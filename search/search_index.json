{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Choice-Learn documentation!","text":"<p>  Large-scale choice modeling through the lens of machine learning  </p> If you are not coming from GitHub, check it out You can also read our academic paper ! <p></p> <p>Choice-Learn is a Python package designed to help you estimate discrete choice models and use them (e.g., assortment optimization plug-in). The package provides ready-to-use datasets and models from the litterature. It also provides a lower level use if you wish to customize any choice model or create your own from scratch. Choice-Learn efficiently handles data with the objective to limit RAM usage. It is made particularly easy to estimate choice models with your own, large datasets.</p> <p>Choice-Learn uses NumPy and pandas as data backend engines and TensorFlow for models.</p> <p>In this documentation you will find examples to be quickly getting started as well as some more in-depth example.</p>"},{"location":"#whats-in-there","title":"What's in there ?","text":"<p>Here is a quick overview of the different functionalities offered by Choice-Learn. Further details are given in the rest of the documentation.</p>"},{"location":"#data","title":"Data","text":"<ul> <li>Custom data handling for choice datasets with possible memory usage optimizations</li> <li>Some Open-Source ready-to use datasets are included within the datasets:<ul> <li>SwissMetro</li> <li>ModeCanada</li> <li>The Train dataset</li> <li>The Heating &amp; Electricity datasets from Kenneth Train</li> <li>Stated car preferences</li> <li>The TaFeng dataset from Kaggle</li> <li>The ICDM-2013 Expedia dataset from Kaggle</li> <li>London Passenger Mode Choice</li> <li>The Bakery Dataset</li> <li>The Badminton Dataset</li> </ul> </li> </ul>"},{"location":"#single-choice-models","title":"Single Choice Models","text":"<ul> <li>Custom modeling</li> <li>Ready to be used models:<ul> <li>Linear Models:<ul> <li>Multinomial Logit</li> <li>Conditional Logit</li> <li>Latent class MNL</li> <li>Nested Logit</li> </ul> </li> <li>Non-Linear Models:<ul> <li>RUMnet</li> <li>TasteNet</li> <li>Learning MNL</li> <li>ResLogit</li> </ul> </li> </ul> </li> </ul>"},{"location":"#multi-purchases-or-basket-models","title":"Multi-Purchases or Basket Models","text":"<ul> <li>Data handling for basket models and preprocessing</li> <li>Shopper model</li> <li>AleaCarta model</li> <li>Basic Attention model</li> </ul>"},{"location":"#tools","title":"Tools","text":"<ul> <li>Assortment Optimization</li> <li>Assortment and Pricing</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>Diverse examples are provided in the How-To section, give it a look !</p>"},{"location":"#introduction-discrete-choice-modeling","title":"Introduction - Discrete Choice Modeling","text":"<p>Discrete choice models aim at explaining or predicting choices over a set of alternatives. Well known use-cases include analyzing people's choice of mean of transport or products purchases in stores.</p> <p>If you are new to choice modeling, you can check this resource. Otherwise, you can also take a look at the introductive example.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#user-installation","title":"User installation","text":"<p>To install the required packages in a virtual environment, run the following command:</p> <p>The easiest is to pip-install the package: <pre><code>pip install choice-learn\n</code></pre></p> <p>Otherwise you can use the git repository to get the latest version: <pre><code>git clone git@github.com:artefactory/choice-learn.git\n</code></pre></p>"},{"location":"#dependencies","title":"Dependencies","text":"<p>Choice-Learn requires the following: - Python (&gt;=3.9) - NumPy (&gt;=1.24) - pandas (&gt;=1.5)</p> <p>For modeling you need: - TensorFlow (&gt;=2.13)</p> <p>Warning: If you are a MAC user with a M1 or M2 chip, importing TensorFlow might lead to Python crashing. In such case, use anaconda to install TensorFlow with <code>conda install -c apple tensorflow</code>.</p> <p>Finally, an optional requirement used for statsitcal reporting and LBFG-S optimization is: - TensorFlow Probability (&gt;=0.20.1)</p> <p>Finally for pricing or assortment optimization, you need either Gurobi or OR-Tools: - gurobipy (&gt;=11.0.0) - ortools (&gt;=9.6.2534)</p>"},{"location":"#contributing","title":"Contributing","text":"<p>You are welcome to contribute to the project ! You can help in various ways: - raise issues - resolve issues already opened - develop new features - provide additional examples of use - fix typos, improve code quality - develop new tests</p> <p>We recommend to first open an issue to discuss your ideas.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you consider this package and any of its feature useful for your research, please cite us:</p> <pre><code>@article{Auriau2024,\n  doi = {10.21105/joss.06899},\n  url = {https://doi.org/10.21105/joss.06899},\n  year = {2024},\n  publisher = {The Open Journal},\n  volume = {9},\n  number = {101},\n  pages = {6899},\n  author = {Vincent Auriau and Ali Aouad and Antoine D\u00e9sir and Emmanuel Malherbe},\n  title = {Choice-Learn: Large-scale choice modeling for operational contexts through the lens of machine learning},\n  journal = {Journal of Open Source Software} }\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The use of this software is under the MIT license, with no limitation of usage, including for commercial applications.</p>"},{"location":"#affiliations","title":"Affiliations","text":"<p>Choice-Learn has been developed through a collaboration between researchers at the Artefact Research Center and the laboratory MICS from CentraleSup\u00e9lec, Universit\u00e9 Paris Saclay.</p> <p> <p>"},{"location":"code/","title":"Code","text":""},{"location":"explanations/","title":"Explanations","text":""},{"location":"how-to-guides/","title":"Introduction","text":"<p>Here are some in-depth examples to help you with mastering Choice-Learn. In particular you will find notebooks to handle:</p> <p></p> <p>DATA - An introduction to data handling with Choice-Learn - More in-depth examples to instantiate a ChoiceDataset - Explanations and examples on how to use Features by IDs if you have RAM usage issues</p> <p></p> <p>MODELS</p> <ul> <li> <p>Linear Models :</p> <ul> <li>MNL: An introduction to choice modeling with the Multi Nomial Logit model</li> <li>cLogit: Tutorials on how to parametrize and fit a Conditional Logit model</li> <li>nLogit: How to parametrize and fit a Nested Logit model</li> <li>Latent Class: A basic examples on how to estimate several Latent Class models</li> </ul> </li> <li> <p>Non-Linear Models :</p> <ul> <li>RUMnet:  Representing Random Utility Choice Models with Neural Networks (Aouad and D\u00e9sir, 2023).</li> <li>TasteNet-MNL: A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability (Han, Pereira, Ben-Akiva and Zegras, 2022)</li> </ul> </li> <li> <p>Diverse :</p> <ul> <li>Logistic Regression: A reproduction of the logistic regression tutorial by scikit-learn</li> <li>On model finetuning: Hyperparameters and learning tools</li> </ul> </li> </ul> <p></p> <p>AUXILIARY TOOLS</p> <p>We currently handle two types of post-processing that leverage choice models:</p> <ul> <li>Assortment Optimization: How to best select a subset of alternatives to sell to a customer.</li> <li>Pricing: How to best select alternative prices - can be combined with assortment optimization.</li> </ul> <p>If you feel like adding adding a dataset, a model, a tool or a usecase algorithm would bring value to the package, feel free to reach out !</p>"},{"location":"tutorials/","title":"Introduction","text":"<p>Here are a few tutorials to get started with the choice-learn package. In particular it shows how handle data and get started with modelling.</p> <p>You will also find more in-depth examples in the How-To section.</p> <ul> <li>Introduction</li> <li>Data Handling</li> <li>Basket Data Handling</li> </ul>"},{"location":"notebooks/models/models_finetuning/","title":"Model finetuning: generic hyper-parameters","text":"<p>A few hyperparameters are shared by most models and can be used to improve model's estimation process and generalization.</p>"},{"location":"notebooks/models/models_finetuning/#optimizer","title":"Optimizer","text":"<p>Two main types of optimizers are available for model estimation with Choice-Learn. You should choose among these two types depending on the model you wish to estimate and the size of your dataset.</p>"},{"location":"notebooks/models/models_finetuning/#quasi-newton-methods","title":"Quasi-Newton methods:","text":"<p>For smaller models and datasets you should use the L-BFGS algorithm by specifying <code>optimizer=\"lbfgs\"</code> in the model instantiation. The algorithm is faster but has a high memory consumption since it needs to evaluate the whole dataset negative log-likelihood for several sets of parameters. You can find more information on the algorithm on Wikipedia and on the implementation on the official TensorFlow documentation.</p>"},{"location":"notebooks/models/models_finetuning/#tolerance","title":"Tolerance","text":"<p>Choice-Learn models have a hyperparameter called <code>tolerance</code> that controls the precision of the L-BFGS algorithm. You can change it to either converge faster or get better precision. The default value is $10^{-8}$ and the algorithm will consider to have reached an optimal when the objective function value has a different smaller than <code>tolerance</code> between two steps.</p>"},{"location":"notebooks/models/models_finetuning/#epochs","title":"Epochs","text":"<p>When using L-BFGS you can also set up the <code>epochs</code> hyperparameter to specify the maximum number of iteration accepted. If an optimal respecting tolerance is not found within this number of iterations it is stopped nonetheless.</p>"},{"location":"notebooks/models/models_finetuning/#gradient-descent-methods","title":"Gradient descent methods:","text":"<p>If you have memory error using L-BFGS you can use gradient-descent base algorithms that are particularly popular for neural networks estimation since they have many parameters. Different versions exist among which SGD, Adam and Adamax are particularly popular. You can select them by specifying <code>optimizer=\"Adam\"</code> in the model instantiation.</p>"},{"location":"notebooks/models/models_finetuning/#learning-rate","title":"Learning Rate","text":"<p>Gradient descent algorithms can be parametrized with the <code>lr</code> argument. The learning rate basically control the step size of each weight update. Default value is $10^{-3}$.</p>"},{"location":"notebooks/models/models_finetuning/#batch-size","title":"Batch Size","text":"<p>The batch size hyperparameters sets up how many data (= choices) should be used in each update of the gradient descent algorithm. A compromise has to be found between higher values taking up a more memory that are more stable but can become slower in terms of convergence time and smaller values with fast iterations but that also introduce more noise. The default value is <code>batch_size=32</code> and you can use <code>batch_size=-1</code> if you want your batch to be the whole dataset.</p>"},{"location":"notebooks/models/models_finetuning/#epochs_1","title":"Epochs","text":"<p>You can also control the number of epochs of gradient descent algorithms with the <code>epochs</code> argument. The default value being 1000 you are strongly advised to change it.</p>"},{"location":"notebooks/models/models_finetuning/#callbacks","title":"Callbacks","text":"<p>You can also use tf.keras.callbacks to have learning rate variation or early stopping strategies. You should create it in pure TensorFlow and give it to the model as a list of callbacks: <code>callbacks=[...]</code>.</p>"},{"location":"notebooks/models/models_finetuning/#regularization","title":"Regularization","text":"<p>A weight regularization can be added during model estimation. The regularization type can be chosen among $L_1$ and $L_2$ and an importance coefficient factor can be specified. For example: <code>model = SimpleMNL(regularization_type=\"l2\", regularization_strength=0.001, **kwargs)</code>.</p> <p>For a short reminder this will set the following estimation objective:  </p> <p>With $W$ the model's parameters and $\\mathbb{P}(c_i|\\mathcal{A}_i)$ the model's probability to choose the chosen item $c_i$.</p>"},{"location":"notebooks/models/models_finetuning/#exit-choice","title":"Exit Choice","text":"<p>You can specify if you want to integrate an exit choice or not during the model's estimation with the argument <code>add_exit_choice</code>. The default value is <code>False</code>, if set to <code>True</code>, the probabilities will integrate an exist choice of utility zero.</p>"},{"location":"notebooks/models/models_finetuning/#label-smoothing","title":"Label Smoothing","text":"<p>You can integrate a label smoothing that sometimes help the optimization process. It is more recommended with descent gradient algorithms.</p> <p>Basically, the probabilities are float in [0, 1]. When &gt; 0, label values are smoothed, meaning the confidence on label values is relaxed. For example, if 0.1, use 0.1 / num_items for non-chosen labels and 0.9 + 0.1 / num_classes for chosen labels.</p> <p>More explanations here.</p>"},{"location":"notebooks/models/models_finetuning/#final-word","title":"Final Word","text":"<p>If you want or need more details checkout the references page of models.</p>"},{"location":"paper/paper/","title":"Introduction","text":"<p>Discrete choice models aim at predicting choice decisions made by individuals from a menu of alternatives, called an assortment. Well-known use cases include predicting a commuter's choice of transportation mode or a customer's purchases. Choice models are able to handle assortment variations, when some alternatives become unavailable or when their features change in different contexts. This adaptability to different scenarios allows these models to be used as inputs for optimization problems, including assortment planning or pricing.</p> <p>Choice-Learn provides a modular suite of choice modeling tools for practitioners and academic researchers to process choice data, and then formulate, estimate and operationalize choice models. The library is structured into two levels of usage, as illustrated in Figure \\ref{fig:gen_org}. The higher-level is designed for fast and easy implementation and the lower-level enables more advanced parameterizations. This structure, inspired by Keras' different endpoints [@Chollet:2015], enables a user-friendly interface. Choice-Learn is designed with the following objectives:</p> <ul> <li>Streamlined: The processing of datasets and the estimation of standard choice models are facilitated by a simple code signature that are consistent with mainstream machine learning packages such as scikit-learn [@Pedregosa:2011].</li> <li>Scalable: Optimized processes are implemented for data storage and models estimation, allowing the use of large datasets and models with a large number of parameters.</li> <li>Flexible: The codebase can be customized to fit different use cases.</li> <li>Models Library: The same package provides implementations of both standard choice models and machine learning-based methods, including neural networks.</li> <li>Downstream operations: Post-processing tools that leverage choice models for assortment planning and pricing are integrated into the library.</li> </ul> <p>The main contributions are summarized in Tables \\ref{tab:comparison1} and \\ref{tab:comparison2}.</p> <p></p> <p>: Comparison of the different packages for data handling and downstream opereations.\\label{tab:comparison1}</p> <p>+--------------+------------------+--------------------+--------------+----------------+ |              | Data             |  Data              |              |                | | Package      | Format           |  Batching          | Assortment   | Pricing        | +:============:+:================:+:==================:+:============:+:==============:+ | Biogeme      | wide             | $\\times$           |   $\\times$   |   $\\times$     | +--------------+------------------+--------------------+--------------+----------------+ | PyLogit      | long             |  $\\times$          |   $\\times$   |   $\\times$     | +--------------+------------------+--------------------+--------------+----------------+ | Torch-Choice | Multi Index      | $\\checkmark$       |   $\\times$   |   $\\times$     | +--------------+------------------+--------------------+--------------+----------------+ | Choice-Learn | Features Storage | $\\checkmark$       | $\\checkmark$ | $\\checkmark$   | +==============+==================+====================+==============+================+</p> <p>: Comparison of the different packages for modelization. CondL, NestL, MixL, and LatC respectively indicate the Conditional Logit, Nested Logit, Mixed Logit and Latent Class models.\\label{tab:comparison2}</p> <p>+--------------+---------------------------------+--------------------+----------------+----------------+----------------------+ |   Package    | Traditional\\                    | NeuralNet\\         | Custom         | Non-Stochastic |  Stochastic          | |              | Models                          |                    | Models         | Optimizer      |  Optimizer           | |              |                                 | Models             |                |                |                      | +:============:+:===============================:+:==================:+:==============:+:==============:+:====================:+ | Biogeme      | CondL, NestL,\\                  | $\\times$           | $\\checkmark$   | Newton BFGS    | $\\quad \\times \\quad$ | |              | MixL, LatC\\                     |                    |                |                |                      | |              | &amp; more\\                         |                    |                |                |                      | |              |                                 |                    |                |                |                      | +--------------+---------------------------------+--------------------+----------------+----------------+----------------------+ | PyLogit      | CondL, NestL,\\                  |  $\\times$          | $\\times$       | BFGS           |   $\\times$           | |              | MixL,\\                          |                    |                |                |                      | |              | Asymmetric\\                     |                    |                |                |                      | |              |                                 |                    |                |                |                      | +--------------+---------------------------------+--------------------+----------------+----------------+----------------------+ | Torch-       | CondL, NestL\\                   | $\\times$           | $\\times$       | L-BFGS         | $\\checkmark$         | | Choice\\      |                                 |                    |                |                |                      | |              |                                 |                    |                |                |                      | +--------------+---------------------------------+--------------------+----------------+----------------+----------------------+ | Choice-Learn | CondL, NestL,\\                  | $\\checkmark$       | $\\checkmark$   | L-BFGS         | $\\checkmark$         | |              | LatC                            |                    |                |                |                      | +==============+=================================+====================+================+================+======================+</p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#statement-of-need","title":"Statement of need","text":"","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#data-and-model-scalability","title":"Data and model scalability","text":"<p><code>Choice-Learn</code>'s data management relies on NumPy [@Harris:2020] with the objective of limiting the memory footprint. It minimizes the repetition of items or customers features and defers the jointure of the full data structure until processing batches of data. The package introduces the FeaturesStorage object, illustrated in Figure \\ref{fig:fbi}, that allows feature values to be referenced only by their ID. These values are substituted to the ID placeholder on the fly in the batching process. For instance, supermarkets features such as surface or position are often stationary. Thus, they can be stored in an auxiliary data structure and in the main dataset, the store where the choice is recorded is only referenced with its ID.</p> <p>The package stands on Tensorflow [@Abadi:2015] for model estimation, offering the possibility to use fast quasi-Newton optimization algorithm such as L-BFGS [@Nocedal:2006] as well as various gradient-descent optimizers [@Tieleman:2012; @Kingma:2017] specialized in handling batches of data. GPU usage is also possible, which can prove to be time-saving. Finally, the TensorFlow backbone ensures an efficient usage in a production environment, for instance within an assortment recommendation software, through deployment and serving tools, such as TFLite and TFServing.</p> <p></p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#flexible-usage-from-linear-utility-to-customized-specification","title":"Flexible usage: From linear utility to customized specification","text":"<p>Choice models following the Random Utility Maximization principle [@McFadden:2000] define the utility of an alternative $i \\in \\mathcal{A}$ as the sum of a deterministic part $U(i)$ and a random error $\\epsilon_i$. If the terms $(\\epsilon_i)_{i \\in \\mathcal{A}}$ are assumed to be independent and Gumbel-distributed, the probability to choose alternative $i$ can be written as the softmax normalization over the available alternatives $j\\in \\mathcal{A}$:</p> <p> </p> <p>The choice-modeler's job is to formulate an adequate utility function $U(.)$ depending on the context. In <code>Choice-Learn</code>, the user can parametrize predefined models or freely specify a custom utility function. To declare a custom model, one needs to inherit the ChoiceModel class and overwrite the <code>compute_batch_utility</code> method as shown in the documentation.</p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#library-of-traditional-random-utility-models-and-machine-learning-based-models","title":"Library of traditional random utility models and machine learning-based models","text":"<p>Traditional parametric choice models, including the Conditional Logit [@Train:1987], often specify the utility function in a linear form. This provides interpretable coefficients but limits the predictive power of the model. Recent works propose the estimation of more complex models, with neural networks approaches [@Han:2022; @Aouad:2023] and tree-based models [@Salvad\u00e9:2024; @AouadMarket:2023]. While existing choice libraries [@Bierlaire:2023; @Brathwaite:2018; @Du:2023] are often not designed to integrate such machine learning-based approaches, <code>Choice-Learn</code> proposes a collection including both types of models.</p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#downstream-operations-assortment-and-pricing-optimization","title":"Downstream operations: Assortment and pricing optimization","text":"<p><code>Choice-Learn</code> offers additional tools for downstream operations, that are not usually integrated in choice modeling libraries. In particular, assortment optimization is a common use case that leverages a choice model to determine the optimal subset of alternatives to offer customers maximizing a certain objective, such as the expected revenue, conversion rate, or social welfare. This framework captures a variety of applications such as assortment planning, display location optimization, and pricing. We provide implementations based on the mixed-integer programming formulation described in [@MendezDiaz:2014], with the option to choose the solver between Gurobi [@Gurobi:2023] and OR-Tools [@ORTools:2024].</p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#memory-usage-a-case-study","title":"Memory usage: a case study","text":"<p>We provide in Figure \\ref{fig:xps} (a) numerical examples of memory usage to showcase the efficiency of the FeaturesStorage. We consider a feature repeated in a dataset, such as a one-hot encoding for locations, represented by a matrix of shape (#locations, #locations) where each row refers to one location.</p> <p>We compare four data handling methods on the Expedia dataset [@Expedia:2013]: pandas.DataFrames [@pandas:2020] in long and wide format, both used in choice modeling packages, Torch-Choice and <code>Choice-Learn</code>. Figure \\ref{fig:xps} (b) shows the results for various sample sizes.</p> <p>Finally, in Figure \\ref{fig:xps} (c) and (d), we observe memory usage gains on a proprietary dataset in brick-and-mortar retailing consisting of the aggregation of more than 4 billion purchases in Konzum supermarkets in Croatia. Focusing on the coffee subcategory, the dataset specifies, for each purchase, which products were available, their prices, as well as a one-hot representation of the supermarket.</p> <p></p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#acknowledgments","title":"Acknowledgments","text":"<p>The authors thank Fortenova^1 and Martin Mo\u017eina for their helpful collaboration and provision of the proprietary dataset.</p>","tags":["Python","choice","operations","machine learning"]},{"location":"paper/paper/#references","title":"References","text":"","tags":["Python","choice","operations","machine learning"]},{"location":"references/basket_models/references_alea_carta/","title":"The AleaCarta Model","text":"<p>Implementation of the AleaCarta model.</p>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta","title":"<code>AleaCarta</code>","text":"<p>             Bases: <code>BaseBasketModel</code></p> <p>Class for the AleaCarta model.</p> <p>Better Capturing Interactions between Products in Retail: Revisited Negative Sampling for Basket Choice Modeling, D\u00e9sir, J.; Auriau, V.; Mo\u017eina, M.; Malherbe, E. (2025), ECML PKDDD</p> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>class AleaCarta(BaseBasketModel):\n    \"\"\"Class for the AleaCarta model.\n\n    Better Capturing Interactions between Products in Retail: Revisited Negative Sampling for\n    Basket Choice Modeling,\n    D\u00e9sir, J.; Auriau, V.; Mo\u017eina, M.; Malherbe, E. (2025), ECML PKDDD\n    \"\"\"\n\n    def __init__(\n        self,\n        item_intercept: bool = True,\n        price_effects: bool = True,\n        seasonal_effects: bool = False,\n        latent_sizes: dict[str] = {\"preferences\": 4, \"price\": 4, \"season\": 4},\n        n_negative_samples: int = 2,\n        optimizer: str = \"adam\",\n        callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n        lr: float = 1e-3,\n        epochs: int = 10,\n        batch_size: int = 32,\n        grad_clip_value: Union[float, None] = None,\n        weight_decay: Union[float, None] = None,\n        momentum: float = 0.0,\n        epsilon_price: float = 1e-5,\n        l2_regularization: float = 0.0,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the AleaCarta model.\n\n        Parameters\n        ----------\n        item_intercept: bool, optional\n            Whether to include item intercept in the model, by default True\n            Corresponds to the item intercept\n        price_effects: bool, optional\n            Whether to include price effects in the model, by default True\n        seasonal_effects: bool, optional\n            Whether to include seasonal effects in the model, by default False\n        latent_sizes: dict[str]\n            Lengths of the vector representation of the latent parameters\n            latent_sizes[\"preferences\"]: length of one vector of theta, gamma\n            latent_sizes[\"price\"]: length of one vector of delta, beta\n            latent_sizes[\"season\"]: length of one vector of nu, mu\n            by default {\"preferences\": 4, \"price\": 4, \"season\": 4}\n        n_negative_samples: int, optional\n            Number of negative samples to draw for each positive sample for the training,\n            by default 2\n            Must be &gt; 0\n        optimizer: str, optional\n            Optimizer to use for training, by default \"adam\"\n        callbacks: tf.keras.callbacks.Callbacklist, optional\n            List of callbacks to add to model.fit, by default None and only add History\n        lr: float, optional\n            Learning rate, by default 1e-3\n        epochs: int, optional\n            Number of epochs, by default 100\n        batch_size: int, optional\n            Batch size, by default 32\n        grad_clip_value: float, optional\n            Value to clip the gradient, by default None\n        weight_decay: float, optional\n            Weight decay, by default None\n        momentum: float, optional\n            Momentum for the optimizer, by default 0. For SGD only\n        epsilon_price: float, optional\n            Epsilon value to add to prices to avoid NaN values (log(0)), by default 1e-5\n        \"\"\"\n        self.item_intercept = item_intercept\n        self.price_effects = price_effects\n        self.seasonal_effects = seasonal_effects\n        self.l2_regularization = l2_regularization\n\n        if \"preferences\" not in latent_sizes.keys():\n            logging.warning(\n                \"No latent size value has been specified for preferences, \"\n                \"switching to default value 4.\"\n            )\n            latent_sizes[\"preferences\"] = 4\n        if \"price\" not in latent_sizes.keys() and self.price_effects:\n            logging.warning(\n                \"No latent size value has been specified for price_effects, \"\n                \"switching to default value 4.\"\n            )\n            latent_sizes[\"price\"] = 4\n        if \"season\" not in latent_sizes.keys() and self.seasonal_effects:\n            logging.warning(\n                \"No latent size value has been specified for seasonal_effects, \"\n                \"switching to default value 4.\"\n            )\n            latent_sizes[\"season\"] = 4\n\n        for val in latent_sizes.keys():\n            if val not in [\"preferences\", \"price\", \"season\"]:\n                raise ValueError(f\"Unknown value for latent_sizes dict: {val}.\")\n\n        if n_negative_samples &lt;= 0:\n            raise ValueError(\"n_negative_samples must be &gt; 0.\")\n\n        self.latent_sizes = latent_sizes\n        self.n_negative_samples = n_negative_samples\n\n        # Add epsilon to prices to avoid NaN values (log(0))\n        self.epsilon_price = epsilon_price\n\n        super().__init__(\n            optimizer=optimizer,\n            callbacks=callbacks,\n            lr=lr,\n            epochs=epochs,\n            batch_size=batch_size,\n            grad_clip_value=grad_clip_value,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            **kwargs,\n        )\n\n        if len(tf.config.get_visible_devices(\"GPU\")):\n            # At least one available GPU\n            self.on_gpu = True\n        else:\n            # No available GPU\n            self.on_gpu = False\n        # /!\\ If a model trained on GPU is loaded on CPU, self.on_gpu must be set\n        # to False manually after loading the model, and vice versa\n\n        self.instantiated = False\n\n    def instantiate(\n        self,\n        n_items: int,\n        n_stores: int = 0,\n    ) -&gt; None:\n        \"\"\"Instantiate the Shopper model.\n\n        Parameters\n        ----------\n        n_items: int\n            Number of items to consider, i.e. the number of items in the dataset\n        n_stores: int\n            Number of stores in the population\n        \"\"\"\n        self.n_items = n_items\n        if n_stores == 0 and self.price_effects:\n            # To take into account the price effects, the number of stores must be &gt; 0\n            # to have a delta embedding\n            # (By default, the store id is 0)\n            n_stores = 1\n        self.n_stores = n_stores\n\n        self.gamma = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_items, self.latent_sizes[\"preferences\"])\n            ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n            trainable=True,\n            name=\"gamma\",\n        )\n        self.theta = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_stores, self.latent_sizes[\"preferences\"])\n            ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n            trainable=True,\n            name=\"theta\",\n        )\n\n        if self.item_intercept:\n            # Add item intercept\n            self.alpha = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(n_items,)  # Dimension for 1 item: 1\n                ),\n                trainable=True,\n                name=\"alpha\",\n            )\n\n        if self.price_effects:\n            # Add price sensitivity\n            self.beta = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(n_items, self.latent_sizes[\"price\"])\n                ),  # Dimension for 1 item: latent_sizes[\"price\"]\n                trainable=True,\n                name=\"beta\",\n            )\n            self.delta = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(n_stores, self.latent_sizes[\"price\"])\n                ),  # Dimension for 1 item: latent_sizes[\"price\"]\n                trainable=True,\n                name=\"delta\",\n            )\n\n        if self.seasonal_effects:\n            # Add seasonal effects\n            self.mu = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(n_items, self.latent_sizes[\"season\"])\n                ),  # Dimension for 1 item: latent_sizes[\"season\"]\n                trainable=True,\n                name=\"mu\",\n            )\n            self.nu = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(52, self.latent_sizes[\"season\"])\n                ),  # Dimension for 1 item: latent_sizes[\"season\"]\n                trainable=True,\n                name=\"nu\",\n            )\n\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self) -&gt; list[tf.Variable]:\n        \"\"\"Latent parameters of the model.\n\n        Returns\n        -------\n        list[tf.Variable]\n            Latent parameters of the model\n        \"\"\"\n        weights = [self.gamma, self.theta]\n\n        if self.item_intercept:\n            weights.append(self.alpha)\n\n        if self.price_effects:\n            weights.extend([self.beta, self.delta])\n\n        if self.seasonal_effects:\n            weights.extend([self.mu, self.nu])\n\n        return weights\n\n    @property\n    def train_iter_method(self) -&gt; str:\n        \"\"\"Method used to generate sub-baskets from a purchased one.\n\n        Available methods are:\n        - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:\n                        (1|0); (2|1); (3|1,2); (4|1,2,3); etc...\n        - 'aleacarta': creates all the sub-baskets with N-1 items:\n                        (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)\n\n        Returns\n        -------\n        str\n            Data generation method.\n        \"\"\"\n        return \"aleacarta\"\n\n    def compute_batch_utility(\n        self,\n        item_batch: Union[np.ndarray, tf.Tensor],\n        basket_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        user_batch: Union[np.ndarray, tf.Tensor],\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the utility of all the items in item_batch given the items in basket_batch.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray or tf.Tensor\n            Batch of the purchased items ID (integers) for which to compute the utility\n            Shape must be (batch_size,)\n            (positive and negative samples concatenated together)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (floats) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            Batch of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n\n        Returns\n        -------\n        item_utilities: tf.Tensor\n            Utility of all the items in item_batch\n            Shape must be (batch_size,)\n        \"\"\"\n        _ = user_batch\n        _ = available_item_batch\n        item_batch = tf.cast(item_batch, dtype=tf.int32)\n        if len(tf.shape(item_batch)) == 1:\n            if len(tf.shape(price_batch)) != 1:\n                raise ValueError(f\"\"\"Arguments price_batch and item_batch should have same shape\n                and are:{item_batch.shape} and {price_batch.shape}\"\"\")\n            item_batch = tf.expand_dims(item_batch, axis=1)\n            price_batch = tf.expand_dims(price_batch, axis=1)\n            squeeze = True\n        else:\n            squeeze = False\n\n        basket_batch = tf.cast(basket_batch, dtype=tf.int32)\n        store_batch = tf.cast(store_batch, dtype=tf.int32)\n        week_batch = tf.cast(week_batch, dtype=tf.int32)\n        price_batch = tf.cast(price_batch, dtype=tf.float32)\n\n        theta_store = tf.gather(self.theta, indices=store_batch)\n        gamma_item = tf.gather(self.gamma, indices=item_batch)\n        # Compute the dot product along the last dimension\n        store_preferences = tf.einsum(\"kj,klj-&gt;kl\", theta_store, gamma_item)\n\n        if self.item_intercept:\n            item_intercept = tf.gather(self.alpha, indices=item_batch)\n        else:\n            item_intercept = tf.zeros_like(store_preferences)\n        if self.price_effects:\n            delta_store = tf.gather(self.delta, indices=store_batch)\n            beta_item = tf.gather(self.beta, indices=item_batch)\n            # Add epsilon to avoid NaN values (log(0))\n            price_effects = (\n                -1\n                # Compute the dot product along the last dimension\n                * tf.einsum(\"kj,klj-&gt;kl\", delta_store, beta_item)\n                * tf.math.log(price_batch + self.epsilon_price)\n            )\n        else:\n            delta_store = tf.zeros_like(store_batch)\n            price_effects = tf.zeros_like(store_preferences)\n\n        if self.seasonal_effects:\n            nu_week = tf.gather(self.nu, indices=week_batch)\n            mu_item = tf.gather(self.mu, indices=item_batch)\n            # Compute the dot product along the last dimension\n            seasonal_effects = tf.einsum(\"kj,klj-&gt;kl\", nu_week, mu_item)\n        else:\n            seasonal_effects = tf.zeros_like(store_preferences)\n\n        # The effects of item intercept, store preferences, price sensitivity\n        # and seasonal effects are combined in the per-item per-trip latent variable\n        psi = tf.reduce_sum(\n            [\n                item_intercept,\n                store_preferences,\n                price_effects,\n                seasonal_effects,\n            ],\n            axis=0,\n        )  # Shape: (batch_size,)\n\n        # Create a RaggedTensor from the indices with padding removed\n        item_indices_ragged = tf.cast(\n            tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n            dtype=tf.int32,\n        )\n\n        if tf.size(item_indices_ragged) == 0:\n            # Empty baskets: no gamma embeddings to gather\n            # (It must be a ragged tensor here because TF's GraphMode requires the same\n            # nested structure to be returned from all branches of a conditional)\n            gamma_by_basket = tf.RaggedTensor.from_tensor(\n                tf.zeros((len(item_batch), 0, self.gamma.shape[1]))\n            )\n            basket_size = tf.ones((len(item_batch),))\n        else:\n            # Gather the embeddings using a ragged tensor of indices\n            gamma_by_basket = tf.ragged.map_flat_values(tf.gather, self.gamma, item_indices_ragged)\n            basket_size = tf.cast(item_indices_ragged.row_lengths(), dtype=tf.float32)\n\n        gamma_by_basket = tf.reduce_sum(gamma_by_basket, axis=1) / tf.maximum(\n            tf.expand_dims(basket_size, axis=-1), 1.0\n        )\n        # Basket interaction: one vs all\n        # Compute the dot product along the last dimension (latent_size)\n        basket_interaction_utility = tf.einsum(\"kj,klj-&gt;kl\", gamma_by_basket, gamma_item)\n\n        # Sum over the items in the basket\n        if squeeze:\n            return tf.gather(psi + basket_interaction_utility, 0, axis=1)\n        return psi + basket_interaction_utility\n\n    def compute_basket_utility(\n        self,\n        basket: Union[None, np.ndarray] = None,\n        store: Union[None, int] = None,\n        week: Union[None, int] = None,\n        prices: Union[None, np.ndarray] = None,\n        available_item_batch: Union[None, np.ndarray] = None,\n        trip: Union[None, Trip] = None,\n    ) -&gt; float:\n        r\"\"\"Compute the utility of an (unordered) basket.\n\n        Corresponds to the sum of all the conditional utilities:\n                \\sum_{i \\in basket} U(i | basket \\ {i})\n        Take as input directly a Trip object or separately basket, store,\n        week and prices.\n\n        Parameters\n        ----------\n        basket: np.ndarray or None, optional\n            ID the of items already in the basket, by default None\n        store: int or None, optional\n            Store id, by default None\n        week: int or None, optional\n            Week number, by default None\n        prices: np.ndarray or None, optional\n            Prices for each purchased item, by default None\n            Shape must be (len(basket),)\n        trip: Trip or None, optional\n            Trip object containing basket, store, week and prices,\n            by default None\n\n        Returns\n        -------\n        float\n            Utility of the (unordered) basket\n        \"\"\"\n        if trip is None:\n            # Trip not provided as an argument\n            # Then basket, store, week and prices must be provided\n            if basket is None or store is None or week is None or prices is None:\n                raise ValueError(\n                    \"If trip is None, then basket, store, week, and prices \"\n                    \"must be provided as arguments.\"\n                )\n\n        else:\n            # Trip directly provided as an argument\n            basket = trip.purchases\n            store = trip.store\n            week = trip.week\n            available_item_batch = trip.assortment\n            prices = [trip.prices[item_id] for item_id in basket]\n\n        len_basket = len(basket)\n\n        # basket_batch[i] = basket without the i-th item\n        basket_batch = np.array(\n            [np.delete(basket, i) for i in range(len_basket)]\n        )  # Shape: (len_basket, len(basket) - 1)\n\n        return tf.reduce_sum(\n            self.compute_batch_utility(\n                item_batch=basket,\n                basket_batch=basket_batch,\n                store_batch=np.array([store] * len_basket),\n                week_batch=np.array([week] * len_basket),\n                price_batch=prices,\n                available_item_batch=available_item_batch,\n                user_batch=None,\n            )\n        ).numpy()\n\n    def get_negative_samples(\n        self,\n        available_items: np.ndarray,\n        purchased_items: np.ndarray,\n        next_item: int,\n        n_samples: int,\n    ) -&gt; list[int]:\n        \"\"\"Sample randomly a set of items.\n\n        (set of items not already purchased and *not necessarily* from the basket)\n\n        Parameters\n        ----------\n        available_items: np.ndarray\n            Matrix indicating the availability (1) or not (0) of the products\n            Shape must be (n_items,)\n        purchased_items: np.ndarray\n            List of items already purchased (already in the basket)\n        next_item: int\n            Next item (to be added in the basket)\n        n_samples: int\n            Number of samples to draw\n\n        Returns\n        -------\n        list[int]\n            Random sample of items, each of them distinct from\n            the next item and from the items already in the basket\n        \"\"\"\n        # Convert inputs to tensors\n        available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n        purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n        next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n        # Get the list of available items based on the availability matrix\n        item_ids = tf.range(self.n_items)\n        available_mask = tf.equal(available_items, 1)\n        assortment = tf.boolean_mask(item_ids, available_mask)\n\n        not_to_be_chosen = tf.concat([purchased_items, tf.expand_dims(next_item, axis=0)], axis=0)\n\n        # Sample negative items from the assortment excluding not_to_be_chosen\n        negative_samples = tf.boolean_mask(\n            tensor=assortment,\n            # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n            mask=~tf.reduce_any(\n                tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n            ),\n        )\n\n        error_message = (\n            \"The number of negative samples to draw must be less than \"\n            \"the number of available items not already purchased and \"\n            \"distinct from the next item.\"\n        )\n        # Raise an error if n_samples &gt; tf.size(negative_samples)\n        tf.debugging.assert_greater_equal(\n            tf.size(negative_samples), n_samples, message=error_message\n        )\n\n        # Randomize the sampling\n        negative_samples = tf.random.shuffle(negative_samples)\n\n        # Keep only n_samples\n        return negative_samples[:n_samples]\n\n    # @tf.function  # Graph mode\n    def compute_batch_loss(\n        self,\n        item_batch: np.ndarray,\n        basket_batch: np.ndarray,\n        future_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        user_batch: np.ndarray,\n    ) -&gt; tuple[tf.Variable]:\n        \"\"\"Compute log-likelihood and loss for one batch of items.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray\n            Batch of purchased items ID (integers)\n            Shape must be (batch_size,)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        future_batch: np.ndarray\n            Batch of items to be purchased in the future (ID of items not yet in the\n            basket) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n            Here for signature reasons, unused for this model\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (floats) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            List of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n\n        Returns\n        -------\n        tf.Variable\n            Value of the loss for the batch (binary cross-entropy),\n            Shape must be (1,)\n        loglikelihood: tf.Variable\n            Computed log-likelihood of the batch of items\n            Approximated by difference of utilities between positive and negative samples\n            Shape must be (1,)\n        \"\"\"\n        _ = user_batch\n        _ = future_batch\n        batch_size = len(item_batch)\n        item_batch = tf.cast(item_batch, dtype=tf.int32)\n\n        # Negative sampling\n        negative_samples = tf.stack(\n            [\n                self.get_negative_samples(\n                    available_items=available_item_batch[idx],\n                    purchased_items=basket_batch[idx],\n                    next_item=item_batch[idx],\n                    n_samples=self.n_negative_samples,\n                )\n                for idx in range(batch_size)\n            ],\n            axis=0,\n        )\n        augmented_item_batch = tf.cast(\n            tf.concat([tf.expand_dims(item_batch, axis=-1), negative_samples], axis=1),\n            dtype=tf.int32,\n        )\n\n        # Each time, pick only the price of the item in augmented_item_batch from the\n        # corresponding price array\n        augmented_price_batch = tf.gather(\n            params=price_batch, indices=augmented_item_batch, batch_dims=1\n        )\n        # Compute the utility of all the available items\n        all_utilities = self.compute_batch_utility(\n            item_batch=augmented_item_batch,\n            basket_batch=basket_batch,\n            store_batch=store_batch,\n            week_batch=week_batch,\n            price_batch=augmented_price_batch,\n            available_item_batch=available_item_batch,\n            user_batch=None,\n        )  # Shape: (batch_size * (n_negative_samples + 1),)\n\n        positive_samples_utilities = tf.gather(params=all_utilities, indices=[0], axis=1)\n        negative_samples_utilities = tf.gather(\n            params=all_utilities, indices=tf.range(1, self.n_negative_samples + 1), axis=1\n        )\n\n        # Log-likelihood of a batch = sum of log-likelihoods of its samples\n        # Add a small epsilon to gain numerical stability (avoid log(0))\n        epsilon = 0.0  # No epsilon added for now\n        loglikelihood = tf.reduce_sum(\n            tf.math.log(\n                tf.sigmoid(\n                    tf.tile(\n                        positive_samples_utilities,\n                        [1, self.n_negative_samples],\n                    )\n                    - negative_samples_utilities\n                )\n                + epsilon\n            ),\n        )  # Shape of loglikelihood: (1,))\n        bce = tf.keras.backend.binary_crossentropy(\n            # Target: 1 for positive samples, 0 for negative samples\n            target=tf.concat(\n                [\n                    tf.ones_like(positive_samples_utilities),\n                    tf.zeros_like(negative_samples_utilities),\n                ],\n                axis=1,\n            ),\n            output=tf.nn.sigmoid(all_utilities),\n        )  # Shape: (batch_size * (n_negative_samples + 1),)\n        ridge_regularization = self.l2_regularization * tf.add_n(\n            [tf.nn.l2_loss(weight) for weight in self.trainable_weights]\n        )\n        # Normalize by the batch size and the number of negative samples\n        return (\n            tf.reduce_sum(bce + ridge_regularization)\n            / (batch_size * (self.n_negative_samples + 1)),\n            loglikelihood,\n        )\n</code></pre>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.train_iter_method","title":"<code>train_iter_method: str</code>  <code>property</code>","text":"<p>Method used to generate sub-baskets from a purchased one.</p> <p>Available methods are: - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:                 (1|0); (2|1); (3|1,2); (4|1,2,3); etc... - 'aleacarta': creates all the sub-baskets with N-1 items:                 (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)</p> <p>Returns:</p> Type Description <code>str</code> <p>Data generation method.</p>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.trainable_weights","title":"<code>trainable_weights: list[tf.Variable]</code>  <code>property</code>","text":"<p>Latent parameters of the model.</p> <p>Returns:</p> Type Description <code>list[Variable]</code> <p>Latent parameters of the model</p>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.__init__","title":"<code>__init__(item_intercept=True, price_effects=True, seasonal_effects=False, latent_sizes={'preferences': 4, 'price': 4, 'season': 4}, n_negative_samples=2, optimizer='adam', callbacks=None, lr=0.001, epochs=10, batch_size=32, grad_clip_value=None, weight_decay=None, momentum=0.0, epsilon_price=1e-05, l2_regularization=0.0, **kwargs)</code>","text":"<p>Initialize the AleaCarta model.</p> <p>Parameters:</p> Name Type Description Default <code>item_intercept</code> <code>bool</code> <p>Whether to include item intercept in the model, by default True Corresponds to the item intercept</p> <code>True</code> <code>price_effects</code> <code>bool</code> <p>Whether to include price effects in the model, by default True</p> <code>True</code> <code>seasonal_effects</code> <code>bool</code> <p>Whether to include seasonal effects in the model, by default False</p> <code>False</code> <code>latent_sizes</code> <code>dict[str]</code> <p>Lengths of the vector representation of the latent parameters latent_sizes[\"preferences\"]: length of one vector of theta, gamma latent_sizes[\"price\"]: length of one vector of delta, beta latent_sizes[\"season\"]: length of one vector of nu, mu by default {\"preferences\": 4, \"price\": 4, \"season\": 4}</p> <code>{'preferences': 4, 'price': 4, 'season': 4}</code> <code>n_negative_samples</code> <code>int</code> <p>Number of negative samples to draw for each positive sample for the training, by default 2 Must be &gt; 0</p> <code>2</code> <code>optimizer</code> <code>str</code> <p>Optimizer to use for training, by default \"adam\"</p> <code>'adam'</code> <code>callbacks</code> <code>Union[CallbackList, None]</code> <p>List of callbacks to add to model.fit, by default None and only add History</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate, by default 1e-3</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of epochs, by default 100</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size, by default 32</p> <code>32</code> <code>grad_clip_value</code> <code>Union[float, None]</code> <p>Value to clip the gradient, by default None</p> <code>None</code> <code>weight_decay</code> <code>Union[float, None]</code> <p>Weight decay, by default None</p> <code>None</code> <code>momentum</code> <code>float</code> <p>Momentum for the optimizer, by default 0. For SGD only</p> <code>0.0</code> <code>epsilon_price</code> <code>float</code> <p>Epsilon value to add to prices to avoid NaN values (log(0)), by default 1e-5</p> <code>1e-05</code> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>def __init__(\n    self,\n    item_intercept: bool = True,\n    price_effects: bool = True,\n    seasonal_effects: bool = False,\n    latent_sizes: dict[str] = {\"preferences\": 4, \"price\": 4, \"season\": 4},\n    n_negative_samples: int = 2,\n    optimizer: str = \"adam\",\n    callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n    lr: float = 1e-3,\n    epochs: int = 10,\n    batch_size: int = 32,\n    grad_clip_value: Union[float, None] = None,\n    weight_decay: Union[float, None] = None,\n    momentum: float = 0.0,\n    epsilon_price: float = 1e-5,\n    l2_regularization: float = 0.0,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the AleaCarta model.\n\n    Parameters\n    ----------\n    item_intercept: bool, optional\n        Whether to include item intercept in the model, by default True\n        Corresponds to the item intercept\n    price_effects: bool, optional\n        Whether to include price effects in the model, by default True\n    seasonal_effects: bool, optional\n        Whether to include seasonal effects in the model, by default False\n    latent_sizes: dict[str]\n        Lengths of the vector representation of the latent parameters\n        latent_sizes[\"preferences\"]: length of one vector of theta, gamma\n        latent_sizes[\"price\"]: length of one vector of delta, beta\n        latent_sizes[\"season\"]: length of one vector of nu, mu\n        by default {\"preferences\": 4, \"price\": 4, \"season\": 4}\n    n_negative_samples: int, optional\n        Number of negative samples to draw for each positive sample for the training,\n        by default 2\n        Must be &gt; 0\n    optimizer: str, optional\n        Optimizer to use for training, by default \"adam\"\n    callbacks: tf.keras.callbacks.Callbacklist, optional\n        List of callbacks to add to model.fit, by default None and only add History\n    lr: float, optional\n        Learning rate, by default 1e-3\n    epochs: int, optional\n        Number of epochs, by default 100\n    batch_size: int, optional\n        Batch size, by default 32\n    grad_clip_value: float, optional\n        Value to clip the gradient, by default None\n    weight_decay: float, optional\n        Weight decay, by default None\n    momentum: float, optional\n        Momentum for the optimizer, by default 0. For SGD only\n    epsilon_price: float, optional\n        Epsilon value to add to prices to avoid NaN values (log(0)), by default 1e-5\n    \"\"\"\n    self.item_intercept = item_intercept\n    self.price_effects = price_effects\n    self.seasonal_effects = seasonal_effects\n    self.l2_regularization = l2_regularization\n\n    if \"preferences\" not in latent_sizes.keys():\n        logging.warning(\n            \"No latent size value has been specified for preferences, \"\n            \"switching to default value 4.\"\n        )\n        latent_sizes[\"preferences\"] = 4\n    if \"price\" not in latent_sizes.keys() and self.price_effects:\n        logging.warning(\n            \"No latent size value has been specified for price_effects, \"\n            \"switching to default value 4.\"\n        )\n        latent_sizes[\"price\"] = 4\n    if \"season\" not in latent_sizes.keys() and self.seasonal_effects:\n        logging.warning(\n            \"No latent size value has been specified for seasonal_effects, \"\n            \"switching to default value 4.\"\n        )\n        latent_sizes[\"season\"] = 4\n\n    for val in latent_sizes.keys():\n        if val not in [\"preferences\", \"price\", \"season\"]:\n            raise ValueError(f\"Unknown value for latent_sizes dict: {val}.\")\n\n    if n_negative_samples &lt;= 0:\n        raise ValueError(\"n_negative_samples must be &gt; 0.\")\n\n    self.latent_sizes = latent_sizes\n    self.n_negative_samples = n_negative_samples\n\n    # Add epsilon to prices to avoid NaN values (log(0))\n    self.epsilon_price = epsilon_price\n\n    super().__init__(\n        optimizer=optimizer,\n        callbacks=callbacks,\n        lr=lr,\n        epochs=epochs,\n        batch_size=batch_size,\n        grad_clip_value=grad_clip_value,\n        weight_decay=weight_decay,\n        momentum=momentum,\n        **kwargs,\n    )\n\n    if len(tf.config.get_visible_devices(\"GPU\")):\n        # At least one available GPU\n        self.on_gpu = True\n    else:\n        # No available GPU\n        self.on_gpu = False\n    # /!\\ If a model trained on GPU is loaded on CPU, self.on_gpu must be set\n    # to False manually after loading the model, and vice versa\n\n    self.instantiated = False\n</code></pre>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.compute_basket_utility","title":"<code>compute_basket_utility(basket=None, store=None, week=None, prices=None, available_item_batch=None, trip=None)</code>","text":"<p>Compute the utility of an (unordered) basket.</p> <p>Corresponds to the sum of all the conditional utilities:         \\sum_{i \\in basket} U(i | basket \\ {i}) Take as input directly a Trip object or separately basket, store, week and prices.</p> <p>Parameters:</p> Name Type Description Default <code>basket</code> <code>Union[None, ndarray]</code> <p>ID the of items already in the basket, by default None</p> <code>None</code> <code>store</code> <code>Union[None, int]</code> <p>Store id, by default None</p> <code>None</code> <code>week</code> <code>Union[None, int]</code> <p>Week number, by default None</p> <code>None</code> <code>prices</code> <code>Union[None, ndarray]</code> <p>Prices for each purchased item, by default None Shape must be (len(basket),)</p> <code>None</code> <code>trip</code> <code>Union[None, Trip]</code> <p>Trip object containing basket, store, week and prices, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Utility of the (unordered) basket</p> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>def compute_basket_utility(\n    self,\n    basket: Union[None, np.ndarray] = None,\n    store: Union[None, int] = None,\n    week: Union[None, int] = None,\n    prices: Union[None, np.ndarray] = None,\n    available_item_batch: Union[None, np.ndarray] = None,\n    trip: Union[None, Trip] = None,\n) -&gt; float:\n    r\"\"\"Compute the utility of an (unordered) basket.\n\n    Corresponds to the sum of all the conditional utilities:\n            \\sum_{i \\in basket} U(i | basket \\ {i})\n    Take as input directly a Trip object or separately basket, store,\n    week and prices.\n\n    Parameters\n    ----------\n    basket: np.ndarray or None, optional\n        ID the of items already in the basket, by default None\n    store: int or None, optional\n        Store id, by default None\n    week: int or None, optional\n        Week number, by default None\n    prices: np.ndarray or None, optional\n        Prices for each purchased item, by default None\n        Shape must be (len(basket),)\n    trip: Trip or None, optional\n        Trip object containing basket, store, week and prices,\n        by default None\n\n    Returns\n    -------\n    float\n        Utility of the (unordered) basket\n    \"\"\"\n    if trip is None:\n        # Trip not provided as an argument\n        # Then basket, store, week and prices must be provided\n        if basket is None or store is None or week is None or prices is None:\n            raise ValueError(\n                \"If trip is None, then basket, store, week, and prices \"\n                \"must be provided as arguments.\"\n            )\n\n    else:\n        # Trip directly provided as an argument\n        basket = trip.purchases\n        store = trip.store\n        week = trip.week\n        available_item_batch = trip.assortment\n        prices = [trip.prices[item_id] for item_id in basket]\n\n    len_basket = len(basket)\n\n    # basket_batch[i] = basket without the i-th item\n    basket_batch = np.array(\n        [np.delete(basket, i) for i in range(len_basket)]\n    )  # Shape: (len_basket, len(basket) - 1)\n\n    return tf.reduce_sum(\n        self.compute_batch_utility(\n            item_batch=basket,\n            basket_batch=basket_batch,\n            store_batch=np.array([store] * len_basket),\n            week_batch=np.array([week] * len_basket),\n            price_batch=prices,\n            available_item_batch=available_item_batch,\n            user_batch=None,\n        )\n    ).numpy()\n</code></pre>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.compute_batch_loss","title":"<code>compute_batch_loss(item_batch, basket_batch, future_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch)</code>","text":"<p>Compute log-likelihood and loss for one batch of items.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>ndarray</code> <p>Batch of purchased items ID (integers) Shape must be (batch_size,)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>future_batch</code> <code>ndarray</code> <p>Batch of items to be purchased in the future (ID of items not yet in the basket) (arrays) for each purchased item Shape must be (batch_size, max_basket_size) Here for signature reasons, unused for this model</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (floats) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>List of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <p>Returns:</p> Name Type Description <code>Variable</code> <p>Value of the loss for the batch (binary cross-entropy), Shape must be (1,)</p> <code>loglikelihood</code> <code>Variable</code> <p>Computed log-likelihood of the batch of items Approximated by difference of utilities between positive and negative samples Shape must be (1,)</p> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>def compute_batch_loss(\n    self,\n    item_batch: np.ndarray,\n    basket_batch: np.ndarray,\n    future_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    user_batch: np.ndarray,\n) -&gt; tuple[tf.Variable]:\n    \"\"\"Compute log-likelihood and loss for one batch of items.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray\n        Batch of purchased items ID (integers)\n        Shape must be (batch_size,)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    future_batch: np.ndarray\n        Batch of items to be purchased in the future (ID of items not yet in the\n        basket) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n        Here for signature reasons, unused for this model\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (floats) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        List of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n\n    Returns\n    -------\n    tf.Variable\n        Value of the loss for the batch (binary cross-entropy),\n        Shape must be (1,)\n    loglikelihood: tf.Variable\n        Computed log-likelihood of the batch of items\n        Approximated by difference of utilities between positive and negative samples\n        Shape must be (1,)\n    \"\"\"\n    _ = user_batch\n    _ = future_batch\n    batch_size = len(item_batch)\n    item_batch = tf.cast(item_batch, dtype=tf.int32)\n\n    # Negative sampling\n    negative_samples = tf.stack(\n        [\n            self.get_negative_samples(\n                available_items=available_item_batch[idx],\n                purchased_items=basket_batch[idx],\n                next_item=item_batch[idx],\n                n_samples=self.n_negative_samples,\n            )\n            for idx in range(batch_size)\n        ],\n        axis=0,\n    )\n    augmented_item_batch = tf.cast(\n        tf.concat([tf.expand_dims(item_batch, axis=-1), negative_samples], axis=1),\n        dtype=tf.int32,\n    )\n\n    # Each time, pick only the price of the item in augmented_item_batch from the\n    # corresponding price array\n    augmented_price_batch = tf.gather(\n        params=price_batch, indices=augmented_item_batch, batch_dims=1\n    )\n    # Compute the utility of all the available items\n    all_utilities = self.compute_batch_utility(\n        item_batch=augmented_item_batch,\n        basket_batch=basket_batch,\n        store_batch=store_batch,\n        week_batch=week_batch,\n        price_batch=augmented_price_batch,\n        available_item_batch=available_item_batch,\n        user_batch=None,\n    )  # Shape: (batch_size * (n_negative_samples + 1),)\n\n    positive_samples_utilities = tf.gather(params=all_utilities, indices=[0], axis=1)\n    negative_samples_utilities = tf.gather(\n        params=all_utilities, indices=tf.range(1, self.n_negative_samples + 1), axis=1\n    )\n\n    # Log-likelihood of a batch = sum of log-likelihoods of its samples\n    # Add a small epsilon to gain numerical stability (avoid log(0))\n    epsilon = 0.0  # No epsilon added for now\n    loglikelihood = tf.reduce_sum(\n        tf.math.log(\n            tf.sigmoid(\n                tf.tile(\n                    positive_samples_utilities,\n                    [1, self.n_negative_samples],\n                )\n                - negative_samples_utilities\n            )\n            + epsilon\n        ),\n    )  # Shape of loglikelihood: (1,))\n    bce = tf.keras.backend.binary_crossentropy(\n        # Target: 1 for positive samples, 0 for negative samples\n        target=tf.concat(\n            [\n                tf.ones_like(positive_samples_utilities),\n                tf.zeros_like(negative_samples_utilities),\n            ],\n            axis=1,\n        ),\n        output=tf.nn.sigmoid(all_utilities),\n    )  # Shape: (batch_size * (n_negative_samples + 1),)\n    ridge_regularization = self.l2_regularization * tf.add_n(\n        [tf.nn.l2_loss(weight) for weight in self.trainable_weights]\n    )\n    # Normalize by the batch size and the number of negative samples\n    return (\n        tf.reduce_sum(bce + ridge_regularization)\n        / (batch_size * (self.n_negative_samples + 1)),\n        loglikelihood,\n    )\n</code></pre>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.compute_batch_utility","title":"<code>compute_batch_utility(item_batch, basket_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch)</code>","text":"<p>Compute the utility of all the items in item_batch given the items in basket_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>Union[ndarray, Tensor]</code> <p>Batch of the purchased items ID (integers) for which to compute the utility Shape must be (batch_size,) (positive and negative samples concatenated together)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (floats) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>Batch of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <p>Returns:</p> Name Type Description <code>item_utilities</code> <code>Tensor</code> <p>Utility of all the items in item_batch Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>def compute_batch_utility(\n    self,\n    item_batch: Union[np.ndarray, tf.Tensor],\n    basket_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    user_batch: Union[np.ndarray, tf.Tensor],\n) -&gt; tf.Tensor:\n    \"\"\"Compute the utility of all the items in item_batch given the items in basket_batch.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray or tf.Tensor\n        Batch of the purchased items ID (integers) for which to compute the utility\n        Shape must be (batch_size,)\n        (positive and negative samples concatenated together)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (floats) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        Batch of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n\n    Returns\n    -------\n    item_utilities: tf.Tensor\n        Utility of all the items in item_batch\n        Shape must be (batch_size,)\n    \"\"\"\n    _ = user_batch\n    _ = available_item_batch\n    item_batch = tf.cast(item_batch, dtype=tf.int32)\n    if len(tf.shape(item_batch)) == 1:\n        if len(tf.shape(price_batch)) != 1:\n            raise ValueError(f\"\"\"Arguments price_batch and item_batch should have same shape\n            and are:{item_batch.shape} and {price_batch.shape}\"\"\")\n        item_batch = tf.expand_dims(item_batch, axis=1)\n        price_batch = tf.expand_dims(price_batch, axis=1)\n        squeeze = True\n    else:\n        squeeze = False\n\n    basket_batch = tf.cast(basket_batch, dtype=tf.int32)\n    store_batch = tf.cast(store_batch, dtype=tf.int32)\n    week_batch = tf.cast(week_batch, dtype=tf.int32)\n    price_batch = tf.cast(price_batch, dtype=tf.float32)\n\n    theta_store = tf.gather(self.theta, indices=store_batch)\n    gamma_item = tf.gather(self.gamma, indices=item_batch)\n    # Compute the dot product along the last dimension\n    store_preferences = tf.einsum(\"kj,klj-&gt;kl\", theta_store, gamma_item)\n\n    if self.item_intercept:\n        item_intercept = tf.gather(self.alpha, indices=item_batch)\n    else:\n        item_intercept = tf.zeros_like(store_preferences)\n    if self.price_effects:\n        delta_store = tf.gather(self.delta, indices=store_batch)\n        beta_item = tf.gather(self.beta, indices=item_batch)\n        # Add epsilon to avoid NaN values (log(0))\n        price_effects = (\n            -1\n            # Compute the dot product along the last dimension\n            * tf.einsum(\"kj,klj-&gt;kl\", delta_store, beta_item)\n            * tf.math.log(price_batch + self.epsilon_price)\n        )\n    else:\n        delta_store = tf.zeros_like(store_batch)\n        price_effects = tf.zeros_like(store_preferences)\n\n    if self.seasonal_effects:\n        nu_week = tf.gather(self.nu, indices=week_batch)\n        mu_item = tf.gather(self.mu, indices=item_batch)\n        # Compute the dot product along the last dimension\n        seasonal_effects = tf.einsum(\"kj,klj-&gt;kl\", nu_week, mu_item)\n    else:\n        seasonal_effects = tf.zeros_like(store_preferences)\n\n    # The effects of item intercept, store preferences, price sensitivity\n    # and seasonal effects are combined in the per-item per-trip latent variable\n    psi = tf.reduce_sum(\n        [\n            item_intercept,\n            store_preferences,\n            price_effects,\n            seasonal_effects,\n        ],\n        axis=0,\n    )  # Shape: (batch_size,)\n\n    # Create a RaggedTensor from the indices with padding removed\n    item_indices_ragged = tf.cast(\n        tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n        dtype=tf.int32,\n    )\n\n    if tf.size(item_indices_ragged) == 0:\n        # Empty baskets: no gamma embeddings to gather\n        # (It must be a ragged tensor here because TF's GraphMode requires the same\n        # nested structure to be returned from all branches of a conditional)\n        gamma_by_basket = tf.RaggedTensor.from_tensor(\n            tf.zeros((len(item_batch), 0, self.gamma.shape[1]))\n        )\n        basket_size = tf.ones((len(item_batch),))\n    else:\n        # Gather the embeddings using a ragged tensor of indices\n        gamma_by_basket = tf.ragged.map_flat_values(tf.gather, self.gamma, item_indices_ragged)\n        basket_size = tf.cast(item_indices_ragged.row_lengths(), dtype=tf.float32)\n\n    gamma_by_basket = tf.reduce_sum(gamma_by_basket, axis=1) / tf.maximum(\n        tf.expand_dims(basket_size, axis=-1), 1.0\n    )\n    # Basket interaction: one vs all\n    # Compute the dot product along the last dimension (latent_size)\n    basket_interaction_utility = tf.einsum(\"kj,klj-&gt;kl\", gamma_by_basket, gamma_item)\n\n    # Sum over the items in the basket\n    if squeeze:\n        return tf.gather(psi + basket_interaction_utility, 0, axis=1)\n    return psi + basket_interaction_utility\n</code></pre>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.get_negative_samples","title":"<code>get_negative_samples(available_items, purchased_items, next_item, n_samples)</code>","text":"<p>Sample randomly a set of items.</p> <p>(set of items not already purchased and not necessarily from the basket)</p> <p>Parameters:</p> Name Type Description Default <code>available_items</code> <code>ndarray</code> <p>Matrix indicating the availability (1) or not (0) of the products Shape must be (n_items,)</p> required <code>purchased_items</code> <code>ndarray</code> <p>List of items already purchased (already in the basket)</p> required <code>next_item</code> <code>int</code> <p>Next item (to be added in the basket)</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to draw</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>Random sample of items, each of them distinct from the next item and from the items already in the basket</p> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>def get_negative_samples(\n    self,\n    available_items: np.ndarray,\n    purchased_items: np.ndarray,\n    next_item: int,\n    n_samples: int,\n) -&gt; list[int]:\n    \"\"\"Sample randomly a set of items.\n\n    (set of items not already purchased and *not necessarily* from the basket)\n\n    Parameters\n    ----------\n    available_items: np.ndarray\n        Matrix indicating the availability (1) or not (0) of the products\n        Shape must be (n_items,)\n    purchased_items: np.ndarray\n        List of items already purchased (already in the basket)\n    next_item: int\n        Next item (to be added in the basket)\n    n_samples: int\n        Number of samples to draw\n\n    Returns\n    -------\n    list[int]\n        Random sample of items, each of them distinct from\n        the next item and from the items already in the basket\n    \"\"\"\n    # Convert inputs to tensors\n    available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n    purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n    next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n    # Get the list of available items based on the availability matrix\n    item_ids = tf.range(self.n_items)\n    available_mask = tf.equal(available_items, 1)\n    assortment = tf.boolean_mask(item_ids, available_mask)\n\n    not_to_be_chosen = tf.concat([purchased_items, tf.expand_dims(next_item, axis=0)], axis=0)\n\n    # Sample negative items from the assortment excluding not_to_be_chosen\n    negative_samples = tf.boolean_mask(\n        tensor=assortment,\n        # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n        mask=~tf.reduce_any(\n            tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n        ),\n    )\n\n    error_message = (\n        \"The number of negative samples to draw must be less than \"\n        \"the number of available items not already purchased and \"\n        \"distinct from the next item.\"\n    )\n    # Raise an error if n_samples &gt; tf.size(negative_samples)\n    tf.debugging.assert_greater_equal(\n        tf.size(negative_samples), n_samples, message=error_message\n    )\n\n    # Randomize the sampling\n    negative_samples = tf.random.shuffle(negative_samples)\n\n    # Keep only n_samples\n    return negative_samples[:n_samples]\n</code></pre>"},{"location":"references/basket_models/references_alea_carta/#choice_learn.basket_models.alea_carta.AleaCarta.instantiate","title":"<code>instantiate(n_items, n_stores=0)</code>","text":"<p>Instantiate the Shopper model.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items to consider, i.e. the number of items in the dataset</p> required <code>n_stores</code> <code>int</code> <p>Number of stores in the population</p> <code>0</code> Source code in <code>choice_learn/basket_models/alea_carta.py</code> <pre><code>def instantiate(\n    self,\n    n_items: int,\n    n_stores: int = 0,\n) -&gt; None:\n    \"\"\"Instantiate the Shopper model.\n\n    Parameters\n    ----------\n    n_items: int\n        Number of items to consider, i.e. the number of items in the dataset\n    n_stores: int\n        Number of stores in the population\n    \"\"\"\n    self.n_items = n_items\n    if n_stores == 0 and self.price_effects:\n        # To take into account the price effects, the number of stores must be &gt; 0\n        # to have a delta embedding\n        # (By default, the store id is 0)\n        n_stores = 1\n    self.n_stores = n_stores\n\n    self.gamma = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n            shape=(n_items, self.latent_sizes[\"preferences\"])\n        ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n        trainable=True,\n        name=\"gamma\",\n    )\n    self.theta = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n            shape=(n_stores, self.latent_sizes[\"preferences\"])\n        ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n        trainable=True,\n        name=\"theta\",\n    )\n\n    if self.item_intercept:\n        # Add item intercept\n        self.alpha = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_items,)  # Dimension for 1 item: 1\n            ),\n            trainable=True,\n            name=\"alpha\",\n        )\n\n    if self.price_effects:\n        # Add price sensitivity\n        self.beta = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_items, self.latent_sizes[\"price\"])\n            ),  # Dimension for 1 item: latent_sizes[\"price\"]\n            trainable=True,\n            name=\"beta\",\n        )\n        self.delta = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_stores, self.latent_sizes[\"price\"])\n            ),  # Dimension for 1 item: latent_sizes[\"price\"]\n            trainable=True,\n            name=\"delta\",\n        )\n\n    if self.seasonal_effects:\n        # Add seasonal effects\n        self.mu = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_items, self.latent_sizes[\"season\"])\n            ),  # Dimension for 1 item: latent_sizes[\"season\"]\n            trainable=True,\n            name=\"mu\",\n        )\n        self.nu = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(52, self.latent_sizes[\"season\"])\n            ),  # Dimension for 1 item: latent_sizes[\"season\"]\n            trainable=True,\n            name=\"nu\",\n        )\n\n    self.instantiated = True\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/","title":"Basic Attention Model","text":"<p>Implementation of an attention-based model for item recommendation.</p>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding","title":"<code>AttentionBasedContextEmbedding</code>","text":"<p>             Bases: <code>BaseBasketModel</code></p> <p>Class for the attention-based model.</p> <p>Wang, Shoujin, Liang Hu, Longbing Cao, Xiaoshui Huang, Defu Lian, and Wei Liu. \"Attention-based transactional context embedding for next-item recommendation.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018.</p> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>class AttentionBasedContextEmbedding(BaseBasketModel):\n    \"\"\"Class for the attention-based model.\n\n    Wang, Shoujin, Liang Hu, Longbing Cao, Xiaoshui Huang, Defu Lian,\n    and Wei Liu. \"Attention-based transactional context embedding for\n    next-item recommendation.\" In Proceedings of the AAAI conference on\n    artificial intelligence, vol. 32, no. 1. 2018.\n    \"\"\"\n\n    def __init__(\n        self,\n        latent_size: int = 4,\n        n_negative_samples: int = 2,\n        nce_distribution=\"natural\",\n        optimizer: str = \"adam\",\n        callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n        lr: float = 1e-3,\n        epochs: int = 10,\n        batch_size: int = 32,\n        grad_clip_value: Union[float, None] = None,\n        weight_decay: Union[float, None] = None,\n        momentum: float = 0.0,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the model with hyperparameters.\n\n        Parameters\n        ----------\n        epochs : int\n            Number of training epochs.\n        lr : float\n            Learning rate for the optimizer.\n        latent_size : int\n            Size of the item embeddings.\n        n_negative_samples : int\n            Number of negative samples to use in training.\n        batch_size : int\n            Size of the batches for training. Default is 50.\n        optimizer : str\n            Optimizer to use for training. Default is \"Adam\".\n        nce_distribution: str\n            Items distribution to be used to compute the NCE Loss\n            Currently available: 'natural' to estimate the distribution\n            from the train dataset and 'uniform' where all items have the\n            same disitrbution, 1/n_items. Default is 'natural'.\n        \"\"\"\n        self.instantiated = False\n\n        self.latent_size = latent_size\n        self.n_negative_samples = n_negative_samples\n        self.nce_distribution = nce_distribution\n\n        super().__init__(\n            optimizer=optimizer,\n            callbacks=callbacks,\n            lr=lr,\n            epochs=epochs,\n            batch_size=batch_size,\n            grad_clip_value=grad_clip_value,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            **kwargs,\n        )\n\n    def instantiate(\n        self,\n        n_items: int,\n    ) -&gt; None:\n        \"\"\"Initialize the model parameters.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of unique items in the dataset.\n        \"\"\"\n        self.n_items = n_items\n\n        self.Wi = tf.Variable(\n            tf.random.normal((self.n_items, self.latent_size), stddev=0.1, seed=42),\n            name=\"Wi\",\n        )\n        self.Wo = tf.Variable(\n            tf.random.normal((self.n_items, self.latent_size), stddev=0.1, seed=42),\n            name=\"Wo\",\n        )\n        self.wa = tf.Variable(tf.random.normal((self.latent_size,), stddev=0.1, seed=42), name=\"wa\")\n\n        self.empty_context_embedding = tf.Variable(\n            tf.random.normal((self.latent_size,), stddev=0.1, seed=42),\n            name=\"empty_context_embedding\",\n        )\n\n        self.loss = NoiseConstrastiveEstimation()\n        self.is_trained = False\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Return the trainable weights of the model.\n\n        Returns\n        -------\n            list\n                List of trainable weights (Wi, wa, Wo).\n        \"\"\"\n        return [self.Wi, self.wa, self.Wo, self.empty_context_embedding]\n\n    @property\n    def train_iter_method(self) -&gt; str:\n        \"\"\"Method used to generate sub-baskets from a purchased one.\n\n        Available methods are:\n        - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:\n                        (1|0); (2|1); (3|1,2); (4|1,2,3); etc...\n        - 'aleacarta': creates all the sub-baskets with N-1 items:\n                        (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)\n\n        Returns\n        -------\n        str\n            Data generation method.\n        \"\"\"\n        return \"aleacarta\"\n\n    def embed_context(self, context_items: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"Return the context embedding matrix.\n\n        Parameters\n        ----------\n            context_items : tf.Tensor\n                [batch_size, variable_length] tf.RaggedTensor\n                Tensor containing the list of the context items.\n\n        Returns\n        -------\n            tf.Tensor\n                [batch_size, latent_size] tf.Tensor\n                Tensor containing the matrix of contexts embeddings.\n        \"\"\"\n        context_embedding = tf.gather(\n            tf.concat([tf.zeros((1, self.latent_size)), self.Wi], axis=0), context_items + 1\n        )\n        e_values = tf.reduce_sum(context_embedding * self.wa, axis=-1)\n        alphas = softmax_with_availabilities(\n            items_logit_by_choice=e_values,\n            available_items_by_choice=tf.where(context_items == -1, 0.0, 1.0),\n        )\n        final_embeddings = tf.reduce_sum(\n            tf.expand_dims(alphas, axis=-1) * context_embedding, axis=1\n        )\n        return tf.where(\n            tf.reduce_sum(final_embeddings, axis=-1, keepdims=True) == 0.0,\n            tf.tile(\n                tf.expand_dims(self.empty_context_embedding, axis=0), (len(final_embeddings), 1)\n            ),\n            final_embeddings,\n        )\n\n    def compute_batch_utility(\n        self,\n        item_batch: Union[np.ndarray, tf.Tensor],\n        basket_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        user_batch: np.ndarray,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the utility of all the items in item_batch given the items in basket_batch.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray or tf.Tensor\n            Batch of the purchased items ID (integers) for which to compute the utility\n            Shape must be (batch_size,)\n            (positive and negative samples concatenated together)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (floats) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            Batch of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n\n        Returns\n        -------\n        item_utilities: tf.Tensor\n            Utility of all the items in item_batch\n            Shape must be (batch_size,)\n        \"\"\"\n        _ = store_batch\n        _ = price_batch\n        _ = week_batch\n        _ = available_item_batch\n        _ = user_batch\n        if len(tf.shape(item_batch)) == 1:\n            item_batch = tf.expand_dims(item_batch, axis=1)\n            squeeze = True\n        else:\n            squeeze = False\n\n        context_embedding = self.embed_context(basket_batch)\n        utilities = tf.einsum(\n            \"kj,klj-&gt;kl\", context_embedding, tf.gather(self.Wo, tf.cast(item_batch, tf.int32))\n        )\n        if squeeze:\n            return tf.gather(utilities, 0, axis=1)\n        return utilities\n\n    def get_negative_samples(\n        self,\n        available_items: np.ndarray,\n        purchased_items: np.ndarray,\n        next_item: int,\n        n_samples: int,\n    ) -&gt; list[int]:\n        \"\"\"Sample randomly a set of items.\n\n        (set of items not already purchased and *not necessarily* from the basket)\n\n        Parameters\n        ----------\n        available_items: np.ndarray\n            Matrix indicating the availability (1) or not (0) of the products\n            Shape must be (n_items,)\n        purchased_items: np.ndarray\n            List of items already purchased (already in the basket)\n        next_item: int\n            Next item (to be added in the basket)\n        n_samples: int\n            Number of samples to draw\n\n        Returns\n        -------\n        list[int]\n            Random sample of items, each of them distinct from\n            the next item and from the items already in the basket\n        \"\"\"\n        # Convert inputs to tensors\n        available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n        purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n        next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n        # Get the list of available items based on the availability matrix\n        item_ids = tf.range(self.n_items)\n        available_mask = tf.equal(available_items, 1)\n        assortment = tf.boolean_mask(item_ids, available_mask)\n\n        not_to_be_chosen = tf.concat([purchased_items, tf.expand_dims(next_item, axis=0)], axis=0)\n\n        # Sample negative items from the assortment excluding not_to_be_chosen\n        negative_samples = tf.boolean_mask(\n            tensor=assortment,\n            # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n            mask=~tf.reduce_any(\n                tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n            ),\n        )\n\n        error_message = (\n            \"The number of negative samples to draw must be less than \"\n            \"the number of available items not already purchased and \"\n            \"distinct from the next item.\"\n        )\n        # Raise an error if n_samples &gt; tf.size(negative_samples)\n        tf.debugging.assert_greater_equal(\n            tf.size(negative_samples), n_samples, message=error_message\n        )\n\n        # Randomize the sampling\n        negative_samples = tf.random.shuffle(negative_samples)\n\n        # Keep only n_samples\n        return negative_samples[:n_samples]\n\n    def _get_items_frequencies(self, dataset: TripDataset) -&gt; tf.Tensor:\n        \"\"\"Count the occurrences of each item in the dataset.\n\n        Parameters\n        ----------\n            dataset : TripDataset\n                Dataset containing the baskets.\n\n        Returns\n        -------\n            tf.Tensor\n                Tensor containing the count of each item.\n        \"\"\"\n        item_counts = np.zeros(self.n_items, dtype=np.int32)\n        for trip in dataset.trips:\n            for item in trip.purchases:\n                item_counts[item] += 1\n        items_distribution = item_counts / item_counts.sum()\n        return tf.constant(items_distribution, dtype=tf.float32)\n\n    def compute_batch_loss(\n        self,\n        item_batch: np.ndarray,\n        basket_batch: np.ndarray,\n        future_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        user_batch: np.ndarray,\n    ) -&gt; tuple[tf.Variable]:\n        \"\"\"Compute log-likelihood and loss for one batch of items.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray\n            Batch of purchased items ID (integers)\n            Shape must be (batch_size,)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        future_batch: np.ndarray\n            Batch of items to be purchased in the future (ID of items not yet in the\n            basket) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n            Here for signature reasons, unused for this model\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (floats) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            List of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n\n        Returns\n        -------\n        tf.Variable\n            Value of the loss for the batch (binary cross-entropy),\n            Shape must be (1,)\n        loglikelihood: tf.Variable\n            Computed log-likelihood of the batch of items\n            Approximated by difference of utilities between positive and negative samples\n            Shape must be (1,)\n        \"\"\"\n        _ = future_batch\n        _ = user_batch\n        negative_samples = tf.stack(\n            [\n                self.get_negative_samples(\n                    available_items=available_item_batch[idx],\n                    purchased_items=basket_batch[idx],\n                    next_item=item_batch[idx],\n                    n_samples=self.n_negative_samples,\n                )\n                for idx in range(len(item_batch))\n            ],\n            axis=0,\n        )\n        pos_score = self.compute_batch_utility(\n            item_batch,\n            basket_batch,\n            store_batch,\n            week_batch,\n            price_batch,\n            available_item_batch,\n            user_batch,\n        )\n        neg_scores = self.compute_batch_utility(\n            item_batch=negative_samples,\n            basket_batch=basket_batch,\n            store_batch=store_batch,\n            week_batch=week_batch,\n            price_batch=price_batch,\n            available_item_batch=available_item_batch,\n            user_batch=user_batch,\n        )\n\n        return self.loss(\n            logit_true=pos_score,\n            logit_negative=neg_scores,\n            freq_true=tf.gather(self.negative_samples_distribution, tf.cast(item_batch, tf.int32)),\n            freq_negative=tf.gather(\n                self.negative_samples_distribution,\n                tf.cast(negative_samples, tf.int32),\n            ),\n        ), 1e-10\n\n    def fit(\n        self,\n        trip_dataset: TripDataset,\n        val_dataset: Union[TripDataset, None] = None,\n        verbose: int = 0,\n    ) -&gt; None:\n        \"\"\"Trains the model for a specified number of epochs.\n\n        Parameters\n        ----------\n            dataset : TripDataset\n                Dataset of baskets to train the model on.\n        \"\"\"\n        if not self.instantiated:\n            self.instantiate(n_items=trip_dataset.n_items)\n\n        if not isinstance(trip_dataset, TripDataset):\n            raise TypeError(\"Dataset must be a TripDataset.\")\n\n        if (\n            max([len(trip.purchases) for trip in trip_dataset.trips]) + self.n_negative_samples\n            &gt; self.n_items\n        ):\n            raise ValueError(\n                \"The number of items in the dataset is less than the number of negative samples.\"\n            )\n\n        if self.nce_distribution == \"natural\":\n            self.negative_samples_distribution = self._get_items_frequencies(trip_dataset)\n        else:\n            self.negative_samples_distribution = (1 / trip_dataset.n_items) * np.ones(\n                (trip_dataset.n_items,)\n            ).astype(\"float32\")\n\n        history = super().fit(trip_dataset=trip_dataset, val_dataset=val_dataset, verbose=verbose)\n\n        self.is_trained = True\n\n        return history\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.train_iter_method","title":"<code>train_iter_method: str</code>  <code>property</code>","text":"<p>Method used to generate sub-baskets from a purchased one.</p> <p>Available methods are: - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:                 (1|0); (2|1); (3|1,2); (4|1,2,3); etc... - 'aleacarta': creates all the sub-baskets with N-1 items:                 (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)</p> <p>Returns:</p> Type Description <code>str</code> <p>Data generation method.</p>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Return the trainable weights of the model.</p> <p>Returns:</p> Type Description <code>    list</code> <p>List of trainable weights (Wi, wa, Wo).</p>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.__init__","title":"<code>__init__(latent_size=4, n_negative_samples=2, nce_distribution='natural', optimizer='adam', callbacks=None, lr=0.001, epochs=10, batch_size=32, grad_clip_value=None, weight_decay=None, momentum=0.0, **kwargs)</code>","text":"<p>Initialize the model with hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>latent_size</code> <code>int</code> <p>Size of the item embeddings.</p> <code>4</code> <code>n_negative_samples</code> <code>int</code> <p>Number of negative samples to use in training.</p> <code>2</code> <code>batch_size</code> <code>int</code> <p>Size of the batches for training. Default is 50.</p> <code>32</code> <code>optimizer</code> <code>str</code> <p>Optimizer to use for training. Default is \"Adam\".</p> <code>'adam'</code> <code>nce_distribution</code> <p>Items distribution to be used to compute the NCE Loss Currently available: 'natural' to estimate the distribution from the train dataset and 'uniform' where all items have the same disitrbution, 1/n_items. Default is 'natural'.</p> <code>'natural'</code> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def __init__(\n    self,\n    latent_size: int = 4,\n    n_negative_samples: int = 2,\n    nce_distribution=\"natural\",\n    optimizer: str = \"adam\",\n    callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n    lr: float = 1e-3,\n    epochs: int = 10,\n    batch_size: int = 32,\n    grad_clip_value: Union[float, None] = None,\n    weight_decay: Union[float, None] = None,\n    momentum: float = 0.0,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the model with hyperparameters.\n\n    Parameters\n    ----------\n    epochs : int\n        Number of training epochs.\n    lr : float\n        Learning rate for the optimizer.\n    latent_size : int\n        Size of the item embeddings.\n    n_negative_samples : int\n        Number of negative samples to use in training.\n    batch_size : int\n        Size of the batches for training. Default is 50.\n    optimizer : str\n        Optimizer to use for training. Default is \"Adam\".\n    nce_distribution: str\n        Items distribution to be used to compute the NCE Loss\n        Currently available: 'natural' to estimate the distribution\n        from the train dataset and 'uniform' where all items have the\n        same disitrbution, 1/n_items. Default is 'natural'.\n    \"\"\"\n    self.instantiated = False\n\n    self.latent_size = latent_size\n    self.n_negative_samples = n_negative_samples\n    self.nce_distribution = nce_distribution\n\n    super().__init__(\n        optimizer=optimizer,\n        callbacks=callbacks,\n        lr=lr,\n        epochs=epochs,\n        batch_size=batch_size,\n        grad_clip_value=grad_clip_value,\n        weight_decay=weight_decay,\n        momentum=momentum,\n        **kwargs,\n    )\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.compute_batch_loss","title":"<code>compute_batch_loss(item_batch, basket_batch, future_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch)</code>","text":"<p>Compute log-likelihood and loss for one batch of items.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>ndarray</code> <p>Batch of purchased items ID (integers) Shape must be (batch_size,)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>future_batch</code> <code>ndarray</code> <p>Batch of items to be purchased in the future (ID of items not yet in the basket) (arrays) for each purchased item Shape must be (batch_size, max_basket_size) Here for signature reasons, unused for this model</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (floats) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>List of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <p>Returns:</p> Name Type Description <code>Variable</code> <p>Value of the loss for the batch (binary cross-entropy), Shape must be (1,)</p> <code>loglikelihood</code> <code>Variable</code> <p>Computed log-likelihood of the batch of items Approximated by difference of utilities between positive and negative samples Shape must be (1,)</p> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def compute_batch_loss(\n    self,\n    item_batch: np.ndarray,\n    basket_batch: np.ndarray,\n    future_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    user_batch: np.ndarray,\n) -&gt; tuple[tf.Variable]:\n    \"\"\"Compute log-likelihood and loss for one batch of items.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray\n        Batch of purchased items ID (integers)\n        Shape must be (batch_size,)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    future_batch: np.ndarray\n        Batch of items to be purchased in the future (ID of items not yet in the\n        basket) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n        Here for signature reasons, unused for this model\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (floats) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        List of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n\n    Returns\n    -------\n    tf.Variable\n        Value of the loss for the batch (binary cross-entropy),\n        Shape must be (1,)\n    loglikelihood: tf.Variable\n        Computed log-likelihood of the batch of items\n        Approximated by difference of utilities between positive and negative samples\n        Shape must be (1,)\n    \"\"\"\n    _ = future_batch\n    _ = user_batch\n    negative_samples = tf.stack(\n        [\n            self.get_negative_samples(\n                available_items=available_item_batch[idx],\n                purchased_items=basket_batch[idx],\n                next_item=item_batch[idx],\n                n_samples=self.n_negative_samples,\n            )\n            for idx in range(len(item_batch))\n        ],\n        axis=0,\n    )\n    pos_score = self.compute_batch_utility(\n        item_batch,\n        basket_batch,\n        store_batch,\n        week_batch,\n        price_batch,\n        available_item_batch,\n        user_batch,\n    )\n    neg_scores = self.compute_batch_utility(\n        item_batch=negative_samples,\n        basket_batch=basket_batch,\n        store_batch=store_batch,\n        week_batch=week_batch,\n        price_batch=price_batch,\n        available_item_batch=available_item_batch,\n        user_batch=user_batch,\n    )\n\n    return self.loss(\n        logit_true=pos_score,\n        logit_negative=neg_scores,\n        freq_true=tf.gather(self.negative_samples_distribution, tf.cast(item_batch, tf.int32)),\n        freq_negative=tf.gather(\n            self.negative_samples_distribution,\n            tf.cast(negative_samples, tf.int32),\n        ),\n    ), 1e-10\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.compute_batch_utility","title":"<code>compute_batch_utility(item_batch, basket_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch)</code>","text":"<p>Compute the utility of all the items in item_batch given the items in basket_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>Union[ndarray, Tensor]</code> <p>Batch of the purchased items ID (integers) for which to compute the utility Shape must be (batch_size,) (positive and negative samples concatenated together)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (floats) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>Batch of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <p>Returns:</p> Name Type Description <code>item_utilities</code> <code>Tensor</code> <p>Utility of all the items in item_batch Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def compute_batch_utility(\n    self,\n    item_batch: Union[np.ndarray, tf.Tensor],\n    basket_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    user_batch: np.ndarray,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the utility of all the items in item_batch given the items in basket_batch.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray or tf.Tensor\n        Batch of the purchased items ID (integers) for which to compute the utility\n        Shape must be (batch_size,)\n        (positive and negative samples concatenated together)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (floats) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        Batch of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n\n    Returns\n    -------\n    item_utilities: tf.Tensor\n        Utility of all the items in item_batch\n        Shape must be (batch_size,)\n    \"\"\"\n    _ = store_batch\n    _ = price_batch\n    _ = week_batch\n    _ = available_item_batch\n    _ = user_batch\n    if len(tf.shape(item_batch)) == 1:\n        item_batch = tf.expand_dims(item_batch, axis=1)\n        squeeze = True\n    else:\n        squeeze = False\n\n    context_embedding = self.embed_context(basket_batch)\n    utilities = tf.einsum(\n        \"kj,klj-&gt;kl\", context_embedding, tf.gather(self.Wo, tf.cast(item_batch, tf.int32))\n    )\n    if squeeze:\n        return tf.gather(utilities, 0, axis=1)\n    return utilities\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.embed_context","title":"<code>embed_context(context_items)</code>","text":"<p>Return the context embedding matrix.</p> <p>Returns:</p> Type Description <code>    tf.Tensor</code> <p>[batch_size, latent_size] tf.Tensor Tensor containing the matrix of contexts embeddings.</p> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def embed_context(self, context_items: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"Return the context embedding matrix.\n\n    Parameters\n    ----------\n        context_items : tf.Tensor\n            [batch_size, variable_length] tf.RaggedTensor\n            Tensor containing the list of the context items.\n\n    Returns\n    -------\n        tf.Tensor\n            [batch_size, latent_size] tf.Tensor\n            Tensor containing the matrix of contexts embeddings.\n    \"\"\"\n    context_embedding = tf.gather(\n        tf.concat([tf.zeros((1, self.latent_size)), self.Wi], axis=0), context_items + 1\n    )\n    e_values = tf.reduce_sum(context_embedding * self.wa, axis=-1)\n    alphas = softmax_with_availabilities(\n        items_logit_by_choice=e_values,\n        available_items_by_choice=tf.where(context_items == -1, 0.0, 1.0),\n    )\n    final_embeddings = tf.reduce_sum(\n        tf.expand_dims(alphas, axis=-1) * context_embedding, axis=1\n    )\n    return tf.where(\n        tf.reduce_sum(final_embeddings, axis=-1, keepdims=True) == 0.0,\n        tf.tile(\n            tf.expand_dims(self.empty_context_embedding, axis=0), (len(final_embeddings), 1)\n        ),\n        final_embeddings,\n    )\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.fit","title":"<code>fit(trip_dataset, val_dataset=None, verbose=0)</code>","text":"<p>Trains the model for a specified number of epochs.</p> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def fit(\n    self,\n    trip_dataset: TripDataset,\n    val_dataset: Union[TripDataset, None] = None,\n    verbose: int = 0,\n) -&gt; None:\n    \"\"\"Trains the model for a specified number of epochs.\n\n    Parameters\n    ----------\n        dataset : TripDataset\n            Dataset of baskets to train the model on.\n    \"\"\"\n    if not self.instantiated:\n        self.instantiate(n_items=trip_dataset.n_items)\n\n    if not isinstance(trip_dataset, TripDataset):\n        raise TypeError(\"Dataset must be a TripDataset.\")\n\n    if (\n        max([len(trip.purchases) for trip in trip_dataset.trips]) + self.n_negative_samples\n        &gt; self.n_items\n    ):\n        raise ValueError(\n            \"The number of items in the dataset is less than the number of negative samples.\"\n        )\n\n    if self.nce_distribution == \"natural\":\n        self.negative_samples_distribution = self._get_items_frequencies(trip_dataset)\n    else:\n        self.negative_samples_distribution = (1 / trip_dataset.n_items) * np.ones(\n            (trip_dataset.n_items,)\n        ).astype(\"float32\")\n\n    history = super().fit(trip_dataset=trip_dataset, val_dataset=val_dataset, verbose=verbose)\n\n    self.is_trained = True\n\n    return history\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.get_negative_samples","title":"<code>get_negative_samples(available_items, purchased_items, next_item, n_samples)</code>","text":"<p>Sample randomly a set of items.</p> <p>(set of items not already purchased and not necessarily from the basket)</p> <p>Parameters:</p> Name Type Description Default <code>available_items</code> <code>ndarray</code> <p>Matrix indicating the availability (1) or not (0) of the products Shape must be (n_items,)</p> required <code>purchased_items</code> <code>ndarray</code> <p>List of items already purchased (already in the basket)</p> required <code>next_item</code> <code>int</code> <p>Next item (to be added in the basket)</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to draw</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>Random sample of items, each of them distinct from the next item and from the items already in the basket</p> Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def get_negative_samples(\n    self,\n    available_items: np.ndarray,\n    purchased_items: np.ndarray,\n    next_item: int,\n    n_samples: int,\n) -&gt; list[int]:\n    \"\"\"Sample randomly a set of items.\n\n    (set of items not already purchased and *not necessarily* from the basket)\n\n    Parameters\n    ----------\n    available_items: np.ndarray\n        Matrix indicating the availability (1) or not (0) of the products\n        Shape must be (n_items,)\n    purchased_items: np.ndarray\n        List of items already purchased (already in the basket)\n    next_item: int\n        Next item (to be added in the basket)\n    n_samples: int\n        Number of samples to draw\n\n    Returns\n    -------\n    list[int]\n        Random sample of items, each of them distinct from\n        the next item and from the items already in the basket\n    \"\"\"\n    # Convert inputs to tensors\n    available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n    purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n    next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n    # Get the list of available items based on the availability matrix\n    item_ids = tf.range(self.n_items)\n    available_mask = tf.equal(available_items, 1)\n    assortment = tf.boolean_mask(item_ids, available_mask)\n\n    not_to_be_chosen = tf.concat([purchased_items, tf.expand_dims(next_item, axis=0)], axis=0)\n\n    # Sample negative items from the assortment excluding not_to_be_chosen\n    negative_samples = tf.boolean_mask(\n        tensor=assortment,\n        # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n        mask=~tf.reduce_any(\n            tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n        ),\n    )\n\n    error_message = (\n        \"The number of negative samples to draw must be less than \"\n        \"the number of available items not already purchased and \"\n        \"distinct from the next item.\"\n    )\n    # Raise an error if n_samples &gt; tf.size(negative_samples)\n    tf.debugging.assert_greater_equal(\n        tf.size(negative_samples), n_samples, message=error_message\n    )\n\n    # Randomize the sampling\n    negative_samples = tf.random.shuffle(negative_samples)\n\n    # Keep only n_samples\n    return negative_samples[:n_samples]\n</code></pre>"},{"location":"references/basket_models/references_basic_attention_model/#choice_learn.basket_models.basic_attention_model.AttentionBasedContextEmbedding.instantiate","title":"<code>instantiate(n_items)</code>","text":"<p>Initialize the model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of unique items in the dataset.</p> required Source code in <code>choice_learn/basket_models/basic_attention_model.py</code> <pre><code>def instantiate(\n    self,\n    n_items: int,\n) -&gt; None:\n    \"\"\"Initialize the model parameters.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of unique items in the dataset.\n    \"\"\"\n    self.n_items = n_items\n\n    self.Wi = tf.Variable(\n        tf.random.normal((self.n_items, self.latent_size), stddev=0.1, seed=42),\n        name=\"Wi\",\n    )\n    self.Wo = tf.Variable(\n        tf.random.normal((self.n_items, self.latent_size), stddev=0.1, seed=42),\n        name=\"Wo\",\n    )\n    self.wa = tf.Variable(tf.random.normal((self.latent_size,), stddev=0.1, seed=42), name=\"wa\")\n\n    self.empty_context_embedding = tf.Variable(\n        tf.random.normal((self.latent_size,), stddev=0.1, seed=42),\n        name=\"empty_context_embedding\",\n    )\n\n    self.loss = NoiseConstrastiveEstimation()\n    self.is_trained = False\n    self.instantiated = True\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/","title":"Self Attention Model","text":"<p>Implementation of an attention-based model for item recommendation.</p>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel","title":"<code>SelfAttentionModel</code>","text":"<p>             Bases: <code>BaseBasketModel</code></p> <p>Class for the self attention model for basket recommendation.</p> <p>Basket Choice Modeling Inspired by the paper: \"Next Item Recommendation with Self-Attention\"  Shuai Zhang, Lina Yao, Yi Tay, and Aixin Sun. The algorithm was modified and adapted to the basket recommendation task.</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>class SelfAttentionModel(BaseBasketModel):\n    \"\"\"Class for the self attention model for basket recommendation.\n\n    Basket Choice Modeling\n    Inspired by the paper: \"Next Item Recommendation with Self-Attention\"  Shuai Zhang, Lina Yao,\n    Yi Tay, and Aixin Sun.\n    The algorithm was modified and adapted to the basket recommendation task.\n    \"\"\"\n\n    def __init__(\n        self,\n        latent_sizes: dict[str, int] = {\"short_term\": 10, \"long_term\": 10},\n        hinge_margin: float = 0.5,\n        short_term_ratio: float = 0.5,\n        n_negative_samples: int = 2,\n        optimizer: str = \"adam\",\n        callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n        lr: float = 1e-3,\n        epochs: int = 10,\n        batch_size: int = 32,\n        grad_clip_value: Union[float, None] = None,\n        weight_decay: Union[float, None] = None,\n        momentum: float = 0.0,\n        l2_regularization: float = 0.0,\n        dropout_rate: float = 0.0,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the model with hyperparameters.\n\n        Parameters\n        ----------\n        latent_size : int\n            Size of the item embeddings.\n        hinge_margin : float\n            Margin parameter for the hinge loss.\n        short_term_weight : float\n            Weighting factor between long-term and short-term preferences.\n        n_negative_samples : int\n            Number of negative samples to use in training.\n        optimizer : str\n            Optimizer to use for training. Default is \"Adam\".\n        callbacks : tf.keras.callbacks.CallbackList or None\n            List of callbacks to use during training. Default is None.\n        lr : float\n            Learning rate for the optimizer.\n        epochs : int\n            Number of training epochs.\n        batch_size : int\n            Size of the batches for training. Default is 32.\n        grad_clip_value : float or None\n            Value for gradient clipping. Default is None (no clipping).\n        weight_decay : float or None\n            Weight decay (L2 regularization) factor. Default is None (no weight decay).\n        momentum : float\n            Momentum factor for optimizers that support it. Default is 0.0.\n        \"\"\"\n        self.instantiated = False\n\n        for val in latent_sizes.keys():\n            if val not in [\"short_term\", \"long_term\"]:\n                raise ValueError(f\"Unknown value for latent_sizes dict: {val}.\")\n        if \"short_term\" not in latent_sizes:\n            latent_sizes[\"short_term\"] = 10\n        if \"long_term\" not in latent_sizes:\n            latent_sizes[\"long_term\"] = 10\n\n        self.hinge_margin = hinge_margin\n        self.short_term_ratio = short_term_ratio\n        self.n_negative_samples = n_negative_samples\n\n        self.latent_sizes = latent_sizes\n        self.d = self.latent_sizes[\"short_term\"]\n        self.d_long = self.latent_sizes[\"long_term\"]\n        self.l2_regularization = l2_regularization\n        self.dropout_rate = dropout_rate\n\n        super().__init__(\n            optimizer=optimizer,\n            callbacks=callbacks,\n            lr=lr,\n            epochs=epochs,\n            batch_size=batch_size,\n            grad_clip_value=grad_clip_value,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            **kwargs,\n        )\n\n    def instantiate(\n        self,\n        n_items: int,\n        n_users: int,\n    ) -&gt; None:\n        \"\"\"Initialize the model parameters.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of unique items in the dataset.\n        n_users : int\n            Number of unique users in the dataset.\n\n        Variables\n        ----------\n        X : tf.Variable\n            Item embedding matrix for short-term preferences, size (n_items, d).\n        V : tf.Variable\n            Item embedding matrix for long-term preferences, size (n_items, d_long).\n        U : tf.Variable\n            User embedding matrix for long-term preferences, size (n_users, d_long).\n        Wq : tf.Variable\n            Weight matrix for query transformation in attention mechanism, size (d, d).\n        Wk : tf.Variable\n            Weight matrix for key transformation in attention mechanism, size (d, d).\n        \"\"\"\n        self.n_items = n_items\n        self.n_users = n_users\n        ##############\n\n        self.X = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(shape=(n_items + 1, self.d)),\n            trainable=True,\n            name=\"X\",\n        )\n\n        self.V = tf.Variable(\n            tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=42)(\n                shape=(n_items, self.d_long)\n            ),\n            trainable=True,\n            name=\"V\",\n        )\n\n        self.U = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(\n                shape=(self.n_users, self.d_long)\n            ),\n            trainable=True,\n            name=\"U\",\n        )\n\n        self.Wq = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(shape=(self.d, self.d)),\n            trainable=True,\n            name=\"Wq\",\n        )\n\n        self.Wk = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(shape=(self.d, self.d)),\n            trainable=True,\n            name=\"Wk\",\n        )\n\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Return the trainable weights of the model.\n\n        Returns\n        -------\n            list\n                List of trainable weights (X, V, U, Wq, Wk).\n        \"\"\"\n        return [self.X, self.V, self.U, self.Wq, self.Wk]\n\n    @property\n    def train_iter_method(self) -&gt; str:\n        \"\"\"Method used to generate sub-baskets from a purchased one.\n\n        Available methods are:\n        - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:\n                        (1|0); (2|1); (3|1,2); (4|1,2,3); etc...\n        - 'aleacarta': creates all the sub-baskets with N-1 items:\n                        (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)\n\n        Returns\n        -------\n        str\n            Data generation method.\n        \"\"\"\n        return \"aleacarta\"\n\n    def masked_attention(self, basket_batch, scaled_scores):\n        \"\"\"Compute the masked attention weights.\n\n        Applying a mask to ignore padding items. Also applied a mask on\n        the diagonal to avoid attending to the same item, if activated\n        \"\"\"\n        batch_size = tf.shape(basket_batch)[0]\n        mask = tf.not_equal(\n            basket_batch, self.n_items\n        )  # shape: (batch_size, L), True si pas padding\n\n        if tf.shape(basket_batch)[1] == 1:\n            attention_weights = tf.ones_like(scaled_scores)  # Shape: (batch_size, L, 1)\n\n        else:\n            # Masque de la diagonale, d\u00e9sactiv\u00e9 pour l'instant\n            diag_mask = tf.eye(tf.shape(basket_batch)[1], batch_shape=[batch_size], dtype=tf.bool)\n            scaled_scores = tf.where(\n                diag_mask,\n                tf.constant(-np.inf, dtype=scaled_scores.dtype),\n                scaled_scores,\n            )\n\n            # Masque des padding items\n            mask_col = tf.expand_dims(mask, axis=1)  # (batch_size, 1, L)\n            scaled_scores = tf.where(\n                mask_col, scaled_scores, tf.constant(-np.inf, dtype=scaled_scores.dtype)\n            )\n\n            all_inf_row = tf.reduce_all(tf.math.is_inf(scaled_scores), axis=-1)  # (batch_size, L)\n            # We set to zero the first value of the rows where all values are -inf to avoid NaNs in\n            # softmax\n            indices = tf.where(all_inf_row)\n            indices_full = tf.concat([indices, tf.zeros_like(indices[:, :1])], axis=1)\n            updates = tf.zeros([tf.shape(indices_full)[0]], dtype=scaled_scores.dtype)\n            scaled_scores = tf.tensor_scatter_nd_update(scaled_scores, indices_full, updates)\n\n            attention_weights = tf.nn.softmax(scaled_scores, axis=-1)  # Shape: (batch_size, L, L)\n\n        return attention_weights\n\n    def embed_basket(self, basket_batch: tf.Tensor, is_training: bool = False) -&gt; tf.Tensor:\n        \"\"\"Return the context embedding matrix.\n\n        Parameters\n        ----------\n            basket_batch : tf.Tensor\n                [batch_size, L]\n                Tensor containing the list of the context items.\n            is_training : bool\n                Whether the model is in training mode or not, to activate dropout if needed.\n\n        Returns\n        -------\n            basket_embedding : tf.Tensor\n                [batch_size, latent_size] tf.Tensor\n                Tensor containing the vector of contexts embeddings.\n            attention_weights : tf.Tensor\n                [batch_size, L, L] tf.Tensor\n                Tensor containing the attention matrix.\n        \"\"\"\n        padding_vector = tf.zeros(shape=[1, self.d])  # Shape (1, d)\n        padded_items = tf.concat([self.X, padding_vector], axis=0)\n        x_basket = tf.gather(padded_items, indices=basket_batch)  # Shape: (batch_size, L, d)\n\n        q_prime = tf.nn.relu(tf.matmul(x_basket, self.Wq))  # Shape: (batch_size, L, d)\n        k_prime = tf.nn.relu(tf.matmul(x_basket, self.Wk))\n\n        if is_training:\n            q_prime = tf.nn.dropout(q_prime, rate=self.dropout_rate)\n            k_prime = tf.nn.dropout(k_prime, rate=self.dropout_rate)\n\n        scores = tf.matmul(q_prime, k_prime, transpose_b=True)\n        scaled_scores = scores / tf.sqrt(float(self.d))\n        attention_weights = self.masked_attention(\n            basket_batch, scaled_scores\n        )  # Shape: (batch_size, L, L)\n\n        attention_output = tf.matmul(attention_weights, x_basket)  # Shape: (batch_size, L, d)\n\n        mask = tf.not_equal(basket_batch, self.n_items)\n        mask_float = tf.cast(mask, dtype=tf.float32)\n        mask_float = tf.expand_dims(mask_float, axis=-1)\n        masked_attention_output = attention_output * mask_float  # (batch_size, L, d)\n\n        # Number of items in each basket (excluding padding)\n        num_items_by_basket = tf.reduce_sum(mask_float, axis=1)  # (batch_size, 1)\n\n        basket_embedding = tf.math.divide_no_nan(\n            tf.reduce_sum(masked_attention_output, axis=1, keepdims=True),\n            num_items_by_basket[:, tf.newaxis, :],\n        )\n        basket_embedding = tf.squeeze(basket_embedding, axis=1)  # Shape: (batch_size,)\n        # ----- Handle empty baskets by assigning a padding embedding -----\n        padding_embedding = self.X[-1]  # shape: (d,)\n        empty_basket_mask = tf.squeeze(tf.equal(num_items_by_basket, 0), axis=1)  # (batch_size,)\n        basket_embedding = tf.where(\n            tf.expand_dims(empty_basket_mask, axis=1),\n            tf.broadcast_to(padding_embedding, tf.shape(basket_embedding)),\n            basket_embedding,\n        )\n        # -----------------------------------------------------------------\n        return basket_embedding, attention_weights\n\n    def compute_batch_short_distance(\n        self,\n        item_batch: Union[np.ndarray, tf.Tensor],\n        basket_embedding: tf.Tensor,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the short distance of the items in item_batch given the items in basket_batch.\n\n        Parameters\n        ----------\n        item_batch: or tf.Tensor\n            Batch of the purchased items ID (integers) for which to compute the distance from their\n            basket.\n            Shape must be (batch_size,)\n            (positive and negative samples concatenated together)\n        basket_embedding: tf.Tensor\n            Batch of context embeddings for each purchased item\n            Shape must be (batch_size, latent_size)\n\n        Returns\n        -------\n        short_term_distance: tf.Tensor\n            Distance of all the items in item_batch from their ground truth embedding (X)\n            Shape must be (batch_size,)\n        \"\"\"\n        x_item_target = tf.gather(self.X, indices=item_batch)  # Shape: (batch_size, d)\n\n        return tf.reduce_sum(\n            tf.square(tf.expand_dims(basket_embedding, axis=1) - x_item_target), axis=-1\n        )\n\n    def compute_batch_long_distance(\n        self,\n        item_batch: Union[np.ndarray, tf.Tensor],\n        user_batch: np.ndarray,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the long distance of all the items in item_batch given the user.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray or tf.Tensor\n            Batch of the purchased items ID (integers) for which to compute the distance from their\n            user.\n            Shape must be (batch_size,)\n            (positive and negative samples concatenated together)\n\n        user_batch: np.ndarray\n            Batch of user IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n\n        Returns\n        -------\n        long_term_distance: tf.Tensor\n            Distance of all the items in item_batch from their ground truth embedding (V)\n            Shape must be (batch_size,)\n        \"\"\"\n        v_future_batch = tf.gather(self.V, indices=item_batch)  # Shape: (batch_size, d)\n\n        u_user_batch = tf.gather(self.U, indices=user_batch)  # Shape: (batch_size, d)\n        return tf.reduce_sum(\n            tf.square(tf.expand_dims(u_user_batch, axis=1) - v_future_batch), axis=-1\n        )  # Shape: (batch_size, 1)\n\n    def compute_batch_distance(\n        self,\n        item_batch: np.ndarray,\n        basket_batch: np.ndarray,\n        user_batch: np.ndarray,\n        is_training: bool,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the total distance (long + short term) of all the items in item_batch.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray\n            Batch of the purchased items ID (integers) for which to compute the distance from their\n            basket.\n            Shape must be (batch_size, None)\n            (positive and negative samples concatenated together)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        user_batch: np.ndarray\n            Batch of user IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        is_training : bool\n            Whether the model is in training mode or not, to activate dropout if needed.\n\n        Returns\n        -------\n        total_distance: tf.Tensor\n            Total distance of all the items in item_batch from their ground truth embeddings\n            Shape must be (batch_size, None)\n        \"\"\"\n        basket_batch_ragged = tf.cast(\n            tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n            dtype=tf.int32,\n        )\n        basket_batch = basket_batch_ragged.to_tensor(self.n_items)\n        basket_embedding, _ = self.embed_basket(basket_batch, is_training)  # Shape: (batch_size, d)\n\n        long_distance = self.compute_batch_long_distance(item_batch, user_batch)\n\n        short_distance = self.compute_batch_short_distance(item_batch, basket_embedding)\n        return self.short_term_ratio * long_distance + (1 - self.short_term_ratio) * short_distance\n\n    def compute_batch_utility(\n        self,\n        item_batch,\n        basket_batch,\n        store_batch,\n        week_batch,\n        price_batch,\n        available_item_batch,\n        user_batch,\n        is_training: bool = False,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the utility of all the items in item_batch.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray\n            Batch of the purchased items ID (integers) for which to compute the utility from their\n            basket.\n            Shape must be (batch_size,)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (floats) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            List of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n        user_batch: np.ndarray\n            Batch of user IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        is_training : bool\n            Whether the model is in training mode or not, to activate dropout if needed.\n\n        Returns\n        -------\n        tf.Tensor\n            Utility of all the items in item_batch\n            Shape must be (batch_size,)\n        \"\"\"\n        _ = store_batch  # Unused for this model\n        _ = week_batch  # Unused for this model\n        _ = price_batch  # Unused for this model\n        _ = available_item_batch  # Unused for this model\n        return -self.compute_batch_distance(\n            item_batch=item_batch,\n            basket_batch=basket_batch,\n            user_batch=user_batch,\n            is_training=is_training,\n        )\n\n    def get_negative_samples(\n        self,\n        available_items: np.ndarray,\n        purchased_items: np.ndarray,\n        next_item: int,\n        n_samples: int,\n    ) -&gt; list[int]:\n        \"\"\"Sample randomly a set of items.\n\n        (set of items not already purchased and *not necessarily* from the basket)\n\n        Parameters\n        ----------\n        available_items: np.ndarray\n            Matrix indicating the availability (1) or not (0) of the products\n            Shape must be (n_items,)\n        purchased_items: np.ndarray\n            List of items already purchased (already in the basket)\n        next_item: int\n            Next item (to be added in the basket)\n        n_samples: int\n            Number of samples to draw\n\n        Returns\n        -------\n        list[int]\n            Random sample of items, each of them distinct from\n            the next item and from the items already in the basket\n        \"\"\"\n        # Convert inputs to tensors\n        available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n        purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n        next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n        # Get the list of available items based on the availability matrix\n        item_ids = tf.range(self.n_items)\n        available_mask = tf.equal(available_items, 1)\n        assortment = tf.boolean_mask(item_ids, available_mask)\n\n        not_to_be_chosen = tf.concat([purchased_items, tf.expand_dims(next_item, axis=0)], axis=0)\n\n        # Sample negative items from the assortment excluding not_to_be_chosen\n        negative_samples = tf.boolean_mask(\n            tensor=assortment,\n            # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n            mask=~tf.reduce_any(\n                tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n            ),\n        )\n\n        error_message = (\n            \"The number of negative samples to draw must be less than \"\n            \"the number of available items not already purchased and \"\n            \"distinct from the next item.\"\n        )\n        # Raise an error if n_samples &gt; tf.size(negative_samples)\n        tf.debugging.assert_greater_equal(\n            tf.size(negative_samples), n_samples, message=error_message\n        )\n\n        # Randomize the sampling\n        negative_samples = tf.random.shuffle(negative_samples)\n\n        # Keep only n_samples\n        return negative_samples[:n_samples]\n\n    def compute_batch_loss(\n        self,\n        item_batch: np.ndarray,\n        basket_batch: np.ndarray,\n        future_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        user_batch: np.ndarray,\n        is_training: bool = True,\n    ) -&gt; tuple[tf.Variable]:\n        \"\"\"Compute total loss.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray\n            Batch of purchased items ID (integers)\n            Shape must be (batch_size,)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        future_batch: np.ndarray\n            Batch of items to be purchased in the future (ID of items not yet in the\n            basket) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n            Here for signature reasons, unused for this model\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (floats) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            List of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n        user_batch: np.ndarray\n            Batch of user IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        is_training: bool\n            Whether the model is in training mode or not, to activate dropout if needed.\n            True by default, cause compute_batch_loss is only used during training.\n\n        Returns\n        -------\n        tf.Variable\n            Value of the loss for the batch (Hinge loss),\n            Shape must be (1,)\n        _: None\n            Placeholder to match the signature of the parent class method\n        \"\"\"\n        _ = future_batch  # Unused for this model\n        _ = store_batch  # Unused for this model\n        _ = week_batch  # Unused for this model\n        _ = price_batch  # Unused for this model\n\n        batch_size = len(item_batch)\n\n        negative_samples = tf.stack(\n            [\n                self.get_negative_samples(\n                    available_items=available_item_batch[idx],\n                    purchased_items=basket_batch[idx],\n                    next_item=item_batch[idx],\n                    n_samples=self.n_negative_samples,\n                )\n                for idx in range(batch_size)\n            ],\n            axis=0,\n        )  # Shape: (batch_size, n_negative_samples)\n\n        item_batch = tf.cast(item_batch, tf.int32)\n        negative_samples = tf.cast(negative_samples, tf.int32)\n\n        augmented_item_batch = tf.cast(\n            tf.concat([tf.expand_dims(item_batch, axis=-1), negative_samples], axis=1),\n            dtype=tf.int32,\n        )  # Shape: (batch_size, 1 + n_negative_samples)\n\n        basket_batch_ragged = tf.cast(\n            tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n            dtype=tf.int32,\n        )\n        basket_batch = basket_batch_ragged.to_tensor(self.n_items)\n        all_distances = self.compute_batch_distance(\n            item_batch=augmented_item_batch,\n            basket_batch=basket_batch,\n            user_batch=user_batch,\n            is_training=is_training,\n        )  # Shape: (batch_size, 1 + n_negative_samples)\n\n        positive_samples_distance = tf.gather(\n            params=all_distances, indices=[0], axis=1\n        )  # (batch_size, 1)\n        neg = tf.gather(\n            params=all_distances, indices=tf.range(1, self.n_negative_samples + 1), axis=1\n        )  # (batch_size, n_negative_samples)\n        pos = tf.tile(\n            positive_samples_distance, [1, self.n_negative_samples]\n        )  # (batch_size, n_negative_samples)\n\n        ridge_regularization = self.l2_regularization * (\n            tf.nn.l2_loss(self.U)\n            + tf.nn.l2_loss(self.V)\n            + tf.nn.l2_loss(self.X)\n            + tf.nn.l2_loss(self.Wq)\n            + tf.nn.l2_loss(self.Wk)\n        )\n\n        hinge_loss = (\n            tf.maximum(float(0), self.hinge_margin + pos - neg) + ridge_regularization\n        )  # (batch_size, n_negative_samples)\n        total_loss = tf.reduce_sum(hinge_loss)\n\n        # Normalize by the batch size and the number of negative samples\n        if tf.reduce_any(self.hinge_margin &gt; 0):\n            return (\n                total_loss / (batch_size * self.n_negative_samples * self.hinge_margin),\n                _,\n            )\n        return total_loss / (batch_size * self.n_negative_samples), _\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.train_iter_method","title":"<code>train_iter_method: str</code>  <code>property</code>","text":"<p>Method used to generate sub-baskets from a purchased one.</p> <p>Available methods are: - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:                 (1|0); (2|1); (3|1,2); (4|1,2,3); etc... - 'aleacarta': creates all the sub-baskets with N-1 items:                 (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)</p> <p>Returns:</p> Type Description <code>str</code> <p>Data generation method.</p>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Return the trainable weights of the model.</p> <p>Returns:</p> Type Description <code>    list</code> <p>List of trainable weights (X, V, U, Wq, Wk).</p>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.__init__","title":"<code>__init__(latent_sizes={'short_term': 10, 'long_term': 10}, hinge_margin=0.5, short_term_ratio=0.5, n_negative_samples=2, optimizer='adam', callbacks=None, lr=0.001, epochs=10, batch_size=32, grad_clip_value=None, weight_decay=None, momentum=0.0, l2_regularization=0.0, dropout_rate=0.0, **kwargs)</code>","text":"<p>Initialize the model with hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>latent_size</code> <code>int</code> <p>Size of the item embeddings.</p> required <code>hinge_margin</code> <code>float</code> <p>Margin parameter for the hinge loss.</p> <code>0.5</code> <code>short_term_weight</code> <code>float</code> <p>Weighting factor between long-term and short-term preferences.</p> required <code>n_negative_samples</code> <code>int</code> <p>Number of negative samples to use in training.</p> <code>2</code> <code>optimizer</code> <code>str</code> <p>Optimizer to use for training. Default is \"Adam\".</p> <code>'adam'</code> <code>callbacks</code> <code>CallbackList or None</code> <p>List of callbacks to use during training. Default is None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Size of the batches for training. Default is 32.</p> <code>32</code> <code>grad_clip_value</code> <code>float or None</code> <p>Value for gradient clipping. Default is None (no clipping).</p> <code>None</code> <code>weight_decay</code> <code>float or None</code> <p>Weight decay (L2 regularization) factor. Default is None (no weight decay).</p> <code>None</code> <code>momentum</code> <code>float</code> <p>Momentum factor for optimizers that support it. Default is 0.0.</p> <code>0.0</code> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def __init__(\n    self,\n    latent_sizes: dict[str, int] = {\"short_term\": 10, \"long_term\": 10},\n    hinge_margin: float = 0.5,\n    short_term_ratio: float = 0.5,\n    n_negative_samples: int = 2,\n    optimizer: str = \"adam\",\n    callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n    lr: float = 1e-3,\n    epochs: int = 10,\n    batch_size: int = 32,\n    grad_clip_value: Union[float, None] = None,\n    weight_decay: Union[float, None] = None,\n    momentum: float = 0.0,\n    l2_regularization: float = 0.0,\n    dropout_rate: float = 0.0,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the model with hyperparameters.\n\n    Parameters\n    ----------\n    latent_size : int\n        Size of the item embeddings.\n    hinge_margin : float\n        Margin parameter for the hinge loss.\n    short_term_weight : float\n        Weighting factor between long-term and short-term preferences.\n    n_negative_samples : int\n        Number of negative samples to use in training.\n    optimizer : str\n        Optimizer to use for training. Default is \"Adam\".\n    callbacks : tf.keras.callbacks.CallbackList or None\n        List of callbacks to use during training. Default is None.\n    lr : float\n        Learning rate for the optimizer.\n    epochs : int\n        Number of training epochs.\n    batch_size : int\n        Size of the batches for training. Default is 32.\n    grad_clip_value : float or None\n        Value for gradient clipping. Default is None (no clipping).\n    weight_decay : float or None\n        Weight decay (L2 regularization) factor. Default is None (no weight decay).\n    momentum : float\n        Momentum factor for optimizers that support it. Default is 0.0.\n    \"\"\"\n    self.instantiated = False\n\n    for val in latent_sizes.keys():\n        if val not in [\"short_term\", \"long_term\"]:\n            raise ValueError(f\"Unknown value for latent_sizes dict: {val}.\")\n    if \"short_term\" not in latent_sizes:\n        latent_sizes[\"short_term\"] = 10\n    if \"long_term\" not in latent_sizes:\n        latent_sizes[\"long_term\"] = 10\n\n    self.hinge_margin = hinge_margin\n    self.short_term_ratio = short_term_ratio\n    self.n_negative_samples = n_negative_samples\n\n    self.latent_sizes = latent_sizes\n    self.d = self.latent_sizes[\"short_term\"]\n    self.d_long = self.latent_sizes[\"long_term\"]\n    self.l2_regularization = l2_regularization\n    self.dropout_rate = dropout_rate\n\n    super().__init__(\n        optimizer=optimizer,\n        callbacks=callbacks,\n        lr=lr,\n        epochs=epochs,\n        batch_size=batch_size,\n        grad_clip_value=grad_clip_value,\n        weight_decay=weight_decay,\n        momentum=momentum,\n        **kwargs,\n    )\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.compute_batch_distance","title":"<code>compute_batch_distance(item_batch, basket_batch, user_batch, is_training)</code>","text":"<p>Compute the total distance (long + short term) of all the items in item_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>ndarray</code> <p>Batch of the purchased items ID (integers) for which to compute the distance from their basket. Shape must be (batch_size, None) (positive and negative samples concatenated together)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>user_batch</code> <code>ndarray</code> <p>Batch of user IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>is_training</code> <code>bool</code> <p>Whether the model is in training mode or not, to activate dropout if needed.</p> required <p>Returns:</p> Name Type Description <code>total_distance</code> <code>Tensor</code> <p>Total distance of all the items in item_batch from their ground truth embeddings Shape must be (batch_size, None)</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def compute_batch_distance(\n    self,\n    item_batch: np.ndarray,\n    basket_batch: np.ndarray,\n    user_batch: np.ndarray,\n    is_training: bool,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the total distance (long + short term) of all the items in item_batch.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray\n        Batch of the purchased items ID (integers) for which to compute the distance from their\n        basket.\n        Shape must be (batch_size, None)\n        (positive and negative samples concatenated together)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    user_batch: np.ndarray\n        Batch of user IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    is_training : bool\n        Whether the model is in training mode or not, to activate dropout if needed.\n\n    Returns\n    -------\n    total_distance: tf.Tensor\n        Total distance of all the items in item_batch from their ground truth embeddings\n        Shape must be (batch_size, None)\n    \"\"\"\n    basket_batch_ragged = tf.cast(\n        tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n        dtype=tf.int32,\n    )\n    basket_batch = basket_batch_ragged.to_tensor(self.n_items)\n    basket_embedding, _ = self.embed_basket(basket_batch, is_training)  # Shape: (batch_size, d)\n\n    long_distance = self.compute_batch_long_distance(item_batch, user_batch)\n\n    short_distance = self.compute_batch_short_distance(item_batch, basket_embedding)\n    return self.short_term_ratio * long_distance + (1 - self.short_term_ratio) * short_distance\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.compute_batch_long_distance","title":"<code>compute_batch_long_distance(item_batch, user_batch)</code>","text":"<p>Compute the long distance of all the items in item_batch given the user.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>Union[ndarray, Tensor]</code> <p>Batch of the purchased items ID (integers) for which to compute the distance from their user. Shape must be (batch_size,) (positive and negative samples concatenated together)</p> required <code>user_batch</code> <code>ndarray</code> <p>Batch of user IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <p>Returns:</p> Name Type Description <code>long_term_distance</code> <code>Tensor</code> <p>Distance of all the items in item_batch from their ground truth embedding (V) Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def compute_batch_long_distance(\n    self,\n    item_batch: Union[np.ndarray, tf.Tensor],\n    user_batch: np.ndarray,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the long distance of all the items in item_batch given the user.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray or tf.Tensor\n        Batch of the purchased items ID (integers) for which to compute the distance from their\n        user.\n        Shape must be (batch_size,)\n        (positive and negative samples concatenated together)\n\n    user_batch: np.ndarray\n        Batch of user IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n\n    Returns\n    -------\n    long_term_distance: tf.Tensor\n        Distance of all the items in item_batch from their ground truth embedding (V)\n        Shape must be (batch_size,)\n    \"\"\"\n    v_future_batch = tf.gather(self.V, indices=item_batch)  # Shape: (batch_size, d)\n\n    u_user_batch = tf.gather(self.U, indices=user_batch)  # Shape: (batch_size, d)\n    return tf.reduce_sum(\n        tf.square(tf.expand_dims(u_user_batch, axis=1) - v_future_batch), axis=-1\n    )  # Shape: (batch_size, 1)\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.compute_batch_loss","title":"<code>compute_batch_loss(item_batch, basket_batch, future_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch, is_training=True)</code>","text":"<p>Compute total loss.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>ndarray</code> <p>Batch of purchased items ID (integers) Shape must be (batch_size,)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>future_batch</code> <code>ndarray</code> <p>Batch of items to be purchased in the future (ID of items not yet in the basket) (arrays) for each purchased item Shape must be (batch_size, max_basket_size) Here for signature reasons, unused for this model</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (floats) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>List of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <code>user_batch</code> <code>ndarray</code> <p>Batch of user IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>is_training</code> <code>bool</code> <p>Whether the model is in training mode or not, to activate dropout if needed. True by default, cause compute_batch_loss is only used during training.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Variable</code> <p>Value of the loss for the batch (Hinge loss), Shape must be (1,)</p> <code>_</code> <code>None</code> <p>Placeholder to match the signature of the parent class method</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def compute_batch_loss(\n    self,\n    item_batch: np.ndarray,\n    basket_batch: np.ndarray,\n    future_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    user_batch: np.ndarray,\n    is_training: bool = True,\n) -&gt; tuple[tf.Variable]:\n    \"\"\"Compute total loss.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray\n        Batch of purchased items ID (integers)\n        Shape must be (batch_size,)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    future_batch: np.ndarray\n        Batch of items to be purchased in the future (ID of items not yet in the\n        basket) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n        Here for signature reasons, unused for this model\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (floats) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        List of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n    user_batch: np.ndarray\n        Batch of user IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    is_training: bool\n        Whether the model is in training mode or not, to activate dropout if needed.\n        True by default, cause compute_batch_loss is only used during training.\n\n    Returns\n    -------\n    tf.Variable\n        Value of the loss for the batch (Hinge loss),\n        Shape must be (1,)\n    _: None\n        Placeholder to match the signature of the parent class method\n    \"\"\"\n    _ = future_batch  # Unused for this model\n    _ = store_batch  # Unused for this model\n    _ = week_batch  # Unused for this model\n    _ = price_batch  # Unused for this model\n\n    batch_size = len(item_batch)\n\n    negative_samples = tf.stack(\n        [\n            self.get_negative_samples(\n                available_items=available_item_batch[idx],\n                purchased_items=basket_batch[idx],\n                next_item=item_batch[idx],\n                n_samples=self.n_negative_samples,\n            )\n            for idx in range(batch_size)\n        ],\n        axis=0,\n    )  # Shape: (batch_size, n_negative_samples)\n\n    item_batch = tf.cast(item_batch, tf.int32)\n    negative_samples = tf.cast(negative_samples, tf.int32)\n\n    augmented_item_batch = tf.cast(\n        tf.concat([tf.expand_dims(item_batch, axis=-1), negative_samples], axis=1),\n        dtype=tf.int32,\n    )  # Shape: (batch_size, 1 + n_negative_samples)\n\n    basket_batch_ragged = tf.cast(\n        tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n        dtype=tf.int32,\n    )\n    basket_batch = basket_batch_ragged.to_tensor(self.n_items)\n    all_distances = self.compute_batch_distance(\n        item_batch=augmented_item_batch,\n        basket_batch=basket_batch,\n        user_batch=user_batch,\n        is_training=is_training,\n    )  # Shape: (batch_size, 1 + n_negative_samples)\n\n    positive_samples_distance = tf.gather(\n        params=all_distances, indices=[0], axis=1\n    )  # (batch_size, 1)\n    neg = tf.gather(\n        params=all_distances, indices=tf.range(1, self.n_negative_samples + 1), axis=1\n    )  # (batch_size, n_negative_samples)\n    pos = tf.tile(\n        positive_samples_distance, [1, self.n_negative_samples]\n    )  # (batch_size, n_negative_samples)\n\n    ridge_regularization = self.l2_regularization * (\n        tf.nn.l2_loss(self.U)\n        + tf.nn.l2_loss(self.V)\n        + tf.nn.l2_loss(self.X)\n        + tf.nn.l2_loss(self.Wq)\n        + tf.nn.l2_loss(self.Wk)\n    )\n\n    hinge_loss = (\n        tf.maximum(float(0), self.hinge_margin + pos - neg) + ridge_regularization\n    )  # (batch_size, n_negative_samples)\n    total_loss = tf.reduce_sum(hinge_loss)\n\n    # Normalize by the batch size and the number of negative samples\n    if tf.reduce_any(self.hinge_margin &gt; 0):\n        return (\n            total_loss / (batch_size * self.n_negative_samples * self.hinge_margin),\n            _,\n        )\n    return total_loss / (batch_size * self.n_negative_samples), _\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.compute_batch_short_distance","title":"<code>compute_batch_short_distance(item_batch, basket_embedding)</code>","text":"<p>Compute the short distance of the items in item_batch given the items in basket_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>Union[ndarray, Tensor]</code> <p>Batch of the purchased items ID (integers) for which to compute the distance from their basket. Shape must be (batch_size,) (positive and negative samples concatenated together)</p> required <code>basket_embedding</code> <code>Tensor</code> <p>Batch of context embeddings for each purchased item Shape must be (batch_size, latent_size)</p> required <p>Returns:</p> Name Type Description <code>short_term_distance</code> <code>Tensor</code> <p>Distance of all the items in item_batch from their ground truth embedding (X) Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def compute_batch_short_distance(\n    self,\n    item_batch: Union[np.ndarray, tf.Tensor],\n    basket_embedding: tf.Tensor,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the short distance of the items in item_batch given the items in basket_batch.\n\n    Parameters\n    ----------\n    item_batch: or tf.Tensor\n        Batch of the purchased items ID (integers) for which to compute the distance from their\n        basket.\n        Shape must be (batch_size,)\n        (positive and negative samples concatenated together)\n    basket_embedding: tf.Tensor\n        Batch of context embeddings for each purchased item\n        Shape must be (batch_size, latent_size)\n\n    Returns\n    -------\n    short_term_distance: tf.Tensor\n        Distance of all the items in item_batch from their ground truth embedding (X)\n        Shape must be (batch_size,)\n    \"\"\"\n    x_item_target = tf.gather(self.X, indices=item_batch)  # Shape: (batch_size, d)\n\n    return tf.reduce_sum(\n        tf.square(tf.expand_dims(basket_embedding, axis=1) - x_item_target), axis=-1\n    )\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.compute_batch_utility","title":"<code>compute_batch_utility(item_batch, basket_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch, is_training=False)</code>","text":"<p>Compute the utility of all the items in item_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <p>Batch of the purchased items ID (integers) for which to compute the utility from their basket. Shape must be (batch_size,)</p> required <code>basket_batch</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>store_batch</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <p>Batch of prices (floats) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <p>List of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <code>user_batch</code> <p>Batch of user IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>is_training</code> <code>bool</code> <p>Whether the model is in training mode or not, to activate dropout if needed.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Utility of all the items in item_batch Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def compute_batch_utility(\n    self,\n    item_batch,\n    basket_batch,\n    store_batch,\n    week_batch,\n    price_batch,\n    available_item_batch,\n    user_batch,\n    is_training: bool = False,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the utility of all the items in item_batch.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray\n        Batch of the purchased items ID (integers) for which to compute the utility from their\n        basket.\n        Shape must be (batch_size,)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (floats) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        List of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n    user_batch: np.ndarray\n        Batch of user IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    is_training : bool\n        Whether the model is in training mode or not, to activate dropout if needed.\n\n    Returns\n    -------\n    tf.Tensor\n        Utility of all the items in item_batch\n        Shape must be (batch_size,)\n    \"\"\"\n    _ = store_batch  # Unused for this model\n    _ = week_batch  # Unused for this model\n    _ = price_batch  # Unused for this model\n    _ = available_item_batch  # Unused for this model\n    return -self.compute_batch_distance(\n        item_batch=item_batch,\n        basket_batch=basket_batch,\n        user_batch=user_batch,\n        is_training=is_training,\n    )\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.embed_basket","title":"<code>embed_basket(basket_batch, is_training=False)</code>","text":"<p>Return the context embedding matrix.</p> <p>Returns:</p> Type Description <code>    basket_embedding : tf.Tensor</code> <pre><code>[batch_size, latent_size] tf.Tensor\nTensor containing the vector of contexts embeddings.\n</code></pre> <p>attention_weights : tf.Tensor     [batch_size, L, L] tf.Tensor     Tensor containing the attention matrix.</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def embed_basket(self, basket_batch: tf.Tensor, is_training: bool = False) -&gt; tf.Tensor:\n    \"\"\"Return the context embedding matrix.\n\n    Parameters\n    ----------\n        basket_batch : tf.Tensor\n            [batch_size, L]\n            Tensor containing the list of the context items.\n        is_training : bool\n            Whether the model is in training mode or not, to activate dropout if needed.\n\n    Returns\n    -------\n        basket_embedding : tf.Tensor\n            [batch_size, latent_size] tf.Tensor\n            Tensor containing the vector of contexts embeddings.\n        attention_weights : tf.Tensor\n            [batch_size, L, L] tf.Tensor\n            Tensor containing the attention matrix.\n    \"\"\"\n    padding_vector = tf.zeros(shape=[1, self.d])  # Shape (1, d)\n    padded_items = tf.concat([self.X, padding_vector], axis=0)\n    x_basket = tf.gather(padded_items, indices=basket_batch)  # Shape: (batch_size, L, d)\n\n    q_prime = tf.nn.relu(tf.matmul(x_basket, self.Wq))  # Shape: (batch_size, L, d)\n    k_prime = tf.nn.relu(tf.matmul(x_basket, self.Wk))\n\n    if is_training:\n        q_prime = tf.nn.dropout(q_prime, rate=self.dropout_rate)\n        k_prime = tf.nn.dropout(k_prime, rate=self.dropout_rate)\n\n    scores = tf.matmul(q_prime, k_prime, transpose_b=True)\n    scaled_scores = scores / tf.sqrt(float(self.d))\n    attention_weights = self.masked_attention(\n        basket_batch, scaled_scores\n    )  # Shape: (batch_size, L, L)\n\n    attention_output = tf.matmul(attention_weights, x_basket)  # Shape: (batch_size, L, d)\n\n    mask = tf.not_equal(basket_batch, self.n_items)\n    mask_float = tf.cast(mask, dtype=tf.float32)\n    mask_float = tf.expand_dims(mask_float, axis=-1)\n    masked_attention_output = attention_output * mask_float  # (batch_size, L, d)\n\n    # Number of items in each basket (excluding padding)\n    num_items_by_basket = tf.reduce_sum(mask_float, axis=1)  # (batch_size, 1)\n\n    basket_embedding = tf.math.divide_no_nan(\n        tf.reduce_sum(masked_attention_output, axis=1, keepdims=True),\n        num_items_by_basket[:, tf.newaxis, :],\n    )\n    basket_embedding = tf.squeeze(basket_embedding, axis=1)  # Shape: (batch_size,)\n    # ----- Handle empty baskets by assigning a padding embedding -----\n    padding_embedding = self.X[-1]  # shape: (d,)\n    empty_basket_mask = tf.squeeze(tf.equal(num_items_by_basket, 0), axis=1)  # (batch_size,)\n    basket_embedding = tf.where(\n        tf.expand_dims(empty_basket_mask, axis=1),\n        tf.broadcast_to(padding_embedding, tf.shape(basket_embedding)),\n        basket_embedding,\n    )\n    # -----------------------------------------------------------------\n    return basket_embedding, attention_weights\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.get_negative_samples","title":"<code>get_negative_samples(available_items, purchased_items, next_item, n_samples)</code>","text":"<p>Sample randomly a set of items.</p> <p>(set of items not already purchased and not necessarily from the basket)</p> <p>Parameters:</p> Name Type Description Default <code>available_items</code> <code>ndarray</code> <p>Matrix indicating the availability (1) or not (0) of the products Shape must be (n_items,)</p> required <code>purchased_items</code> <code>ndarray</code> <p>List of items already purchased (already in the basket)</p> required <code>next_item</code> <code>int</code> <p>Next item (to be added in the basket)</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to draw</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>Random sample of items, each of them distinct from the next item and from the items already in the basket</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def get_negative_samples(\n    self,\n    available_items: np.ndarray,\n    purchased_items: np.ndarray,\n    next_item: int,\n    n_samples: int,\n) -&gt; list[int]:\n    \"\"\"Sample randomly a set of items.\n\n    (set of items not already purchased and *not necessarily* from the basket)\n\n    Parameters\n    ----------\n    available_items: np.ndarray\n        Matrix indicating the availability (1) or not (0) of the products\n        Shape must be (n_items,)\n    purchased_items: np.ndarray\n        List of items already purchased (already in the basket)\n    next_item: int\n        Next item (to be added in the basket)\n    n_samples: int\n        Number of samples to draw\n\n    Returns\n    -------\n    list[int]\n        Random sample of items, each of them distinct from\n        the next item and from the items already in the basket\n    \"\"\"\n    # Convert inputs to tensors\n    available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n    purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n    next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n    # Get the list of available items based on the availability matrix\n    item_ids = tf.range(self.n_items)\n    available_mask = tf.equal(available_items, 1)\n    assortment = tf.boolean_mask(item_ids, available_mask)\n\n    not_to_be_chosen = tf.concat([purchased_items, tf.expand_dims(next_item, axis=0)], axis=0)\n\n    # Sample negative items from the assortment excluding not_to_be_chosen\n    negative_samples = tf.boolean_mask(\n        tensor=assortment,\n        # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n        mask=~tf.reduce_any(\n            tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n        ),\n    )\n\n    error_message = (\n        \"The number of negative samples to draw must be less than \"\n        \"the number of available items not already purchased and \"\n        \"distinct from the next item.\"\n    )\n    # Raise an error if n_samples &gt; tf.size(negative_samples)\n    tf.debugging.assert_greater_equal(\n        tf.size(negative_samples), n_samples, message=error_message\n    )\n\n    # Randomize the sampling\n    negative_samples = tf.random.shuffle(negative_samples)\n\n    # Keep only n_samples\n    return negative_samples[:n_samples]\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.instantiate","title":"<code>instantiate(n_items, n_users)</code>","text":"<p>Initialize the model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of unique items in the dataset.</p> required <code>n_users</code> <code>int</code> <p>Number of unique users in the dataset.</p> required Variables <p>X : tf.Variable     Item embedding matrix for short-term preferences, size (n_items, d). V : tf.Variable     Item embedding matrix for long-term preferences, size (n_items, d_long). U : tf.Variable     User embedding matrix for long-term preferences, size (n_users, d_long). Wq : tf.Variable     Weight matrix for query transformation in attention mechanism, size (d, d). Wk : tf.Variable     Weight matrix for key transformation in attention mechanism, size (d, d).</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def instantiate(\n    self,\n    n_items: int,\n    n_users: int,\n) -&gt; None:\n    \"\"\"Initialize the model parameters.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of unique items in the dataset.\n    n_users : int\n        Number of unique users in the dataset.\n\n    Variables\n    ----------\n    X : tf.Variable\n        Item embedding matrix for short-term preferences, size (n_items, d).\n    V : tf.Variable\n        Item embedding matrix for long-term preferences, size (n_items, d_long).\n    U : tf.Variable\n        User embedding matrix for long-term preferences, size (n_users, d_long).\n    Wq : tf.Variable\n        Weight matrix for query transformation in attention mechanism, size (d, d).\n    Wk : tf.Variable\n        Weight matrix for key transformation in attention mechanism, size (d, d).\n    \"\"\"\n    self.n_items = n_items\n    self.n_users = n_users\n    ##############\n\n    self.X = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(shape=(n_items + 1, self.d)),\n        trainable=True,\n        name=\"X\",\n    )\n\n    self.V = tf.Variable(\n        tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=42)(\n            shape=(n_items, self.d_long)\n        ),\n        trainable=True,\n        name=\"V\",\n    )\n\n    self.U = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(\n            shape=(self.n_users, self.d_long)\n        ),\n        trainable=True,\n        name=\"U\",\n    )\n\n    self.Wq = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(shape=(self.d, self.d)),\n        trainable=True,\n        name=\"Wq\",\n    )\n\n    self.Wk = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=0.01, seed=42)(shape=(self.d, self.d)),\n        trainable=True,\n        name=\"Wk\",\n    )\n\n    self.instantiated = True\n</code></pre>"},{"location":"references/basket_models/references_self_attention_model/#choice_learn.basket_models.self_attention_model.SelfAttentionModel.masked_attention","title":"<code>masked_attention(basket_batch, scaled_scores)</code>","text":"<p>Compute the masked attention weights.</p> <p>Applying a mask to ignore padding items. Also applied a mask on the diagonal to avoid attending to the same item, if activated</p> Source code in <code>choice_learn/basket_models/self_attention_model.py</code> <pre><code>def masked_attention(self, basket_batch, scaled_scores):\n    \"\"\"Compute the masked attention weights.\n\n    Applying a mask to ignore padding items. Also applied a mask on\n    the diagonal to avoid attending to the same item, if activated\n    \"\"\"\n    batch_size = tf.shape(basket_batch)[0]\n    mask = tf.not_equal(\n        basket_batch, self.n_items\n    )  # shape: (batch_size, L), True si pas padding\n\n    if tf.shape(basket_batch)[1] == 1:\n        attention_weights = tf.ones_like(scaled_scores)  # Shape: (batch_size, L, 1)\n\n    else:\n        # Masque de la diagonale, d\u00e9sactiv\u00e9 pour l'instant\n        diag_mask = tf.eye(tf.shape(basket_batch)[1], batch_shape=[batch_size], dtype=tf.bool)\n        scaled_scores = tf.where(\n            diag_mask,\n            tf.constant(-np.inf, dtype=scaled_scores.dtype),\n            scaled_scores,\n        )\n\n        # Masque des padding items\n        mask_col = tf.expand_dims(mask, axis=1)  # (batch_size, 1, L)\n        scaled_scores = tf.where(\n            mask_col, scaled_scores, tf.constant(-np.inf, dtype=scaled_scores.dtype)\n        )\n\n        all_inf_row = tf.reduce_all(tf.math.is_inf(scaled_scores), axis=-1)  # (batch_size, L)\n        # We set to zero the first value of the rows where all values are -inf to avoid NaNs in\n        # softmax\n        indices = tf.where(all_inf_row)\n        indices_full = tf.concat([indices, tf.zeros_like(indices[:, :1])], axis=1)\n        updates = tf.zeros([tf.shape(indices_full)[0]], dtype=scaled_scores.dtype)\n        scaled_scores = tf.tensor_scatter_nd_update(scaled_scores, indices_full, updates)\n\n        attention_weights = tf.nn.softmax(scaled_scores, axis=-1)  # Shape: (batch_size, L, L)\n\n    return attention_weights\n</code></pre>"},{"location":"references/basket_models/references_shopper/","title":"SHOPPER Model","text":"<p>Implementation of the Shopper model.</p>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper","title":"<code>Shopper</code>","text":"<p>             Bases: <code>BaseBasketModel</code></p> <p>Class for the Shopper model.</p> <p>SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements, Ruiz, F. J. R.; Athey, S.; Blei, D. M. (2019)</p> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>class Shopper(BaseBasketModel):\n    \"\"\"Class for the Shopper model.\n\n    SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements,\n    Ruiz, F. J. R.; Athey, S.; Blei, D. M. (2019)\n    \"\"\"\n\n    def __init__(\n        self,\n        item_intercept: bool = True,\n        price_effects: bool = False,\n        seasonal_effects: bool = False,\n        think_ahead: bool = False,\n        latent_sizes: dict[str] = {\"preferences\": 4, \"price\": 4, \"season\": 4},\n        n_negative_samples: int = 2,\n        optimizer: str = \"adam\",\n        callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n        lr: float = 1e-3,\n        epochs: int = 10,\n        batch_size: int = 32,\n        grad_clip_value: Union[float, None] = None,\n        weight_decay: Union[float, None] = None,\n        momentum: float = 0.0,\n        epsilon_price: float = 1e-5,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the Shopper model.\n\n        Parameters\n        ----------\n        item_intercept: bool, optional\n            Whether to include item intercept in the model, by default True\n            Corresponds to the item intercept\n        price_effects: bool, optional\n            Whether to include price effects in the model, by default True\n        seasonal_effects: bool, optional\n            Whether to include seasonal effects in the model, by default True\n        think_ahead: bool, optional\n            Whether to include \"thinking ahead\" in the model, by default False\n        latent_sizes: dict[str]\n            Lengths of the vector representation of the latent parameters\n            latent_sizes[\"preferences\"]: length of one vector of theta, alpha, rho\n            latent_sizes[\"price\"]: length of one vector of gamma, beta\n            latent_sizes[\"season\"]: length of one vector of delta, mu\n            by default {\"preferences\": 4, \"price\": 4, \"season\": 4}\n        n_negative_samples: int, optional\n            Number of negative samples to draw for each positive sample for the training,\n            by default 2\n            Must be &gt; 0\n        optimizer: str, optional\n            Optimizer to use for training, by default \"adam\"\n        callbacks: tf.keras.callbacks.Callbacklist, optional\n            List of callbacks to add to model.fit, by default None and only add History\n        lr: float, optional\n            Learning rate, by default 1e-3\n        epochs: int, optional\n            Number of epochs, by default 100\n        batch_size: int, optional\n            Batch size, by default 32\n        grad_clip_value: float, optional\n            Value to clip the gradient, by default None\n        weight_decay: float, optional\n            Weight decay, by default None\n        momentum: float, optional\n            Momentum for the optimizer, by default 0. For SGD only\n        epsilon_price: float, optional\n            Epsilon value to add to prices to avoid NaN values (log(0)), by default 1e-5\n        \"\"\"\n        self.item_intercept = item_intercept\n        self.price_effects = price_effects\n        self.seasonal_effects = seasonal_effects\n        self.think_ahead = think_ahead\n\n        if \"preferences\" not in latent_sizes.keys():\n            logging.warning(\n                \"No latent size value has been specified for preferences, \"\n                \"switching to default value 4.\"\n            )\n            latent_sizes[\"preferences\"] = 4\n        if \"price\" not in latent_sizes.keys() and self.price_effects:\n            logging.warning(\n                \"No latent size value has been specified for price_effects, \"\n                \"switching to default value 4.\"\n            )\n            latent_sizes[\"price\"] = 4\n        if \"season\" not in latent_sizes.keys() and self.seasonal_effects:\n            logging.warning(\n                \"No latent size value has been specified for seasonal_effects, \"\n                \"switching to default value 4.\"\n            )\n            latent_sizes[\"season\"] = 4\n\n        for val in latent_sizes.keys():\n            if val not in [\"preferences\", \"price\", \"season\"]:\n                raise ValueError(f\"Unknown value for latent_sizes dict: {val}.\")\n\n        if n_negative_samples &lt;= 0:\n            raise ValueError(\"n_negative_samples must be &gt; 0.\")\n\n        self.latent_sizes = latent_sizes\n        self.n_negative_samples = n_negative_samples\n\n        self.epsilon_price = epsilon_price\n\n        super().__init__(\n            optimizer=optimizer,\n            callbacks=callbacks,\n            lr=lr,\n            epochs=epochs,\n            batch_size=batch_size,\n            grad_clip_value=grad_clip_value,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            **kwargs,\n        )\n\n        if len(tf.config.get_visible_devices(\"GPU\")):\n            # At least one available GPU\n            self.on_gpu = True\n        else:\n            # No available GPU\n            self.on_gpu = False\n        # /!\\ If a model trained on GPU is loaded on CPU, self.on_gpu must be set\n        # to False manually after loading the model, and vice versa\n\n        self.instantiated = False\n\n    def instantiate(\n        self,\n        n_items: int,\n        n_stores: int = 0,\n    ) -&gt; None:\n        \"\"\"Instantiate the Shopper model.\n\n        Parameters\n        ----------\n        n_items: int\n            Number of items to consider, i.e. the number of items in the dataset\n            (includes the checkout item)\n        n_stores: int\n            Number of stores in the population\n        \"\"\"\n        self.n_items = n_items\n        if n_stores == 0 and self.price_effects:\n            # To take into account the price effects, the number of stores must be &gt; 0\n            # to have a gamma embedding\n            # (By default, the store id is 0)\n            n_stores = 1\n        self.n_stores = n_stores\n\n        self.rho = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                shape=(n_items, self.latent_sizes[\"preferences\"])\n            ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n            trainable=True,\n            name=\"rho\",\n        )\n        self.alpha = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                shape=(n_items, self.latent_sizes[\"preferences\"])\n            ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n            trainable=True,\n            name=\"alpha\",\n        )\n        self.theta = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                shape=(n_stores, self.latent_sizes[\"preferences\"])\n            ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n            trainable=True,\n            name=\"theta\",\n        )\n\n        if self.item_intercept:\n            # Add item intercept\n            self.lambda_ = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                    # No lambda for the checkout item (set to 0 later)\n                    shape=(n_items - 1,)  # Dimension for 1 item: 1\n                ),\n                trainable=True,\n                name=\"lambda_\",\n            )\n\n        if self.price_effects:\n            # Add price sensitivity\n            self.beta = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                    shape=(n_items, self.latent_sizes[\"price\"])\n                ),  # Dimension for 1 item: latent_sizes[\"price\"]\n                trainable=True,\n                name=\"beta\",\n            )\n            self.gamma = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                    shape=(n_stores, self.latent_sizes[\"price\"])\n                ),  # Dimension for 1 item: latent_sizes[\"price\"]\n                trainable=True,\n                name=\"gamma\",\n            )\n\n        if self.seasonal_effects:\n            # Add seasonal effects\n            self.mu = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(n_items, self.latent_sizes[\"season\"])\n                ),  # Dimension for 1 item: latent_sizes[\"season\"]\n                trainable=True,\n                name=\"mu\",\n            )\n            self.delta = tf.Variable(\n                tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                    shape=(52, self.latent_sizes[\"season\"])\n                ),  # Dimension for 1 item: latent_sizes[\"season\"]\n                trainable=True,\n                name=\"delta\",\n            )\n\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self) -&gt; list[tf.Variable]:\n        \"\"\"Latent parameters of the model.\n\n        Returns\n        -------\n        list[tf.Variable]\n            Latent parameters of the model\n        \"\"\"\n        weights = [self.rho, self.alpha, self.theta]\n\n        if self.item_intercept:\n            weights.append(self.lambda_)\n\n        if self.price_effects:\n            weights.extend([self.beta, self.gamma])\n\n        if self.seasonal_effects:\n            weights.extend([self.mu, self.delta])\n\n        return weights\n\n    @property\n    def train_iter_method(self):\n        \"\"\"Method used to generate sub-baskets from a purchased one.\n\n        Available methods are:\n        - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:\n                        (1|0); (2|1); (3|1,2); (4|1,2,3); etc...\n        - 'aleacarta': creates all the sub-baskets with N-1 items:\n                        (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)\n\n        Returns\n        -------\n        str\n            Data generation method.\n        \"\"\"\n        return \"shopper\"\n\n    def thinking_ahead(\n        self,\n        item_batch: Union[np.ndarray, tf.Tensor],\n        ragged_basket_batch: tf.RaggedTensor,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        theta_store: tf.Tensor,\n        gamma_store: tf.Tensor,\n        delta_week: tf.Tensor,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the utility of all the items in item_batch.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray or tf.Tensor\n            Batch of the purchased items ID (integers) for which to compute the utility\n            Shape must be (batch_size,)\n            (positive and negative samples concatenated together)\n        ragged_basket_batch: tf.RaggedTensor\n            Batch of baskets (ID of items already in the baskets) (arrays) without padding\n            for each purchased item\n            Shape must be (batch_size, None)\n        price_batch: np.ndarray\n            Batch of prices (integers) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            Batch of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n        theta_store: tf.Tensor\n            Slices from theta embedding gathered according to the indices that correspond\n            to the store of each purchased item in the batch\n            Shape must be (batch_size, latent_sizes[\"preferences\"])\n        gamma_store: tf.Tensor\n            Slices from gamma embedding gathered according to the indices that correspond\n            to the store of each purchased item in the batch\n            Shape must be (batch_size, latent_sizes[\"price\"])\n        delta_week: tf.Tensor\n            Slices from delta embedding gathered according to the indices that correspond\n            to the week of each purchased item in the batch\n            Shape must be (batch_size, latent_sizes[\"season\"])\n\n        Returns\n        -------\n        total_next_step_utilities: tf.Tensor\n            Nex step utility of all the items in item_batch\n            Shape must be (batch_size,)\n        \"\"\"\n        total_next_step_utilities = tf.zeros_like(item_batch, dtype=tf.float32)\n        # Compute the next step item utility for each element of the batch, one by one\n        # TODO: avoid a for loop on ragged_basket_batch at a later stage\n        for idx in tf.range(ragged_basket_batch.shape[0]):\n            basket = tf.gather(ragged_basket_batch, idx)\n            if len(basket) != 0 and basket[-1] == 0:\n                # No thinking ahead when the basket ends already with the checkout item 0\n                total_next_step_utilities = tf.tensor_scatter_nd_update(\n                    tensor=total_next_step_utilities, indices=[[idx]], updates=[0]\n                )\n\n            else:\n                # Basket with the hypothetical current item\n                next_basket = tf.concat([basket, [item_batch[idx]]], axis=0)\n                # Get the list of available items based on the availability matrix\n                item_ids = tf.range(self.n_items)\n                available_mask = tf.equal(available_item_batch[idx], 1)\n                assortment = tf.boolean_mask(item_ids, available_mask)\n                hypothetical_next_purchases = tf.boolean_mask(\n                    assortment,\n                    ~tf.reduce_any(\n                        tf.equal(tf.expand_dims(assortment, axis=1), next_basket), axis=1\n                    ),\n                )\n                # Check if there are still items to purchase during the next step\n                if len(hypothetical_next_purchases) == 0:\n                    # No more items to purchase: next step impossible\n                    total_next_step_utilities = tf.tensor_scatter_nd_update(\n                        tensor=total_next_step_utilities, indices=[[idx]], updates=[0]\n                    )\n                else:\n                    # Compute the dot product along the last dimension between the embeddings\n                    # of the given store's theta and alpha of all the items\n                    hypothetical_store_preferences = tf.reduce_sum(\n                        theta_store[idx] * self.alpha, axis=1\n                    )\n\n                    if self.item_intercept:\n                        # Manually enforce the lambda of the checkout item to be 0\n                        # (equivalent to translating the lambda values)\n                        hypothetical_item_intercept = tf.concat([[0.0], self.lambda_], axis=0)\n                    else:\n                        hypothetical_item_intercept = tf.zeros_like(hypothetical_store_preferences)\n\n                    if self.price_effects:\n                        hypothetical_price_effects = (\n                            -1\n                            # Compute the dot product along the last dimension between\n                            # the embeddings of the given store's gamma and beta\n                            # of all the items\n                            * tf.reduce_sum(gamma_store[idx] * self.beta, axis=1)\n                            * tf.math.log(price_batch[idx] + self.epsilon_price)\n                        )\n                    else:\n                        hypothetical_price_effects = tf.zeros_like(hypothetical_store_preferences)\n\n                    if self.seasonal_effects:\n                        # Compute the dot product along the last dimension between the embeddings\n                        # of delta of the given week and mu of all the items\n                        hypothetical_seasonal_effects = tf.reduce_sum(\n                            delta_week[idx] * self.mu, axis=1\n                        )\n                    else:\n                        hypothetical_seasonal_effects = tf.zeros_like(\n                            hypothetical_store_preferences\n                        )\n\n                    # The effects of item intercept, store preferences, price sensitivity\n                    # and seasonal effects are combined in the per-item per-trip latent variable\n                    hypothetical_psi = tf.reduce_sum(\n                        [\n                            hypothetical_item_intercept,  # 0 if self.item_intercept is False\n                            hypothetical_store_preferences,\n                            hypothetical_price_effects,  # 0 if self.price_effects is False\n                            hypothetical_seasonal_effects,  # 0 if self.seasonal_effects is False\n                        ],\n                        axis=0,\n                    )  # Shape: (n_items,)\n\n                    # Shape: (len(hypothetical_next_purchases),)\n                    next_psi = tf.gather(hypothetical_psi, indices=hypothetical_next_purchases)\n\n                    # Consider hypothetical \"next\" item one by one\n                    next_step_basket_interaction_utilities = tf.zeros(\n                        (len(hypothetical_next_purchases),), dtype=tf.float32\n                    )\n                    for inner_idx in tf.range(len(hypothetical_next_purchases)):\n                        next_item_id = tf.gather(hypothetical_next_purchases, inner_idx)\n                        rho_next_item = tf.gather(\n                            self.rho, indices=next_item_id\n                        )  # Shape: (latent_size,)\n                        # Gather the embeddings using a tensor of indices\n                        # (before ensure that indices are integers)\n                        next_alpha_by_basket = tf.gather(\n                            self.alpha, indices=tf.cast(next_basket, dtype=tf.int32)\n                        )  # Shape: (len(next_basket), latent_size)\n                        # Divide the sum of alpha embeddings by the number of items\n                        # in the basket of the next step (always &gt; 0)\n                        next_alpha_average = tf.reduce_sum(next_alpha_by_basket, axis=0) / tf.cast(\n                            len(next_basket), dtype=tf.float32\n                        )  # Shape: (latent_size,)\n                        next_step_basket_interaction_utilities = tf.tensor_scatter_nd_update(\n                            tensor=next_step_basket_interaction_utilities,\n                            indices=[[inner_idx]],\n                            # Compute the dot product along the last dimension, shape: (1,)\n                            updates=[tf.reduce_sum(rho_next_item * next_alpha_average)],\n                        )\n\n                    # Optimal next step: take the maximum utility among all possible next purchases\n                    next_step_utility = tf.reduce_max(\n                        next_psi + next_step_basket_interaction_utilities, axis=0\n                    )  # Shape: (1,)\n                    total_next_step_utilities = tf.tensor_scatter_nd_update(\n                        tensor=total_next_step_utilities,\n                        indices=[[idx]],\n                        updates=[next_step_utility],\n                    )\n\n        return total_next_step_utilities  # Shape: (batch_size,)\n\n    def compute_batch_utility(\n        self,\n        item_batch: Union[np.ndarray, tf.Tensor],\n        basket_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        user_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n    ) -&gt; tf.Tensor:\n        \"\"\"Compute the utility of all the items in item_batch.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray or tf.Tensor\n            Batch of the purchased items ID (integers) for which to compute the utility\n            Shape must be (batch_size,)\n            (positive and negative samples concatenated together)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (integers) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            Batch of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n\n        Returns\n        -------\n        item_utilities: tf.Tensor\n            Utility of all the items in item_batch\n            Shape must be (batch_size,)\n        \"\"\"\n        _ = user_batch\n        item_batch = tf.cast(item_batch, dtype=tf.int32)\n        basket_batch = tf.cast(basket_batch, dtype=tf.int32)\n        store_batch = tf.cast(store_batch, dtype=tf.int32)\n        week_batch = tf.cast(week_batch, dtype=tf.int32)\n        price_batch = tf.cast(price_batch, dtype=tf.float32)\n        available_item_batch = tf.cast(available_item_batch, dtype=tf.int32)\n\n        if len(tf.shape(item_batch)) == 1:\n            if len(tf.shape(price_batch)) != 1:\n                raise ValueError(f\"\"\"Arguments price_batch and item_batch should have same shape\n                and are:{item_batch.shape} and {price_batch.shape}\"\"\")\n            item_batch = tf.expand_dims(item_batch, axis=1)\n            price_batch = tf.expand_dims(price_batch, axis=1)\n            squeeze = True\n        else:\n            squeeze = False\n\n        theta_store = tf.gather(self.theta, indices=store_batch)\n        alpha_item = tf.gather(self.alpha, indices=item_batch)\n        # Compute the dot product along the last dimension\n        store_preferences = tf.einsum(\"kj,klj-&gt;kl\", theta_store, alpha_item)\n\n        if self.item_intercept:\n            # Manually enforce the lambda of the checkout item to be 0\n            # (equivalent to translating the lambda values)\n            item_intercept = tf.gather(tf.concat([[0.0], self.lambda_], axis=0), indices=item_batch)\n        else:\n            item_intercept = tf.zeros_like(store_preferences)\n\n        if self.price_effects:\n            gamma_store = tf.gather(self.gamma, indices=store_batch)\n            beta_item = tf.gather(self.beta, indices=item_batch)\n            # Add epsilon to avoid NaN values (log(0))\n            price_effects = (\n                -1\n                # Compute the dot product along the last dimension\n                * tf.einsum(\"kj,klj-&gt;kl\", gamma_store, beta_item)\n                * tf.math.log(price_batch + self.epsilon_price)\n            )\n        else:\n            gamma_store = tf.zeros_like(store_batch)\n            price_effects = tf.zeros_like(store_preferences)\n\n        if self.seasonal_effects:\n            delta_week = tf.gather(self.delta, indices=week_batch)\n            mu_item = tf.gather(self.mu, indices=item_batch)\n            # Compute the dot product along the last dimension\n            seasonal_effects = tf.einsum(\"kj,klj-&gt;kl\", delta_week, mu_item)\n        else:\n            delta_week = tf.zeros_like(week_batch)\n            seasonal_effects = tf.zeros_like(store_preferences)\n\n        # The effects of item intercept, store preferences, price sensitivity\n        # and seasonal effects are combined in the per-item per-trip latent variable\n        psi = tf.reduce_sum(\n            [\n                item_intercept,\n                store_preferences,\n                price_effects,\n                seasonal_effects,\n            ],\n            axis=0,\n        )  # Shape: (batch_size,)\n\n        # Apply boolean mask to mask out the padding value -1\n        masked_baskets = tf.where(\n            condition=basket_batch &gt; -1,  # If False: padding value -1\n            x=1,  # Output where condition is True\n            y=0,  # Output where condition is False\n        )\n        # Number of items in each basket\n        count_items_in_basket = tf.reduce_sum(masked_baskets, axis=1)\n\n        # Create a RaggedTensor from the indices with padding removed\n        item_indices_ragged = tf.cast(\n            tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n            dtype=tf.int32,\n        )\n\n        if tf.size(item_indices_ragged) == 0:\n            # Empty baskets: no alpha embeddings to gather\n            # (It must be a ragged tensor here because TF's GraphMode requires the same\n            # nested structure to be returned from all branches of a conditional)\n            alpha_by_basket = tf.RaggedTensor.from_tensor(\n                tf.zeros((len(item_batch), 0, self.alpha.shape[1]))\n            )\n        else:\n            # Gather the embeddings using a ragged tensor of indices\n            alpha_by_basket = tf.ragged.map_flat_values(tf.gather, self.alpha, item_indices_ragged)\n\n        # Compute the sum of the alpha embeddings for each basket\n        alpha_sum = tf.reduce_sum(alpha_by_basket, axis=1)\n\n        rho_item = tf.gather(self.rho, indices=item_batch)\n\n        # Divide each sum of alpha embeddings by the number of items in the corresponding basket\n        # Avoid NaN values (division by 0)\n        count_items_in_basket_expanded = tf.expand_dims(\n            tf.cast(count_items_in_basket, dtype=tf.float32), -1\n        )\n\n        # Apply boolean mask for case distinction\n        alpha_average = tf.where(\n            condition=count_items_in_basket_expanded != 0,  # If True: count_items_in_basket &gt; 0\n            x=alpha_sum / count_items_in_basket_expanded,  # Output if condition is True\n            y=tf.zeros_like(alpha_sum),  # Output if condition is False\n        )\n\n        # Compute the dot product along the last dimension\n        basket_interaction_utility = tf.einsum(\"kj,klj-&gt;kl\", alpha_average, rho_item)\n\n        item_utilities = psi + basket_interaction_utility\n\n        # No thinking ahead\n        if not self.think_ahead:\n            if squeeze:\n                return tf.gather(item_utilities, 0, axis=1)\n            return item_utilities\n\n        # Thinking ahead\n        next_step_utilities = tf.stack(\n            [\n                self.thinking_ahead(\n                    item_batch=tf.gather(item_batch, i, axis=1),\n                    ragged_basket_batch=item_indices_ragged,\n                    price_batch=price_batch,\n                    available_item_batch=available_item_batch,\n                    theta_store=theta_store,\n                    gamma_store=gamma_store,  # 0 if self.price_effects is False\n                    delta_week=delta_week,  # 0 if self.seasonal_effects is False\n                )\n                for i in range(tf.shape(item_batch)[1])\n            ],\n            axis=-1,\n        )\n\n        if squeeze:\n            return tf.gather(item_utilities + next_step_utilities, 0, axis=1)\n        return item_utilities + next_step_utilities\n\n    def get_negative_samples(\n        self,\n        available_items: np.ndarray,\n        purchased_items: np.ndarray,\n        future_purchases: np.ndarray,\n        next_item: int,\n        n_samples: int,\n    ) -&gt; list[int]:\n        \"\"\"Sample randomly a set of items.\n\n        (set of items not already purchased and *not necessarily* from the basket)\n\n        Parameters\n        ----------\n        available_items: np.ndarray\n            Matrix indicating the availability (1) or not (0) of the products\n            Shape must be (n_items,)\n        purchased_items: np.ndarray\n            List of items already purchased (already in the basket)\n        future_purchases: np.ndarray\n            List of items to be purchased in the future (not yet in the basket)\n        next_item: int\n            Next item (to be added in the basket)\n        n_samples: int\n            Number of samples to draw\n\n        Returns\n        -------\n        list[int]\n            Random sample of items, each of them distinct from\n            the next item and from the items already in the basket\n        \"\"\"\n        # Convert inputs to tensors\n        available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n        purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n        future_purchases = tf.cast(tf.convert_to_tensor(future_purchases), dtype=tf.int32)\n        next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n        # Get the list of available items based on the availability matrix\n        item_ids = tf.range(self.n_items)\n        available_mask = tf.equal(available_items, 1)\n        assortment = tf.boolean_mask(item_ids, available_mask)\n\n        not_to_be_chosen = tf.concat(\n            [purchased_items, future_purchases, tf.expand_dims(next_item, axis=0)], axis=0\n        )\n\n        # Ensure that the checkout item 0 can be picked as a negative sample\n        # if it is not the next item\n        # (otherwise 0 is always in not_to_be_chosen because it's in future_purchases)\n        if not tf.equal(next_item, 0):\n            not_to_be_chosen = tf.boolean_mask(not_to_be_chosen, not_to_be_chosen != 0)\n\n        # Sample negative items from the assortment excluding not_to_be_chosen\n        negative_samples = tf.boolean_mask(\n            tensor=assortment,\n            # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n            mask=~tf.reduce_any(\n                tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n            ),\n        )\n\n        error_message = (\n            \"The number of negative samples to draw must be less than \"\n            \"the number of available items not already purchased and \"\n            \"distinct from the next item.\"\n        )\n        # Raise an error if n_samples &gt; tf.size(negative_samples)\n        tf.debugging.assert_greater_equal(\n            tf.size(negative_samples), n_samples, message=error_message\n        )\n\n        # Randomize the sampling\n        negative_samples = tf.random.shuffle(negative_samples)\n\n        # Keep only n_samples\n        return negative_samples[:n_samples]\n\n    @tf.function  # Graph mode\n    def compute_batch_loss(\n        self,\n        item_batch: np.ndarray,\n        basket_batch: np.ndarray,\n        future_batch: np.ndarray,\n        store_batch: np.ndarray,\n        week_batch: np.ndarray,\n        price_batch: np.ndarray,\n        available_item_batch: np.ndarray,\n        user_batch: np.ndarray,\n    ) -&gt; tuple[tf.Variable]:\n        \"\"\"Compute log-likelihood and loss for one batch of items.\n\n        Parameters\n        ----------\n        item_batch: np.ndarray\n            Batch of purchased items ID (integers)\n            Shape must be (batch_size,)\n        basket_batch: np.ndarray\n            Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        future_batch: np.ndarray\n            Batch of items to be purchased in the future (ID of items not yet in the\n            basket) (arrays) for each purchased item\n            Shape must be (batch_size, max_basket_size)\n        store_batch: np.ndarray\n            Batch of store IDs (integers) for each purchased item\n            Shape must be (batch_size,)\n        week_batch: np.ndarray\n            Batch of week numbers (integers) for each purchased item\n            Shape must be (batch_size,)\n        price_batch: np.ndarray\n            Batch of prices (integers) for each purchased item\n            Shape must be (batch_size,)\n        available_item_batch: np.ndarray\n            List of availability matrices (indicating the availability (1) or not (0)\n            of the products) (arrays) for each purchased item\n            Shape must be (batch_size, n_items)\n\n        Returns\n        -------\n        batch_loss: tf.Variable\n            Value of the loss for the batch (normalized negative log-likelihood),\n            Shape must be (1,)\n        loglikelihood: tf.Variable\n            Computed log-likelihood of the batch of items\n            Approximated by difference of utilities between positive and negative samples\n            Shape must be (1,)\n        \"\"\"\n        _ = user_batch\n        batch_size = len(item_batch)\n        item_batch = tf.cast(item_batch, dtype=tf.int32)\n\n        # Negative sampling\n        negative_samples = tf.reshape(\n            tf.transpose(\n                tf.reshape(\n                    tf.concat(\n                        [\n                            self.get_negative_samples(\n                                available_items=available_item_batch[idx],\n                                purchased_items=basket_batch[idx],\n                                future_purchases=future_batch[idx],\n                                next_item=item_batch[idx],\n                                n_samples=self.n_negative_samples,\n                            )\n                            for idx in range(batch_size)\n                        ],\n                        axis=0,\n                    ),\n                    # Reshape to have at the beginning of the array all the first negative samples\n                    # of all positive samples, then all the second negative samples, etc.\n                    # (same logic as for the calls to np.tile)\n                    [batch_size, self.n_negative_samples],\n                ),\n            ),\n            # Flatten 2D --&gt; 1D\n            shape=[-1],\n        )\n\n        augmented_item_batch = tf.cast(\n            tf.concat([item_batch, negative_samples], axis=0), dtype=tf.int32\n        )\n        prices_tiled = tf.tile(price_batch, [self.n_negative_samples + 1, 1])\n        # Each time, pick only the price of the item in augmented_item_batch from the\n        # corresponding price array\n        augmented_price_batch = tf.gather(\n            params=prices_tiled,\n            indices=augmented_item_batch,\n            # batch_dims=1 is equivalent to having an outer loop over\n            # the first axis of params and indices\n            batch_dims=1,\n        )\n\n        # Compute the utility of all the available items\n        all_utilities = self.compute_batch_utility(\n            item_batch=augmented_item_batch,\n            basket_batch=tf.tile(basket_batch, [self.n_negative_samples + 1, 1]),\n            store_batch=tf.tile(store_batch, [self.n_negative_samples + 1]),\n            week_batch=tf.tile(week_batch, [self.n_negative_samples + 1]),\n            price_batch=augmented_price_batch,\n            available_item_batch=tf.tile(available_item_batch, [self.n_negative_samples + 1, 1]),\n            user_batch=tf.tile(user_batch, [self.n_negative_samples + 1]),\n        )\n\n        positive_samples_utilities = tf.gather(all_utilities, tf.range(batch_size))\n        negative_samples_utilities = tf.gather(\n            all_utilities, tf.range(batch_size, tf.shape(all_utilities)[0])\n        )\n\n        # Log-likelihood of a batch = sum of log-likelihoods of its samples\n        # Add a small epsilon to gain numerical stability (avoid log(0))\n        epsilon = 0.0  # No epsilon added for now\n        loglikelihood = tf.reduce_sum(\n            tf.math.log(\n                tf.sigmoid(\n                    tf.tile(\n                        positive_samples_utilities,\n                        [self.n_negative_samples],\n                    )\n                    - negative_samples_utilities\n                )\n                + epsilon\n            ),\n        )  # Shape of loglikelihood: (1,)\n\n        # Maximize the predicted log-likelihood (ie minimize the negative log-likelihood)\n        # normalized by the batch size and the number of negative samples\n        batch_loss = -1 * loglikelihood / (batch_size * self.n_negative_samples)\n\n        return batch_loss, loglikelihood\n</code></pre>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.train_iter_method","title":"<code>train_iter_method</code>  <code>property</code>","text":"<p>Method used to generate sub-baskets from a purchased one.</p> <p>Available methods are: - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:                 (1|0); (2|1); (3|1,2); (4|1,2,3); etc... - 'aleacarta': creates all the sub-baskets with N-1 items:                 (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)</p> <p>Returns:</p> Type Description <code>str</code> <p>Data generation method.</p>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.trainable_weights","title":"<code>trainable_weights: list[tf.Variable]</code>  <code>property</code>","text":"<p>Latent parameters of the model.</p> <p>Returns:</p> Type Description <code>list[Variable]</code> <p>Latent parameters of the model</p>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.__init__","title":"<code>__init__(item_intercept=True, price_effects=False, seasonal_effects=False, think_ahead=False, latent_sizes={'preferences': 4, 'price': 4, 'season': 4}, n_negative_samples=2, optimizer='adam', callbacks=None, lr=0.001, epochs=10, batch_size=32, grad_clip_value=None, weight_decay=None, momentum=0.0, epsilon_price=1e-05, **kwargs)</code>","text":"<p>Initialize the Shopper model.</p> <p>Parameters:</p> Name Type Description Default <code>item_intercept</code> <code>bool</code> <p>Whether to include item intercept in the model, by default True Corresponds to the item intercept</p> <code>True</code> <code>price_effects</code> <code>bool</code> <p>Whether to include price effects in the model, by default True</p> <code>False</code> <code>seasonal_effects</code> <code>bool</code> <p>Whether to include seasonal effects in the model, by default True</p> <code>False</code> <code>think_ahead</code> <code>bool</code> <p>Whether to include \"thinking ahead\" in the model, by default False</p> <code>False</code> <code>latent_sizes</code> <code>dict[str]</code> <p>Lengths of the vector representation of the latent parameters latent_sizes[\"preferences\"]: length of one vector of theta, alpha, rho latent_sizes[\"price\"]: length of one vector of gamma, beta latent_sizes[\"season\"]: length of one vector of delta, mu by default {\"preferences\": 4, \"price\": 4, \"season\": 4}</p> <code>{'preferences': 4, 'price': 4, 'season': 4}</code> <code>n_negative_samples</code> <code>int</code> <p>Number of negative samples to draw for each positive sample for the training, by default 2 Must be &gt; 0</p> <code>2</code> <code>optimizer</code> <code>str</code> <p>Optimizer to use for training, by default \"adam\"</p> <code>'adam'</code> <code>callbacks</code> <code>Union[CallbackList, None]</code> <p>List of callbacks to add to model.fit, by default None and only add History</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate, by default 1e-3</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of epochs, by default 100</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size, by default 32</p> <code>32</code> <code>grad_clip_value</code> <code>Union[float, None]</code> <p>Value to clip the gradient, by default None</p> <code>None</code> <code>weight_decay</code> <code>Union[float, None]</code> <p>Weight decay, by default None</p> <code>None</code> <code>momentum</code> <code>float</code> <p>Momentum for the optimizer, by default 0. For SGD only</p> <code>0.0</code> <code>epsilon_price</code> <code>float</code> <p>Epsilon value to add to prices to avoid NaN values (log(0)), by default 1e-5</p> <code>1e-05</code> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>def __init__(\n    self,\n    item_intercept: bool = True,\n    price_effects: bool = False,\n    seasonal_effects: bool = False,\n    think_ahead: bool = False,\n    latent_sizes: dict[str] = {\"preferences\": 4, \"price\": 4, \"season\": 4},\n    n_negative_samples: int = 2,\n    optimizer: str = \"adam\",\n    callbacks: Union[tf.keras.callbacks.CallbackList, None] = None,\n    lr: float = 1e-3,\n    epochs: int = 10,\n    batch_size: int = 32,\n    grad_clip_value: Union[float, None] = None,\n    weight_decay: Union[float, None] = None,\n    momentum: float = 0.0,\n    epsilon_price: float = 1e-5,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the Shopper model.\n\n    Parameters\n    ----------\n    item_intercept: bool, optional\n        Whether to include item intercept in the model, by default True\n        Corresponds to the item intercept\n    price_effects: bool, optional\n        Whether to include price effects in the model, by default True\n    seasonal_effects: bool, optional\n        Whether to include seasonal effects in the model, by default True\n    think_ahead: bool, optional\n        Whether to include \"thinking ahead\" in the model, by default False\n    latent_sizes: dict[str]\n        Lengths of the vector representation of the latent parameters\n        latent_sizes[\"preferences\"]: length of one vector of theta, alpha, rho\n        latent_sizes[\"price\"]: length of one vector of gamma, beta\n        latent_sizes[\"season\"]: length of one vector of delta, mu\n        by default {\"preferences\": 4, \"price\": 4, \"season\": 4}\n    n_negative_samples: int, optional\n        Number of negative samples to draw for each positive sample for the training,\n        by default 2\n        Must be &gt; 0\n    optimizer: str, optional\n        Optimizer to use for training, by default \"adam\"\n    callbacks: tf.keras.callbacks.Callbacklist, optional\n        List of callbacks to add to model.fit, by default None and only add History\n    lr: float, optional\n        Learning rate, by default 1e-3\n    epochs: int, optional\n        Number of epochs, by default 100\n    batch_size: int, optional\n        Batch size, by default 32\n    grad_clip_value: float, optional\n        Value to clip the gradient, by default None\n    weight_decay: float, optional\n        Weight decay, by default None\n    momentum: float, optional\n        Momentum for the optimizer, by default 0. For SGD only\n    epsilon_price: float, optional\n        Epsilon value to add to prices to avoid NaN values (log(0)), by default 1e-5\n    \"\"\"\n    self.item_intercept = item_intercept\n    self.price_effects = price_effects\n    self.seasonal_effects = seasonal_effects\n    self.think_ahead = think_ahead\n\n    if \"preferences\" not in latent_sizes.keys():\n        logging.warning(\n            \"No latent size value has been specified for preferences, \"\n            \"switching to default value 4.\"\n        )\n        latent_sizes[\"preferences\"] = 4\n    if \"price\" not in latent_sizes.keys() and self.price_effects:\n        logging.warning(\n            \"No latent size value has been specified for price_effects, \"\n            \"switching to default value 4.\"\n        )\n        latent_sizes[\"price\"] = 4\n    if \"season\" not in latent_sizes.keys() and self.seasonal_effects:\n        logging.warning(\n            \"No latent size value has been specified for seasonal_effects, \"\n            \"switching to default value 4.\"\n        )\n        latent_sizes[\"season\"] = 4\n\n    for val in latent_sizes.keys():\n        if val not in [\"preferences\", \"price\", \"season\"]:\n            raise ValueError(f\"Unknown value for latent_sizes dict: {val}.\")\n\n    if n_negative_samples &lt;= 0:\n        raise ValueError(\"n_negative_samples must be &gt; 0.\")\n\n    self.latent_sizes = latent_sizes\n    self.n_negative_samples = n_negative_samples\n\n    self.epsilon_price = epsilon_price\n\n    super().__init__(\n        optimizer=optimizer,\n        callbacks=callbacks,\n        lr=lr,\n        epochs=epochs,\n        batch_size=batch_size,\n        grad_clip_value=grad_clip_value,\n        weight_decay=weight_decay,\n        momentum=momentum,\n        **kwargs,\n    )\n\n    if len(tf.config.get_visible_devices(\"GPU\")):\n        # At least one available GPU\n        self.on_gpu = True\n    else:\n        # No available GPU\n        self.on_gpu = False\n    # /!\\ If a model trained on GPU is loaded on CPU, self.on_gpu must be set\n    # to False manually after loading the model, and vice versa\n\n    self.instantiated = False\n</code></pre>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.compute_batch_loss","title":"<code>compute_batch_loss(item_batch, basket_batch, future_batch, store_batch, week_batch, price_batch, available_item_batch, user_batch)</code>","text":"<p>Compute log-likelihood and loss for one batch of items.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>ndarray</code> <p>Batch of purchased items ID (integers) Shape must be (batch_size,)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>future_batch</code> <code>ndarray</code> <p>Batch of items to be purchased in the future (ID of items not yet in the basket) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (integers) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>List of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <p>Returns:</p> Name Type Description <code>batch_loss</code> <code>Variable</code> <p>Value of the loss for the batch (normalized negative log-likelihood), Shape must be (1,)</p> <code>loglikelihood</code> <code>Variable</code> <p>Computed log-likelihood of the batch of items Approximated by difference of utilities between positive and negative samples Shape must be (1,)</p> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>@tf.function  # Graph mode\ndef compute_batch_loss(\n    self,\n    item_batch: np.ndarray,\n    basket_batch: np.ndarray,\n    future_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    user_batch: np.ndarray,\n) -&gt; tuple[tf.Variable]:\n    \"\"\"Compute log-likelihood and loss for one batch of items.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray\n        Batch of purchased items ID (integers)\n        Shape must be (batch_size,)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    future_batch: np.ndarray\n        Batch of items to be purchased in the future (ID of items not yet in the\n        basket) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (integers) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        List of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n\n    Returns\n    -------\n    batch_loss: tf.Variable\n        Value of the loss for the batch (normalized negative log-likelihood),\n        Shape must be (1,)\n    loglikelihood: tf.Variable\n        Computed log-likelihood of the batch of items\n        Approximated by difference of utilities between positive and negative samples\n        Shape must be (1,)\n    \"\"\"\n    _ = user_batch\n    batch_size = len(item_batch)\n    item_batch = tf.cast(item_batch, dtype=tf.int32)\n\n    # Negative sampling\n    negative_samples = tf.reshape(\n        tf.transpose(\n            tf.reshape(\n                tf.concat(\n                    [\n                        self.get_negative_samples(\n                            available_items=available_item_batch[idx],\n                            purchased_items=basket_batch[idx],\n                            future_purchases=future_batch[idx],\n                            next_item=item_batch[idx],\n                            n_samples=self.n_negative_samples,\n                        )\n                        for idx in range(batch_size)\n                    ],\n                    axis=0,\n                ),\n                # Reshape to have at the beginning of the array all the first negative samples\n                # of all positive samples, then all the second negative samples, etc.\n                # (same logic as for the calls to np.tile)\n                [batch_size, self.n_negative_samples],\n            ),\n        ),\n        # Flatten 2D --&gt; 1D\n        shape=[-1],\n    )\n\n    augmented_item_batch = tf.cast(\n        tf.concat([item_batch, negative_samples], axis=0), dtype=tf.int32\n    )\n    prices_tiled = tf.tile(price_batch, [self.n_negative_samples + 1, 1])\n    # Each time, pick only the price of the item in augmented_item_batch from the\n    # corresponding price array\n    augmented_price_batch = tf.gather(\n        params=prices_tiled,\n        indices=augmented_item_batch,\n        # batch_dims=1 is equivalent to having an outer loop over\n        # the first axis of params and indices\n        batch_dims=1,\n    )\n\n    # Compute the utility of all the available items\n    all_utilities = self.compute_batch_utility(\n        item_batch=augmented_item_batch,\n        basket_batch=tf.tile(basket_batch, [self.n_negative_samples + 1, 1]),\n        store_batch=tf.tile(store_batch, [self.n_negative_samples + 1]),\n        week_batch=tf.tile(week_batch, [self.n_negative_samples + 1]),\n        price_batch=augmented_price_batch,\n        available_item_batch=tf.tile(available_item_batch, [self.n_negative_samples + 1, 1]),\n        user_batch=tf.tile(user_batch, [self.n_negative_samples + 1]),\n    )\n\n    positive_samples_utilities = tf.gather(all_utilities, tf.range(batch_size))\n    negative_samples_utilities = tf.gather(\n        all_utilities, tf.range(batch_size, tf.shape(all_utilities)[0])\n    )\n\n    # Log-likelihood of a batch = sum of log-likelihoods of its samples\n    # Add a small epsilon to gain numerical stability (avoid log(0))\n    epsilon = 0.0  # No epsilon added for now\n    loglikelihood = tf.reduce_sum(\n        tf.math.log(\n            tf.sigmoid(\n                tf.tile(\n                    positive_samples_utilities,\n                    [self.n_negative_samples],\n                )\n                - negative_samples_utilities\n            )\n            + epsilon\n        ),\n    )  # Shape of loglikelihood: (1,)\n\n    # Maximize the predicted log-likelihood (ie minimize the negative log-likelihood)\n    # normalized by the batch size and the number of negative samples\n    batch_loss = -1 * loglikelihood / (batch_size * self.n_negative_samples)\n\n    return batch_loss, loglikelihood\n</code></pre>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.compute_batch_utility","title":"<code>compute_batch_utility(item_batch, basket_batch, store_batch, week_batch, price_batch, user_batch, available_item_batch)</code>","text":"<p>Compute the utility of all the items in item_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>Union[ndarray, Tensor]</code> <p>Batch of the purchased items ID (integers) for which to compute the utility Shape must be (batch_size,) (positive and negative samples concatenated together)</p> required <code>basket_batch</code> <code>ndarray</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item Shape must be (batch_size, max_basket_size)</p> required <code>store_batch</code> <code>ndarray</code> <p>Batch of store IDs (integers) for each purchased item Shape must be (batch_size,)</p> required <code>week_batch</code> <code>ndarray</code> <p>Batch of week numbers (integers) for each purchased item Shape must be (batch_size,)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (integers) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>Batch of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <p>Returns:</p> Name Type Description <code>item_utilities</code> <code>Tensor</code> <p>Utility of all the items in item_batch Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>def compute_batch_utility(\n    self,\n    item_batch: Union[np.ndarray, tf.Tensor],\n    basket_batch: np.ndarray,\n    store_batch: np.ndarray,\n    week_batch: np.ndarray,\n    price_batch: np.ndarray,\n    user_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the utility of all the items in item_batch.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray or tf.Tensor\n        Batch of the purchased items ID (integers) for which to compute the utility\n        Shape must be (batch_size,)\n        (positive and negative samples concatenated together)\n    basket_batch: np.ndarray\n        Batch of baskets (ID of items already in the baskets) (arrays) for each purchased item\n        Shape must be (batch_size, max_basket_size)\n    store_batch: np.ndarray\n        Batch of store IDs (integers) for each purchased item\n        Shape must be (batch_size,)\n    week_batch: np.ndarray\n        Batch of week numbers (integers) for each purchased item\n        Shape must be (batch_size,)\n    price_batch: np.ndarray\n        Batch of prices (integers) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        Batch of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n\n    Returns\n    -------\n    item_utilities: tf.Tensor\n        Utility of all the items in item_batch\n        Shape must be (batch_size,)\n    \"\"\"\n    _ = user_batch\n    item_batch = tf.cast(item_batch, dtype=tf.int32)\n    basket_batch = tf.cast(basket_batch, dtype=tf.int32)\n    store_batch = tf.cast(store_batch, dtype=tf.int32)\n    week_batch = tf.cast(week_batch, dtype=tf.int32)\n    price_batch = tf.cast(price_batch, dtype=tf.float32)\n    available_item_batch = tf.cast(available_item_batch, dtype=tf.int32)\n\n    if len(tf.shape(item_batch)) == 1:\n        if len(tf.shape(price_batch)) != 1:\n            raise ValueError(f\"\"\"Arguments price_batch and item_batch should have same shape\n            and are:{item_batch.shape} and {price_batch.shape}\"\"\")\n        item_batch = tf.expand_dims(item_batch, axis=1)\n        price_batch = tf.expand_dims(price_batch, axis=1)\n        squeeze = True\n    else:\n        squeeze = False\n\n    theta_store = tf.gather(self.theta, indices=store_batch)\n    alpha_item = tf.gather(self.alpha, indices=item_batch)\n    # Compute the dot product along the last dimension\n    store_preferences = tf.einsum(\"kj,klj-&gt;kl\", theta_store, alpha_item)\n\n    if self.item_intercept:\n        # Manually enforce the lambda of the checkout item to be 0\n        # (equivalent to translating the lambda values)\n        item_intercept = tf.gather(tf.concat([[0.0], self.lambda_], axis=0), indices=item_batch)\n    else:\n        item_intercept = tf.zeros_like(store_preferences)\n\n    if self.price_effects:\n        gamma_store = tf.gather(self.gamma, indices=store_batch)\n        beta_item = tf.gather(self.beta, indices=item_batch)\n        # Add epsilon to avoid NaN values (log(0))\n        price_effects = (\n            -1\n            # Compute the dot product along the last dimension\n            * tf.einsum(\"kj,klj-&gt;kl\", gamma_store, beta_item)\n            * tf.math.log(price_batch + self.epsilon_price)\n        )\n    else:\n        gamma_store = tf.zeros_like(store_batch)\n        price_effects = tf.zeros_like(store_preferences)\n\n    if self.seasonal_effects:\n        delta_week = tf.gather(self.delta, indices=week_batch)\n        mu_item = tf.gather(self.mu, indices=item_batch)\n        # Compute the dot product along the last dimension\n        seasonal_effects = tf.einsum(\"kj,klj-&gt;kl\", delta_week, mu_item)\n    else:\n        delta_week = tf.zeros_like(week_batch)\n        seasonal_effects = tf.zeros_like(store_preferences)\n\n    # The effects of item intercept, store preferences, price sensitivity\n    # and seasonal effects are combined in the per-item per-trip latent variable\n    psi = tf.reduce_sum(\n        [\n            item_intercept,\n            store_preferences,\n            price_effects,\n            seasonal_effects,\n        ],\n        axis=0,\n    )  # Shape: (batch_size,)\n\n    # Apply boolean mask to mask out the padding value -1\n    masked_baskets = tf.where(\n        condition=basket_batch &gt; -1,  # If False: padding value -1\n        x=1,  # Output where condition is True\n        y=0,  # Output where condition is False\n    )\n    # Number of items in each basket\n    count_items_in_basket = tf.reduce_sum(masked_baskets, axis=1)\n\n    # Create a RaggedTensor from the indices with padding removed\n    item_indices_ragged = tf.cast(\n        tf.ragged.boolean_mask(basket_batch, basket_batch != -1),\n        dtype=tf.int32,\n    )\n\n    if tf.size(item_indices_ragged) == 0:\n        # Empty baskets: no alpha embeddings to gather\n        # (It must be a ragged tensor here because TF's GraphMode requires the same\n        # nested structure to be returned from all branches of a conditional)\n        alpha_by_basket = tf.RaggedTensor.from_tensor(\n            tf.zeros((len(item_batch), 0, self.alpha.shape[1]))\n        )\n    else:\n        # Gather the embeddings using a ragged tensor of indices\n        alpha_by_basket = tf.ragged.map_flat_values(tf.gather, self.alpha, item_indices_ragged)\n\n    # Compute the sum of the alpha embeddings for each basket\n    alpha_sum = tf.reduce_sum(alpha_by_basket, axis=1)\n\n    rho_item = tf.gather(self.rho, indices=item_batch)\n\n    # Divide each sum of alpha embeddings by the number of items in the corresponding basket\n    # Avoid NaN values (division by 0)\n    count_items_in_basket_expanded = tf.expand_dims(\n        tf.cast(count_items_in_basket, dtype=tf.float32), -1\n    )\n\n    # Apply boolean mask for case distinction\n    alpha_average = tf.where(\n        condition=count_items_in_basket_expanded != 0,  # If True: count_items_in_basket &gt; 0\n        x=alpha_sum / count_items_in_basket_expanded,  # Output if condition is True\n        y=tf.zeros_like(alpha_sum),  # Output if condition is False\n    )\n\n    # Compute the dot product along the last dimension\n    basket_interaction_utility = tf.einsum(\"kj,klj-&gt;kl\", alpha_average, rho_item)\n\n    item_utilities = psi + basket_interaction_utility\n\n    # No thinking ahead\n    if not self.think_ahead:\n        if squeeze:\n            return tf.gather(item_utilities, 0, axis=1)\n        return item_utilities\n\n    # Thinking ahead\n    next_step_utilities = tf.stack(\n        [\n            self.thinking_ahead(\n                item_batch=tf.gather(item_batch, i, axis=1),\n                ragged_basket_batch=item_indices_ragged,\n                price_batch=price_batch,\n                available_item_batch=available_item_batch,\n                theta_store=theta_store,\n                gamma_store=gamma_store,  # 0 if self.price_effects is False\n                delta_week=delta_week,  # 0 if self.seasonal_effects is False\n            )\n            for i in range(tf.shape(item_batch)[1])\n        ],\n        axis=-1,\n    )\n\n    if squeeze:\n        return tf.gather(item_utilities + next_step_utilities, 0, axis=1)\n    return item_utilities + next_step_utilities\n</code></pre>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.get_negative_samples","title":"<code>get_negative_samples(available_items, purchased_items, future_purchases, next_item, n_samples)</code>","text":"<p>Sample randomly a set of items.</p> <p>(set of items not already purchased and not necessarily from the basket)</p> <p>Parameters:</p> Name Type Description Default <code>available_items</code> <code>ndarray</code> <p>Matrix indicating the availability (1) or not (0) of the products Shape must be (n_items,)</p> required <code>purchased_items</code> <code>ndarray</code> <p>List of items already purchased (already in the basket)</p> required <code>future_purchases</code> <code>ndarray</code> <p>List of items to be purchased in the future (not yet in the basket)</p> required <code>next_item</code> <code>int</code> <p>Next item (to be added in the basket)</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to draw</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>Random sample of items, each of them distinct from the next item and from the items already in the basket</p> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>def get_negative_samples(\n    self,\n    available_items: np.ndarray,\n    purchased_items: np.ndarray,\n    future_purchases: np.ndarray,\n    next_item: int,\n    n_samples: int,\n) -&gt; list[int]:\n    \"\"\"Sample randomly a set of items.\n\n    (set of items not already purchased and *not necessarily* from the basket)\n\n    Parameters\n    ----------\n    available_items: np.ndarray\n        Matrix indicating the availability (1) or not (0) of the products\n        Shape must be (n_items,)\n    purchased_items: np.ndarray\n        List of items already purchased (already in the basket)\n    future_purchases: np.ndarray\n        List of items to be purchased in the future (not yet in the basket)\n    next_item: int\n        Next item (to be added in the basket)\n    n_samples: int\n        Number of samples to draw\n\n    Returns\n    -------\n    list[int]\n        Random sample of items, each of them distinct from\n        the next item and from the items already in the basket\n    \"\"\"\n    # Convert inputs to tensors\n    available_items = tf.cast(tf.convert_to_tensor(available_items), dtype=tf.int32)\n    purchased_items = tf.cast(tf.convert_to_tensor(purchased_items), dtype=tf.int32)\n    future_purchases = tf.cast(tf.convert_to_tensor(future_purchases), dtype=tf.int32)\n    next_item = tf.cast(tf.convert_to_tensor(next_item), dtype=tf.int32)\n\n    # Get the list of available items based on the availability matrix\n    item_ids = tf.range(self.n_items)\n    available_mask = tf.equal(available_items, 1)\n    assortment = tf.boolean_mask(item_ids, available_mask)\n\n    not_to_be_chosen = tf.concat(\n        [purchased_items, future_purchases, tf.expand_dims(next_item, axis=0)], axis=0\n    )\n\n    # Ensure that the checkout item 0 can be picked as a negative sample\n    # if it is not the next item\n    # (otherwise 0 is always in not_to_be_chosen because it's in future_purchases)\n    if not tf.equal(next_item, 0):\n        not_to_be_chosen = tf.boolean_mask(not_to_be_chosen, not_to_be_chosen != 0)\n\n    # Sample negative items from the assortment excluding not_to_be_chosen\n    negative_samples = tf.boolean_mask(\n        tensor=assortment,\n        # Reduce the 2nd dimension of the boolean mask to get a 1D mask\n        mask=~tf.reduce_any(\n            tf.equal(tf.expand_dims(assortment, axis=1), not_to_be_chosen), axis=1\n        ),\n    )\n\n    error_message = (\n        \"The number of negative samples to draw must be less than \"\n        \"the number of available items not already purchased and \"\n        \"distinct from the next item.\"\n    )\n    # Raise an error if n_samples &gt; tf.size(negative_samples)\n    tf.debugging.assert_greater_equal(\n        tf.size(negative_samples), n_samples, message=error_message\n    )\n\n    # Randomize the sampling\n    negative_samples = tf.random.shuffle(negative_samples)\n\n    # Keep only n_samples\n    return negative_samples[:n_samples]\n</code></pre>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.instantiate","title":"<code>instantiate(n_items, n_stores=0)</code>","text":"<p>Instantiate the Shopper model.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items to consider, i.e. the number of items in the dataset (includes the checkout item)</p> required <code>n_stores</code> <code>int</code> <p>Number of stores in the population</p> <code>0</code> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>def instantiate(\n    self,\n    n_items: int,\n    n_stores: int = 0,\n) -&gt; None:\n    \"\"\"Instantiate the Shopper model.\n\n    Parameters\n    ----------\n    n_items: int\n        Number of items to consider, i.e. the number of items in the dataset\n        (includes the checkout item)\n    n_stores: int\n        Number of stores in the population\n    \"\"\"\n    self.n_items = n_items\n    if n_stores == 0 and self.price_effects:\n        # To take into account the price effects, the number of stores must be &gt; 0\n        # to have a gamma embedding\n        # (By default, the store id is 0)\n        n_stores = 1\n    self.n_stores = n_stores\n\n    self.rho = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n            shape=(n_items, self.latent_sizes[\"preferences\"])\n        ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n        trainable=True,\n        name=\"rho\",\n    )\n    self.alpha = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n            shape=(n_items, self.latent_sizes[\"preferences\"])\n        ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n        trainable=True,\n        name=\"alpha\",\n    )\n    self.theta = tf.Variable(\n        tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n            shape=(n_stores, self.latent_sizes[\"preferences\"])\n        ),  # Dimension for 1 item: latent_sizes[\"preferences\"]\n        trainable=True,\n        name=\"theta\",\n    )\n\n    if self.item_intercept:\n        # Add item intercept\n        self.lambda_ = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                # No lambda for the checkout item (set to 0 later)\n                shape=(n_items - 1,)  # Dimension for 1 item: 1\n            ),\n            trainable=True,\n            name=\"lambda_\",\n        )\n\n    if self.price_effects:\n        # Add price sensitivity\n        self.beta = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                shape=(n_items, self.latent_sizes[\"price\"])\n            ),  # Dimension for 1 item: latent_sizes[\"price\"]\n            trainable=True,\n            name=\"beta\",\n        )\n        self.gamma = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=1.0, seed=42)(\n                shape=(n_stores, self.latent_sizes[\"price\"])\n            ),  # Dimension for 1 item: latent_sizes[\"price\"]\n            trainable=True,\n            name=\"gamma\",\n        )\n\n    if self.seasonal_effects:\n        # Add seasonal effects\n        self.mu = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(n_items, self.latent_sizes[\"season\"])\n            ),  # Dimension for 1 item: latent_sizes[\"season\"]\n            trainable=True,\n            name=\"mu\",\n        )\n        self.delta = tf.Variable(\n            tf.random_normal_initializer(mean=0, stddev=0.1, seed=42)(\n                shape=(52, self.latent_sizes[\"season\"])\n            ),  # Dimension for 1 item: latent_sizes[\"season\"]\n            trainable=True,\n            name=\"delta\",\n        )\n\n    self.instantiated = True\n</code></pre>"},{"location":"references/basket_models/references_shopper/#choice_learn.basket_models.shopper.Shopper.thinking_ahead","title":"<code>thinking_ahead(item_batch, ragged_basket_batch, price_batch, available_item_batch, theta_store, gamma_store, delta_week)</code>","text":"<p>Compute the utility of all the items in item_batch.</p> <p>Parameters:</p> Name Type Description Default <code>item_batch</code> <code>Union[ndarray, Tensor]</code> <p>Batch of the purchased items ID (integers) for which to compute the utility Shape must be (batch_size,) (positive and negative samples concatenated together)</p> required <code>ragged_basket_batch</code> <code>RaggedTensor</code> <p>Batch of baskets (ID of items already in the baskets) (arrays) without padding for each purchased item Shape must be (batch_size, None)</p> required <code>price_batch</code> <code>ndarray</code> <p>Batch of prices (integers) for each purchased item Shape must be (batch_size,)</p> required <code>available_item_batch</code> <code>ndarray</code> <p>Batch of availability matrices (indicating the availability (1) or not (0) of the products) (arrays) for each purchased item Shape must be (batch_size, n_items)</p> required <code>theta_store</code> <code>Tensor</code> <p>Slices from theta embedding gathered according to the indices that correspond to the store of each purchased item in the batch Shape must be (batch_size, latent_sizes[\"preferences\"])</p> required <code>gamma_store</code> <code>Tensor</code> <p>Slices from gamma embedding gathered according to the indices that correspond to the store of each purchased item in the batch Shape must be (batch_size, latent_sizes[\"price\"])</p> required <code>delta_week</code> <code>Tensor</code> <p>Slices from delta embedding gathered according to the indices that correspond to the week of each purchased item in the batch Shape must be (batch_size, latent_sizes[\"season\"])</p> required <p>Returns:</p> Name Type Description <code>total_next_step_utilities</code> <code>Tensor</code> <p>Nex step utility of all the items in item_batch Shape must be (batch_size,)</p> Source code in <code>choice_learn/basket_models/shopper.py</code> <pre><code>def thinking_ahead(\n    self,\n    item_batch: Union[np.ndarray, tf.Tensor],\n    ragged_basket_batch: tf.RaggedTensor,\n    price_batch: np.ndarray,\n    available_item_batch: np.ndarray,\n    theta_store: tf.Tensor,\n    gamma_store: tf.Tensor,\n    delta_week: tf.Tensor,\n) -&gt; tf.Tensor:\n    \"\"\"Compute the utility of all the items in item_batch.\n\n    Parameters\n    ----------\n    item_batch: np.ndarray or tf.Tensor\n        Batch of the purchased items ID (integers) for which to compute the utility\n        Shape must be (batch_size,)\n        (positive and negative samples concatenated together)\n    ragged_basket_batch: tf.RaggedTensor\n        Batch of baskets (ID of items already in the baskets) (arrays) without padding\n        for each purchased item\n        Shape must be (batch_size, None)\n    price_batch: np.ndarray\n        Batch of prices (integers) for each purchased item\n        Shape must be (batch_size,)\n    available_item_batch: np.ndarray\n        Batch of availability matrices (indicating the availability (1) or not (0)\n        of the products) (arrays) for each purchased item\n        Shape must be (batch_size, n_items)\n    theta_store: tf.Tensor\n        Slices from theta embedding gathered according to the indices that correspond\n        to the store of each purchased item in the batch\n        Shape must be (batch_size, latent_sizes[\"preferences\"])\n    gamma_store: tf.Tensor\n        Slices from gamma embedding gathered according to the indices that correspond\n        to the store of each purchased item in the batch\n        Shape must be (batch_size, latent_sizes[\"price\"])\n    delta_week: tf.Tensor\n        Slices from delta embedding gathered according to the indices that correspond\n        to the week of each purchased item in the batch\n        Shape must be (batch_size, latent_sizes[\"season\"])\n\n    Returns\n    -------\n    total_next_step_utilities: tf.Tensor\n        Nex step utility of all the items in item_batch\n        Shape must be (batch_size,)\n    \"\"\"\n    total_next_step_utilities = tf.zeros_like(item_batch, dtype=tf.float32)\n    # Compute the next step item utility for each element of the batch, one by one\n    # TODO: avoid a for loop on ragged_basket_batch at a later stage\n    for idx in tf.range(ragged_basket_batch.shape[0]):\n        basket = tf.gather(ragged_basket_batch, idx)\n        if len(basket) != 0 and basket[-1] == 0:\n            # No thinking ahead when the basket ends already with the checkout item 0\n            total_next_step_utilities = tf.tensor_scatter_nd_update(\n                tensor=total_next_step_utilities, indices=[[idx]], updates=[0]\n            )\n\n        else:\n            # Basket with the hypothetical current item\n            next_basket = tf.concat([basket, [item_batch[idx]]], axis=0)\n            # Get the list of available items based on the availability matrix\n            item_ids = tf.range(self.n_items)\n            available_mask = tf.equal(available_item_batch[idx], 1)\n            assortment = tf.boolean_mask(item_ids, available_mask)\n            hypothetical_next_purchases = tf.boolean_mask(\n                assortment,\n                ~tf.reduce_any(\n                    tf.equal(tf.expand_dims(assortment, axis=1), next_basket), axis=1\n                ),\n            )\n            # Check if there are still items to purchase during the next step\n            if len(hypothetical_next_purchases) == 0:\n                # No more items to purchase: next step impossible\n                total_next_step_utilities = tf.tensor_scatter_nd_update(\n                    tensor=total_next_step_utilities, indices=[[idx]], updates=[0]\n                )\n            else:\n                # Compute the dot product along the last dimension between the embeddings\n                # of the given store's theta and alpha of all the items\n                hypothetical_store_preferences = tf.reduce_sum(\n                    theta_store[idx] * self.alpha, axis=1\n                )\n\n                if self.item_intercept:\n                    # Manually enforce the lambda of the checkout item to be 0\n                    # (equivalent to translating the lambda values)\n                    hypothetical_item_intercept = tf.concat([[0.0], self.lambda_], axis=0)\n                else:\n                    hypothetical_item_intercept = tf.zeros_like(hypothetical_store_preferences)\n\n                if self.price_effects:\n                    hypothetical_price_effects = (\n                        -1\n                        # Compute the dot product along the last dimension between\n                        # the embeddings of the given store's gamma and beta\n                        # of all the items\n                        * tf.reduce_sum(gamma_store[idx] * self.beta, axis=1)\n                        * tf.math.log(price_batch[idx] + self.epsilon_price)\n                    )\n                else:\n                    hypothetical_price_effects = tf.zeros_like(hypothetical_store_preferences)\n\n                if self.seasonal_effects:\n                    # Compute the dot product along the last dimension between the embeddings\n                    # of delta of the given week and mu of all the items\n                    hypothetical_seasonal_effects = tf.reduce_sum(\n                        delta_week[idx] * self.mu, axis=1\n                    )\n                else:\n                    hypothetical_seasonal_effects = tf.zeros_like(\n                        hypothetical_store_preferences\n                    )\n\n                # The effects of item intercept, store preferences, price sensitivity\n                # and seasonal effects are combined in the per-item per-trip latent variable\n                hypothetical_psi = tf.reduce_sum(\n                    [\n                        hypothetical_item_intercept,  # 0 if self.item_intercept is False\n                        hypothetical_store_preferences,\n                        hypothetical_price_effects,  # 0 if self.price_effects is False\n                        hypothetical_seasonal_effects,  # 0 if self.seasonal_effects is False\n                    ],\n                    axis=0,\n                )  # Shape: (n_items,)\n\n                # Shape: (len(hypothetical_next_purchases),)\n                next_psi = tf.gather(hypothetical_psi, indices=hypothetical_next_purchases)\n\n                # Consider hypothetical \"next\" item one by one\n                next_step_basket_interaction_utilities = tf.zeros(\n                    (len(hypothetical_next_purchases),), dtype=tf.float32\n                )\n                for inner_idx in tf.range(len(hypothetical_next_purchases)):\n                    next_item_id = tf.gather(hypothetical_next_purchases, inner_idx)\n                    rho_next_item = tf.gather(\n                        self.rho, indices=next_item_id\n                    )  # Shape: (latent_size,)\n                    # Gather the embeddings using a tensor of indices\n                    # (before ensure that indices are integers)\n                    next_alpha_by_basket = tf.gather(\n                        self.alpha, indices=tf.cast(next_basket, dtype=tf.int32)\n                    )  # Shape: (len(next_basket), latent_size)\n                    # Divide the sum of alpha embeddings by the number of items\n                    # in the basket of the next step (always &gt; 0)\n                    next_alpha_average = tf.reduce_sum(next_alpha_by_basket, axis=0) / tf.cast(\n                        len(next_basket), dtype=tf.float32\n                    )  # Shape: (latent_size,)\n                    next_step_basket_interaction_utilities = tf.tensor_scatter_nd_update(\n                        tensor=next_step_basket_interaction_utilities,\n                        indices=[[inner_idx]],\n                        # Compute the dot product along the last dimension, shape: (1,)\n                        updates=[tf.reduce_sum(rho_next_item * next_alpha_average)],\n                    )\n\n                # Optimal next step: take the maximum utility among all possible next purchases\n                next_step_utility = tf.reduce_max(\n                    next_psi + next_step_basket_interaction_utilities, axis=0\n                )  # Shape: (1,)\n                total_next_step_utilities = tf.tensor_scatter_nd_update(\n                    tensor=total_next_step_utilities,\n                    indices=[[idx]],\n                    updates=[next_step_utility],\n                )\n\n    return total_next_step_utilities  # Shape: (batch_size,)\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/","title":"Trip and TripDataset Data Structures","text":"<p>Classes to handle datasets with baskets of products.</p>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.Trip","title":"<code>Trip</code>","text":"<p>Class for a trip.</p> <p>A trip is a sequence of purchases made at a specific time and at a specific store with given prices and a specific assortment. It can be seen as the content of a time-stamped purchase receipt with store identification.</p> <p>Trip = (purchases, store, week, prices, assortment)</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>class Trip:\n    \"\"\"Class for a trip.\n\n    A trip is a sequence of purchases made at a specific time and at a\n    specific store with given prices and a specific assortment. It can\n    be seen as the content of a time-stamped purchase receipt with store\n    identification.\n\n    Trip = (purchases, store, week, prices, assortment)\n    \"\"\"\n\n    def __init__(\n        self,\n        purchases: np.ndarray,\n        prices: np.ndarray,\n        assortment: Union[int, np.ndarray],\n        store: int = 0,\n        week: int = 0,\n        user_id: int = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the trip.\n\n        Parameters\n        ----------\n        purchases: np.ndarray\n            List of the ID of the purchased items, 0 to n_items - 1 (0-indexed)\n            Shape must be (len_basket,), the last item is the checkout item 0\n        store: int\n            Store ID, 0 to n_stores - 1 (0-indexed)\n        week: int\n            Week number, 0 to 51 (0-indexed)\n        prices: np.ndarray\n            Prices of all the items in the dataset\n            Shape must be (n_items,) with n_items the number of items in\n            the TripDataset\n        assortment: int or np.ndarray\n            Assortment ID (int) corresponding to the assortment (ie its index in\n            self.available_items) OR availability matrix (np.ndarray) of the\n            assortment (binary vector of length n_items where 1 means the item\n            is available and 0 means the item is not available)\n            An assortment is the list of available items of a specific store at a given time\n        \"\"\"\n        if week not in range(52):\n            raise ValueError(\"Week number must be between 0 and 51, inclusive.\")\n\n        # Constitutive elements of a trip\n        self.purchases = purchases\n        self.store = store\n        self.week = week\n        self.prices = prices\n        self.assortment = assortment\n        self.user_id = user_id\n\n        self.trip_length = len(purchases)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return short representation of the trip.\n\n        Returns\n        -------\n        str\n            Representation of the trip\n        \"\"\"\n        desc = f\"Trip with {self.trip_length} purchases {self.purchases}\"\n        desc += f\" at store {self.store} in week {self.week} by user {self.user_id}\"\n        desc += f\" with prices {self.prices} and assortment {self.assortment}\"\n        return desc\n\n    def get_items_up_to_index(self, i: int) -&gt; np.ndarray:\n        \"\"\"Get items up to index i.\n\n        Parameters\n        ----------\n        i: int\n            Index of the item to get\n\n        Returns\n        -------\n        np.ndarray\n            List of items up to index i (excluded)\n            Shape must be (i,)\n        \"\"\"\n        return self.purchases[:i]\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.Trip.__init__","title":"<code>__init__(purchases, prices, assortment, store=0, week=0, user_id=0)</code>","text":"<p>Initialize the trip.</p> <p>Parameters:</p> Name Type Description Default <code>purchases</code> <code>ndarray</code> <p>List of the ID of the purchased items, 0 to n_items - 1 (0-indexed) Shape must be (len_basket,), the last item is the checkout item 0</p> required <code>store</code> <code>int</code> <p>Store ID, 0 to n_stores - 1 (0-indexed)</p> <code>0</code> <code>week</code> <code>int</code> <p>Week number, 0 to 51 (0-indexed)</p> <code>0</code> <code>prices</code> <code>ndarray</code> <p>Prices of all the items in the dataset Shape must be (n_items,) with n_items the number of items in the TripDataset</p> required <code>assortment</code> <code>Union[int, ndarray]</code> <p>Assortment ID (int) corresponding to the assortment (ie its index in self.available_items) OR availability matrix (np.ndarray) of the assortment (binary vector of length n_items where 1 means the item is available and 0 means the item is not available) An assortment is the list of available items of a specific store at a given time</p> required Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __init__(\n    self,\n    purchases: np.ndarray,\n    prices: np.ndarray,\n    assortment: Union[int, np.ndarray],\n    store: int = 0,\n    week: int = 0,\n    user_id: int = 0,\n) -&gt; None:\n    \"\"\"Initialize the trip.\n\n    Parameters\n    ----------\n    purchases: np.ndarray\n        List of the ID of the purchased items, 0 to n_items - 1 (0-indexed)\n        Shape must be (len_basket,), the last item is the checkout item 0\n    store: int\n        Store ID, 0 to n_stores - 1 (0-indexed)\n    week: int\n        Week number, 0 to 51 (0-indexed)\n    prices: np.ndarray\n        Prices of all the items in the dataset\n        Shape must be (n_items,) with n_items the number of items in\n        the TripDataset\n    assortment: int or np.ndarray\n        Assortment ID (int) corresponding to the assortment (ie its index in\n        self.available_items) OR availability matrix (np.ndarray) of the\n        assortment (binary vector of length n_items where 1 means the item\n        is available and 0 means the item is not available)\n        An assortment is the list of available items of a specific store at a given time\n    \"\"\"\n    if week not in range(52):\n        raise ValueError(\"Week number must be between 0 and 51, inclusive.\")\n\n    # Constitutive elements of a trip\n    self.purchases = purchases\n    self.store = store\n    self.week = week\n    self.prices = prices\n    self.assortment = assortment\n    self.user_id = user_id\n\n    self.trip_length = len(purchases)\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.Trip.__str__","title":"<code>__str__()</code>","text":"<p>Return short representation of the trip.</p> <p>Returns:</p> Type Description <code>str</code> <p>Representation of the trip</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return short representation of the trip.\n\n    Returns\n    -------\n    str\n        Representation of the trip\n    \"\"\"\n    desc = f\"Trip with {self.trip_length} purchases {self.purchases}\"\n    desc += f\" at store {self.store} in week {self.week} by user {self.user_id}\"\n    desc += f\" with prices {self.prices} and assortment {self.assortment}\"\n    return desc\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.Trip.get_items_up_to_index","title":"<code>get_items_up_to_index(i)</code>","text":"<p>Get items up to index i.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the item to get</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>List of items up to index i (excluded) Shape must be (i,)</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_items_up_to_index(self, i: int) -&gt; np.ndarray:\n    \"\"\"Get items up to index i.\n\n    Parameters\n    ----------\n    i: int\n        Index of the item to get\n\n    Returns\n    -------\n    np.ndarray\n        List of items up to index i (excluded)\n        Shape must be (i,)\n    \"\"\"\n    return self.purchases[:i]\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset","title":"<code>TripDataset</code>","text":"<p>Class for a dataset of trips.</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>class TripDataset:\n    \"\"\"Class for a dataset of trips.\"\"\"\n\n    def __init__(\n        self,\n        trips: list[Trip],\n        available_items: np.ndarray,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Parameters\n        ----------\n        trips: list[Trip]\n            List of trips\n            Length must be n_trips\n        available_items: np.ndarray\n            Array of availability matrices\n            available_items[i]: availability matrix of the assortment whose ID is i\n            (The availability matrix is a binary vector of length n_items\n            where 1 means the item is available and 0 means the item is not available)\n            Shape must be (n_assortments, n_items)\n        \"\"\"\n        self.trips = trips\n        self.max_length = max([trip.trip_length for trip in self.trips])\n        self.n_samples = len(self.get_transactions())\n        self.available_items = available_items\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of trips in the dataset.\n\n        Returns\n        -------\n        int\n            Number of trips in the dataset\n        \"\"\"\n        return len(self.trips)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return short representation of the dataset.\n\n        Returns\n        -------\n        str\n            Representation of the dataset\n        \"\"\"\n        return f\"TripDataset with {len(self)} trips\"\n\n    def __iter__(self) -&gt; iter:\n        \"\"\"Iterate over the trips in the dataset.\n\n        Returns\n        -------\n        iter\n            Iterator over the trips\n        \"\"\"\n        return iter(self.trips)\n\n    def concatenate(self, other: object, inplace: bool = False) -&gt; object:\n        \"\"\"Add a dataset to another.\n\n        Parameters\n        ----------\n        other: TripDataset\n            Dataset to add\n        inplace: bool\n            Whether to add the dataset in-place or not, by default False\n\n        Returns\n        -------\n        TripDataset\n            Concatenated dataset\n        \"\"\"\n        if inplace:  # Add another dataset to the current one (in-place)\n            # Concatenate the list of trips\n            self.trips += other.trips\n            # Update the attributes of the TripDataset\n            self.max_length = max([trip.trip_length for trip in self.trips])\n            self.n_samples = len(self.get_transactions())\n            # Concatenate the arrays of availability matrices\n            # /!\\ When concatenating 2 TripDatasets, the indices of the availability matrices\n            # changes\n            self.available_items = np.concatenate(\n                (self.available_items, other.available_items), axis=0\n            )\n            return self\n\n        # Else: create a new dataset by adding 2 datasets together\n        return TripDataset(\n            # Concatenate the list of trips\n            trips=self.trips + other.trips,\n            # Concatenate the arrays of availability matrices\n            # /!\\ When concatenating 2 TripDatasets, the indices of the availability matrices\n            # changes\n            available_items=np.concatenate((self.available_items, other.available_items), axis=0),\n        )\n\n    def get_trip(self, index: int) -&gt; Trip:\n        \"\"\"Return the trip at the given index.\n\n        Parameters\n        ----------\n        index: int\n            Index of the trip to get\n\n        Returns\n        -------\n        Trip\n            Trip at the given index\n        \"\"\"\n        return self.trips[index]\n\n    def get_transactions(self) -&gt; np.ndarray:\n        \"\"\"Return the transactions of the TripDataset.\n\n        One transaction is a quadruplet (store, trip, item, user_id).\n\n        Returns\n        -------\n        dict\n            Transactions of the TripDataset\n            keys: trans_id\n            values: (store, trip, item)\n        \"\"\"\n        transactions = {}\n\n        trans_id = 0\n        for i, trip in enumerate(self.trips):\n            for item in trip.purchases:\n                transactions[trans_id] = (trip.store, i, item, trip.user_id)\n                trans_id += 1\n\n        return transactions\n\n    def get_all_items(self) -&gt; np.ndarray:\n        \"\"\"Return the list of all items available in the dataset.\n\n        Returns\n        -------\n        np.ndarray\n            List of items available in the dataset\n        \"\"\"\n        return np.arange(self.n_items)\n\n    def get_all_baskets(self) -&gt; np.ndarray:\n        \"\"\"Return the list of all baskets in the dataset.\n\n        Returns\n        -------\n        np.ndarray\n            List of baskets in the dataset\n        \"\"\"\n        return np.array([self.trips[i].purchases for i in range(len(self))])\n\n    def get_all_stores(self) -&gt; np.ndarray:\n        \"\"\"Return the list of all stores in the dataset.\n\n        Returns\n        -------\n        np.ndarray\n            List of stores in the dataset\n        \"\"\"\n        # If preprocessing working well, equal to [0, 1, ..., n_stores - 1]\n        return np.array(list({self.trips[i].store for i in range(len(self))}))\n\n    def get_all_weeks(self) -&gt; np.ndarray:\n        \"\"\"Return the list of all weeks in the dataset.\n\n        Returns\n        -------\n        np.ndarray\n            List of weeks in the dataset\n        \"\"\"\n        # If preprocessing working well, equal to [0, 1, ..., 51 or 52]\n        return np.array(list({self.trips[i].week for i in range(len(self))}))\n\n    def get_all_prices(self) -&gt; np.ndarray:\n        \"\"\"Return the list of all price arrays in the dataset.\n\n        Returns\n        -------\n        np.ndarray\n            List of price arrays in the dataset\n        \"\"\"\n        return np.array([self.trips[i].prices for i in range(len(self))])\n\n    def get_all_users(self, shuffled: bool = False) -&gt; np.ndarray:\n        \"\"\"Return the list of all users in the dataset.\n\n        Parameters\n        ----------\n        shuffled: bool\n            Whether to shuffle the list of users or not, by default False\n\n        Returns\n        -------\n        np.ndarray\n            List of users in the dataset\n        \"\"\"\n        if shuffled:\n            user_ids = list({self.trips[i].user_id for i in range(len(self))})\n            random.shuffle(user_ids)  # nosec\n            return np.array(user_ids)\n        return np.array(list({self.trips[i].user_id for i in range(len(self))}))\n\n    @property\n    def n_items(self) -&gt; int:\n        \"\"\"Return the number of items available in the dataset.\n\n        Returns\n        -------\n        int\n            Number of items available in the dataset\n        \"\"\"\n        return self.available_items.shape[1]\n\n    @property\n    def n_stores(self) -&gt; int:\n        \"\"\"Return the number of stores in the dataset.\n\n        Returns\n        -------\n        int\n            Number of stores in the dataset\n        \"\"\"\n        return len(self.get_all_stores())\n\n    @property\n    def n_users(self) -&gt; int:\n        \"\"\"Return the number of users in the dataset.\n\n        Returns\n        -------\n        int\n            Number of users in the dataset\n        \"\"\"\n        return len(self.get_all_users())\n\n    @property\n    def n_assortments(self) -&gt; int:\n        \"\"\"Return the number of assortments in the dataset.\n\n        Returns\n        -------\n        int\n            Number of assortments in the dataset\n        \"\"\"\n        return self.available_items.shape[0]\n\n    def get_one_vs_all_augmented_data_from_trip_index(\n        self,\n        trip_index: int,\n    ) -&gt; tuple[np.ndarray]:\n        \"\"\"Get augmented data from a trip index.\n\n        Augmented data consists in removing one item from the basket that will be used\n        as a target from the remaining items. It is done for all items, leading to returning:\n            - items,\n            - padded baskets with an item removed,\n            - stores,\n            - weeks,\n            - prices,\n            - available items.\n            - user_id\n\n        Parameters\n        ----------\n        trip_index: int\n            Index of the trip from which to get the data\n\n        Returns\n        -------\n        tuple[np.ndarray]\n            For each sample (ie transaction) from the trip:\n            item, basket, store, week, prices, available items\n            Length must be 6\n        \"\"\"\n        # Get the trip from the index\n        trip = self.trips[trip_index]\n        length_trip = len(trip.purchases)\n        permuted_purchases = np.array(trip.purchases)\n\n        # Create new baskets with one item removed that will be used as target\n        # (len(basket) new baskets created)\n        # And pad the truncated baskets with -1 to have the same length (because we need\n        # numpy arrays for tiling and numpy arrays must have the same length)\n        padded_purchases_lacking_one_item = np.array(\n            [\n                np.concatenate(\n                    (\n                        permuted_purchases[:i],\n                        # Pad the removed item with -1\n                        [-1],\n                        permuted_purchases[i + 1 :],\n                        # Pad to have the same length\n                        -1 * np.ones(self.max_length - length_trip),\n                    )\n                )\n                for i in range(0, length_trip)\n            ],\n            dtype=int,\n        )\n\n        if not (isinstance(trip.assortment, np.ndarray) or isinstance(trip.assortment, list)):\n            # Then it is the assortment ID (ie its index in self.available_items)\n            assortment = self.available_items[trip.assortment]\n        else:  # np.ndarray\n            # Then it is directly the availability matrix\n            assortment = trip.assortment\n\n        if not (isinstance(trip.prices, np.ndarray) or isinstance(trip.prices, list)):\n            # Then it is the assortment ID (ie its index in self.available_items)\n            prices = self.prices[trip.prices]\n        else:  # np.ndarray\n            # Then it is directly the availability matrix\n            prices = trip.prices\n\n        # Each item is linked to a basket, a store, a week, prices and an assortment\n        return (\n            permuted_purchases,  # Items\n            padded_purchases_lacking_one_item,  # Baskets\n            np.empty((0, self.max_length), dtype=int),  # Future purchases\n            np.full(length_trip, trip.store),  # Stores\n            np.full(length_trip, trip.week),  # Weeks\n            np.tile(prices, (length_trip, 1)),  # Prices\n            np.tile(assortment, (length_trip, 1)),  # Available items\n            np.full(length_trip, trip.user_id),  # User IDs\n        )\n\n    def get_subbaskets_augmented_data_from_trip_index(\n        self,\n        trip_index: int,\n    ) -&gt; tuple[np.ndarray]:\n        \"\"\"Get augmented data from a trip index.\n\n        Augmented data includes all the transactions obtained sequentially from the trip.\n        In particular, items in the basket are shuffled and sub-baskets are built iteratively\n        with the next item that will be used as a target. In particular, it leads to:\n            - permuted items,\n            - permuted, truncated and padded baskets,\n            - padded future purchases based on the baskets,\n            - stores,\n            - weeks,\n            - prices,\n            - available items.\n\n        Parameters\n        ----------\n        trip_index: int\n            Index of the trip from which to get the data\n\n        Returns\n        -------\n        tuple[np.ndarray]\n            For each sample (ie transaction) from the trip:\n            item, basket, future purchases, store, week, prices, available items\n            Length must be 7\n        \"\"\"\n        # Get the trip from the index\n        trip = self.trips[trip_index]\n        length_trip = len(trip.purchases)\n\n        # Draw a random permutation of the items in the basket without the checkout item 0\n        # TODO at a later stage: improve by sampling several permutations here\n        permutation_list = list(permutations(range(length_trip - 1)))\n        permutation = random.sample(permutation_list, 1)[0]  # nosec\n\n        # Permute the basket while keeping the checkout item 0 at the end\n        permuted_purchases = np.array([trip.purchases[j] for j in permutation] + [0])\n\n        # Truncate the baskets: for each batch sample, we consider the truncation possibilities\n        # ranging from an empty basket to the basket with all the elements except the checkout item\n        # And pad the truncated baskets with -1 to have the same length (because we need\n        # numpy arrays for tiling and numpy arrays must have the same length)\n        padded_truncated_purchases = np.array(\n            [\n                np.concatenate((permuted_purchases[:i], -1 * np.ones(self.max_length - i)))\n                for i in range(0, length_trip)\n            ],\n            dtype=int,\n        )\n\n        # padded_future_purchases are the complements of padded_truncated_purchases, ie the\n        # items that are not yet in the (permuted) basket but that we know will be purchased\n        # during the next steps of the trip\n        # Pad the future purchases with -1 to have the same length\n        padded_future_purchases = np.array(\n            [\n                np.concatenate(\n                    (\n                        permuted_purchases[i + 1 :],\n                        -1 * np.ones(self.max_length - len(permuted_purchases) + i + 1),\n                    )\n                )\n                for i in range(0, length_trip)\n            ],\n            dtype=int,\n        )\n\n        if isinstance(trip.assortment, int):\n            # Then it is the assortment ID (ie its index in self.available_items)\n            assortment = self.available_items[trip.assortment]\n        else:  # np.ndarray\n            # Then it is directly the availability matrix\n            assortment = trip.assortment\n\n        # Each item is linked to a basket, the future purchases,\n        # a store, a week, prices and an assortment\n        return (\n            permuted_purchases,  # Items\n            padded_truncated_purchases,  # Baskets\n            padded_future_purchases,  # Future purchases\n            np.full(length_trip, trip.store),  # Stores\n            np.full(length_trip, trip.week),  # Weeks\n            np.tile(trip.prices, (length_trip, 1)),  # Prices\n            np.tile(assortment, (length_trip, 1)),  # Available items\n            np.full(length_trip, trip.user_id),  # User IDs\n        )\n\n    def get_sequential_data_from_trip_index(\n        self,\n        trip_index: int,\n        sequence_length: int = 5,\n        n_future_purchases: int = 3,\n    ) -&gt; tuple[np.ndarray]:\n        \"\"\"Get augmented data from a trip index for sequential recommendation.\n\n        Parameters\n        ----------\n        trip_index: int\n            Index of the trip from which to get the data\n        sequence_length: int\n            Lenght of sequence we consider: example sequence_length=5 means\n            we consider the 5th item as target and the first 5 items as the basket.\n        n_future_purchases: int\n            Number of future purchases to consider: example n_future_purchases=3\n            means we consider the next 3 items after the target item as future purchases.\n\n        Returns\n        -------\n        tuple[np.ndarray]\n            For each sample (ie transaction) from the trip:\n            item, basket, future purchases, store, week, prices, available items, user_id\n        \"\"\"\n        # Get the trip from the index\n        trip = self.trips[trip_index]\n        purchases = np.array(trip.purchases)\n\n        padded_truncated_purchases = np.array(\n            [purchases[:sequence_length]],\n            dtype=int,\n        )\n\n        padded_future_purchases = np.array(\n            [\n                np.pad(\n                    purchases[sequence_length + 1 : sequence_length + 1 + n_future_purchases],\n                    (\n                        0,\n                        max(\n                            0,\n                            n_future_purchases\n                            - len(\n                                purchases[\n                                    sequence_length + 1 : sequence_length + 1 + n_future_purchases\n                                ]\n                            ),\n                        ),\n                    ),\n                    constant_values=-1,\n                )\n            ],\n            dtype=int,\n        )\n        if isinstance(trip.assortment, int):\n            # Then it is the assortment ID (ie its index in self.available_items)\n            assortment = self.available_items[trip.assortment]\n        else:  # np.ndarray\n            # Then it is directly the availability matrix\n            assortment = trip.assortment\n\n        return (\n            np.array([purchases[sequence_length]]),  # Items\n            padded_truncated_purchases,  # Baskets\n            padded_future_purchases,  # Future purchases\n            np.array([trip.store]),  # Stores\n            np.array([trip.week]),  # Weeks\n            np.array(trip.prices),  # Prices\n            np.array([assortment]),  # Available items\n            np.array([trip.user_id]),  # User IDs\n        )\n\n    def iter_batch(\n        self,\n        batch_size: int,\n        shuffle: bool = False,\n        data_method: str = \"shopper\",\n    ) -&gt; object:\n        \"\"\"Iterate over a TripDataset to return batches of items of length batch_size.\n\n        Parameters\n        ----------\n        batch_size: int\n            Batch size (number of items in the batch)\n        shuffle: bool\n            Whether or not to shuffle the dataset\n        data_method: str\n            Method used to generate sub-baskets from a purchased one. Available methods are:\n            - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:\n                         (1|0); (2|1); (3|1,2); (4|1,2,3); etc...\n            - 'aleacarta': creates all the sub-baskets with N-1 items:\n                           (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)\n\n        Yields\n        ------\n        tuple[np.ndarray]\n            For each item in the batch: item, basket, future purchases,\n            store, week, prices, available items, user_id\n        \"\"\"\n        # Get trip indexes\n        num_trips = len(self)\n        trip_indexes = np.arange(num_trips)\n\n        # Shuffle trip indexes\n        # TODO: shuffling on the trip indexes or on the item indexes?\n        if shuffle:\n            trip_indexes = np.random.default_rng().permutation(trip_indexes)\n\n        # Initialize the buffer\n        buffer = (\n            np.empty(0, dtype=int),  # Items\n            np.empty((0, self.max_length), dtype=int),  # Baskets\n            np.empty((0, self.max_length), dtype=int),  # Future purchases\n            np.empty(0, dtype=int),  # Stores\n            np.empty(0, dtype=int),  # Weeks\n            np.empty((0, self.n_items), dtype=int),  # Prices\n            np.empty((0, self.n_items), dtype=int),  # Available items\n            np.empty(0, dtype=int),  # User IDs\n        )\n\n        if data_method == \"sequential\":\n            buffer = (\n                np.empty(0, dtype=int),  # Items\n                np.empty((0, 5), dtype=int),  # Baskets\n                np.empty((0, 3), dtype=int),  # Future purchases\n                np.empty(0, dtype=int),  # Stores\n                np.empty(0, dtype=int),  # Weeks\n                np.empty((0, self.n_items), dtype=int),  # Prices\n                np.empty((0, self.n_items), dtype=int),  # Available items\n                np.empty(0, dtype=int),  # User IDs\n            )\n\n        if batch_size == -1:\n            # Get the whole dataset in one batch\n            for trip_index in trip_indexes:\n                if data_method == \"shopper\":\n                    additional_trip_data = self.get_subbaskets_augmented_data_from_trip_index(\n                        trip_index\n                    )\n                elif data_method == \"aleacarta\":\n                    additional_trip_data = self.get_one_vs_all_augmented_data_from_trip_index(\n                        trip_index\n                    )\n                elif data_method == \"sequential\":\n                    additional_trip_data = self.get_sequential_data_from_trip_index(trip_index)\n                else:\n                    raise ValueError(f\"Unknown data method: {data_method}\")\n\n                buffer = tuple(\n                    np.concatenate((buffer[i], additional_trip_data[i])) for i in range(len(buffer))\n                )\n\n            # Yield the whole dataset\n            yield buffer\n\n        else:\n            # Yield batches of size batch_size while going through all the trips\n            index = 0\n            outer_break = False\n            while index &lt; num_trips:\n                # Fill the buffer with trips' augmented data until it reaches the batch size\n                while len(buffer[0]) &lt; batch_size:\n                    if index &gt;= num_trips:\n                        # Then the buffer is not full but there are no more trips to consider\n                        # Yield the batch partially filled\n                        yield buffer\n\n                        # Exit the TWO while loops when all trips have been considered\n                        outer_break = True\n                        break\n\n                    else:\n                        # Consider a new trip to fill the buffer\n                        if data_method == \"shopper\":\n                            additional_trip_data = (\n                                self.get_subbaskets_augmented_data_from_trip_index(\n                                    trip_indexes[index]\n                                )\n                            )\n                        elif data_method == \"aleacarta\":\n                            additional_trip_data = (\n                                self.get_one_vs_all_augmented_data_from_trip_index(\n                                    trip_indexes[index]\n                                )\n                            )\n                        elif data_method == \"sequential\":\n                            additional_trip_data = self.get_sequential_data_from_trip_index(\n                                trip_indexes[index]\n                            )\n                        else:\n                            raise ValueError(f\"Unknown data method: {data_method}\")\n                        index += 1\n\n                        # Fill the buffer with the new trip\n                        buffer = tuple(\n                            np.concatenate((buffer[i], additional_trip_data[i]))\n                            for i in range(len(buffer))\n                        )\n\n                if outer_break:\n                    break\n\n                # Once the buffer is full, get the batch and update the next buffer\n                batch = tuple(buffer[i][:batch_size] for i in range(len(buffer)))\n                buffer = tuple(buffer[i][batch_size:] for i in range(len(buffer)))\n\n                # Yield the batch\n                yield batch\n\n    def __getitem__(self, index: Union[int, list, np.ndarray, range, slice]) -&gt; object:\n        \"\"\"Return a TripDataset object populated with the trips at index.\n\n        Parameters\n        ----------\n        index: int, list[int], np.ndarray, range or list\n            Index or list of indices of the trip(s) to get\n\n        Returns\n        -------\n        Trip or list[Trip]\n            Trip at the given index or list of trips at the given indices\n        \"\"\"\n        if isinstance(index, int):\n            return TripDataset(\n                trips=[self.trips[index]],\n                available_items=self.available_items,\n            )\n        if isinstance(index, (list, np.ndarray, range)):\n            return TripDataset(\n                trips=[self.trips[i] for i in index],\n                available_items=self.available_items,\n            )\n        if isinstance(index, slice):\n            return TripDataset(\n                trips=self.trips[index],\n                available_items=self.available_items,\n            )\n\n        raise TypeError(\"Type of index must be int, list, np.ndarray, range or slice.\")\n\n    def iter_batch_evaluate(\n        self,\n        trip_batch_size: int,\n    ) -&gt; object:\n        \"\"\"Iterate over a TripDataset to return batches w/ trip_batch_size Trips's subsamples.\n\n        Parameters\n        ----------\n        trip_batch_size: int\n            Batch size (number of Trips in the batch)\n\n        Yields\n        ------\n        tuple[np.ndarray]\n            For each item in the batch: item, basket, future purchases,\n            store, week, prices, available items\n            Length must 8\n        \"\"\"\n        # Get trip indexes\n        num_trips = len(self)\n        trip_indexes = np.arange(num_trips)\n\n        # Initialize the buffer\n        buffer = (\n            np.empty(0, dtype=int),  # Items\n            np.empty((0, self.max_length), dtype=int),  # Baskets\n            np.empty((0, self.max_length), dtype=int),  # Future purchases\n            np.empty(0, dtype=int),  # Stores\n            np.empty(0, dtype=int),  # Weeks\n            np.empty((0, self.n_items), dtype=int),  # Prices\n            np.empty((0, self.n_items), dtype=int),  # Available items\n        )\n\n        if trip_batch_size == -1:\n            # Get the whole dataset in one batch\n            identifiers = []\n            for trip_index in trip_indexes:\n                additional_trip_data = self.get_one_vs_all_augmented_data_from_trip_index(\n                    trip_index\n                )\n                buffer = tuple(\n                    np.concatenate((buffer[i], additional_trip_data[i])) for i in range(len(buffer))\n                )\n                identifiers.extend([trip_index] * len(additional_trip_data[0]))\n\n            # Yield the whole dataset\n            yield buffer, np.array(identifiers)\n\n        else:\n            # Yield batches of size batch_size while going through all the trips\n            index = 0\n            outer_break = False\n            while index &lt; num_trips:\n                trip_identifier = []\n                buffer = (\n                    np.empty(0, dtype=int),  # Items\n                    np.empty((0, self.max_length), dtype=int),  # Baskets\n                    np.empty((0, self.max_length), dtype=int),  # Future purchases\n                    np.empty(0, dtype=int),  # Stores\n                    np.empty(0, dtype=int),  # Weeks\n                    np.empty((0, self.n_items), dtype=int),  # Prices\n                    np.empty((0, self.n_items), dtype=int),  # Available items\n                    np.empty(0, dtype=int),  # Users\n                )\n                while np.max(trip_identifier, initial=-1) + 1 &lt; trip_batch_size:\n                    if index &gt;= num_trips:\n                        # Then the buffer is not full but there are no more trips to consider\n                        # Yield the batch partially filled\n                        yield buffer, np.array(trip_identifier)\n\n                        # Exit the TWO while loops when all trips have been considered\n                        outer_break = True\n                        break\n\n                    else:\n                        # Consider a new trip to fill the buffer\n                        additional_trip_data = self.get_one_vs_all_augmented_data_from_trip_index(\n                            trip_indexes[index]\n                        )\n                        index += 1\n\n                        # Fill the buffer with the new trip\n                        buffer = tuple(\n                            np.concatenate((buffer[i], additional_trip_data[i]))\n                            for i in range(len(buffer))\n                        )\n                        trip_identifier.extend(\n                            [np.max(trip_identifier, initial=-1) + 1] * len(additional_trip_data[0])\n                        )\n\n                if outer_break:\n                    break\n\n                # Yield the batch\n                yield buffer, np.array(trip_identifier)\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.n_assortments","title":"<code>n_assortments: int</code>  <code>property</code>","text":"<p>Return the number of assortments in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of assortments in the dataset</p>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.n_items","title":"<code>n_items: int</code>  <code>property</code>","text":"<p>Return the number of items available in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of items available in the dataset</p>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.n_stores","title":"<code>n_stores: int</code>  <code>property</code>","text":"<p>Return the number of stores in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of stores in the dataset</p>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.n_users","title":"<code>n_users: int</code>  <code>property</code>","text":"<p>Return the number of users in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of users in the dataset</p>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return a TripDataset object populated with the trips at index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, list, ndarray, range, slice]</code> <p>Index or list of indices of the trip(s) to get</p> required <p>Returns:</p> Type Description <code>Trip or list[Trip]</code> <p>Trip at the given index or list of trips at the given indices</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __getitem__(self, index: Union[int, list, np.ndarray, range, slice]) -&gt; object:\n    \"\"\"Return a TripDataset object populated with the trips at index.\n\n    Parameters\n    ----------\n    index: int, list[int], np.ndarray, range or list\n        Index or list of indices of the trip(s) to get\n\n    Returns\n    -------\n    Trip or list[Trip]\n        Trip at the given index or list of trips at the given indices\n    \"\"\"\n    if isinstance(index, int):\n        return TripDataset(\n            trips=[self.trips[index]],\n            available_items=self.available_items,\n        )\n    if isinstance(index, (list, np.ndarray, range)):\n        return TripDataset(\n            trips=[self.trips[i] for i in index],\n            available_items=self.available_items,\n        )\n    if isinstance(index, slice):\n        return TripDataset(\n            trips=self.trips[index],\n            available_items=self.available_items,\n        )\n\n    raise TypeError(\"Type of index must be int, list, np.ndarray, range or slice.\")\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.__init__","title":"<code>__init__(trips, available_items)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>trips</code> <code>list[Trip]</code> <p>List of trips Length must be n_trips</p> required <code>available_items</code> <code>ndarray</code> <p>Array of availability matrices available_items[i]: availability matrix of the assortment whose ID is i (The availability matrix is a binary vector of length n_items where 1 means the item is available and 0 means the item is not available) Shape must be (n_assortments, n_items)</p> required Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __init__(\n    self,\n    trips: list[Trip],\n    available_items: np.ndarray,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Parameters\n    ----------\n    trips: list[Trip]\n        List of trips\n        Length must be n_trips\n    available_items: np.ndarray\n        Array of availability matrices\n        available_items[i]: availability matrix of the assortment whose ID is i\n        (The availability matrix is a binary vector of length n_items\n        where 1 means the item is available and 0 means the item is not available)\n        Shape must be (n_assortments, n_items)\n    \"\"\"\n    self.trips = trips\n    self.max_length = max([trip.trip_length for trip in self.trips])\n    self.n_samples = len(self.get_transactions())\n    self.available_items = available_items\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the trips in the dataset.</p> <p>Returns:</p> Type Description <code>iter</code> <p>Iterator over the trips</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __iter__(self) -&gt; iter:\n    \"\"\"Iterate over the trips in the dataset.\n\n    Returns\n    -------\n    iter\n        Iterator over the trips\n    \"\"\"\n    return iter(self.trips)\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of trips in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of trips in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of trips in the dataset.\n\n    Returns\n    -------\n    int\n        Number of trips in the dataset\n    \"\"\"\n    return len(self.trips)\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.__str__","title":"<code>__str__()</code>","text":"<p>Return short representation of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>Representation of the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return short representation of the dataset.\n\n    Returns\n    -------\n    str\n        Representation of the dataset\n    \"\"\"\n    return f\"TripDataset with {len(self)} trips\"\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.concatenate","title":"<code>concatenate(other, inplace=False)</code>","text":"<p>Add a dataset to another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>Dataset to add</p> required <code>inplace</code> <code>bool</code> <p>Whether to add the dataset in-place or not, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>TripDataset</code> <p>Concatenated dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def concatenate(self, other: object, inplace: bool = False) -&gt; object:\n    \"\"\"Add a dataset to another.\n\n    Parameters\n    ----------\n    other: TripDataset\n        Dataset to add\n    inplace: bool\n        Whether to add the dataset in-place or not, by default False\n\n    Returns\n    -------\n    TripDataset\n        Concatenated dataset\n    \"\"\"\n    if inplace:  # Add another dataset to the current one (in-place)\n        # Concatenate the list of trips\n        self.trips += other.trips\n        # Update the attributes of the TripDataset\n        self.max_length = max([trip.trip_length for trip in self.trips])\n        self.n_samples = len(self.get_transactions())\n        # Concatenate the arrays of availability matrices\n        # /!\\ When concatenating 2 TripDatasets, the indices of the availability matrices\n        # changes\n        self.available_items = np.concatenate(\n            (self.available_items, other.available_items), axis=0\n        )\n        return self\n\n    # Else: create a new dataset by adding 2 datasets together\n    return TripDataset(\n        # Concatenate the list of trips\n        trips=self.trips + other.trips,\n        # Concatenate the arrays of availability matrices\n        # /!\\ When concatenating 2 TripDatasets, the indices of the availability matrices\n        # changes\n        available_items=np.concatenate((self.available_items, other.available_items), axis=0),\n    )\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_all_baskets","title":"<code>get_all_baskets()</code>","text":"<p>Return the list of all baskets in the dataset.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of baskets in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_all_baskets(self) -&gt; np.ndarray:\n    \"\"\"Return the list of all baskets in the dataset.\n\n    Returns\n    -------\n    np.ndarray\n        List of baskets in the dataset\n    \"\"\"\n    return np.array([self.trips[i].purchases for i in range(len(self))])\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_all_items","title":"<code>get_all_items()</code>","text":"<p>Return the list of all items available in the dataset.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of items available in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_all_items(self) -&gt; np.ndarray:\n    \"\"\"Return the list of all items available in the dataset.\n\n    Returns\n    -------\n    np.ndarray\n        List of items available in the dataset\n    \"\"\"\n    return np.arange(self.n_items)\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_all_prices","title":"<code>get_all_prices()</code>","text":"<p>Return the list of all price arrays in the dataset.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of price arrays in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_all_prices(self) -&gt; np.ndarray:\n    \"\"\"Return the list of all price arrays in the dataset.\n\n    Returns\n    -------\n    np.ndarray\n        List of price arrays in the dataset\n    \"\"\"\n    return np.array([self.trips[i].prices for i in range(len(self))])\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_all_stores","title":"<code>get_all_stores()</code>","text":"<p>Return the list of all stores in the dataset.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of stores in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_all_stores(self) -&gt; np.ndarray:\n    \"\"\"Return the list of all stores in the dataset.\n\n    Returns\n    -------\n    np.ndarray\n        List of stores in the dataset\n    \"\"\"\n    # If preprocessing working well, equal to [0, 1, ..., n_stores - 1]\n    return np.array(list({self.trips[i].store for i in range(len(self))}))\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_all_users","title":"<code>get_all_users(shuffled=False)</code>","text":"<p>Return the list of all users in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>shuffled</code> <code>bool</code> <p>Whether to shuffle the list of users or not, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of users in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_all_users(self, shuffled: bool = False) -&gt; np.ndarray:\n    \"\"\"Return the list of all users in the dataset.\n\n    Parameters\n    ----------\n    shuffled: bool\n        Whether to shuffle the list of users or not, by default False\n\n    Returns\n    -------\n    np.ndarray\n        List of users in the dataset\n    \"\"\"\n    if shuffled:\n        user_ids = list({self.trips[i].user_id for i in range(len(self))})\n        random.shuffle(user_ids)  # nosec\n        return np.array(user_ids)\n    return np.array(list({self.trips[i].user_id for i in range(len(self))}))\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_all_weeks","title":"<code>get_all_weeks()</code>","text":"<p>Return the list of all weeks in the dataset.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of weeks in the dataset</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_all_weeks(self) -&gt; np.ndarray:\n    \"\"\"Return the list of all weeks in the dataset.\n\n    Returns\n    -------\n    np.ndarray\n        List of weeks in the dataset\n    \"\"\"\n    # If preprocessing working well, equal to [0, 1, ..., 51 or 52]\n    return np.array(list({self.trips[i].week for i in range(len(self))}))\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_one_vs_all_augmented_data_from_trip_index","title":"<code>get_one_vs_all_augmented_data_from_trip_index(trip_index)</code>","text":"<p>Get augmented data from a trip index.</p> <p>Augmented data consists in removing one item from the basket that will be used as a target from the remaining items. It is done for all items, leading to returning:     - items,     - padded baskets with an item removed,     - stores,     - weeks,     - prices,     - available items.     - user_id</p> <p>Parameters:</p> Name Type Description Default <code>trip_index</code> <code>int</code> <p>Index of the trip from which to get the data</p> required <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>For each sample (ie transaction) from the trip: item, basket, store, week, prices, available items Length must be 6</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_one_vs_all_augmented_data_from_trip_index(\n    self,\n    trip_index: int,\n) -&gt; tuple[np.ndarray]:\n    \"\"\"Get augmented data from a trip index.\n\n    Augmented data consists in removing one item from the basket that will be used\n    as a target from the remaining items. It is done for all items, leading to returning:\n        - items,\n        - padded baskets with an item removed,\n        - stores,\n        - weeks,\n        - prices,\n        - available items.\n        - user_id\n\n    Parameters\n    ----------\n    trip_index: int\n        Index of the trip from which to get the data\n\n    Returns\n    -------\n    tuple[np.ndarray]\n        For each sample (ie transaction) from the trip:\n        item, basket, store, week, prices, available items\n        Length must be 6\n    \"\"\"\n    # Get the trip from the index\n    trip = self.trips[trip_index]\n    length_trip = len(trip.purchases)\n    permuted_purchases = np.array(trip.purchases)\n\n    # Create new baskets with one item removed that will be used as target\n    # (len(basket) new baskets created)\n    # And pad the truncated baskets with -1 to have the same length (because we need\n    # numpy arrays for tiling and numpy arrays must have the same length)\n    padded_purchases_lacking_one_item = np.array(\n        [\n            np.concatenate(\n                (\n                    permuted_purchases[:i],\n                    # Pad the removed item with -1\n                    [-1],\n                    permuted_purchases[i + 1 :],\n                    # Pad to have the same length\n                    -1 * np.ones(self.max_length - length_trip),\n                )\n            )\n            for i in range(0, length_trip)\n        ],\n        dtype=int,\n    )\n\n    if not (isinstance(trip.assortment, np.ndarray) or isinstance(trip.assortment, list)):\n        # Then it is the assortment ID (ie its index in self.available_items)\n        assortment = self.available_items[trip.assortment]\n    else:  # np.ndarray\n        # Then it is directly the availability matrix\n        assortment = trip.assortment\n\n    if not (isinstance(trip.prices, np.ndarray) or isinstance(trip.prices, list)):\n        # Then it is the assortment ID (ie its index in self.available_items)\n        prices = self.prices[trip.prices]\n    else:  # np.ndarray\n        # Then it is directly the availability matrix\n        prices = trip.prices\n\n    # Each item is linked to a basket, a store, a week, prices and an assortment\n    return (\n        permuted_purchases,  # Items\n        padded_purchases_lacking_one_item,  # Baskets\n        np.empty((0, self.max_length), dtype=int),  # Future purchases\n        np.full(length_trip, trip.store),  # Stores\n        np.full(length_trip, trip.week),  # Weeks\n        np.tile(prices, (length_trip, 1)),  # Prices\n        np.tile(assortment, (length_trip, 1)),  # Available items\n        np.full(length_trip, trip.user_id),  # User IDs\n    )\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_sequential_data_from_trip_index","title":"<code>get_sequential_data_from_trip_index(trip_index, sequence_length=5, n_future_purchases=3)</code>","text":"<p>Get augmented data from a trip index for sequential recommendation.</p> <p>Parameters:</p> Name Type Description Default <code>trip_index</code> <code>int</code> <p>Index of the trip from which to get the data</p> required <code>sequence_length</code> <code>int</code> <p>Lenght of sequence we consider: example sequence_length=5 means we consider the 5th item as target and the first 5 items as the basket.</p> <code>5</code> <code>n_future_purchases</code> <code>int</code> <p>Number of future purchases to consider: example n_future_purchases=3 means we consider the next 3 items after the target item as future purchases.</p> <code>3</code> <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>For each sample (ie transaction) from the trip: item, basket, future purchases, store, week, prices, available items, user_id</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_sequential_data_from_trip_index(\n    self,\n    trip_index: int,\n    sequence_length: int = 5,\n    n_future_purchases: int = 3,\n) -&gt; tuple[np.ndarray]:\n    \"\"\"Get augmented data from a trip index for sequential recommendation.\n\n    Parameters\n    ----------\n    trip_index: int\n        Index of the trip from which to get the data\n    sequence_length: int\n        Lenght of sequence we consider: example sequence_length=5 means\n        we consider the 5th item as target and the first 5 items as the basket.\n    n_future_purchases: int\n        Number of future purchases to consider: example n_future_purchases=3\n        means we consider the next 3 items after the target item as future purchases.\n\n    Returns\n    -------\n    tuple[np.ndarray]\n        For each sample (ie transaction) from the trip:\n        item, basket, future purchases, store, week, prices, available items, user_id\n    \"\"\"\n    # Get the trip from the index\n    trip = self.trips[trip_index]\n    purchases = np.array(trip.purchases)\n\n    padded_truncated_purchases = np.array(\n        [purchases[:sequence_length]],\n        dtype=int,\n    )\n\n    padded_future_purchases = np.array(\n        [\n            np.pad(\n                purchases[sequence_length + 1 : sequence_length + 1 + n_future_purchases],\n                (\n                    0,\n                    max(\n                        0,\n                        n_future_purchases\n                        - len(\n                            purchases[\n                                sequence_length + 1 : sequence_length + 1 + n_future_purchases\n                            ]\n                        ),\n                    ),\n                ),\n                constant_values=-1,\n            )\n        ],\n        dtype=int,\n    )\n    if isinstance(trip.assortment, int):\n        # Then it is the assortment ID (ie its index in self.available_items)\n        assortment = self.available_items[trip.assortment]\n    else:  # np.ndarray\n        # Then it is directly the availability matrix\n        assortment = trip.assortment\n\n    return (\n        np.array([purchases[sequence_length]]),  # Items\n        padded_truncated_purchases,  # Baskets\n        padded_future_purchases,  # Future purchases\n        np.array([trip.store]),  # Stores\n        np.array([trip.week]),  # Weeks\n        np.array(trip.prices),  # Prices\n        np.array([assortment]),  # Available items\n        np.array([trip.user_id]),  # User IDs\n    )\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_subbaskets_augmented_data_from_trip_index","title":"<code>get_subbaskets_augmented_data_from_trip_index(trip_index)</code>","text":"<p>Get augmented data from a trip index.</p> <p>Augmented data includes all the transactions obtained sequentially from the trip. In particular, items in the basket are shuffled and sub-baskets are built iteratively with the next item that will be used as a target. In particular, it leads to:     - permuted items,     - permuted, truncated and padded baskets,     - padded future purchases based on the baskets,     - stores,     - weeks,     - prices,     - available items.</p> <p>Parameters:</p> Name Type Description Default <code>trip_index</code> <code>int</code> <p>Index of the trip from which to get the data</p> required <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>For each sample (ie transaction) from the trip: item, basket, future purchases, store, week, prices, available items Length must be 7</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_subbaskets_augmented_data_from_trip_index(\n    self,\n    trip_index: int,\n) -&gt; tuple[np.ndarray]:\n    \"\"\"Get augmented data from a trip index.\n\n    Augmented data includes all the transactions obtained sequentially from the trip.\n    In particular, items in the basket are shuffled and sub-baskets are built iteratively\n    with the next item that will be used as a target. In particular, it leads to:\n        - permuted items,\n        - permuted, truncated and padded baskets,\n        - padded future purchases based on the baskets,\n        - stores,\n        - weeks,\n        - prices,\n        - available items.\n\n    Parameters\n    ----------\n    trip_index: int\n        Index of the trip from which to get the data\n\n    Returns\n    -------\n    tuple[np.ndarray]\n        For each sample (ie transaction) from the trip:\n        item, basket, future purchases, store, week, prices, available items\n        Length must be 7\n    \"\"\"\n    # Get the trip from the index\n    trip = self.trips[trip_index]\n    length_trip = len(trip.purchases)\n\n    # Draw a random permutation of the items in the basket without the checkout item 0\n    # TODO at a later stage: improve by sampling several permutations here\n    permutation_list = list(permutations(range(length_trip - 1)))\n    permutation = random.sample(permutation_list, 1)[0]  # nosec\n\n    # Permute the basket while keeping the checkout item 0 at the end\n    permuted_purchases = np.array([trip.purchases[j] for j in permutation] + [0])\n\n    # Truncate the baskets: for each batch sample, we consider the truncation possibilities\n    # ranging from an empty basket to the basket with all the elements except the checkout item\n    # And pad the truncated baskets with -1 to have the same length (because we need\n    # numpy arrays for tiling and numpy arrays must have the same length)\n    padded_truncated_purchases = np.array(\n        [\n            np.concatenate((permuted_purchases[:i], -1 * np.ones(self.max_length - i)))\n            for i in range(0, length_trip)\n        ],\n        dtype=int,\n    )\n\n    # padded_future_purchases are the complements of padded_truncated_purchases, ie the\n    # items that are not yet in the (permuted) basket but that we know will be purchased\n    # during the next steps of the trip\n    # Pad the future purchases with -1 to have the same length\n    padded_future_purchases = np.array(\n        [\n            np.concatenate(\n                (\n                    permuted_purchases[i + 1 :],\n                    -1 * np.ones(self.max_length - len(permuted_purchases) + i + 1),\n                )\n            )\n            for i in range(0, length_trip)\n        ],\n        dtype=int,\n    )\n\n    if isinstance(trip.assortment, int):\n        # Then it is the assortment ID (ie its index in self.available_items)\n        assortment = self.available_items[trip.assortment]\n    else:  # np.ndarray\n        # Then it is directly the availability matrix\n        assortment = trip.assortment\n\n    # Each item is linked to a basket, the future purchases,\n    # a store, a week, prices and an assortment\n    return (\n        permuted_purchases,  # Items\n        padded_truncated_purchases,  # Baskets\n        padded_future_purchases,  # Future purchases\n        np.full(length_trip, trip.store),  # Stores\n        np.full(length_trip, trip.week),  # Weeks\n        np.tile(trip.prices, (length_trip, 1)),  # Prices\n        np.tile(assortment, (length_trip, 1)),  # Available items\n        np.full(length_trip, trip.user_id),  # User IDs\n    )\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_transactions","title":"<code>get_transactions()</code>","text":"<p>Return the transactions of the TripDataset.</p> <p>One transaction is a quadruplet (store, trip, item, user_id).</p> <p>Returns:</p> Type Description <code>dict</code> <p>Transactions of the TripDataset keys: trans_id values: (store, trip, item)</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_transactions(self) -&gt; np.ndarray:\n    \"\"\"Return the transactions of the TripDataset.\n\n    One transaction is a quadruplet (store, trip, item, user_id).\n\n    Returns\n    -------\n    dict\n        Transactions of the TripDataset\n        keys: trans_id\n        values: (store, trip, item)\n    \"\"\"\n    transactions = {}\n\n    trans_id = 0\n    for i, trip in enumerate(self.trips):\n        for item in trip.purchases:\n            transactions[trans_id] = (trip.store, i, item, trip.user_id)\n            trans_id += 1\n\n    return transactions\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.get_trip","title":"<code>get_trip(index)</code>","text":"<p>Return the trip at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the trip to get</p> required <p>Returns:</p> Type Description <code>Trip</code> <p>Trip at the given index</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def get_trip(self, index: int) -&gt; Trip:\n    \"\"\"Return the trip at the given index.\n\n    Parameters\n    ----------\n    index: int\n        Index of the trip to get\n\n    Returns\n    -------\n    Trip\n        Trip at the given index\n    \"\"\"\n    return self.trips[index]\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.iter_batch","title":"<code>iter_batch(batch_size, shuffle=False, data_method='shopper')</code>","text":"<p>Iterate over a TripDataset to return batches of items of length batch_size.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size (number of items in the batch)</p> required <code>shuffle</code> <code>bool</code> <p>Whether or not to shuffle the dataset</p> <code>False</code> <code>data_method</code> <code>str</code> <p>Method used to generate sub-baskets from a purchased one. Available methods are: - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:              (1|0); (2|1); (3|1,2); (4|1,2,3); etc... - 'aleacarta': creates all the sub-baskets with N-1 items:                (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)</p> <code>'shopper'</code> <p>Yields:</p> Type Description <code>tuple[ndarray]</code> <p>For each item in the batch: item, basket, future purchases, store, week, prices, available items, user_id</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def iter_batch(\n    self,\n    batch_size: int,\n    shuffle: bool = False,\n    data_method: str = \"shopper\",\n) -&gt; object:\n    \"\"\"Iterate over a TripDataset to return batches of items of length batch_size.\n\n    Parameters\n    ----------\n    batch_size: int\n        Batch size (number of items in the batch)\n    shuffle: bool\n        Whether or not to shuffle the dataset\n    data_method: str\n        Method used to generate sub-baskets from a purchased one. Available methods are:\n        - 'shopper': randomly orders the purchases and creates the ordered sub-baskets:\n                     (1|0); (2|1); (3|1,2); (4|1,2,3); etc...\n        - 'aleacarta': creates all the sub-baskets with N-1 items:\n                       (4|1,2,3); (3|1,2,4); (2|1,3,4); (1|2,3,4)\n\n    Yields\n    ------\n    tuple[np.ndarray]\n        For each item in the batch: item, basket, future purchases,\n        store, week, prices, available items, user_id\n    \"\"\"\n    # Get trip indexes\n    num_trips = len(self)\n    trip_indexes = np.arange(num_trips)\n\n    # Shuffle trip indexes\n    # TODO: shuffling on the trip indexes or on the item indexes?\n    if shuffle:\n        trip_indexes = np.random.default_rng().permutation(trip_indexes)\n\n    # Initialize the buffer\n    buffer = (\n        np.empty(0, dtype=int),  # Items\n        np.empty((0, self.max_length), dtype=int),  # Baskets\n        np.empty((0, self.max_length), dtype=int),  # Future purchases\n        np.empty(0, dtype=int),  # Stores\n        np.empty(0, dtype=int),  # Weeks\n        np.empty((0, self.n_items), dtype=int),  # Prices\n        np.empty((0, self.n_items), dtype=int),  # Available items\n        np.empty(0, dtype=int),  # User IDs\n    )\n\n    if data_method == \"sequential\":\n        buffer = (\n            np.empty(0, dtype=int),  # Items\n            np.empty((0, 5), dtype=int),  # Baskets\n            np.empty((0, 3), dtype=int),  # Future purchases\n            np.empty(0, dtype=int),  # Stores\n            np.empty(0, dtype=int),  # Weeks\n            np.empty((0, self.n_items), dtype=int),  # Prices\n            np.empty((0, self.n_items), dtype=int),  # Available items\n            np.empty(0, dtype=int),  # User IDs\n        )\n\n    if batch_size == -1:\n        # Get the whole dataset in one batch\n        for trip_index in trip_indexes:\n            if data_method == \"shopper\":\n                additional_trip_data = self.get_subbaskets_augmented_data_from_trip_index(\n                    trip_index\n                )\n            elif data_method == \"aleacarta\":\n                additional_trip_data = self.get_one_vs_all_augmented_data_from_trip_index(\n                    trip_index\n                )\n            elif data_method == \"sequential\":\n                additional_trip_data = self.get_sequential_data_from_trip_index(trip_index)\n            else:\n                raise ValueError(f\"Unknown data method: {data_method}\")\n\n            buffer = tuple(\n                np.concatenate((buffer[i], additional_trip_data[i])) for i in range(len(buffer))\n            )\n\n        # Yield the whole dataset\n        yield buffer\n\n    else:\n        # Yield batches of size batch_size while going through all the trips\n        index = 0\n        outer_break = False\n        while index &lt; num_trips:\n            # Fill the buffer with trips' augmented data until it reaches the batch size\n            while len(buffer[0]) &lt; batch_size:\n                if index &gt;= num_trips:\n                    # Then the buffer is not full but there are no more trips to consider\n                    # Yield the batch partially filled\n                    yield buffer\n\n                    # Exit the TWO while loops when all trips have been considered\n                    outer_break = True\n                    break\n\n                else:\n                    # Consider a new trip to fill the buffer\n                    if data_method == \"shopper\":\n                        additional_trip_data = (\n                            self.get_subbaskets_augmented_data_from_trip_index(\n                                trip_indexes[index]\n                            )\n                        )\n                    elif data_method == \"aleacarta\":\n                        additional_trip_data = (\n                            self.get_one_vs_all_augmented_data_from_trip_index(\n                                trip_indexes[index]\n                            )\n                        )\n                    elif data_method == \"sequential\":\n                        additional_trip_data = self.get_sequential_data_from_trip_index(\n                            trip_indexes[index]\n                        )\n                    else:\n                        raise ValueError(f\"Unknown data method: {data_method}\")\n                    index += 1\n\n                    # Fill the buffer with the new trip\n                    buffer = tuple(\n                        np.concatenate((buffer[i], additional_trip_data[i]))\n                        for i in range(len(buffer))\n                    )\n\n            if outer_break:\n                break\n\n            # Once the buffer is full, get the batch and update the next buffer\n            batch = tuple(buffer[i][:batch_size] for i in range(len(buffer)))\n            buffer = tuple(buffer[i][batch_size:] for i in range(len(buffer)))\n\n            # Yield the batch\n            yield batch\n</code></pre>"},{"location":"references/basket_models/data/references_dataset/#choice_learn.basket_models.data.basket_dataset.TripDataset.iter_batch_evaluate","title":"<code>iter_batch_evaluate(trip_batch_size)</code>","text":"<p>Iterate over a TripDataset to return batches w/ trip_batch_size Trips's subsamples.</p> <p>Parameters:</p> Name Type Description Default <code>trip_batch_size</code> <code>int</code> <p>Batch size (number of Trips in the batch)</p> required <p>Yields:</p> Type Description <code>tuple[ndarray]</code> <p>For each item in the batch: item, basket, future purchases, store, week, prices, available items Length must 8</p> Source code in <code>choice_learn/basket_models/data/basket_dataset.py</code> <pre><code>def iter_batch_evaluate(\n    self,\n    trip_batch_size: int,\n) -&gt; object:\n    \"\"\"Iterate over a TripDataset to return batches w/ trip_batch_size Trips's subsamples.\n\n    Parameters\n    ----------\n    trip_batch_size: int\n        Batch size (number of Trips in the batch)\n\n    Yields\n    ------\n    tuple[np.ndarray]\n        For each item in the batch: item, basket, future purchases,\n        store, week, prices, available items\n        Length must 8\n    \"\"\"\n    # Get trip indexes\n    num_trips = len(self)\n    trip_indexes = np.arange(num_trips)\n\n    # Initialize the buffer\n    buffer = (\n        np.empty(0, dtype=int),  # Items\n        np.empty((0, self.max_length), dtype=int),  # Baskets\n        np.empty((0, self.max_length), dtype=int),  # Future purchases\n        np.empty(0, dtype=int),  # Stores\n        np.empty(0, dtype=int),  # Weeks\n        np.empty((0, self.n_items), dtype=int),  # Prices\n        np.empty((0, self.n_items), dtype=int),  # Available items\n    )\n\n    if trip_batch_size == -1:\n        # Get the whole dataset in one batch\n        identifiers = []\n        for trip_index in trip_indexes:\n            additional_trip_data = self.get_one_vs_all_augmented_data_from_trip_index(\n                trip_index\n            )\n            buffer = tuple(\n                np.concatenate((buffer[i], additional_trip_data[i])) for i in range(len(buffer))\n            )\n            identifiers.extend([trip_index] * len(additional_trip_data[0]))\n\n        # Yield the whole dataset\n        yield buffer, np.array(identifiers)\n\n    else:\n        # Yield batches of size batch_size while going through all the trips\n        index = 0\n        outer_break = False\n        while index &lt; num_trips:\n            trip_identifier = []\n            buffer = (\n                np.empty(0, dtype=int),  # Items\n                np.empty((0, self.max_length), dtype=int),  # Baskets\n                np.empty((0, self.max_length), dtype=int),  # Future purchases\n                np.empty(0, dtype=int),  # Stores\n                np.empty(0, dtype=int),  # Weeks\n                np.empty((0, self.n_items), dtype=int),  # Prices\n                np.empty((0, self.n_items), dtype=int),  # Available items\n                np.empty(0, dtype=int),  # Users\n            )\n            while np.max(trip_identifier, initial=-1) + 1 &lt; trip_batch_size:\n                if index &gt;= num_trips:\n                    # Then the buffer is not full but there are no more trips to consider\n                    # Yield the batch partially filled\n                    yield buffer, np.array(trip_identifier)\n\n                    # Exit the TWO while loops when all trips have been considered\n                    outer_break = True\n                    break\n\n                else:\n                    # Consider a new trip to fill the buffer\n                    additional_trip_data = self.get_one_vs_all_augmented_data_from_trip_index(\n                        trip_indexes[index]\n                    )\n                    index += 1\n\n                    # Fill the buffer with the new trip\n                    buffer = tuple(\n                        np.concatenate((buffer[i], additional_trip_data[i]))\n                        for i in range(len(buffer))\n                    )\n                    trip_identifier.extend(\n                        [np.max(trip_identifier, initial=-1) + 1] * len(additional_trip_data[0])\n                    )\n\n            if outer_break:\n                break\n\n            # Yield the batch\n            yield buffer, np.array(trip_identifier)\n</code></pre>"},{"location":"references/basket_models/data/references_preprocessing/","title":"Data Preprocessing for Basket Models","text":"<p>Datasets loader.</p>"},{"location":"references/basket_models/data/references_preprocessing/#choice_learn.basket_models.data.preprocessing.csv_to_df","title":"<code>csv_to_df(data_file_name, data_module=OS_DATA_MODULE, sep='')</code>","text":"<p>Load and return the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <code>str</code> <p>Name of the csv file to load</p> required <code>data_module</code> <code>str</code> <p>Path to directory containing the data file, by default DATA_MODULE</p> <code>OS_DATA_MODULE</code> <code>encoding</code> <p>Encoding method of file, by default \"utf-8\"</p> required <code>sep</code> <code>str</code> <p>Separator used in the csv file, by default ''</p> <code>''</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Loaded dataset</p> Source code in <code>choice_learn/basket_models/data/preprocessing.py</code> <pre><code>def csv_to_df(\n    data_file_name: str, data_module: str = OS_DATA_MODULE, sep: str = \"\"\n) -&gt; pd.DataFrame:\n    \"\"\"Load and return the dataset.\n\n    Parameters\n    ----------\n    data_file_name: str\n        Name of the csv file to load\n    data_module: str, optional\n        Path to directory containing the data file, by default DATA_MODULE\n    encoding: str, optional\n        Encoding method of file, by default \"utf-8\"\n    sep: str, optional\n        Separator used in the csv file, by default ''\n\n    Returns\n    -------\n    pd.DataFrame\n        Loaded dataset\n    \"\"\"\n    path = os.path.join(data_module, data_file_name)\n\n    return pd.read_csv(path, sep=sep)\n</code></pre>"},{"location":"references/basket_models/data/references_preprocessing/#choice_learn.basket_models.data.preprocessing.from_csv","title":"<code>from_csv(data_file_name, nrows=None, sep=None, store_id_col='store_id', item_id_col='item_id', session_id_col='session_id', quantity_col='quantity', week_id_col='week_id', price_col='price')</code>","text":"<p>Build a TripDataset from a csv file (with preprocessing).</p> <p>The csv file should contain the following columns: - store_id - item_id - session_id - quantity - week_id - price (not necessarily with these names).</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <code>str</code> <p>Name of the csv file to load</p> required <code>nrows</code> <code>Union[int, None]</code> <p>Number of rows to load, by default None</p> <code>None</code> <code>store_id_col</code> <code>str</code> <p>Name of the store id column, by default \"store_id\"</p> <code>'store_id'</code> <code>item_id_col</code> <code>str</code> <p>Name of the item id column, by default \"item_id\"</p> <code>'item_id'</code> <code>session_id_col</code> <code>str</code> <p>Name of the session id column, by default \"session_id\"</p> <code>'session_id'</code> <code>quantity_col</code> <code>str</code> <p>Name of the quantity column, by default \"quantity\"</p> <code>'quantity'</code> <code>price_col</code> <code>str</code> <p>Name of the price column, by default \"price\"</p> <code>'price'</code> <code>week_id_col</code> <code>str</code> <p>Name of the week id column, by default \"week_id\"</p> <code>'week_id'</code> <p>Returns:</p> Name Type Description <code>trip_dataset</code> <code>TripDataset</code> <p>TripDataset built from the csv files (with preprocessing)</p> <code>n_items</code> <code>int</code> <p>Number of distinct items in the dataset</p> <code>n_stores</code> <code>int</code> <p>Number of distinct stores in the dataset</p> <code>n_trips</code> <code>int</code> <p>Number of distinct trips in the dataset</p> <code>(trip_dataset, n_items, n_stores, n_trips)</code> Source code in <code>choice_learn/basket_models/data/preprocessing.py</code> <pre><code>def from_csv(\n    data_file_name: str,\n    nrows: Union[int, None] = None,\n    sep: str = None,\n    store_id_col: str = \"store_id\",\n    item_id_col: str = \"item_id\",\n    session_id_col: str = \"session_id\",\n    quantity_col: str = \"quantity\",\n    week_id_col: str = \"week_id\",\n    price_col: str = \"price\",\n) -&gt; tuple[TripDataset]:\n    \"\"\"Build a TripDataset from a csv file (with preprocessing).\n\n    The csv file should contain the following columns:\n    - store_id\n    - item_id\n    - session_id\n    - quantity\n    - week_id\n    - price\n    (not necessarily with these names).\n\n    Parameters\n    ----------\n    data_file_name: str\n        Name of the csv file to load\n    nrows: int, optional\n        Number of rows to load, by default None\n    store_id_col: str, optional\n        Name of the store id column, by default \"store_id\"\n    item_id_col: str, optional\n        Name of the item id column, by default \"item_id\"\n    session_id_col: str, optional\n        Name of the session id column, by default \"session_id\"\n    quantity_col: str, optional\n        Name of the quantity column, by default \"quantity\"\n    price_col: str, optional\n        Name of the price column, by default \"price\"\n    week_id_col: str, optional\n        Name of the week id column, by default \"week_id\"\n\n    Returns\n    -------\n    trip_dataset: TripDataset\n        TripDataset built from the csv files (with preprocessing)\n    n_items: int\n        Number of distinct items in the dataset\n    n_stores: int\n        Number of distinct stores in the dataset\n    n_trips: int\n        Number of distinct trips in the dataset\n\n    trip_dataset, n_items, n_stores, n_trips\n    \"\"\"\n    # Load the data and select the first nrows\n    dataset = csv_to_df(data_file_name=data_file_name, data_module=OS_DATA_MODULE, sep=sep)\n    print(\n        \"Number of transactions in the total dataset: \",\n        f\"{dataset.shape[0]}\",\n    )\n    dataset = dataset.iloc[:nrows]\n\n    # Print some statistics about the dataset\n    dataset_grouped_by_item = csv_to_df(\n        data_file_name=data_file_name, data_module=OS_DATA_MODULE, sep=sep\n    ).groupby([\"item_id\"])\n    dataset_grouped_by_trip = csv_to_df(\n        data_file_name=data_file_name, data_module=OS_DATA_MODULE, sep=sep\n    ).groupby([\"session_id\", \"store_id\"])\n    print(f\"Nb of items in the total dataset: {dataset_grouped_by_item.ngroups}\")\n    print(f\"Nb of trips in the total dataset: {dataset_grouped_by_trip.ngroups}\")\n    print(\n        \"Average number of items per trip in the total dataset: \",\n        f\"{dataset_grouped_by_trip.size().mean()}\",\n    )\n    print(\n        \"Min and max number of items per trip in the total dataset: \"\n        f\"{dataset_grouped_by_trip.size().min()}, {dataset_grouped_by_trip.size().max()}\"\n    )\n\n    # Rename columns\n    dataset = dataset.rename(\n        columns={\n            store_id_col: \"store_id\",\n            item_id_col: \"item_id\",\n            session_id_col: \"session_id\",\n            quantity_col: \"quantity\",\n            price_col: \"price\",\n            week_id_col: \"week_id\",\n        }\n    )\n\n    print(f\"Before mapping the item ids: {dataset['item_id'].unique()=}\\n\")\n\n    # Map the indexes\n    for column in dataset.columns:\n        if column[-3:] == \"_id\":\n            print(f\"Remapping {column}\")\n            if column == \"item_id\":\n                # 1-index mapping (the checkout item 0 is counted in n_items)\n                map_indexes(dataset, column, index_start=1)\n            else:\n                # O-index mapping\n                map_indexes(dataset, column, index_start=0)\n    # /!\\ TODO: the same ids accross datasets are not necessarily mapped to the same index\n    # --&gt; The remapping should be done on the whole dataset after having merged the\n    # different subsets?\n\n    print(f\"After mapping the item ids: {dataset['item_id'].unique()=}\\n\")\n\n    # Normalize the raw prices (the price for a given trip is divided by the per-item mean price)\n    # Drop rows with NaN values in the price column\n    dataset = dataset.dropna(subset=[\"price\"])\n\n    # Division by the mean of the prices of the given item in the different trips\n    dataset[\"price\"] = dataset[\"price\"] / dataset.groupby(\"item_id\")[\"price\"].transform(\"mean\")\n    # Other possibility : division by the mean of the prices of the items in the given trip\n    # dataset[\"price\"] = dataset[\"price\"] / dataset.groupby(\"session_id\")[\"price\"].transform(\n    #     \"mean\"\n    # )\n\n    n_items = dataset[\"item_id\"].nunique() + 1  # +1 for the checkout item\n    n_stores = dataset[\"store_id\"].nunique()\n\n    # Divide the data into trips\n    dataset_trips = []\n\n    grouped_sessions = list(dataset.groupby(\"session_id\"))\n    for trip_idx, (trip_id, trip_data) in enumerate(dataset.groupby(\"session_id\")):\n        purchases = trip_data[\"item_id\"].tolist()\n        store = trip_data[\"store_id\"].tolist()\n        # All the trips of a given session have the same week_id\n        week = trip_data[\"week_id\"].tolist()[0]\n\n        if len(purchases) != len(set(purchases)):\n            # Remove duplicates while preserving order\n            purchases = list(dict.fromkeys(purchases))\n\n        # Create price array with error handling\n        # (Price of checkout item 0: 1 or another default value &gt; 0)\n        # (-1 means that the price has not already been set)\n        prices = np.array([1] + [-1] * (n_items - 1))\n\n        # 1. Get the price of each item in the trip\n        for item_id, session_id in zip(purchases, trip_data[\"session_id\"]):\n            try:\n                if isinstance(\n                    dataset.set_index([\"item_id\", \"session_id\"]).loc[(item_id, session_id)][\n                        \"price\"\n                    ],\n                    pd.Series,\n                ):\n                    # Then the price is a Pandas series (same value repeated)\n                    if (\n                        (\n                            dataset.set_index([\"item_id\", \"session_id\"])\n                            .loc[(item_id, session_id)][\"price\"]\n                            .to_numpy()[0]\n                        )\n                        == (\n                            dataset.set_index([\"item_id\", \"session_id\"])\n                            .loc[(item_id, session_id)][\"price\"]\n                            .to_numpy()[0]\n                        )\n                    ):\n                        # Ensure that the price is not NaN\n                        # (The price is NaN when there is no item_id in session_id)\n                        prices[item_id] = (\n                            dataset.set_index([\"item_id\", \"session_id\"])\n                            .loc[(item_id, session_id)][\"price\"]\n                            .to_numpy()[0]\n                        )\n                    else:\n                        prices[item_id] = 1  # Or another default value &gt; 0\n                else:\n                    # Then the price is a scalar\n                    if (\n                        dataset.set_index([\"item_id\", \"session_id\"]).loc[(item_id, session_id)][\n                            \"price\"\n                        ]\n                        == dataset.set_index([\"item_id\", \"session_id\"]).loc[(item_id, session_id)][\n                            \"price\"\n                        ]\n                    ):\n                        # Ensure that the price is not NaN\n                        # (The price is NaN when there is no item_id in session_id)\n                        prices[item_id] = dataset.set_index([\"item_id\", \"session_id\"]).loc[\n                            (item_id, session_id)\n                        ][\"price\"]\n                    else:\n                        prices[item_id] = 1  # Or another default value &gt; 0\n\n            except KeyError:\n                prices[item_id] = 1  # Or another default value &gt; 0\n\n        # 2. Approximate the price of the items not in the trip with\n        # the price of the same item in the previous or next trip\n        for item_id in range(n_items):\n            if prices[item_id] == -1:\n                found_price = False\n                step = 1\n                while not found_price:\n                    # Proceed step by step to find the price of the item\n                    # in the k-th previous or the k-th next trip\n                    prev_session_id, prev_session_data = None, None\n                    next_session_id, next_session_data = None, None\n\n                    if trip_idx - step &gt;= 0:\n                        prev_session_id, prev_session_data = grouped_sessions[trip_idx - step]\n                    if trip_idx + step &lt; len(grouped_sessions):\n                        next_session_id, next_session_data = grouped_sessions[trip_idx + step]\n\n                    if (\n                        prev_session_data is not None\n                        and item_id in prev_session_data[\"item_id\"].tolist()\n                    ):\n                        # If item_id is in the previous trip, take the\n                        # price of the item in the previous trip\n                        if isinstance(\n                            dataset.set_index([\"item_id\", \"session_id\"]).loc[\n                                (item_id, prev_session_id)\n                            ][\"price\"],\n                            pd.Series,\n                        ):\n                            # Then the price is a Pandas series (same value repeated)\n                            prices[item_id] = (\n                                dataset.set_index([\"item_id\", \"session_id\"])\n                                .loc[(item_id, prev_session_id)][\"price\"]\n                                .to_numpy()[0]\n                            )\n                        else:\n                            # Then the price is a scalar\n                            prices[item_id] = dataset.set_index([\"item_id\", \"session_id\"]).loc[\n                                (item_id, prev_session_id)\n                            ][\"price\"]\n                        found_price = True\n\n                    elif (\n                        next_session_data is not None\n                        and item_id in next_session_data[\"item_id\"].tolist()\n                    ):\n                        # If item_id is in the next session, take the\n                        # price of the item in the next trip\n                        if isinstance(\n                            dataset.set_index([\"item_id\", \"session_id\"]).loc[\n                                (item_id, next_session_id)\n                            ][\"price\"],\n                            pd.Series,\n                        ):\n                            # Then the price is a Pandas series (same value repeated)\n                            prices[item_id] = (\n                                dataset.set_index([\"item_id\", \"session_id\"])\n                                .loc[(item_id, next_session_id)][\"price\"]\n                                .to_numpy()[0]\n                            )\n                        else:\n                            # Then the price is a scalar\n                            prices[item_id] = dataset.set_index([\"item_id\", \"session_id\"]).loc[\n                                (item_id, next_session_id)\n                            ][\"price\"]\n                        found_price = True\n\n                    if trip_idx - step &lt; 0 and trip_idx + step &gt;= len(grouped_sessions):\n                        # Then we have checked all possible previous and next trips\n                        break\n\n                    step += 1\n\n                if not found_price:\n                    prices[item_id] = 1  # Or another default value &gt; 0\n\n        for store_id in store:\n            purchases_store = trip_data[trip_data[\"store_id\"] == store_id][\"item_id\"].tolist()\n            dataset_trips.append(\n                Trip(\n                    purchases=purchases_store + [0],  # Add the checkout item 0 at the end\n                    store=store_id,\n                    week=week,\n                    prices=prices,\n                    assortment=0,  # TODO: Add the assortment\n                )\n            )\n\n    # Build the TripDatasets\n    assortments = np.expand_dims(np.ones(n_items), axis=0)  # TODO: Add the assortments\n    trip_dataset = TripDataset(trips=dataset_trips, assortments=assortments)\n\n    n_trips = len(trip_dataset)\n\n    print(f\"{n_items=}, {n_stores=} and {n_trips=}\")\n\n    return trip_dataset, n_items, n_stores, n_trips\n</code></pre>"},{"location":"references/basket_models/data/references_preprocessing/#choice_learn.basket_models.data.preprocessing.map_indexes","title":"<code>map_indexes(df, column_name, index_start)</code>","text":"<p>Create the mapping and map the values of a column to indexes.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the column to map</p> required <code>column_name</code> <code>str</code> <p>Name of the column to map</p> required <code>index_start</code> <code>int</code> <p>Index to start the mapping from</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Mapping from values to indexes</p> Source code in <code>choice_learn/basket_models/data/preprocessing.py</code> <pre><code>def map_indexes(df: pd.DataFrame, column_name: str, index_start: int) -&gt; dict:\n    \"\"\"Create the mapping and map the values of a column to indexes.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        DataFrame containing the column to map\n    column_name: str\n        Name of the column to map\n    index_start: int\n        Index to start the mapping from\n\n    Returns\n    -------\n    dict\n        Mapping from values to indexes\n    \"\"\"\n    unique_values = df[column_name].unique()\n    # Index the items id starting from index_start\n    mapping = {value: index + index_start for index, value in enumerate(unique_values)}\n    df[column_name] = df[column_name].map(mapping)\n\n    return mapping\n</code></pre>"},{"location":"references/basket_models/datasets/references_bakery/","title":"Bakery Dataset as TripDataset","text":"<p>Base TripDataset loader for the Bakery dataset from Benson et al. (2018).</p>"},{"location":"references/basket_models/datasets/references_bakery/#choice_learn.basket_models.datasets.bakery.load_bakery","title":"<code>load_bakery(as_frame=False, load_5_25_version=False)</code>","text":"<p>Load the bakery dataset from uchoice-Bakery.txt.</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as TripDataset, by default False.</p> <code>False</code> <code>load_5_25_version</code> <code>bool</code> <p>Whether to return the 5-25 version of the dataset, by default False.</p> <code>False</code> Source code in <code>choice_learn/basket_models/datasets/bakery.py</code> <pre><code>def load_bakery(as_frame=False, load_5_25_version=False):\n    \"\"\"Load the bakery dataset from uchoice-Bakery.txt.\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as TripDataset,\n        by default False.\n    load_5_25_version : bool, optional\n        Whether to return the 5-25 version of the dataset,\n        by default False.\n    \"\"\"\n    url = \"https://drive.usercontent.google.com/u/0/uc?id=1qV8qmiHTq6y5fwgN0_hRXyKreNKrF72E&amp;export=download\"\n    data_file_name = download_from_url(url)\n\n    archive_path = get_path(data_file_name)\n    # We put the extracted files in the data directory\n    with tarfile.open(archive_path, \"r:gz\") as tar:\n        # Here are the files we are downloading\n        file_names = tar.getnames()\n\n        # Security check for safe member paths (defense-in-depth for all Python versions)\n        for member in tar.getmembers():\n            member_path = os.path.normpath(member.name)\n            if (\n                Path(member_path).is_absolute()\n                or member_path.startswith(\"..\")\n                or \"..\" in member_path.split(os.path.sep)\n            ):\n                raise ValueError(f\"tar archive contains unsafe path: {member.name}\")\n\n        # We extract all the files\n        if sys.version_info &gt;= (3, 12):\n            tar.extractall(path=archive_path.parent, filter=\"data\")  # nosec\n        else:\n            tar.extractall(path=archive_path.parent)  # nosec\n\n        # We want to read the uchoice-Bakery.txt file (second file in the archive)\n    if load_5_25_version:\n        csv_file_to_read = \"uchoice-Bakery/uchoice-Bakery-5-25.txt\"\n    else:\n        csv_file_to_read = \"uchoice-Bakery/uchoice-Bakery.txt\"\n    if csv_file_to_read not in file_names:\n        raise FileNotFoundError(f\"'{csv_file_to_read}' not found in the archive.\")\n\n    noms_colonnes = [\n        \"article_1\",\n        \"article_2\",\n        \"article_3\",\n        \"article_4\",\n        \"article_5\",\n        \"article_6\",\n        \"article_7\",\n        \"article_8\",\n    ]\n\n    # likewise get_path function\n    path = archive_path.parent / csv_file_to_read\n    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=noms_colonnes)\n\n    if as_frame:\n        return df\n\n    n_item = int(df.max().max())\n\n    # Apparently all items are available at each trip\n    availability_matrix = np.array([[1] * n_item])\n\n    list_purchases = [[int(item) - 1 for item in row if pd.notna(item)] for row in df.to_numpy()]\n\n    # Dummy prices, all equal to 1\n    prices = np.array([[1] * n_item])\n    trips_list = [\n        Trip(purchases=purchases, assortment=0, prices=prices) for purchases in list_purchases\n    ]\n\n    return TripDataset(trips=trips_list, available_items=availability_matrix)\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/","title":"Synthetic (Badminton) Dataset as TripDataset","text":"<p>Data generation module for synthetic basket data.</p>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator","title":"<code>SyntheticDataGenerator</code>","text":"<p>Class to generate synthetic basket data based on predefined item sets and their relations.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>class SyntheticDataGenerator:\n    \"\"\"Class to generate synthetic basket data based on predefined item sets and their relations.\"\"\"\n\n    def __init__(\n        self,\n        items_nest: dict,\n        nests_interactions: list,\n        proba_complementary_items: float = 0.7,\n        proba_neutral_items: float = 0.15,\n        noise_proba: float = 0.05,\n        plant_seed: int = None,\n        user_profile: dict = None,\n    ) -&gt; None:\n        \"\"\"Initialize the data generator with parameters for basket generation.\n\n        Parameters\n        ----------\n            items_nest : dict\n                Dictionary defining item sets and their relations.\n                Key should be next index and values list of items indexes, e.g.\n                items_nests = {0:[0, 1, 2],\n                               1: [3, 4, 5],\n                               2: [6],\n                               3: [7]}\n            nests_interactions: list\n                List of interactions between nests for each nest. Symmetry should\n                be ensure by users, e.g.\n                nests_interactions = [[\"\", \"compl\", \"neutral\", \"neutral\"],\n                                      [\"compl\", \"\", \"neutral\", \"neutral\"],\n                                      [\"neutral\", \"neutral\", \"\", \"neutral\"],\n                                      [\"neutral\", \"neutral\", \"neutral\", \"\"]]\n            user_profile = {0:{ \"nest\" : 0, \"item\" : 0},\n                            1: {\"nest\" : 0, \"item\" : 1},\n                            2: {\"nest\" : 0, \"item\" : 2}}\n                Dictionary defining user profiles with preferred nest and item. Structure is:\n                {user_id: {\"nest\": nest, \"item\": preferred_item}, ...}\n            proba_complementary_items : float\n                Probability of adding complementary items to the basket.\n            proba_neutral_items : float\n                Probability of adding neutral items to the basket.\n            noise_proba : float\n                Probability of adding noise items to the basket.\n        \"\"\"\n        self.proba_complementary_items = proba_complementary_items\n        self.proba_neutral_items = proba_neutral_items\n        self.noise_proba = noise_proba\n        self.items_nest = items_nest\n        self.nests_interactions = nests_interactions\n        self.user_profile = user_profile\n\n        if plant_seed is not None:\n            np.random.seed(plant_seed)\n\n    def get_available_sets(self, assortment_items: np.ndarray = None) -&gt; np.ndarray:\n        \"\"\"Return the available nests based on the current assortment.\n\n        Parameters\n        ----------\n            assortment : int or np.ndarray, optional\n                Index of the assortment or an array representing the assortment.\n\n        Returns\n        -------\n            np.ndarray\n                List of keys from items_nest\n                Where the first item set intersects with the current assortment.\n        \"\"\"\n        return np.array(\n            list(\n                key\n                for key, value in self.items_nest.items()\n                if set(value).intersection(set(assortment_items))\n            )\n        )\n\n    def select_first_item(\n        self,\n        available_sets: np.ndarray,\n        available_items: np.ndarray,\n        user_id: int = None,\n    ) -&gt; tuple:\n        \"\"\"Select the first item and its nest randomly from the available sets.\n\n        Parameters\n        ----------\n            available_sets : np.ndarray\n                List of available sets from which to select the first item.\n            available_items : np.ndarray\n                List of available items in the current assortment.\n\n        Returns\n        -------\n            tuple\n                A tuple containing the first item and its corresponding nest.\n        \"\"\"\n        chosen_nest = np.random.choice(available_sets)\n\n        chosen_item = np.random.choice(\n            [i for i in self.items_nest[chosen_nest] if i in available_items]\n        )\n        if self.user_profile is not None and self.user_profile[user_id][\"nest\"] == chosen_nest:\n            if self.user_profile[user_id][\"item\"] in available_items and np.random.rand() &lt; 0.7:\n                chosen_item = self.user_profile[user_id][\"item\"]\n\n        return chosen_item, chosen_nest\n\n    def complete_basket(\n        self,\n        first_item: int,\n        first_nest: int,\n        available_items: np.ndarray,\n        user_id: int = None,\n    ) -&gt; list:\n        \"\"\"Completes the basket by adding items based on the relations of the first item.\n\n        Parameters\n        ----------\n            first_item : int\n                The first item to be added to the basket.\n            first_nest : int\n                The nest corresponding to the first item.\n            available_items: np.ndarray\n                Avaialbe item IDs\n\n        Returns\n        -------\n            list\n                list next basket items.\n        \"\"\"\n        basket = [first_item]\n        interactions = self.nests_interactions[first_nest]\n        for nest_id, items in self.items_nest.items():\n            if (\n                interactions[nest_id] == \"compl\"\n                and np.random.random() &lt; self.proba_complementary_items\n            ):\n                try:\n                    if (\n                        self.user_profile is not None\n                        and self.user_profile[user_id][\"nest\"] == nest_id\n                    ):\n                        if (\n                            self.user_profile[user_id][\"item\"] in available_items\n                            and np.random.rand() &lt; 0.7\n                        ):\n                            basket.append(self.user_profile[user_id][\"item\"])\n                        else:\n                            basket.append(\n                                np.random.choice([i for i in items if i in available_items])\n                            )\n                    else:\n                        basket.append(np.random.choice([i for i in items if i in available_items]))\n                except ValueError:\n                    logging.warning(\n                        f\"Warning: No more complementary items available in nest {nest_id}\"\n                    )\n                    pass\n            elif (\n                interactions[nest_id] == \"neutral\" and np.random.random() &lt; self.proba_neutral_items\n            ):\n                try:\n                    basket.append(np.random.choice([i for i in items if i in available_items]))\n                except ValueError:\n                    logging.warning(f\"Warning: No more neutral items available in nest {nest_id}\")\n                    pass\n        return basket\n\n    def add_noise(self, basket: list, available_items) -&gt; list:\n        \"\"\"Add noise items to the basket based on the defined noise probability.\n\n        Parameters\n        ----------\n            basket : list\n                The current basket of items.\n\n        Returns\n        -------\n            list\n                A list containing the items in the basket, potentially with noise items added.\n        \"\"\"\n        if np.random.rand() &lt;= self.noise_proba:\n            try:\n                basket.append(np.random.choice([i for i in available_items if i not in basket]))\n            except IndexError:\n                logging.warning(\n                    \"Warning: No more items available to add as noise.Returning the current basket.\"\n                )\n            except ValueError:\n                logging.warning(\n                    \"Warning: No more items available to add as noise.Returning the current basket.\"\n                )\n        return basket\n\n    def generate_basket(\n        self,\n        assortment: Union[int, np.ndarray] = None,\n        len_basket: int = None,\n        user_id: int = None,\n    ) -&gt; list:\n        \"\"\"Generate a basket of items based on the defined item sets and their relations.\n\n        Parameters\n        ----------\n            assortment : np.ndarray, optional\n                Index of the assortment or an array representing the assortment.\n                1 represent sold items while 0 represnet missing items.\n            len_basket : int, optional\n                Length of the basket to be generated.\n                If None, the basket length is determined by the available sets.\n\n        Returns\n        -------\n            array\n                array of items in the generated basket.\n        \"\"\"\n        available_items = np.where(assortment &gt; 0)[0]\n        available_sets = self.get_available_sets(available_items)\n\n        if len(available_sets) != 0:\n            first_chosen_item, first_chosen_nest = self.select_first_item(\n                available_sets=available_sets,\n                available_items=available_items,\n                user_id=user_id,\n            )\n            basket = self.complete_basket(\n                first_chosen_item,\n                first_chosen_nest,\n                available_items=available_items,\n                user_id=user_id,\n            )\n            basket = self.add_noise(basket, available_items=available_items)\n        else:\n            basket = []\n\n        if len_basket is not None:\n            if not isinstance(len_basket, int) or len_basket &lt; 1:\n                raise TypeError(\"len_basket should be an integer larger than 0.\")\n            if len(basket) &lt; len_basket:\n                basket = self.generate_basket(assortment, len_basket, user_id=user_id)\n            else:\n                basket = np.random.choice(basket, len_basket, replace=False)\n\n        return np.array(basket)\n\n    def generate_trip(\n        self, assortment: Union[int, np.ndarray] = None, len_basket: int = None\n    ) -&gt; Trip:\n        \"\"\"Generate a trip object from the generated basket.\n\n        Parameters\n        ----------\n            assortment : int or np.ndarray\n                Index of the assortment or an array representing the assortment.\n            len_basket : int, optional\n                Length of the basket to be generated.\n                If None, the basket length is determined by the\n                available sets.\n\n        Returns\n        -------\n            Trip\n                A Trip object containing the generated basket.\n        \"\"\"\n        user_id = (\n            np.random.randint(0, len(self.user_profile)) if self.user_profile is not None else None\n        )\n        basket = self.generate_basket(assortment, len_basket=len_basket, user_id=user_id).astype(\n            int\n        )\n        return Trip(\n            purchases=basket,\n            # Assuming uniform price of 1.0 for simplicity\n            prices=np.ones((1, len(assortment))),\n            assortment=assortment,\n            user_id=user_id,\n        )\n\n    def generate_trip_dataset(\n        self,\n        n_baskets: int = 400,\n        assortments_matrix: np.ndarray = None,\n        len_basket: int = None,\n    ) -&gt; TripDataset:\n        \"\"\"Generate a TripDataset from the generated baskets.\n\n        Parameters\n        ----------\n            n_baskets : int, optional\n                Number of baskets to generate. If None, uses the default value.\n            assortment_matrix : list of sets, optional\n                Matrix of assortments to use for generating baskets.\n                If None, uses the default assortment matrix.\n                shape (n_assortments, n_items)\n            len_basket : int, optional\n                Length of the basket to be generated.\n                If None, the basket length is determined by the\n                available sets.\n\n        Returns\n        -------\n            TripDataset\n                A TripDataset object containing the generated baskets.\n        \"\"\"\n        trips = []\n        assortments = []\n        assortment_id = np.random.randint(0, len(assortments_matrix))\n        for _ in range(n_baskets):\n            trip = self.generate_trip(assortments_matrix[assortment_id], len_basket=len_basket)\n            assortments.append(assortments_matrix[assortment_id])\n            trips.append(trip)\n\n        return TripDataset(trips, np.array(assortments_matrix))\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.__init__","title":"<code>__init__(items_nest, nests_interactions, proba_complementary_items=0.7, proba_neutral_items=0.15, noise_proba=0.05, plant_seed=None, user_profile=None)</code>","text":"<p>Initialize the data generator with parameters for basket generation.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def __init__(\n    self,\n    items_nest: dict,\n    nests_interactions: list,\n    proba_complementary_items: float = 0.7,\n    proba_neutral_items: float = 0.15,\n    noise_proba: float = 0.05,\n    plant_seed: int = None,\n    user_profile: dict = None,\n) -&gt; None:\n    \"\"\"Initialize the data generator with parameters for basket generation.\n\n    Parameters\n    ----------\n        items_nest : dict\n            Dictionary defining item sets and their relations.\n            Key should be next index and values list of items indexes, e.g.\n            items_nests = {0:[0, 1, 2],\n                           1: [3, 4, 5],\n                           2: [6],\n                           3: [7]}\n        nests_interactions: list\n            List of interactions between nests for each nest. Symmetry should\n            be ensure by users, e.g.\n            nests_interactions = [[\"\", \"compl\", \"neutral\", \"neutral\"],\n                                  [\"compl\", \"\", \"neutral\", \"neutral\"],\n                                  [\"neutral\", \"neutral\", \"\", \"neutral\"],\n                                  [\"neutral\", \"neutral\", \"neutral\", \"\"]]\n        user_profile = {0:{ \"nest\" : 0, \"item\" : 0},\n                        1: {\"nest\" : 0, \"item\" : 1},\n                        2: {\"nest\" : 0, \"item\" : 2}}\n            Dictionary defining user profiles with preferred nest and item. Structure is:\n            {user_id: {\"nest\": nest, \"item\": preferred_item}, ...}\n        proba_complementary_items : float\n            Probability of adding complementary items to the basket.\n        proba_neutral_items : float\n            Probability of adding neutral items to the basket.\n        noise_proba : float\n            Probability of adding noise items to the basket.\n    \"\"\"\n    self.proba_complementary_items = proba_complementary_items\n    self.proba_neutral_items = proba_neutral_items\n    self.noise_proba = noise_proba\n    self.items_nest = items_nest\n    self.nests_interactions = nests_interactions\n    self.user_profile = user_profile\n\n    if plant_seed is not None:\n        np.random.seed(plant_seed)\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.add_noise","title":"<code>add_noise(basket, available_items)</code>","text":"<p>Add noise items to the basket based on the defined noise probability.</p> <p>Returns:</p> Type Description <code>    list</code> <p>A list containing the items in the basket, potentially with noise items added.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def add_noise(self, basket: list, available_items) -&gt; list:\n    \"\"\"Add noise items to the basket based on the defined noise probability.\n\n    Parameters\n    ----------\n        basket : list\n            The current basket of items.\n\n    Returns\n    -------\n        list\n            A list containing the items in the basket, potentially with noise items added.\n    \"\"\"\n    if np.random.rand() &lt;= self.noise_proba:\n        try:\n            basket.append(np.random.choice([i for i in available_items if i not in basket]))\n        except IndexError:\n            logging.warning(\n                \"Warning: No more items available to add as noise.Returning the current basket.\"\n            )\n        except ValueError:\n            logging.warning(\n                \"Warning: No more items available to add as noise.Returning the current basket.\"\n            )\n    return basket\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.complete_basket","title":"<code>complete_basket(first_item, first_nest, available_items, user_id=None)</code>","text":"<p>Completes the basket by adding items based on the relations of the first item.</p> <p>Returns:</p> Type Description <code>    list</code> <p>list next basket items.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def complete_basket(\n    self,\n    first_item: int,\n    first_nest: int,\n    available_items: np.ndarray,\n    user_id: int = None,\n) -&gt; list:\n    \"\"\"Completes the basket by adding items based on the relations of the first item.\n\n    Parameters\n    ----------\n        first_item : int\n            The first item to be added to the basket.\n        first_nest : int\n            The nest corresponding to the first item.\n        available_items: np.ndarray\n            Avaialbe item IDs\n\n    Returns\n    -------\n        list\n            list next basket items.\n    \"\"\"\n    basket = [first_item]\n    interactions = self.nests_interactions[first_nest]\n    for nest_id, items in self.items_nest.items():\n        if (\n            interactions[nest_id] == \"compl\"\n            and np.random.random() &lt; self.proba_complementary_items\n        ):\n            try:\n                if (\n                    self.user_profile is not None\n                    and self.user_profile[user_id][\"nest\"] == nest_id\n                ):\n                    if (\n                        self.user_profile[user_id][\"item\"] in available_items\n                        and np.random.rand() &lt; 0.7\n                    ):\n                        basket.append(self.user_profile[user_id][\"item\"])\n                    else:\n                        basket.append(\n                            np.random.choice([i for i in items if i in available_items])\n                        )\n                else:\n                    basket.append(np.random.choice([i for i in items if i in available_items]))\n            except ValueError:\n                logging.warning(\n                    f\"Warning: No more complementary items available in nest {nest_id}\"\n                )\n                pass\n        elif (\n            interactions[nest_id] == \"neutral\" and np.random.random() &lt; self.proba_neutral_items\n        ):\n            try:\n                basket.append(np.random.choice([i for i in items if i in available_items]))\n            except ValueError:\n                logging.warning(f\"Warning: No more neutral items available in nest {nest_id}\")\n                pass\n    return basket\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.generate_basket","title":"<code>generate_basket(assortment=None, len_basket=None, user_id=None)</code>","text":"<p>Generate a basket of items based on the defined item sets and their relations.</p> <p>Returns:</p> Type Description <code>    array</code> <p>array of items in the generated basket.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def generate_basket(\n    self,\n    assortment: Union[int, np.ndarray] = None,\n    len_basket: int = None,\n    user_id: int = None,\n) -&gt; list:\n    \"\"\"Generate a basket of items based on the defined item sets and their relations.\n\n    Parameters\n    ----------\n        assortment : np.ndarray, optional\n            Index of the assortment or an array representing the assortment.\n            1 represent sold items while 0 represnet missing items.\n        len_basket : int, optional\n            Length of the basket to be generated.\n            If None, the basket length is determined by the available sets.\n\n    Returns\n    -------\n        array\n            array of items in the generated basket.\n    \"\"\"\n    available_items = np.where(assortment &gt; 0)[0]\n    available_sets = self.get_available_sets(available_items)\n\n    if len(available_sets) != 0:\n        first_chosen_item, first_chosen_nest = self.select_first_item(\n            available_sets=available_sets,\n            available_items=available_items,\n            user_id=user_id,\n        )\n        basket = self.complete_basket(\n            first_chosen_item,\n            first_chosen_nest,\n            available_items=available_items,\n            user_id=user_id,\n        )\n        basket = self.add_noise(basket, available_items=available_items)\n    else:\n        basket = []\n\n    if len_basket is not None:\n        if not isinstance(len_basket, int) or len_basket &lt; 1:\n            raise TypeError(\"len_basket should be an integer larger than 0.\")\n        if len(basket) &lt; len_basket:\n            basket = self.generate_basket(assortment, len_basket, user_id=user_id)\n        else:\n            basket = np.random.choice(basket, len_basket, replace=False)\n\n    return np.array(basket)\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.generate_trip","title":"<code>generate_trip(assortment=None, len_basket=None)</code>","text":"<p>Generate a trip object from the generated basket.</p> <p>Returns:</p> Type Description <code>    Trip</code> <p>A Trip object containing the generated basket.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def generate_trip(\n    self, assortment: Union[int, np.ndarray] = None, len_basket: int = None\n) -&gt; Trip:\n    \"\"\"Generate a trip object from the generated basket.\n\n    Parameters\n    ----------\n        assortment : int or np.ndarray\n            Index of the assortment or an array representing the assortment.\n        len_basket : int, optional\n            Length of the basket to be generated.\n            If None, the basket length is determined by the\n            available sets.\n\n    Returns\n    -------\n        Trip\n            A Trip object containing the generated basket.\n    \"\"\"\n    user_id = (\n        np.random.randint(0, len(self.user_profile)) if self.user_profile is not None else None\n    )\n    basket = self.generate_basket(assortment, len_basket=len_basket, user_id=user_id).astype(\n        int\n    )\n    return Trip(\n        purchases=basket,\n        # Assuming uniform price of 1.0 for simplicity\n        prices=np.ones((1, len(assortment))),\n        assortment=assortment,\n        user_id=user_id,\n    )\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.generate_trip_dataset","title":"<code>generate_trip_dataset(n_baskets=400, assortments_matrix=None, len_basket=None)</code>","text":"<p>Generate a TripDataset from the generated baskets.</p> <p>Returns:</p> Type Description <code>    TripDataset</code> <p>A TripDataset object containing the generated baskets.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def generate_trip_dataset(\n    self,\n    n_baskets: int = 400,\n    assortments_matrix: np.ndarray = None,\n    len_basket: int = None,\n) -&gt; TripDataset:\n    \"\"\"Generate a TripDataset from the generated baskets.\n\n    Parameters\n    ----------\n        n_baskets : int, optional\n            Number of baskets to generate. If None, uses the default value.\n        assortment_matrix : list of sets, optional\n            Matrix of assortments to use for generating baskets.\n            If None, uses the default assortment matrix.\n            shape (n_assortments, n_items)\n        len_basket : int, optional\n            Length of the basket to be generated.\n            If None, the basket length is determined by the\n            available sets.\n\n    Returns\n    -------\n        TripDataset\n            A TripDataset object containing the generated baskets.\n    \"\"\"\n    trips = []\n    assortments = []\n    assortment_id = np.random.randint(0, len(assortments_matrix))\n    for _ in range(n_baskets):\n        trip = self.generate_trip(assortments_matrix[assortment_id], len_basket=len_basket)\n        assortments.append(assortments_matrix[assortment_id])\n        trips.append(trip)\n\n    return TripDataset(trips, np.array(assortments_matrix))\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.get_available_sets","title":"<code>get_available_sets(assortment_items=None)</code>","text":"<p>Return the available nests based on the current assortment.</p> <p>Returns:</p> Type Description <code>    np.ndarray</code> <p>List of keys from items_nest Where the first item set intersects with the current assortment.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def get_available_sets(self, assortment_items: np.ndarray = None) -&gt; np.ndarray:\n    \"\"\"Return the available nests based on the current assortment.\n\n    Parameters\n    ----------\n        assortment : int or np.ndarray, optional\n            Index of the assortment or an array representing the assortment.\n\n    Returns\n    -------\n        np.ndarray\n            List of keys from items_nest\n            Where the first item set intersects with the current assortment.\n    \"\"\"\n    return np.array(\n        list(\n            key\n            for key, value in self.items_nest.items()\n            if set(value).intersection(set(assortment_items))\n        )\n    )\n</code></pre>"},{"location":"references/basket_models/datasets/references_synthetic_dataset/#choice_learn.basket_models.datasets.synthetic_dataset.SyntheticDataGenerator.select_first_item","title":"<code>select_first_item(available_sets, available_items, user_id=None)</code>","text":"<p>Select the first item and its nest randomly from the available sets.</p> <p>Returns:</p> Type Description <code>    tuple</code> <p>A tuple containing the first item and its corresponding nest.</p> Source code in <code>choice_learn/basket_models/datasets/synthetic_dataset.py</code> <pre><code>def select_first_item(\n    self,\n    available_sets: np.ndarray,\n    available_items: np.ndarray,\n    user_id: int = None,\n) -&gt; tuple:\n    \"\"\"Select the first item and its nest randomly from the available sets.\n\n    Parameters\n    ----------\n        available_sets : np.ndarray\n            List of available sets from which to select the first item.\n        available_items : np.ndarray\n            List of available items in the current assortment.\n\n    Returns\n    -------\n        tuple\n            A tuple containing the first item and its corresponding nest.\n    \"\"\"\n    chosen_nest = np.random.choice(available_sets)\n\n    chosen_item = np.random.choice(\n        [i for i in self.items_nest[chosen_nest] if i in available_items]\n    )\n    if self.user_profile is not None and self.user_profile[user_id][\"nest\"] == chosen_nest:\n        if self.user_profile[user_id][\"item\"] in available_items and np.random.rand() &lt; 0.7:\n            chosen_item = self.user_profile[user_id][\"item\"]\n\n    return chosen_item, chosen_nest\n</code></pre>"},{"location":"references/basket_models/utils/references_permutation/","title":"Permutation","text":"<p>Generation of all the permutations of an iterable.</p>"},{"location":"references/basket_models/utils/references_permutation/#choice_learn.basket_models.utils.permutation.permutations","title":"<code>permutations(iterable, r=None)</code>","text":"<p>Generate all the r length permutations of an iterable (n factorial possibilities).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; permutations('ABCD', 2)\n</code></pre> <p>'AB', 'AC', 'AD', 'BA', 'BC', 'BD', 'CA', 'CB', 'CD', 'DA', 'DB', 'DC'     &gt;&gt;&gt; permutations(range(3)) '012', '021', '102', '120', '201', '210'</p> <p>Code taken from https://docs.python.org/3/library/itertools.html.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Union[list, ndarray, tuple]</code> <p>Iterable to generate the permutations from</p> required <code>r</code> <code>Union[int, None]</code> <p>Length of the permutations, by default None If None, then r defaults to the length of the iterable</p> <code>None</code> <p>Returns:</p> Type Description <code>generator</code> <p>Generator of permutations</p> Source code in <code>choice_learn/basket_models/utils/permutation.py</code> <pre><code>def permutations(iterable: Union[list, np.ndarray, tuple], r: Union[int, None] = None) -&gt; iter:\n    \"\"\"Generate all the r length permutations of an iterable (n factorial possibilities).\n\n    Examples\n    --------\n        &gt;&gt;&gt; permutations('ABCD', 2)\n    'AB', 'AC', 'AD', 'BA', 'BC', 'BD', 'CA', 'CB', 'CD', 'DA', 'DB', 'DC'\n        &gt;&gt;&gt; permutations(range(3))\n    '012', '021', '102', '120', '201', '210'\n\n    Code taken from https://docs.python.org/3/library/itertools.html.\n\n    Parameters\n    ----------\n    iterable: iterable (list, np.ndarray or tuple)\n        Iterable to generate the permutations from\n    r: int, optional\n        Length of the permutations, by default None\n        If None, then r defaults to the length of the iterable\n\n    Returns\n    -------\n    generator\n       Generator of permutations\n    \"\"\"\n    pool = tuple(iterable)\n    n = len(pool)\n    r = n if r is None else r\n    if r &gt; n:\n        return\n\n    indices = list(range(n))\n    cycles = list(range(n, n - r, -1))\n    yield tuple(pool[i] for i in indices[:r])\n\n    while n:\n        for i in reversed(range(r)):\n            cycles[i] -= 1\n            if cycles[i] == 0:\n                indices[i:] = indices[i + 1 :] + indices[i : i + 1]\n                cycles[i] = n - i\n            else:\n                j = cycles[i]\n                indices[i], indices[-j] = indices[-j], indices[i]\n                yield tuple(pool[i] for i in indices[:r])\n                break\n        else:\n            return\n</code></pre>"},{"location":"references/data/references_choice_dataset/","title":"ChoiceDataset","text":"<p>Main classes to handle assortment data.</p>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset","title":"<code>ChoiceDataset</code>","text":"<p>ChoiceDataset is the main class to handle assortment data minimizing RAM usage.</p> <p>The choices are given as a ragged list of choices for each session. It is particularly useful if several (a lot) of choices happen during the same session. For example if we have the same customer buying several items during the same session, all its choices can be regrouped under the same session_features. Limits data duplication in such cases.</p> <p>The class has same methods/arguments as ChoiceDataset with a slight difference with self.choices being a ragged list. The returned features in self.getitem are the same as ChoiceDataset. When calling getitem(index) we map index to a session index and a choice index within the session.</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>class ChoiceDataset:\n    \"\"\"ChoiceDataset is the main class to handle assortment data minimizing RAM usage.\n\n    The choices are given as a ragged list of choices\n    for each session. It is particularly useful if several (a lot) of choices happen\n    during the same session. For example if we have the same customer buying several\n    items during the same session, all its choices\n    can be regrouped under the same session_features. Limits data duplication in such cases.\n\n    The class has same methods/arguments as ChoiceDataset with a slight difference with\n    self.choices being a ragged list. The returned features in self.__getitem__ are the same\n    as ChoiceDataset. When calling __getitem__(index) we map index to a session index and a\n    choice index within the session.\n    \"\"\"\n\n    def __init__(\n        self,\n        choices,  # Should not have None as default value ?\n        shared_features_by_choice=None,  # as many as choices.  values or ids (look at key)\n        items_features_by_choice=None,\n        available_items_by_choice=None,\n        features_by_ids=[],  # list of (name, FeaturesStorage)\n        shared_features_by_choice_names=None,\n        items_features_by_choice_names=None,\n    ):\n        \"\"\"Build the ChoiceDataset.\n\n        Parameters\n        ----------\n        choices: list or np.ndarray\n            list of chosen items indexes\n        shared_features_by_choice : tuple of (array_like, )\n            matrix of shape (num_choices, num_shared_features) containing the features of the\n            different choices that are common to all items (e.g. store features,\n            customer features, etc...)\n        items_features_by_choice : tuple of (array_like, ), default is None\n            matrix of shape (num_choices, num_items, num_items_features)\n            containing the features\n            of the items that change over time (e.g. price, promotion, etc...), default is None\n        available_items_by_choice : array_like\n            matrix of shape (num_choices, num_items) containing the availabilities of the items\n            over the different choices, default is None\n        features_by_ids : list of (name, FeaturesStorage)\n            List of Storage objects. Their name must correspond to a feature name\n            among shared_features_by_choice or items_features_by_choice\n            and their ids must match to those features values. Default is []\n        shared_features_by_choice_names : tuple of (array_like, )\n            list of names of the shared_features_by_choice, default is None\n            Shapes must match with shared_features_by_choice\n        items_features_by_choice_names : tuple of (array_like, )\n            list of names of the items_features_by_choice, default is None\n            Shapes must match with items_features_by_choice\n        \"\"\"\n        if choices is None:\n            # Done to keep a logical order of arguments, and has logic: choices have to be specified\n            raise ValueError(\"Choices must be specified, got None\")\n\n        # --------- [\u00a0Handling features type given as tuples or not ] --------- #\n\n        # If shared_features_by_choice is not given as tuple, transform it internally as a tuple\n        # A bit longer because can be None and need to also handle names\n        if shared_features_by_choice is not None:\n            if not isinstance(shared_features_by_choice, tuple):\n                self._return_shared_features_by_choice_tuple = False\n                if shared_features_by_choice_names is not None:\n                    if len(shared_features_by_choice[0]) != len(shared_features_by_choice_names):\n                        raise ValueError(\n                            f\"\"\"Number of features given does not match\n                                         number of features names given:\n                                           {len(shared_features_by_choice[0])} and\n                                            {len(shared_features_by_choice_names)}\"\"\"\n                        )\n                else:\n                    logging.warning(\n                        \"\"\"Shared Features Names were not provided, will not be able to\n                                    fit models needing them such as Conditional Logit.\"\"\"\n                    )\n\n                shared_features_by_choice_names = (shared_features_by_choice_names,)\n                shared_features_by_choice = (shared_features_by_choice,)\n\n            # choices_features is already a tuple, names are given, checking consistency\n            else:\n                self._return_shared_features_by_choice_tuple = True\n                if shared_features_by_choice_names is not None:\n                    for sub_k, (sub_features, sub_names) in enumerate(\n                        zip(shared_features_by_choice, shared_features_by_choice_names)\n                    ):\n                        if len(sub_features[0]) != len(sub_names):\n                            raise ValueError(\n                                f\"\"\"{sub_k}-th given shared_features_by_choice and\n                                shared_features_by_choice_names shapes do not match:\n                                {len(sub_features[0])} and {len(sub_names)}.\"\"\"\n                            )\n                # In this case names are missing, still transform it as a tuple\n                else:\n                    logging.warning(\n                        \"\"\"Shared Features Names were not provided, will not be able to\n                                    fit models needing them such as Conditional Logit.\"\"\"\n                    )\n                    shared_features_by_choice_names = (None,) * len(shared_features_by_choice)\n        else:\n            self._return_shared_features_by_choice_tuple = False\n\n        # If items_features_by_choice is not given as tuple, transform it internally as a tuple\n        # A bit longer because can be None and need to also handle names\n\n        if not isinstance(items_features_by_choice, tuple) and items_features_by_choice is not None:\n            self._return_items_features_by_choice_tuple = False\n            if items_features_by_choice_names is not None:\n                if len(items_features_by_choice[0][0]) != len(items_features_by_choice_names):\n                    raise ValueError(\n                        f\"\"\"Number of items_features_by_choice given does not match\n                                     number of items_features_by_choice_names given:\n                                     {len(items_features_by_choice[0][0])} and\n                                     {len(items_features_by_choice_names)}\"\"\"\n                    )\n            else:\n                logging.warning(\n                    \"\"\"Items Features Names were not provided, will not be able to\n                                fit models needing them such as Conditional Logit.\"\"\"\n                )\n            items_features_by_choice = (items_features_by_choice,)\n            items_features_by_choice_names = (items_features_by_choice_names,)\n\n        # items_features_by_choice is already a tuple, names are given, checking consistency\n        elif items_features_by_choice is not None and items_features_by_choice_names is not None:\n            for sub_k, (sub_features, sub_names) in enumerate(\n                zip(items_features_by_choice, items_features_by_choice_names)\n            ):\n                # Split if feature is full FeaturesStorage\n                if np.array(sub_features).ndim == 1:\n                    # check features_by_ids\n                    logging.info(\n                        \"feature of dimension 1 detected -  a FeatureByIDs MUST be provided\"\n                    )\n                    for fbid in features_by_ids:\n                        if fbid.name == sub_names[0]:\n                            logging.info(\"FeatureByIDs found\")\n                            break\n                    else:\n                        raise ValueError(\n                            \"\"\"FeatureByIDs must be provided when items_features\\\n                                of shape (n_choices, 1) is given.\"\"\"\n                        )\n\n                elif len(sub_features[0][0]) != len(sub_names):\n                    raise ValueError(\n                        f\"\"\"{sub_k}-th given items_features_by_choice with names\n                        {sub_names} and\n                        items_features_by_choice_names shapes do not match:\n                        {len(sub_features[0][0])} and {len(sub_names)}.\"\"\"\n                    )\n            self._return_items_features_by_choice_tuple = True\n\n        # In this case names are missing, still transform it as a tuple\n        elif items_features_by_choice is not None:\n            logging.warning(\n                \"\"\"Items Features Names were not provided, will not be able to\n                            fit models needing them such as Conditional Logit.\"\"\"\n            )\n            self._return_items_features_by_choice_tuple = True\n            items_features_by_choice_names = (None,) * len(items_features_by_choice)\n\n        else:\n            self._return_items_features_by_choice_tuple = False\n\n        # --------- [Normalizing features types (DataFrame, List, etc...) -&gt; np.ndarray] --------- #\n        #\n        # Part of this code is for handling features given as pandas.DataFrame\n        # Basically it transforms them to be internally stocked as np.ndarray and keep columns\n        # names as features names\n\n        # Handling shared features\n        if shared_features_by_choice is not None:\n            for i, feature in enumerate(shared_features_by_choice):\n                if isinstance(feature, pd.DataFrame):\n                    # Ordering choices by id ?\n                    if \"choice_id\" in feature.columns:\n                        feature = feature.set_index(\"choice_id\")\n                    shared_features_by_choice = (\n                        shared_features_by_choice[:i]\n                        + (feature.loc[np.sort(feature.index)].to_numpy(),)\n                        + shared_features_by_choice[i + 1 :]\n                    )\n                    if shared_features_by_choice_names[i] is not None:\n                        logging.warning(\n                            f\"\"\"shared_features_by_choice_names {shared_features_by_choice_names[i]}\n                            were given. They will be overwritten with DF columns names:\n                            {feature.columns}\"\"\"\n                        )\n                    shared_features_by_choice_names = (\n                        shared_features_by_choice_names[:i]\n                        + (feature.columns,)\n                        + shared_features_by_choice_names[i + 1 :]\n                    )\n                elif isinstance(feature, list):\n                    shared_features_by_choice = (\n                        shared_features_by_choice[:i]\n                        + (np.array(feature),)\n                        + shared_features_by_choice[i + 1 :]\n                    )\n        # Handling items_features_by_choice\n        if items_features_by_choice is not None:\n            for i, feature in enumerate(items_features_by_choice):\n                if isinstance(feature, pd.DataFrame):\n                    # Ordering choices by id ?\n                    # TODO: here choice_id was context_id &gt; make sure this change does not affect\n                    # some code somewhere\n                    if \"choice_id\" in feature.columns:\n                        if \"item_id\" in feature.columns:\n                            all_items = np.sort(feature.item_id.unique())\n                            feature_array = []\n                            temp_availabilities = []\n                            for sess in np.sort(feature.choice_id.unique()):\n                                sess_df = feature.loc[feature.choice_id == sess]\n                                sess_df = sess_df[\n                                    sess_df.columns.difference([\"choice_id\"])\n                                ].set_index(\"item_id\")\n                                sess_feature = []\n                                choice_availabilities = []\n                                for item in all_items:\n                                    if item in sess_df.index:\n                                        sess_feature.append(sess_df.loc[item].to_numpy())\n                                        choice_availabilities.append(1)\n                                    else:\n                                        sess_feature.append(np.zeros(len(sess_df.columns)))\n                                        choice_availabilities.append(0)\n                                feature_array.append(sess_feature)\n                                temp_availabilities.append(choice_availabilities)\n\n                            items_features_by_choice = (\n                                items_features_by_choice[:i]\n                                + (np.stack(feature_array, axis=0),)\n                                + items_features_by_choice[i + 1 :]\n                            )\n\n                            if items_features_by_choice_names[i] is not None:\n                                logging.warning(\n                                    f\"\"\"items_features_by_choice_names\n                                    {items_features_by_choice_names[i]} were given. They will be\n                                    overwritten with DF columns names: {feature.columns}\"\"\"\n                                )\n                            items_features_by_choice_names = (\n                                items_features_by_choice_names[:i]\n                                + (sess_df.columns,)\n                                + items_features_by_choice_names[i + 1 :]\n                            )\n                            if (\n                                available_items_by_choice is None\n                                and len(np.unique(temp_availabilities)) &gt; 1\n                            ):\n                                logging.info(\n                                    f\"\"\"available_items_by_choice were not given and computed from\n                                    {i}-th items_features_by_choice.\"\"\"\n                                )\n                                available_items_by_choice = np.array(temp_availabilities)\n                        else:\n                            feature_array = []\n                            for sess in np.sort(feature.choice_id.unique()):\n                                sess_df = feature.loc[feature.choice_id == sess]\n                                sess_df = sess_df[sess_df.columns.difference([\"choice_id\"])]\n                                sess_feature = sess_df.to_numpy()\n                                feature_array.append(sess_feature)\n\n                            items_features_by_choice = (\n                                items_features_by_choice[:i]\n                                + (np.stack(feature_array, axis=0),)\n                                + items_features_by_choice[i + 1 :]\n                            )\n                            if items_features_by_choice_names[i] is not None:\n                                logging.warning(\n                                    f\"\"\"items_features_by_choice_names\n                                    {items_features_by_choice_names[i]} were given. They will be\n                                    overwritten with DF columns names: {feature.columns}\"\"\"\n                                )\n                            items_features_by_choice_names = (\n                                items_features_by_choice_names[:i]\n                                + (feature.columns.difference([\"choice_id\"]),)\n                                + items_features_by_choice_names[i + 1 :]\n                            )\n                    else:\n                        raise ValueError(\n                            f\"\"\"A 'choice_id' column must be integrated in {i}-th\n                            items_features_by_choice DF, in order to identify each choice.\"\"\"\n                        )\n                elif isinstance(feature, list):\n                    items_features_by_choice = (\n                        items_features_by_choice[:i]\n                        + (np.array(feature),)\n                        + items_features_by_choice[i + 1 :]\n                    )\n        # Handling available_items_by_choice\n        if available_items_by_choice is not None:\n            if isinstance(available_items_by_choice, list):\n                available_items_by_choice = np.array(\n                    available_items_by_choice,\n                    dtype=object,  # Are you sure ?\n                )\n            elif isinstance(available_items_by_choice, pd.DataFrame):\n                if \"choice_id\" in available_items_by_choice.columns:\n                    if \"item_id\" in available_items_by_choice.columns:\n                        av_array = []\n                        for sess in np.sort(available_items_by_choice.choice_id.unique()):\n                            sess_df = available_items_by_choice.loc[\n                                available_items_by_choice.choice_id == sess\n                            ]\n                            sess_df = sess_df.drop(\"choice_id\", axis=1)\n                            sess_df = sess_df.set_index(\"item_id\")\n                            av_array.append(sess_df.loc[np.sort(sess_df.index)].to_numpy())\n                        available_items_by_choice = np.squeeze(np.array(av_array))\n                    else:\n                        av_array = []\n                        for sess in np.sort(available_items_by_choice.choice_id.unique()):\n                            sess_df = available_items_by_choice.loc[\n                                available_items_by_choice.choice_id == sess\n                            ]\n                            sess_df = sess_df.drop(\"choice_id\", axis=1)\n                            av_array.append(sess_df.to_numpy())\n                        available_items_by_choice = np.squeeze(np.array(av_array))\n                else:\n                    logging.info(\n                        \"No 'choice_id' column found in available_items_by_choice DF, using index\"\n                    )\n                    available_items_by_choice = available_items_by_choice.to_numpy().reshape(\n                        len(choices), -1\n                    )\n\n        # Handling choices\n        # Choices must then be given as the name of the chosen item\n        # Items are sorted by name and attributed an index\n        # TODO: Keep items_id as an attribute ?\n        if isinstance(choices, pd.DataFrame):\n            # Ordering choices by id\n            if \"choice_id\" in choices.columns:\n                choices = choices.set_index(\"choice_id\")\n            choices = choices.loc[np.sort(choices.index)]\n            items = np.sort(np.unique(choices.to_numpy()))\n            # items is the value (str) of the item\n            choices = [np.where(items == c)[0] for c in np.squeeze(choices.to_numpy())]\n            choices = np.squeeze(choices)\n        elif isinstance(choices, pd.Series):\n            choices = choices.to_numpy()\n        elif isinstance(choices, list):\n            choices = np.array(choices)\n\n        # Setting attributes of ChoiceDataset\n        self.shared_features_by_choice = shared_features_by_choice\n        self.items_features_by_choice = items_features_by_choice\n        self.available_items_by_choice = available_items_by_choice\n        self.choices = choices\n\n        for fid in features_by_ids:\n            if not isinstance(fid, Storage):\n                raise ValueError(\"FeaturesByID must be Storage object\")\n        self.features_by_ids = features_by_ids\n\n        self.shared_features_by_choice_names = shared_features_by_choice_names\n        self.items_features_by_choice_names = items_features_by_choice_names\n\n        # What about typing ? should build after check to change it ?\n        (\n            self.shared_features_by_choice_map,\n            self.items_features_by_choice_map,\n        ) = self._build_features_by_ids()\n        self.check_features_by_ids()\n\n        # self.n_choices = len(self.choices)\n\n        # Different consitency checks to ensure everything is coherent\n        self._check_dataset()  # Should handle alone if np.arrays are squeezed\n        self._return_types = self._check_types()\n        self._check_names()\n        # Build .iloc method\n        self.indexer = ChoiceDatasetIndexer(self)\n\n    def _build_features_by_ids(self):\n        \"\"\"Build mapping function.\n\n        Those mapping functions are so that at indexing,\n        the features are rebuilt with the features by id.\n\n        Returns\n        -------\n        tuple\n            indexes and features_by_id of shared_features_by_choice\n        tuple\n            indexes and features_by_id of items_features_by_choice\n        \"\"\"\n        if len(self.features_by_ids) == 0:\n            return {}, {}\n\n        found_av_fid = False\n        for fid in self.features_by_ids:\n            if fid.name == \"available_items_by_choice\":\n                logging.warning(\"FeaturesStorage for available_items_by_choice detected.\")\n                if self.available_items_by_choice is None:\n                    raise ValueError(\n                        \"\"\"Cannot provide availabilities_by_choice as\\\n                        features_by_ids without indexes.\"\"\"\n                    )\n                self.available_items_by_choice = (fid, self.available_items_by_choice)\n                found_av_fid = True\n\n        if (\n            self.shared_features_by_choice_names is None\n            and self.items_features_by_choice_names is None\n            and not found_av_fid\n        ):\n            raise ValueError(\n                \"\"\"\"Features names are needed to match id columns with features_by_id,\n                and none were given. It is possible to either give them as arguments or to pass\n                features as pandas.DataFrames.\"\"\"\n            )\n        if (\n            isinstance(self.shared_features_by_choice_names, tuple)\n            and self.shared_features_by_choice_names[0] is None\n            and isinstance(self.items_features_by_choice_names, tuple)\n            and self.items_features_by_choice_names[0] is None\n            and not found_av_fid\n        ):\n            raise ValueError(\n                \"\"\"\"Features names are needed to match id columns with features_by_id,\n                and none were given. It is possible to either give them as arguments or to pass\n                features as pandas.DataFrames.\"\"\"\n            )\n\n        shared_features_map = {}\n        items_features_map = {}\n\n        if self.shared_features_by_choice_names is not None:\n            for i, feature in enumerate(self.shared_features_by_choice_names):\n                if feature is not None:\n                    for j, column_name in enumerate(feature):\n                        for feature_by_id in self.features_by_ids:\n                            if column_name == feature_by_id.name:\n                                index_dict = shared_features_map.get(i, {})\n                                index_dict[j] = feature_by_id\n                                shared_features_map[i] = index_dict\n                                logging.info(\n                                    f\"\"\"Feature by ID found for shared_features_by_choice:\n                                    {feature_by_id.name}\"\"\"\n                                )\n\n                                # We test a subset of IDs\n                                test_values = self.shared_features_by_choice[i][:, j][\n                                    np.arange(0, len(self.choices), 10)\n                                ]\n                                try:\n                                    for val in test_values:\n                                        feature_by_id.batch[val]\n                                except KeyError:\n                                    raise ValueError(\n                                        f\"\"\"Key {val} in Shared Feature {column_name}\n                                                     not found in {feature_by_id.name}\"\"\"\n                                    )\n\n        if self.items_features_by_choice_names is not None:\n            for i, feature in enumerate(self.items_features_by_choice_names):\n                if feature is not None:\n                    for k, column_name in enumerate(feature):\n                        for feature_by_id in self.features_by_ids:\n                            if column_name == feature_by_id.name:\n                                index_dict = items_features_map.get(i, {})\n                                index_dict[k] = feature_by_id\n                                items_features_map[i] = index_dict\n                                logging.info(\n                                    f\"\"\"Feature by ID found for items_features_by_choice:\n                                    {feature_by_id.name}\"\"\"\n                                )\n\n                                # We test a subset of the IDs\n                                if self.items_features_by_choice[i].ndim == 1:\n                                    test_values = self.items_features_by_choice[i][\n                                        np.arange(0, len(self.choices), 10)\n                                    ]\n                                else:\n                                    test_values = self.items_features_by_choice[i][:, :, k][\n                                        np.arange(0, len(self.choices), 10)\n                                    ]\n                                try:\n                                    for val in test_values:\n                                        feature_by_id.batch[val]\n                                except KeyError:\n                                    raise ValueError(\n                                        f\"\"\"Key {val} in Items Feature {column_name}\n                                                     not found in {feature_by_id.name}\"\"\"\n                                    )\n        # Checking number of found features_by_id\n        num_ff_maps = sum([len(val) for val in shared_features_map.values()])\n        num_if_maps = sum([len(val) for val in items_features_map.values()])\n        if num_ff_maps + num_if_maps != len(self.features_by_ids) - found_av_fid:\n            raise ValueError(\"Some features_by_ids were not matched with features_names.\")\n\n        return shared_features_map, items_features_map\n\n    def check_features_by_ids(self, batch_size=128):\n        \"\"\"Verify that all IDs given in features exist in the corresponding FeaturesStorage.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            batch size used to sample the FeaturesStorage, by default 128\n\n        Returns\n        -------\n        bool\n            Whether the check was successful or not\n        \"\"\"\n        for index_1 in self.shared_features_by_choice_map:\n            for index_2 in self.shared_features_by_choice_map[index_1]:\n                all_values = np.unique(self.shared_features_by_choice[index_1][:, index_2])\n                for i in range(len(all_values) // batch_size + 1):\n                    self.shared_features_by_choice_map[index_1][index_2].batch[\n                        all_values[i * batch_size : (i + 1) * batch_size]\n                    ]\n\n        for index_1 in self.items_features_by_choice_map:\n            for index_2 in self.items_features_by_choice_map[index_1]:\n                if self.items_features_by_choice[index_1].ndim == 1:\n                    all_values = np.unique(self.items_features_by_choice[index_1])\n                else:\n                    all_values = np.unique(self.items_features_by_choice[index_1][:, :, index_2])\n                for i in range(len(all_values) // batch_size + 1):\n                    self.items_features_by_choice_map[index_1][index_2].batch[\n                        all_values[i * batch_size : (i + 1) * batch_size]\n                    ]\n        logging.info(\"Features by ID checked: all IDs have values\")\n        return True\n\n    def _check_dataset(self):\n        \"\"\"Verify that the shapes of the different features are consistent.\n\n        Particularly:\n            - Over number of items\n            - Over number of choices\n        &gt; Verifies that the choices have coherent values.\n        \"\"\"\n        self._check_num_items_shapes()\n        self._check_num_sessions_shapes()\n        self._check_choices_coherence()\n\n    def _check_num_items_shapes(self):\n        \"\"\"Verify that the shapes of the different features are consistent over number of items.\n\n        Particularly:\n            - items_features_by_choice\n            - available_items_by_choice\n        &gt; Sets the argument base_num_items.\n        \"\"\"\n        if self.items_features_by_choice is not None:\n            if self.items_features_by_choice[0].ndim == 1:\n                # items_features_by_choice fully integrated into a FeaturesStorage\n                base_num_items = self.items_features_by_choice_map[0][0].shape[1]\n            else:\n                base_num_items = self.items_features_by_choice[0].shape[1]\n        elif self.available_items_by_choice is not None:\n            if isinstance(self.available_items_by_choice, tuple):\n                base_num_items = (\n                    self.available_items_by_choice[0].get_element_from_index(0).shape[0]\n                )\n            else:\n                base_num_items = self.available_items_by_choice.shape[1]\n        else:\n            logging.warning(\n                \"No items features or items availabilities are defined. Using max value of choices\"\n            )\n            base_num_items = len(np.unique(self.choices))\n\n        logging.info(f\"Number of detected items is {base_num_items}\")\n        self.base_num_items = base_num_items\n\n        if self.items_features_by_choice is not None:\n            for k, items_feature in enumerate(self.items_features_by_choice):\n                if items_feature.ndim == 1:\n                    features_shape = self.items_features_by_choice_map[k][0].shape\n                    if features_shape[1] != base_num_items:\n                        raise ValueError(\n                            f\"\"\"{k}-th 'items_features_by_choice' shape does not match the\n                            detected number of items:\n                            ({items_feature.shape[1]} and {base_num_items})\"\"\"\n                        )\n                else:\n                    if items_feature.shape[1] != base_num_items:\n                        raise ValueError(\n                            f\"\"\"{k}-th 'items_features_by_choice' shape does not match the\n                            detected number of items:\n                            ({items_feature.shape[1]} and {base_num_items})\"\"\"\n                        )\n        if self.available_items_by_choice is not None:\n            if isinstance(self.available_items_by_choice, tuple):\n                extract = self.available_items_by_choice[0].batch[\n                    self.available_items_by_choice[1][0]\n                ]\n            else:\n                extract = self.available_items_by_choice[0]\n            if len(extract) != base_num_items:\n                raise ValueError(\n                    f\"\"\"'available_items_by_choice' shape does not match the\n                        detected number of items: ({len(extract)}\n                        and {base_num_items})\"\"\"\n                )\n\n    def _check_num_sessions_shapes(self):\n        \"\"\"Verify that the shapes of the different features are consistent over nb of sessions.\n\n        Particularly:\n            - shared_features_by_choice\n            - items_features_by_choice\n            - available_items_by_choice\n        &gt; Sets self.base_num_choices argument.\n        \"\"\"\n        self.n_choices = len(self.choices)\n\n        if self.shared_features_by_choice is not None:\n            for k, feature in enumerate(self.shared_features_by_choice):\n                if feature.shape[0] != self.n_choices:\n                    raise ValueError(\n                        f\"\"\"{k}-th 'shared_features_by_choice' shape does not match\n                         the number of choices detected: ({feature.shape[0]}, {self.n_choices})\"\"\"\n                    )\n\n        if self.items_features_by_choice is not None:\n            for k, items_feature in enumerate(self.items_features_by_choice):\n                if items_feature.shape[0] != self.n_choices:\n                    raise ValueError(\n                        f\"\"\"{k}-th 'items_features_by_choice' shape does not match\n                         the number of choices detected: ({items_feature.shape[0]} and\n                         {self.n_choices})\"\"\"\n                    )\n        if self.available_items_by_choice is not None:\n            if isinstance(self.available_items_by_choice, tuple):\n                if len(self.available_items_by_choice[1]) != self.n_choices:\n                    raise ValueError(\n                        f\"\"\"Given 'available_items_by_choice' shape does not match\n                        the number of choices detected: ({len(self.available_items_by_choice[1])}\n                        and {self.n_choices})\"\"\"\n                    )\n            else:\n                if self.available_items_by_choice.shape[0] != self.n_choices:\n                    raise ValueError(\n                        f\"\"\"Given 'available_items_by_choice' shape does not match\n                            the number of choices detected:\n                            ({self.available_items_by_choice.shape[0]}\n                            and {self.n_choices})\"\"\"\n                    )\n\n    def _check_choices_coherence(self):\n        \"\"\"Verify that the choices are coherent with the nb of items present in other features.\n\n        Particularly:\n            - There is no choice index higher than detected number of items\n            - All items are present at least once in the choices\n        \"\"\"\n        if np.max(self.choices) &gt; self.base_num_items - 1:\n            msg = f\"Choices values not coherent with number of items given in features.  \\\n            In particular, max value of choices is {np.max(self.choices)} while number of  \\\n            items is {self.base_num_items}\"\n            raise ValueError(msg)\n\n        unique_choices = set(np.unique(self.choices).flatten())\n        missing_choices = set(np.arange(start=0, stop=self.base_num_items, step=1)) - unique_choices\n        if len(missing_choices) &gt; 0:\n            logging.warning(f\"Some choices never happen in the dataset: {missing_choices}\")\n\n    def _check_types(self):\n        \"\"\"Check types of elements and store it in order to return right types.\n\n        Particularly:\n            - Either int32 or float32 consistently for features.\n                float32 is to be preferred unless One-Hot encoding is used.\n            - float32 for available_items_by_choice\n            - int32 for choices\n        \"\"\"\n        return_types = []\n\n        shared_features_types = []\n        if self.shared_features_by_choice is not None:\n            for feature in self.shared_features_by_choice:\n                if np.issubdtype(feature[0].dtype, np.integer):\n                    shared_features_types.append(np.int32)\n                else:\n                    shared_features_types.append(np.float32)\n        for indexes, f_dict in self.shared_features_by_choice_map.items():\n            sample_dtype = next(iter(f_dict.values())).get_storage_type()\n            shared_features_types[indexes] = sample_dtype\n        return_types.append(tuple(shared_features_types))\n\n        items_features_types = []\n        if self.items_features_by_choice is not None:\n            for items_feat in self.items_features_by_choice:\n                if np.issubdtype(items_feat[0].dtype, np.integer):\n                    items_features_types.append(np.int32)\n                else:\n                    items_features_types.append(np.float32)\n        for indexes, f_dict in self.items_features_by_choice_map.items():\n            sample_dtype = next(iter(f_dict.values())).get_storage_type()\n            items_features_types[indexes] = sample_dtype\n        return_types.append(tuple(items_features_types))\n        return_types.append(np.float32)\n        return_types.append(np.int32)\n\n        return return_types\n\n    def _check_names(self):\n        \"\"\"Verify that names and features shapes are consistent with each other.\"\"\"\n        if self.shared_features_by_choice_names is not None:\n            for k, (name, features) in enumerate(\n                zip(self.shared_features_by_choice_names, self.shared_features_by_choice)\n            ):\n                if name is not None:\n                    if len(name) != features.shape[1]:\n                        raise ValueError(\n                            f\"\"\"Specified {k}th shared_features_by_choice_name has length\n                            {len(name)}while shared_features_by_choice has\n                            {features.shape[1]} elements.\"\"\"\n                        )\n\n        if self.items_features_by_choice_names is not None:\n            for k, (\n                name,\n                features,\n            ) in enumerate(zip(self.items_features_by_choice_names, self.items_features_by_choice)):\n                if name is not None:\n                    if features.ndim &gt; 1:\n                        if len(name) != features.shape[2]:\n                            raise ValueError(\n                                f\"Specified {k}th\\\n                            items_features_by_choice_names has length {len(name)} while \\\n                            items_features_by_choice has {features.shape[2]} elements.\"\n                            )\n                    elif len(name) != 1:\n                        raise ValueError(\n                            f\"Specified {k}th items_features_by_choice_names has length {len(name)}\\\n                            while items_features_by_choice has 1 element.\"\n                        )\n\n    def __len__(self):\n        \"\"\"Return length of the dataset e.g. total number of choices.\n\n        Returns\n        -------\n        int\n            total number of choices\n        \"\"\"\n        return len(self.choices)\n\n    def __str__(self):\n        \"\"\"Return short representation of ChoiceDataset.\n\n        Returns\n        -------\n        str\n            short representation of ChoiceDataset\n        \"\"\"\n        template = \"\"\"First choice is: Shared Features by choice: {}\\n\n                      Items Features by choice: {}\\nAvailable items by choice: {}\\n\n                      Choices: {}\"\"\"\n        return template.format(\n            self.batch[0][0], self.batch[0][1], self.batch[0][2], self.batch[0][3]\n        )\n\n    def get_n_items(self):\n        \"\"\"Access the total number of different items.\n\n        Returns\n        -------\n        int\n            total number of different items\n        \"\"\"\n        return self.base_num_items\n\n    def get_n_choices(self):\n        \"\"\"Access the total number of different choices.\n\n        Redundant with __len__ method.\n\n        Returns\n        -------\n        int\n            total number of different choices\n        \"\"\"\n        return len(self)\n\n    @classmethod\n    def _long_df_to_items_features_array(\n        cls,\n        df,\n        features,\n        items_id_column=\"item_id\",\n        choices_id_column=\"choice_id\",\n        items_index=None,\n        choices_index=None,\n    ):\n        \"\"\"Build items_features_by_choice and available_items_by_choice from dataframe.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            Dataframe containing all the features for each item and sessions\n        items_index : list\n            List of items identifiers\n        choices_index : list\n            List of unique identifiers of choices\n        features : list\n            List of columns of df that represents the items_features (for sessions_items_features)\n        items_id_column: str, optional\n            Name of the column containing the item ids, default is \"items_id\"\n        choices_id_column: str, optional\n            Name of the column containing the choices ids, default is \"choice_id\"\n\n        Returns\n        -------\n        np.ndarray of shape (n_choices, n_items, n_features)\n            Corresponding items_features_by_choice\n        np.ndarray of shape (n_choices, n_items)\n            Corresponding availabilities\n        \"\"\"\n        try:\n            features.remove(\"context_id\")\n        except (AttributeError, ValueError):\n            pass\n        try:\n            features.remove(\"item_id\")\n        except (AttributeError, ValueError):\n            pass\n\n        if choices_index is None:\n            choices_index = np.sort(df[choices_id_column].unique().to_numpy())\n        if items_index is None:\n            items_index = np.sort(df[items_id_column].unique().to_numpy())\n\n        items_features_by_choice = []\n        available_items_by_choice = []\n        for sess in choices_index:\n            sess_df = df.loc[df[choices_id_column] == sess]\n\n            # All items were available for the choice\n            if len(sess_df) == len(items_index):\n                sess_df = sess_df.T\n                sess_df.columns = sess_df.loc[items_id_column]\n                if features is not None:\n                    items_features_by_choice.append(sess_df[items_index].loc[features].T.values)\n                available_items_by_choice.append(np.ones(len(items_index)).astype(\"float32\"))\n\n            # Some items were not available for the choice\n            else:\n                sess_feats = []\n                sess_av = []\n                for item in items_index:\n                    item_df = sess_df.loc[sess_df[items_id_column] == item]\n                    if len(item_df) &gt; 0:\n                        if features is not None:\n                            sess_feats.append(item_df[features].to_numpy()[0])\n                        sess_av.append(1.0)\n                    else:\n                        if features is not None:\n                            # Unavailable items features are filled with zeros\n                            sess_feats.append(np.zeros(len(features)))\n                        sess_av.append(0.0)\n                items_features_by_choice.append(sess_feats)\n                available_items_by_choice.append(sess_av)\n\n        if features is not None:\n            items_features_by_choice = np.array(items_features_by_choice)\n        else:\n            items_features_by_choice = None\n        return items_features_by_choice, np.array(available_items_by_choice).astype(\"float32\")\n\n    @classmethod\n    def from_single_wide_df(\n        cls,\n        df,\n        items_id,\n        shared_features_columns=None,\n        items_features_suffixes=None,\n        items_features_prefixes=None,\n        available_items_suffix=None,\n        available_items_prefix=None,\n        delimiter=\"_\",\n        choices_column=\"choice\",\n        choice_format=\"items_id\",\n    ):\n        \"\"\"Build numpy arrays for ChoiceDataset from a single dataframe in wide format.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            dataframe in Wide format\n        items_id : list\n            List of items ids\n        shared_features_columns : list, optional\n            List of columns of the dataframe that are shared_features_by_choice, default is None\n        items_features_prefixes : list, optional\n            Prefixes of the columns of the dataframe that are items_features_by_choice,\n            default is None\n        items_features_suffixes : list, optional\n            Suffixes of the columns of the dataframe that are items_features_by_choice,\n            default is None\n        available_items_prefix: str, optional\n            Prefix of the columns of the dataframe that precise available_items_by_choice,\n            default is None\n        available_items_suffix: str, optional\n            Suffix of the columns of the dataframe that precise available_items_by_choice,\n            default is None\n        delimiter: str, optional\n            Delimiter used to separate the given prefix or suffixes and the features names,\n            default is \"_\"\n        choice_column: str, optional\n            Name of the column containing the choices, default is \"choice\"\n        choice_format: str, optional\n            How choice is indicated in df, either \"items_id\" or \"items_index\",\n            default is \"items_id\"\n\n        Returns\n        -------\n        ChoiceDataset\n            corresponding ChoiceDataset\n        \"\"\"\n        if available_items_prefix is not None and available_items_suffix is not None:\n            raise ValueError(\n                \"You cannot give both available_items_prefix and\\\n                    available_items_suffix.\"\n            )\n        if choice_format not in [\"items_index\", \"items_id\"]:\n            logging.warning(\"choice_format not understood, defaulting to 'items_index'\")\n\n        if shared_features_columns is not None:\n            shared_features_by_choice = df[shared_features_columns].to_numpy()\n            shared_features_by_choice_names = shared_features_columns\n        else:\n            shared_features_by_choice = None\n            shared_features_by_choice_names = None\n\n        if items_features_suffixes is not None and items_features_prefixes is not None:\n            # The list of features names is the concatenation of the two lists of\n            # prefixes and suffixes\n            items_features_names = items_features_prefixes + items_features_suffixes\n            items_features_by_choice = []\n            for item in items_id:\n                columns = [f\"{feature}{delimiter}{item}\" for feature in items_features_prefixes] + [\n                    f\"{item}{delimiter}{feature}\" for feature in items_features_suffixes\n                ]\n                for col in columns:\n                    if col not in df.columns:\n                        logging.warning(\n                            f\"Column {col} was not in DataFrame,\\\n                            dummy creation of the feature with zeros.\"\n                        )\n                        df[col] = 0\n                items_features_by_choice.append(df[columns].to_numpy())\n            items_features_by_choice = np.stack(items_features_by_choice, axis=1)\n        elif items_features_suffixes is not None:\n            items_features_names = items_features_suffixes\n            items_features_by_choice = []\n            for item in items_id:\n                columns = [f\"{item}{delimiter}{feature}\" for feature in items_features_suffixes]\n                for col in columns:\n                    if col not in df.columns:\n                        logging.warning(\n                            f\"Column {col} was not in DataFrame,\\\n                            dummy creation of the feature with zeros.\"\n                        )\n                        df[col] = 0\n                items_features_by_choice.append(df[columns].to_numpy())\n            items_features_by_choice = np.stack(items_features_by_choice, axis=1)\n        elif items_features_prefixes is not None:\n            items_features_names = items_features_prefixes\n            items_features_by_choice = []\n            for item in items_id:\n                columns = [f\"{feature}{delimiter}{item}\" for feature in items_features_prefixes]\n                for col in columns:\n                    if col not in df.columns:\n                        logging.warning(\n                            f\"Column {col} was not in DataFrame,\\\n                            dummy creation of the feature with zeros.\"\n                        )\n                        df[col] = 0\n                items_features_by_choice.append(df[columns].to_numpy())\n            items_features_by_choice = np.stack(items_features_by_choice, axis=1)\n        else:\n            items_features_by_choice = None\n            items_features_names = None\n\n        if available_items_suffix is not None:\n            if isinstance(available_items_suffix, list):\n                if not len(available_items_suffix) == len(items_id):\n                    raise ValueError(\n                        \"You have given a list of columns for availabilities.\"\n                        \"We consider that it is one for each item however lenghts do not match\"\n                    )\n                logging.info(\"You have given a list of columns for availabilities.\")\n                logging.info(\"Each column will be matched to an item, given their order\")\n                available_items_by_choice = df[available_items_suffix].to_numpy()\n            else:\n                columns = [f\"{item}{delimiter}{available_items_suffix}\" for item in items_id]\n                available_items_by_choice = df[columns].to_numpy()\n        elif available_items_prefix is not None:\n            if isinstance(available_items_prefix, list):\n                if not len(available_items_prefix) == len(items_id):\n                    raise ValueError(\n                        \"You have given a list of columns for availabilities.\"\n                        \"We consider that it is one for each item however lenghts do not match\"\n                    )\n                logging.info(\"You have given a list of columns for availabilities.\")\n                logging.info(\"Each column will be matched to an item, given their order\")\n                available_items_by_choice = df[available_items_prefix].to_numpy()\n            else:\n                columns = [f\"{available_items_prefix}{delimiter}{item}\" for item in items_id]\n                available_items_by_choice = df[columns].to_numpy()\n        else:\n            available_items_by_choice = None\n\n        choices = df[choices_column].to_numpy()\n        if choice_format == \"items_id\":\n            if items_id is None:\n                raise ValueError(\"items_id must be given to use choice_format='items_id'\")\n            items_id = np.array(items_id)\n            choices = np.squeeze([np.where(items_id == c)[0] for c in choices])\n            if choices.size == 0:\n                raise ValueError(\"No choice found in the items_id list\")\n\n        return ChoiceDataset(\n            shared_features_by_choice=shared_features_by_choice,\n            shared_features_by_choice_names=shared_features_by_choice_names,\n            items_features_by_choice=items_features_by_choice,\n            items_features_by_choice_names=items_features_names,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n\n    @classmethod\n    def from_single_long_df(\n        cls,\n        df,\n        choices_column=\"choice\",\n        items_id_column=\"item_id\",\n        choices_id_column=\"choice_id\",\n        shared_features_columns=None,\n        items_features_columns=None,\n        choice_format=\"items_id\",\n    ):\n        \"\"\"Build numpy arrays for ChoiceDataset from a single dataframe in long format.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            dataframe in Long format\n        choices_column: str, optional\n            Name of the column containing the choices, default is \"choice\"\n        items_id_column: str, optional\n            Name of the column containing the item ids, default is \"items_id\"\n        choices_id_column: str, optional\n            Name of the column containing the choice ids. It is used to identify all rows\n            about a single choice, default is \"choice_id\"\n        shared_features_columns : list\n            Columns of the dataframe that are shared_features_by_choice, default is None\n        items_features_columns : list\n            Columns of the dataframe that are items_features_by_choice, default is None\n        choice_format: str, optional\n            How choice is indicated in df, either \"items_name\" or \"one_zero\",\n            default is \"items_id\"\n\n        Returns\n        -------\n        ChoiceDataset\n            corresponding ChoiceDataset\n        \"\"\"\n        # Ordering items and choices by id\n        items = np.sort(df[items_id_column].unique())\n        choices_ids = np.sort(df[choices_id_column].unique())\n\n        if shared_features_columns is not None:\n            shared_features_by_choice = df[\n                shared_features_columns + [choices_id_column]\n            ].drop_duplicates()\n            shared_features_by_choice = shared_features_by_choice.set_index(choices_id_column)\n            shared_features_by_choice = shared_features_by_choice.loc[choices_ids].to_numpy()\n\n            shared_features_by_choice_names = shared_features_columns\n        else:\n            shared_features_by_choice = None\n            shared_features_by_choice_names = None\n\n        (\n            items_features_by_choice,\n            avaialble_items_by_choice,\n        ) = cls._long_df_to_items_features_array(\n            df,\n            features=items_features_columns,\n            items_id_column=items_id_column,\n            choices_id_column=choices_id_column,\n            items_index=items,\n            choices_index=choices_ids,\n        )\n\n        items_features_by_choice_names = items_features_columns\n\n        if choice_format == \"items_id\":\n            choices = df[[choices_column, choices_id_column]].drop_duplicates(choices_id_column)\n            choices = choices.set_index(choices_id_column)\n            choices = choices.loc[choices_ids].to_numpy()\n            # items is the value (str) of the item\n            choices = np.squeeze([np.where(items == c)[0] for c in choices])\n        elif choice_format == \"one_zero\":\n            choices = df[[items_id_column, choices_column, choices_id_column]]\n            choices = choices.loc[choices[choices_column] == 1]\n            choices = choices.set_index(choices_id_column)\n            choices = (\n                choices.loc[choices_ids][items_id_column]\n                .map({k: v for v, k in enumerate(items)})\n                .to_numpy()\n            )\n        else:\n            raise ValueError(\n                f\"choice_format {choice_format} not recognized. Must be in ['items_id', 'one_zero']\"\n            )\n        return ChoiceDataset(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=avaialble_items_by_choice,\n            choices=choices,\n            shared_features_by_choice_names=shared_features_by_choice_names,\n            items_features_by_choice_names=items_features_by_choice_names,\n        )\n\n    def save(self):\n        \"\"\"Save the dataset.\"\"\"\n        raise NotImplementedError\n\n    def summary(self):\n        \"\"\"Display a summary of the dataset.\"\"\"\n        print(\"%=====================================================================%\")\n        print(\"%%% Summary of the dataset:\")\n        print(\"%=====================================================================%\")\n        print(\"Number of items:\", self.get_n_items())\n        print(\n            \"Number of choices:\",\n            len(self),\n        )\n        print(\"%=====================================================================%\")\n\n        if self.shared_features_by_choice is not None:\n            print(\" Shared Features by Choice:\")\n            print(f\" {sum([f.shape[1] for f in self.shared_features_by_choice])} shared features\")\n            if self.shared_features_by_choice_names is not None:\n                if self.shared_features_by_choice_names[0] is not None:\n                    print(f\" with names: {self.shared_features_by_choice_names}\")\n        else:\n            print(\" No Shared Features by Choice registered\")\n        print(\"\\n\")\n\n        if self.items_features_by_choice is not None:\n            if self.items_features_by_choice[0] is not None:\n                print(\" Items Features by Choice:\")\n                print(\n                    f\"\"\"{\n                        sum(\n                            [\n                                f.shape[2] if f.ndim == 3 else 1\n                                for f in self.items_features_by_choice\n                            ]\n                        )\n                    } items features \"\"\"\n                )\n                if self.items_features_by_choice_names is not None:\n                    if self.items_features_by_choice_names[0] is not None:\n                        print(f\" with names: {self.items_features_by_choice_names}\")\n        else:\n            print(\" No Items Features by Choice registered\")\n        print(\"%=====================================================================%\")\n        return \"\"\n\n    def get_choices_batch(self, choices_indexes, features=None):\n        \"\"\"Access a chunk of data within the ChoiceDataset from choice indexes.\n\n        Parameters\n        ----------\n        choices_indexes : int or list of int or slice\n            indexes of the choices (that will be mapped to choice &amp; session indexes) to return\n        features : list of str, optional\n            list of features to return. None returns all of them, default is None.\n\n        Returns\n        -------\n        tuple of (array_like, )\n            tuple of arrays containing a batch of shared_features_by_choice\n        tuple of (array_like, )\n            tuple of arrays containing a batch of items_features_by_choice\n        array_like\n            array containing a batch of availables_items_by_choice\n        array_like\n            array containing a batch of choices\n        \"\"\"\n        _ = features\n        if isinstance(choices_indexes, list):\n            if np.array(choices_indexes).ndim &gt; 1:\n                raise ValueError(\n                    \"\"\"ChoiceDataset unidimensional can only be batched along choices\n                                 dimension received a list with several axis of indexing.\"\"\"\n                )\n            if self.shared_features_by_choice is None:\n                shared_features_by_choice = None\n            else:\n                shared_features_by_choice = list(\n                    shared_features_by_choice[choices_indexes]\n                    # .astype(self._return_types[1][i])\n                    for i, shared_features_by_choice in enumerate(self.shared_features_by_choice)\n                )\n\n            if self.items_features_by_choice is None:\n                items_features_by_choice = None\n            else:\n                items_features_by_choice = list(\n                    items_features_by_choice[choices_indexes]\n                    # .astype(self._return_types[2][i])\n                    for _, items_features_by_choice in enumerate(self.items_features_by_choice)\n                )\n            if self.available_items_by_choice is None:\n                available_items_by_choice = np.ones(\n                    (len(choices_indexes), self.base_num_items)\n                ).astype(\"float32\")\n            else:\n                if isinstance(self.available_items_by_choice, tuple):\n                    available_items_by_choice = self.available_items_by_choice[0].batch[\n                        self.available_items_by_choice[1][choices_indexes]\n                    ]\n                else:\n                    available_items_by_choice = self.available_items_by_choice[choices_indexes]\n                # .astype(self._return_types[3])\n\n            choices = self.choices[choices_indexes].astype(self._return_types[3])\n\n            if len(self.shared_features_by_choice_map) &gt; 0:\n                mapped_features = []\n                for tuple_index in range(len(shared_features_by_choice)):\n                    if tuple_index in self.shared_features_by_choice_map.keys():\n                        feat_ind_min = 0\n                        unstacked_feat = []\n                        for feature_index in np.sort(\n                            list(self.shared_features_by_choice_map[tuple_index].keys())\n                        ):\n                            if feat_ind_min != feature_index:\n                                unstacked_feat.append(\n                                    shared_features_by_choice[tuple_index][\n                                        :, feat_ind_min:feature_index\n                                    ]\n                                )\n                            unstacked_feat.append(\n                                self.shared_features_by_choice_map[tuple_index][\n                                    feature_index\n                                ].batch[shared_features_by_choice[tuple_index][:, feature_index]]\n                            )\n                            feat_ind_min = feature_index + 1\n                        if feat_ind_min != shared_features_by_choice[tuple_index].shape[1]:\n                            unstacked_feat.append(\n                                shared_features_by_choice[tuple_index][:, feat_ind_min:]\n                            )\n                        mapped_features.append(np.concatenate(unstacked_feat, axis=1))\n                    else:\n                        mapped_features.append(shared_features_by_choice[tuple_index])\n\n                shared_features_by_choice = mapped_features\n\n            if len(self.items_features_by_choice_map) &gt; 0:\n                mapped_features = []\n                for tuple_index in range(len(items_features_by_choice)):\n                    if tuple_index in self.items_features_by_choice_map.keys():\n                        if items_features_by_choice[tuple_index].ndim == 1:\n                            mapped_features.append(\n                                self.items_features_by_choice_map[tuple_index][0].batch[\n                                    items_features_by_choice[tuple_index]\n                                ]\n                            )\n                        else:\n                            feat_ind_min = 0\n                            unstacked_feat = []\n                            for feature_index in np.sort(\n                                list(self.items_features_by_choice_map[tuple_index].keys())\n                            ):\n                                if feat_ind_min != feature_index:\n                                    unstacked_feat.append(\n                                        items_features_by_choice[tuple_index][\n                                            :, :, feat_ind_min:feature_index\n                                        ]\n                                    )\n                                unstacked_feat.append(\n                                    self.items_features_by_choice_map[tuple_index][\n                                        feature_index\n                                    ].batch[\n                                        items_features_by_choice[tuple_index][:, :, feature_index]\n                                    ]\n                                )\n                                feat_ind_min = feature_index + 1\n                            if feat_ind_min != items_features_by_choice[tuple_index].shape[2]:\n                                unstacked_feat.append(\n                                    shared_features_by_choice[tuple_index][:, :, feat_ind_min:]\n                                )\n                            mapped_features.append(np.concatenate(unstacked_feat, axis=2))\n                    else:\n                        mapped_features.append(items_features_by_choice[tuple_index])\n\n                items_features_by_choice = mapped_features\n\n            if shared_features_by_choice is not None:\n                for i in range(len(shared_features_by_choice)):\n                    shared_features_by_choice[i] = shared_features_by_choice[i].astype(\n                        self._return_types[0][i]\n                    )\n                if not self._return_shared_features_by_choice_tuple:\n                    shared_features_by_choice = shared_features_by_choice[0]\n                else:\n                    shared_features_by_choice = tuple(shared_features_by_choice)\n\n            if items_features_by_choice is not None:\n                for i in range(len(items_features_by_choice)):\n                    items_features_by_choice[i] = items_features_by_choice[i].astype(\n                        self._return_types[1][i]\n                    )\n                # items_features_by_choice were not given as a tuple, so we return do not return\n                # it as a tuple\n                if not self._return_items_features_by_choice_tuple:\n                    items_features_by_choice = items_features_by_choice[0]\n                else:\n                    items_features_by_choice = tuple(items_features_by_choice)\n\n            return (\n                shared_features_by_choice,\n                items_features_by_choice,\n                available_items_by_choice,\n                choices,\n            )\n\n        if isinstance(choices_indexes, slice):\n            return self.get_choices_batch(\n                list(range(*choices_indexes.indices(self.choices.shape[0])))\n            )\n\n        choices_indexes = [choices_indexes]\n        (\n            shared_features_by_choices,\n            items_features_by_choice,\n            available_items_by_choice,\n            choice,\n        ) = self.get_choices_batch(choices_indexes)\n        if shared_features_by_choices is not None:\n            if isinstance(shared_features_by_choices, tuple):\n                shared_features_by_choices = tuple(feat[0] for feat in shared_features_by_choices)\n            else:\n                shared_features_by_choices = shared_features_by_choices[0]\n        if items_features_by_choice is not None:\n            if isinstance(items_features_by_choice, tuple):\n                items_features_by_choice = tuple(feat[0] for feat in items_features_by_choice)\n            else:\n                items_features_by_choice = items_features_by_choice[0]\n\n        return (\n            shared_features_by_choices,\n            items_features_by_choice,\n            available_items_by_choice[0],\n            choice[0],\n        )\n\n    def __getitem__(self, choices_indexes):\n        \"\"\"Create a sub-ChoiceDataset with only a subset of choices, from their indexes.\n\n        Parameters\n        ----------\n        choices_indexes : np.ndarray\n            indexes of the choices to keep, shape should be (num_choices,)\n\n        Returns\n        -------\n        ChoiceDataset\n            ChoiceDataset with only the sessions indexed by indexes\n        \"\"\"\n        if isinstance(choices_indexes, int):\n            choices_indexes = [choices_indexes]\n        elif isinstance(choices_indexes, slice):\n            return self.__getitem__(list(range(*choices_indexes.indices(len(self.choices)))))\n\n        try:\n            if self.shared_features_by_choice[0] is None:\n                shared_features_by_choice = None\n            else:\n                shared_features_by_choice = tuple(\n                    self.shared_features_by_choice[i][choices_indexes]\n                    for i in range(len(self.shared_features_by_choice))\n                )\n                if not self._return_shared_features_by_choice_tuple:\n                    shared_features_by_choice = shared_features_by_choice[0]\n        except TypeError:\n            shared_features_by_choice = None\n\n        try:\n            if self.items_features_by_choice[0] is None:\n                items_features_by_choice = None\n            else:\n                items_features_by_choice = tuple(\n                    self.items_features_by_choice[i][choices_indexes]\n                    for i in range(len(self.items_features_by_choice))\n                )\n                if not self._return_items_features_by_choice_tuple:\n                    items_features_by_choice = items_features_by_choice[0]\n        except TypeError:\n            items_features_by_choice = None\n\n        try:\n            if self.shared_features_by_choice_names[0] is None:\n                shared_features_by_choice_names = None\n            else:\n                shared_features_by_choice_names = self.shared_features_by_choice_names\n                if not self._return_shared_features_by_choice_tuple:\n                    shared_features_by_choice_names = shared_features_by_choice_names[0]\n        except TypeError:\n            shared_features_by_choice_names = None\n        try:\n            if self.items_features_by_choice_names[0] is None:\n                items_features_by_choice_names = None\n            else:\n                items_features_by_choice_names = self.items_features_by_choice_names\n                if not self._return_items_features_by_choice_tuple:\n                    items_features_by_choice_names = items_features_by_choice_names[0]\n        except TypeError:\n            items_features_by_choice_names = None\n\n        try:\n            if isinstance(self.available_items_by_choice, tuple):\n                available_items_by_choice = self.available_items_by_choice[1][choices_indexes]\n            else:\n                available_items_by_choice = self.available_items_by_choice[choices_indexes]\n        except TypeError:\n            available_items_by_choice = None\n\n        return ChoiceDataset(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=[self.choices[i] for i in choices_indexes],\n            shared_features_by_choice_names=shared_features_by_choice_names,\n            items_features_by_choice_names=items_features_by_choice_names,\n            features_by_ids=self.features_by_ids,\n        )\n\n    @property\n    def batch(self):\n        \"\"\"Indexer. Corresponds to get_choice_batch, but with [] logic.\"\"\"\n        return self.indexer\n\n    def iter_batch(self, batch_size, shuffle=False, sample_weight=None):\n        \"\"\"Iterate over dataset return batches of length batch_size.\n\n        Newer version.\n\n        Parameters\n        ----------\n        batch_size : int\n            batch size to set\n        shuffle: bool\n            Whether or not to shuffle the dataset\n        sample_weight : Iterable\n            list of weights to be returned with the right indexing during the shuffling\n        \"\"\"\n        if sample_weight is not None and isinstance(sample_weight, list):\n            sample_weight = np.array(sample_weight)\n        if batch_size == -1 or batch_size == len(self):\n            yield self.indexer.get_full_dataset(sample_weight=sample_weight)\n        else:\n            # Get indexes for each choice\n            num_choices = len(self)\n            indexes = np.arange(num_choices)\n\n            # Shuffle indexes\n            if shuffle and not batch_size == -1:\n                indexes = np.random.permutation(indexes)\n\n            yielded_size = 0\n            while yielded_size &lt; num_choices:\n                # Return sample_weight if not None, for index matching\n                batch_indexes = indexes[yielded_size : yielded_size + batch_size].tolist()\n                if sample_weight is not None:\n                    yield (\n                        self.batch[batch_indexes],\n                        sample_weight[batch_indexes],\n                    )\n                else:\n                    yield self.batch[batch_indexes]\n                yielded_size += batch_size\n\n    def filter(self, bool_list):\n        \"\"\"Filter over sessions indexes following bool.\n\n        Parameters\n        ----------\n        bool_list : list of boolean\n            list of booleans of length self.get_n_choices() to filter choices.\n            True to keep, False to discard.\n        \"\"\"\n        indexes = [i for i, keep in enumerate(bool_list) if keep]\n        return self[indexes]\n\n    def get_n_shared_features(self):\n        \"\"\"Access the number of shared features.\n\n        Returns\n        -------\n        int\n            number of shared items features\n        \"\"\"\n        if self.shared_features_by_choice is not None:\n            n_features = 0\n            for shared_features in self.shared_features_by_choice:\n                n_features += shared_features.shape[1]\n            return n_features\n        return 0\n\n    def get_n_items_features(self):\n        \"\"\"Access the number of items features.\n\n        Returns\n        -------\n        int\n            number of items features\n        \"\"\"\n        if self.items_features_by_choice is not None:\n            n_features = 0\n            for items_features in self.items_features_by_choice:\n                n_features += items_features.shape[2]\n            return n_features\n        return 0\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Indexer. Corresponds to get_choice_batch, but with [] logic.</p>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.__getitem__","title":"<code>__getitem__(choices_indexes)</code>","text":"<p>Create a sub-ChoiceDataset with only a subset of choices, from their indexes.</p> <p>Parameters:</p> Name Type Description Default <code>choices_indexes</code> <code>ndarray</code> <p>indexes of the choices to keep, shape should be (num_choices,)</p> required <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>ChoiceDataset with only the sessions indexed by indexes</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def __getitem__(self, choices_indexes):\n    \"\"\"Create a sub-ChoiceDataset with only a subset of choices, from their indexes.\n\n    Parameters\n    ----------\n    choices_indexes : np.ndarray\n        indexes of the choices to keep, shape should be (num_choices,)\n\n    Returns\n    -------\n    ChoiceDataset\n        ChoiceDataset with only the sessions indexed by indexes\n    \"\"\"\n    if isinstance(choices_indexes, int):\n        choices_indexes = [choices_indexes]\n    elif isinstance(choices_indexes, slice):\n        return self.__getitem__(list(range(*choices_indexes.indices(len(self.choices)))))\n\n    try:\n        if self.shared_features_by_choice[0] is None:\n            shared_features_by_choice = None\n        else:\n            shared_features_by_choice = tuple(\n                self.shared_features_by_choice[i][choices_indexes]\n                for i in range(len(self.shared_features_by_choice))\n            )\n            if not self._return_shared_features_by_choice_tuple:\n                shared_features_by_choice = shared_features_by_choice[0]\n    except TypeError:\n        shared_features_by_choice = None\n\n    try:\n        if self.items_features_by_choice[0] is None:\n            items_features_by_choice = None\n        else:\n            items_features_by_choice = tuple(\n                self.items_features_by_choice[i][choices_indexes]\n                for i in range(len(self.items_features_by_choice))\n            )\n            if not self._return_items_features_by_choice_tuple:\n                items_features_by_choice = items_features_by_choice[0]\n    except TypeError:\n        items_features_by_choice = None\n\n    try:\n        if self.shared_features_by_choice_names[0] is None:\n            shared_features_by_choice_names = None\n        else:\n            shared_features_by_choice_names = self.shared_features_by_choice_names\n            if not self._return_shared_features_by_choice_tuple:\n                shared_features_by_choice_names = shared_features_by_choice_names[0]\n    except TypeError:\n        shared_features_by_choice_names = None\n    try:\n        if self.items_features_by_choice_names[0] is None:\n            items_features_by_choice_names = None\n        else:\n            items_features_by_choice_names = self.items_features_by_choice_names\n            if not self._return_items_features_by_choice_tuple:\n                items_features_by_choice_names = items_features_by_choice_names[0]\n    except TypeError:\n        items_features_by_choice_names = None\n\n    try:\n        if isinstance(self.available_items_by_choice, tuple):\n            available_items_by_choice = self.available_items_by_choice[1][choices_indexes]\n        else:\n            available_items_by_choice = self.available_items_by_choice[choices_indexes]\n    except TypeError:\n        available_items_by_choice = None\n\n    return ChoiceDataset(\n        shared_features_by_choice=shared_features_by_choice,\n        items_features_by_choice=items_features_by_choice,\n        available_items_by_choice=available_items_by_choice,\n        choices=[self.choices[i] for i in choices_indexes],\n        shared_features_by_choice_names=shared_features_by_choice_names,\n        items_features_by_choice_names=items_features_by_choice_names,\n        features_by_ids=self.features_by_ids,\n    )\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.__init__","title":"<code>__init__(choices, shared_features_by_choice=None, items_features_by_choice=None, available_items_by_choice=None, features_by_ids=[], shared_features_by_choice_names=None, items_features_by_choice_names=None)</code>","text":"<p>Build the ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>choices</code> <p>list of chosen items indexes</p> required <code>shared_features_by_choice</code> <code>tuple of (array_like, )</code> <p>matrix of shape (num_choices, num_shared_features) containing the features of the different choices that are common to all items (e.g. store features, customer features, etc...)</p> <code>None</code> <code>items_features_by_choice</code> <code>tuple of (array_like, )</code> <p>matrix of shape (num_choices, num_items, num_items_features) containing the features of the items that change over time (e.g. price, promotion, etc...), default is None</p> <code>is None</code> <code>available_items_by_choice</code> <code>array_like</code> <p>matrix of shape (num_choices, num_items) containing the availabilities of the items over the different choices, default is None</p> <code>None</code> <code>features_by_ids</code> <code>list of (name, FeaturesStorage)</code> <p>List of Storage objects. Their name must correspond to a feature name among shared_features_by_choice or items_features_by_choice and their ids must match to those features values. Default is []</p> <code>[]</code> <code>shared_features_by_choice_names</code> <code>tuple of (array_like, )</code> <p>list of names of the shared_features_by_choice, default is None Shapes must match with shared_features_by_choice</p> <code>None</code> <code>items_features_by_choice_names</code> <code>tuple of (array_like, )</code> <p>list of names of the items_features_by_choice, default is None Shapes must match with items_features_by_choice</p> <code>None</code> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def __init__(\n    self,\n    choices,  # Should not have None as default value ?\n    shared_features_by_choice=None,  # as many as choices.  values or ids (look at key)\n    items_features_by_choice=None,\n    available_items_by_choice=None,\n    features_by_ids=[],  # list of (name, FeaturesStorage)\n    shared_features_by_choice_names=None,\n    items_features_by_choice_names=None,\n):\n    \"\"\"Build the ChoiceDataset.\n\n    Parameters\n    ----------\n    choices: list or np.ndarray\n        list of chosen items indexes\n    shared_features_by_choice : tuple of (array_like, )\n        matrix of shape (num_choices, num_shared_features) containing the features of the\n        different choices that are common to all items (e.g. store features,\n        customer features, etc...)\n    items_features_by_choice : tuple of (array_like, ), default is None\n        matrix of shape (num_choices, num_items, num_items_features)\n        containing the features\n        of the items that change over time (e.g. price, promotion, etc...), default is None\n    available_items_by_choice : array_like\n        matrix of shape (num_choices, num_items) containing the availabilities of the items\n        over the different choices, default is None\n    features_by_ids : list of (name, FeaturesStorage)\n        List of Storage objects. Their name must correspond to a feature name\n        among shared_features_by_choice or items_features_by_choice\n        and their ids must match to those features values. Default is []\n    shared_features_by_choice_names : tuple of (array_like, )\n        list of names of the shared_features_by_choice, default is None\n        Shapes must match with shared_features_by_choice\n    items_features_by_choice_names : tuple of (array_like, )\n        list of names of the items_features_by_choice, default is None\n        Shapes must match with items_features_by_choice\n    \"\"\"\n    if choices is None:\n        # Done to keep a logical order of arguments, and has logic: choices have to be specified\n        raise ValueError(\"Choices must be specified, got None\")\n\n    # --------- [\u00a0Handling features type given as tuples or not ] --------- #\n\n    # If shared_features_by_choice is not given as tuple, transform it internally as a tuple\n    # A bit longer because can be None and need to also handle names\n    if shared_features_by_choice is not None:\n        if not isinstance(shared_features_by_choice, tuple):\n            self._return_shared_features_by_choice_tuple = False\n            if shared_features_by_choice_names is not None:\n                if len(shared_features_by_choice[0]) != len(shared_features_by_choice_names):\n                    raise ValueError(\n                        f\"\"\"Number of features given does not match\n                                     number of features names given:\n                                       {len(shared_features_by_choice[0])} and\n                                        {len(shared_features_by_choice_names)}\"\"\"\n                    )\n            else:\n                logging.warning(\n                    \"\"\"Shared Features Names were not provided, will not be able to\n                                fit models needing them such as Conditional Logit.\"\"\"\n                )\n\n            shared_features_by_choice_names = (shared_features_by_choice_names,)\n            shared_features_by_choice = (shared_features_by_choice,)\n\n        # choices_features is already a tuple, names are given, checking consistency\n        else:\n            self._return_shared_features_by_choice_tuple = True\n            if shared_features_by_choice_names is not None:\n                for sub_k, (sub_features, sub_names) in enumerate(\n                    zip(shared_features_by_choice, shared_features_by_choice_names)\n                ):\n                    if len(sub_features[0]) != len(sub_names):\n                        raise ValueError(\n                            f\"\"\"{sub_k}-th given shared_features_by_choice and\n                            shared_features_by_choice_names shapes do not match:\n                            {len(sub_features[0])} and {len(sub_names)}.\"\"\"\n                        )\n            # In this case names are missing, still transform it as a tuple\n            else:\n                logging.warning(\n                    \"\"\"Shared Features Names were not provided, will not be able to\n                                fit models needing them such as Conditional Logit.\"\"\"\n                )\n                shared_features_by_choice_names = (None,) * len(shared_features_by_choice)\n    else:\n        self._return_shared_features_by_choice_tuple = False\n\n    # If items_features_by_choice is not given as tuple, transform it internally as a tuple\n    # A bit longer because can be None and need to also handle names\n\n    if not isinstance(items_features_by_choice, tuple) and items_features_by_choice is not None:\n        self._return_items_features_by_choice_tuple = False\n        if items_features_by_choice_names is not None:\n            if len(items_features_by_choice[0][0]) != len(items_features_by_choice_names):\n                raise ValueError(\n                    f\"\"\"Number of items_features_by_choice given does not match\n                                 number of items_features_by_choice_names given:\n                                 {len(items_features_by_choice[0][0])} and\n                                 {len(items_features_by_choice_names)}\"\"\"\n                )\n        else:\n            logging.warning(\n                \"\"\"Items Features Names were not provided, will not be able to\n                            fit models needing them such as Conditional Logit.\"\"\"\n            )\n        items_features_by_choice = (items_features_by_choice,)\n        items_features_by_choice_names = (items_features_by_choice_names,)\n\n    # items_features_by_choice is already a tuple, names are given, checking consistency\n    elif items_features_by_choice is not None and items_features_by_choice_names is not None:\n        for sub_k, (sub_features, sub_names) in enumerate(\n            zip(items_features_by_choice, items_features_by_choice_names)\n        ):\n            # Split if feature is full FeaturesStorage\n            if np.array(sub_features).ndim == 1:\n                # check features_by_ids\n                logging.info(\n                    \"feature of dimension 1 detected -  a FeatureByIDs MUST be provided\"\n                )\n                for fbid in features_by_ids:\n                    if fbid.name == sub_names[0]:\n                        logging.info(\"FeatureByIDs found\")\n                        break\n                else:\n                    raise ValueError(\n                        \"\"\"FeatureByIDs must be provided when items_features\\\n                            of shape (n_choices, 1) is given.\"\"\"\n                    )\n\n            elif len(sub_features[0][0]) != len(sub_names):\n                raise ValueError(\n                    f\"\"\"{sub_k}-th given items_features_by_choice with names\n                    {sub_names} and\n                    items_features_by_choice_names shapes do not match:\n                    {len(sub_features[0][0])} and {len(sub_names)}.\"\"\"\n                )\n        self._return_items_features_by_choice_tuple = True\n\n    # In this case names are missing, still transform it as a tuple\n    elif items_features_by_choice is not None:\n        logging.warning(\n            \"\"\"Items Features Names were not provided, will not be able to\n                        fit models needing them such as Conditional Logit.\"\"\"\n        )\n        self._return_items_features_by_choice_tuple = True\n        items_features_by_choice_names = (None,) * len(items_features_by_choice)\n\n    else:\n        self._return_items_features_by_choice_tuple = False\n\n    # --------- [Normalizing features types (DataFrame, List, etc...) -&gt; np.ndarray] --------- #\n    #\n    # Part of this code is for handling features given as pandas.DataFrame\n    # Basically it transforms them to be internally stocked as np.ndarray and keep columns\n    # names as features names\n\n    # Handling shared features\n    if shared_features_by_choice is not None:\n        for i, feature in enumerate(shared_features_by_choice):\n            if isinstance(feature, pd.DataFrame):\n                # Ordering choices by id ?\n                if \"choice_id\" in feature.columns:\n                    feature = feature.set_index(\"choice_id\")\n                shared_features_by_choice = (\n                    shared_features_by_choice[:i]\n                    + (feature.loc[np.sort(feature.index)].to_numpy(),)\n                    + shared_features_by_choice[i + 1 :]\n                )\n                if shared_features_by_choice_names[i] is not None:\n                    logging.warning(\n                        f\"\"\"shared_features_by_choice_names {shared_features_by_choice_names[i]}\n                        were given. They will be overwritten with DF columns names:\n                        {feature.columns}\"\"\"\n                    )\n                shared_features_by_choice_names = (\n                    shared_features_by_choice_names[:i]\n                    + (feature.columns,)\n                    + shared_features_by_choice_names[i + 1 :]\n                )\n            elif isinstance(feature, list):\n                shared_features_by_choice = (\n                    shared_features_by_choice[:i]\n                    + (np.array(feature),)\n                    + shared_features_by_choice[i + 1 :]\n                )\n    # Handling items_features_by_choice\n    if items_features_by_choice is not None:\n        for i, feature in enumerate(items_features_by_choice):\n            if isinstance(feature, pd.DataFrame):\n                # Ordering choices by id ?\n                # TODO: here choice_id was context_id &gt; make sure this change does not affect\n                # some code somewhere\n                if \"choice_id\" in feature.columns:\n                    if \"item_id\" in feature.columns:\n                        all_items = np.sort(feature.item_id.unique())\n                        feature_array = []\n                        temp_availabilities = []\n                        for sess in np.sort(feature.choice_id.unique()):\n                            sess_df = feature.loc[feature.choice_id == sess]\n                            sess_df = sess_df[\n                                sess_df.columns.difference([\"choice_id\"])\n                            ].set_index(\"item_id\")\n                            sess_feature = []\n                            choice_availabilities = []\n                            for item in all_items:\n                                if item in sess_df.index:\n                                    sess_feature.append(sess_df.loc[item].to_numpy())\n                                    choice_availabilities.append(1)\n                                else:\n                                    sess_feature.append(np.zeros(len(sess_df.columns)))\n                                    choice_availabilities.append(0)\n                            feature_array.append(sess_feature)\n                            temp_availabilities.append(choice_availabilities)\n\n                        items_features_by_choice = (\n                            items_features_by_choice[:i]\n                            + (np.stack(feature_array, axis=0),)\n                            + items_features_by_choice[i + 1 :]\n                        )\n\n                        if items_features_by_choice_names[i] is not None:\n                            logging.warning(\n                                f\"\"\"items_features_by_choice_names\n                                {items_features_by_choice_names[i]} were given. They will be\n                                overwritten with DF columns names: {feature.columns}\"\"\"\n                            )\n                        items_features_by_choice_names = (\n                            items_features_by_choice_names[:i]\n                            + (sess_df.columns,)\n                            + items_features_by_choice_names[i + 1 :]\n                        )\n                        if (\n                            available_items_by_choice is None\n                            and len(np.unique(temp_availabilities)) &gt; 1\n                        ):\n                            logging.info(\n                                f\"\"\"available_items_by_choice were not given and computed from\n                                {i}-th items_features_by_choice.\"\"\"\n                            )\n                            available_items_by_choice = np.array(temp_availabilities)\n                    else:\n                        feature_array = []\n                        for sess in np.sort(feature.choice_id.unique()):\n                            sess_df = feature.loc[feature.choice_id == sess]\n                            sess_df = sess_df[sess_df.columns.difference([\"choice_id\"])]\n                            sess_feature = sess_df.to_numpy()\n                            feature_array.append(sess_feature)\n\n                        items_features_by_choice = (\n                            items_features_by_choice[:i]\n                            + (np.stack(feature_array, axis=0),)\n                            + items_features_by_choice[i + 1 :]\n                        )\n                        if items_features_by_choice_names[i] is not None:\n                            logging.warning(\n                                f\"\"\"items_features_by_choice_names\n                                {items_features_by_choice_names[i]} were given. They will be\n                                overwritten with DF columns names: {feature.columns}\"\"\"\n                            )\n                        items_features_by_choice_names = (\n                            items_features_by_choice_names[:i]\n                            + (feature.columns.difference([\"choice_id\"]),)\n                            + items_features_by_choice_names[i + 1 :]\n                        )\n                else:\n                    raise ValueError(\n                        f\"\"\"A 'choice_id' column must be integrated in {i}-th\n                        items_features_by_choice DF, in order to identify each choice.\"\"\"\n                    )\n            elif isinstance(feature, list):\n                items_features_by_choice = (\n                    items_features_by_choice[:i]\n                    + (np.array(feature),)\n                    + items_features_by_choice[i + 1 :]\n                )\n    # Handling available_items_by_choice\n    if available_items_by_choice is not None:\n        if isinstance(available_items_by_choice, list):\n            available_items_by_choice = np.array(\n                available_items_by_choice,\n                dtype=object,  # Are you sure ?\n            )\n        elif isinstance(available_items_by_choice, pd.DataFrame):\n            if \"choice_id\" in available_items_by_choice.columns:\n                if \"item_id\" in available_items_by_choice.columns:\n                    av_array = []\n                    for sess in np.sort(available_items_by_choice.choice_id.unique()):\n                        sess_df = available_items_by_choice.loc[\n                            available_items_by_choice.choice_id == sess\n                        ]\n                        sess_df = sess_df.drop(\"choice_id\", axis=1)\n                        sess_df = sess_df.set_index(\"item_id\")\n                        av_array.append(sess_df.loc[np.sort(sess_df.index)].to_numpy())\n                    available_items_by_choice = np.squeeze(np.array(av_array))\n                else:\n                    av_array = []\n                    for sess in np.sort(available_items_by_choice.choice_id.unique()):\n                        sess_df = available_items_by_choice.loc[\n                            available_items_by_choice.choice_id == sess\n                        ]\n                        sess_df = sess_df.drop(\"choice_id\", axis=1)\n                        av_array.append(sess_df.to_numpy())\n                    available_items_by_choice = np.squeeze(np.array(av_array))\n            else:\n                logging.info(\n                    \"No 'choice_id' column found in available_items_by_choice DF, using index\"\n                )\n                available_items_by_choice = available_items_by_choice.to_numpy().reshape(\n                    len(choices), -1\n                )\n\n    # Handling choices\n    # Choices must then be given as the name of the chosen item\n    # Items are sorted by name and attributed an index\n    # TODO: Keep items_id as an attribute ?\n    if isinstance(choices, pd.DataFrame):\n        # Ordering choices by id\n        if \"choice_id\" in choices.columns:\n            choices = choices.set_index(\"choice_id\")\n        choices = choices.loc[np.sort(choices.index)]\n        items = np.sort(np.unique(choices.to_numpy()))\n        # items is the value (str) of the item\n        choices = [np.where(items == c)[0] for c in np.squeeze(choices.to_numpy())]\n        choices = np.squeeze(choices)\n    elif isinstance(choices, pd.Series):\n        choices = choices.to_numpy()\n    elif isinstance(choices, list):\n        choices = np.array(choices)\n\n    # Setting attributes of ChoiceDataset\n    self.shared_features_by_choice = shared_features_by_choice\n    self.items_features_by_choice = items_features_by_choice\n    self.available_items_by_choice = available_items_by_choice\n    self.choices = choices\n\n    for fid in features_by_ids:\n        if not isinstance(fid, Storage):\n            raise ValueError(\"FeaturesByID must be Storage object\")\n    self.features_by_ids = features_by_ids\n\n    self.shared_features_by_choice_names = shared_features_by_choice_names\n    self.items_features_by_choice_names = items_features_by_choice_names\n\n    # What about typing ? should build after check to change it ?\n    (\n        self.shared_features_by_choice_map,\n        self.items_features_by_choice_map,\n    ) = self._build_features_by_ids()\n    self.check_features_by_ids()\n\n    # self.n_choices = len(self.choices)\n\n    # Different consitency checks to ensure everything is coherent\n    self._check_dataset()  # Should handle alone if np.arrays are squeezed\n    self._return_types = self._check_types()\n    self._check_names()\n    # Build .iloc method\n    self.indexer = ChoiceDatasetIndexer(self)\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return length of the dataset e.g. total number of choices.</p> <p>Returns:</p> Type Description <code>int</code> <p>total number of choices</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return length of the dataset e.g. total number of choices.\n\n    Returns\n    -------\n    int\n        total number of choices\n    \"\"\"\n    return len(self.choices)\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.__str__","title":"<code>__str__()</code>","text":"<p>Return short representation of ChoiceDataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>short representation of ChoiceDataset</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def __str__(self):\n    \"\"\"Return short representation of ChoiceDataset.\n\n    Returns\n    -------\n    str\n        short representation of ChoiceDataset\n    \"\"\"\n    template = \"\"\"First choice is: Shared Features by choice: {}\\n\n                  Items Features by choice: {}\\nAvailable items by choice: {}\\n\n                  Choices: {}\"\"\"\n    return template.format(\n        self.batch[0][0], self.batch[0][1], self.batch[0][2], self.batch[0][3]\n    )\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.check_features_by_ids","title":"<code>check_features_by_ids(batch_size=128)</code>","text":"<p>Verify that all IDs given in features exist in the corresponding FeaturesStorage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>batch size used to sample the FeaturesStorage, by default 128</p> <code>128</code> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the check was successful or not</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def check_features_by_ids(self, batch_size=128):\n    \"\"\"Verify that all IDs given in features exist in the corresponding FeaturesStorage.\n\n    Parameters\n    ----------\n    batch_size : int, optional\n        batch size used to sample the FeaturesStorage, by default 128\n\n    Returns\n    -------\n    bool\n        Whether the check was successful or not\n    \"\"\"\n    for index_1 in self.shared_features_by_choice_map:\n        for index_2 in self.shared_features_by_choice_map[index_1]:\n            all_values = np.unique(self.shared_features_by_choice[index_1][:, index_2])\n            for i in range(len(all_values) // batch_size + 1):\n                self.shared_features_by_choice_map[index_1][index_2].batch[\n                    all_values[i * batch_size : (i + 1) * batch_size]\n                ]\n\n    for index_1 in self.items_features_by_choice_map:\n        for index_2 in self.items_features_by_choice_map[index_1]:\n            if self.items_features_by_choice[index_1].ndim == 1:\n                all_values = np.unique(self.items_features_by_choice[index_1])\n            else:\n                all_values = np.unique(self.items_features_by_choice[index_1][:, :, index_2])\n            for i in range(len(all_values) // batch_size + 1):\n                self.items_features_by_choice_map[index_1][index_2].batch[\n                    all_values[i * batch_size : (i + 1) * batch_size]\n                ]\n    logging.info(\"Features by ID checked: all IDs have values\")\n    return True\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.filter","title":"<code>filter(bool_list)</code>","text":"<p>Filter over sessions indexes following bool.</p> <p>Parameters:</p> Name Type Description Default <code>bool_list</code> <code>list of boolean</code> <p>list of booleans of length self.get_n_choices() to filter choices. True to keep, False to discard.</p> required Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def filter(self, bool_list):\n    \"\"\"Filter over sessions indexes following bool.\n\n    Parameters\n    ----------\n    bool_list : list of boolean\n        list of booleans of length self.get_n_choices() to filter choices.\n        True to keep, False to discard.\n    \"\"\"\n    indexes = [i for i, keep in enumerate(bool_list) if keep]\n    return self[indexes]\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.from_single_long_df","title":"<code>from_single_long_df(df, choices_column='choice', items_id_column='item_id', choices_id_column='choice_id', shared_features_columns=None, items_features_columns=None, choice_format='items_id')</code>  <code>classmethod</code>","text":"<p>Build numpy arrays for ChoiceDataset from a single dataframe in long format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe in Long format</p> required <code>choices_column</code> <p>Name of the column containing the choices, default is \"choice\"</p> <code>'choice'</code> <code>items_id_column</code> <p>Name of the column containing the item ids, default is \"items_id\"</p> <code>'item_id'</code> <code>choices_id_column</code> <p>Name of the column containing the choice ids. It is used to identify all rows about a single choice, default is \"choice_id\"</p> <code>'choice_id'</code> <code>shared_features_columns</code> <code>list</code> <p>Columns of the dataframe that are shared_features_by_choice, default is None</p> <code>None</code> <code>items_features_columns</code> <code>list</code> <p>Columns of the dataframe that are items_features_by_choice, default is None</p> <code>None</code> <code>choice_format</code> <p>How choice is indicated in df, either \"items_name\" or \"one_zero\", default is \"items_id\"</p> <code>'items_id'</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>corresponding ChoiceDataset</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>@classmethod\ndef from_single_long_df(\n    cls,\n    df,\n    choices_column=\"choice\",\n    items_id_column=\"item_id\",\n    choices_id_column=\"choice_id\",\n    shared_features_columns=None,\n    items_features_columns=None,\n    choice_format=\"items_id\",\n):\n    \"\"\"Build numpy arrays for ChoiceDataset from a single dataframe in long format.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        dataframe in Long format\n    choices_column: str, optional\n        Name of the column containing the choices, default is \"choice\"\n    items_id_column: str, optional\n        Name of the column containing the item ids, default is \"items_id\"\n    choices_id_column: str, optional\n        Name of the column containing the choice ids. It is used to identify all rows\n        about a single choice, default is \"choice_id\"\n    shared_features_columns : list\n        Columns of the dataframe that are shared_features_by_choice, default is None\n    items_features_columns : list\n        Columns of the dataframe that are items_features_by_choice, default is None\n    choice_format: str, optional\n        How choice is indicated in df, either \"items_name\" or \"one_zero\",\n        default is \"items_id\"\n\n    Returns\n    -------\n    ChoiceDataset\n        corresponding ChoiceDataset\n    \"\"\"\n    # Ordering items and choices by id\n    items = np.sort(df[items_id_column].unique())\n    choices_ids = np.sort(df[choices_id_column].unique())\n\n    if shared_features_columns is not None:\n        shared_features_by_choice = df[\n            shared_features_columns + [choices_id_column]\n        ].drop_duplicates()\n        shared_features_by_choice = shared_features_by_choice.set_index(choices_id_column)\n        shared_features_by_choice = shared_features_by_choice.loc[choices_ids].to_numpy()\n\n        shared_features_by_choice_names = shared_features_columns\n    else:\n        shared_features_by_choice = None\n        shared_features_by_choice_names = None\n\n    (\n        items_features_by_choice,\n        avaialble_items_by_choice,\n    ) = cls._long_df_to_items_features_array(\n        df,\n        features=items_features_columns,\n        items_id_column=items_id_column,\n        choices_id_column=choices_id_column,\n        items_index=items,\n        choices_index=choices_ids,\n    )\n\n    items_features_by_choice_names = items_features_columns\n\n    if choice_format == \"items_id\":\n        choices = df[[choices_column, choices_id_column]].drop_duplicates(choices_id_column)\n        choices = choices.set_index(choices_id_column)\n        choices = choices.loc[choices_ids].to_numpy()\n        # items is the value (str) of the item\n        choices = np.squeeze([np.where(items == c)[0] for c in choices])\n    elif choice_format == \"one_zero\":\n        choices = df[[items_id_column, choices_column, choices_id_column]]\n        choices = choices.loc[choices[choices_column] == 1]\n        choices = choices.set_index(choices_id_column)\n        choices = (\n            choices.loc[choices_ids][items_id_column]\n            .map({k: v for v, k in enumerate(items)})\n            .to_numpy()\n        )\n    else:\n        raise ValueError(\n            f\"choice_format {choice_format} not recognized. Must be in ['items_id', 'one_zero']\"\n        )\n    return ChoiceDataset(\n        shared_features_by_choice=shared_features_by_choice,\n        items_features_by_choice=items_features_by_choice,\n        available_items_by_choice=avaialble_items_by_choice,\n        choices=choices,\n        shared_features_by_choice_names=shared_features_by_choice_names,\n        items_features_by_choice_names=items_features_by_choice_names,\n    )\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.from_single_wide_df","title":"<code>from_single_wide_df(df, items_id, shared_features_columns=None, items_features_suffixes=None, items_features_prefixes=None, available_items_suffix=None, available_items_prefix=None, delimiter='_', choices_column='choice', choice_format='items_id')</code>  <code>classmethod</code>","text":"<p>Build numpy arrays for ChoiceDataset from a single dataframe in wide format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe in Wide format</p> required <code>items_id</code> <code>list</code> <p>List of items ids</p> required <code>shared_features_columns</code> <code>list</code> <p>List of columns of the dataframe that are shared_features_by_choice, default is None</p> <code>None</code> <code>items_features_prefixes</code> <code>list</code> <p>Prefixes of the columns of the dataframe that are items_features_by_choice, default is None</p> <code>None</code> <code>items_features_suffixes</code> <code>list</code> <p>Suffixes of the columns of the dataframe that are items_features_by_choice, default is None</p> <code>None</code> <code>available_items_prefix</code> <p>Prefix of the columns of the dataframe that precise available_items_by_choice, default is None</p> <code>None</code> <code>available_items_suffix</code> <p>Suffix of the columns of the dataframe that precise available_items_by_choice, default is None</p> <code>None</code> <code>delimiter</code> <p>Delimiter used to separate the given prefix or suffixes and the features names, default is \"_\"</p> <code>'_'</code> <code>choice_column</code> <p>Name of the column containing the choices, default is \"choice\"</p> required <code>choice_format</code> <p>How choice is indicated in df, either \"items_id\" or \"items_index\", default is \"items_id\"</p> <code>'items_id'</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>corresponding ChoiceDataset</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>@classmethod\ndef from_single_wide_df(\n    cls,\n    df,\n    items_id,\n    shared_features_columns=None,\n    items_features_suffixes=None,\n    items_features_prefixes=None,\n    available_items_suffix=None,\n    available_items_prefix=None,\n    delimiter=\"_\",\n    choices_column=\"choice\",\n    choice_format=\"items_id\",\n):\n    \"\"\"Build numpy arrays for ChoiceDataset from a single dataframe in wide format.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        dataframe in Wide format\n    items_id : list\n        List of items ids\n    shared_features_columns : list, optional\n        List of columns of the dataframe that are shared_features_by_choice, default is None\n    items_features_prefixes : list, optional\n        Prefixes of the columns of the dataframe that are items_features_by_choice,\n        default is None\n    items_features_suffixes : list, optional\n        Suffixes of the columns of the dataframe that are items_features_by_choice,\n        default is None\n    available_items_prefix: str, optional\n        Prefix of the columns of the dataframe that precise available_items_by_choice,\n        default is None\n    available_items_suffix: str, optional\n        Suffix of the columns of the dataframe that precise available_items_by_choice,\n        default is None\n    delimiter: str, optional\n        Delimiter used to separate the given prefix or suffixes and the features names,\n        default is \"_\"\n    choice_column: str, optional\n        Name of the column containing the choices, default is \"choice\"\n    choice_format: str, optional\n        How choice is indicated in df, either \"items_id\" or \"items_index\",\n        default is \"items_id\"\n\n    Returns\n    -------\n    ChoiceDataset\n        corresponding ChoiceDataset\n    \"\"\"\n    if available_items_prefix is not None and available_items_suffix is not None:\n        raise ValueError(\n            \"You cannot give both available_items_prefix and\\\n                available_items_suffix.\"\n        )\n    if choice_format not in [\"items_index\", \"items_id\"]:\n        logging.warning(\"choice_format not understood, defaulting to 'items_index'\")\n\n    if shared_features_columns is not None:\n        shared_features_by_choice = df[shared_features_columns].to_numpy()\n        shared_features_by_choice_names = shared_features_columns\n    else:\n        shared_features_by_choice = None\n        shared_features_by_choice_names = None\n\n    if items_features_suffixes is not None and items_features_prefixes is not None:\n        # The list of features names is the concatenation of the two lists of\n        # prefixes and suffixes\n        items_features_names = items_features_prefixes + items_features_suffixes\n        items_features_by_choice = []\n        for item in items_id:\n            columns = [f\"{feature}{delimiter}{item}\" for feature in items_features_prefixes] + [\n                f\"{item}{delimiter}{feature}\" for feature in items_features_suffixes\n            ]\n            for col in columns:\n                if col not in df.columns:\n                    logging.warning(\n                        f\"Column {col} was not in DataFrame,\\\n                        dummy creation of the feature with zeros.\"\n                    )\n                    df[col] = 0\n            items_features_by_choice.append(df[columns].to_numpy())\n        items_features_by_choice = np.stack(items_features_by_choice, axis=1)\n    elif items_features_suffixes is not None:\n        items_features_names = items_features_suffixes\n        items_features_by_choice = []\n        for item in items_id:\n            columns = [f\"{item}{delimiter}{feature}\" for feature in items_features_suffixes]\n            for col in columns:\n                if col not in df.columns:\n                    logging.warning(\n                        f\"Column {col} was not in DataFrame,\\\n                        dummy creation of the feature with zeros.\"\n                    )\n                    df[col] = 0\n            items_features_by_choice.append(df[columns].to_numpy())\n        items_features_by_choice = np.stack(items_features_by_choice, axis=1)\n    elif items_features_prefixes is not None:\n        items_features_names = items_features_prefixes\n        items_features_by_choice = []\n        for item in items_id:\n            columns = [f\"{feature}{delimiter}{item}\" for feature in items_features_prefixes]\n            for col in columns:\n                if col not in df.columns:\n                    logging.warning(\n                        f\"Column {col} was not in DataFrame,\\\n                        dummy creation of the feature with zeros.\"\n                    )\n                    df[col] = 0\n            items_features_by_choice.append(df[columns].to_numpy())\n        items_features_by_choice = np.stack(items_features_by_choice, axis=1)\n    else:\n        items_features_by_choice = None\n        items_features_names = None\n\n    if available_items_suffix is not None:\n        if isinstance(available_items_suffix, list):\n            if not len(available_items_suffix) == len(items_id):\n                raise ValueError(\n                    \"You have given a list of columns for availabilities.\"\n                    \"We consider that it is one for each item however lenghts do not match\"\n                )\n            logging.info(\"You have given a list of columns for availabilities.\")\n            logging.info(\"Each column will be matched to an item, given their order\")\n            available_items_by_choice = df[available_items_suffix].to_numpy()\n        else:\n            columns = [f\"{item}{delimiter}{available_items_suffix}\" for item in items_id]\n            available_items_by_choice = df[columns].to_numpy()\n    elif available_items_prefix is not None:\n        if isinstance(available_items_prefix, list):\n            if not len(available_items_prefix) == len(items_id):\n                raise ValueError(\n                    \"You have given a list of columns for availabilities.\"\n                    \"We consider that it is one for each item however lenghts do not match\"\n                )\n            logging.info(\"You have given a list of columns for availabilities.\")\n            logging.info(\"Each column will be matched to an item, given their order\")\n            available_items_by_choice = df[available_items_prefix].to_numpy()\n        else:\n            columns = [f\"{available_items_prefix}{delimiter}{item}\" for item in items_id]\n            available_items_by_choice = df[columns].to_numpy()\n    else:\n        available_items_by_choice = None\n\n    choices = df[choices_column].to_numpy()\n    if choice_format == \"items_id\":\n        if items_id is None:\n            raise ValueError(\"items_id must be given to use choice_format='items_id'\")\n        items_id = np.array(items_id)\n        choices = np.squeeze([np.where(items_id == c)[0] for c in choices])\n        if choices.size == 0:\n            raise ValueError(\"No choice found in the items_id list\")\n\n    return ChoiceDataset(\n        shared_features_by_choice=shared_features_by_choice,\n        shared_features_by_choice_names=shared_features_by_choice_names,\n        items_features_by_choice=items_features_by_choice,\n        items_features_by_choice_names=items_features_names,\n        available_items_by_choice=available_items_by_choice,\n        choices=choices,\n    )\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.get_choices_batch","title":"<code>get_choices_batch(choices_indexes, features=None)</code>","text":"<p>Access a chunk of data within the ChoiceDataset from choice indexes.</p> <p>Parameters:</p> Name Type Description Default <code>choices_indexes</code> <code>int or list of int or slice</code> <p>indexes of the choices (that will be mapped to choice &amp; session indexes) to return</p> required <code>features</code> <code>list of str</code> <p>list of features to return. None returns all of them, default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple of (array_like, )</code> <p>tuple of arrays containing a batch of shared_features_by_choice</p> <code>tuple of (array_like, )</code> <p>tuple of arrays containing a batch of items_features_by_choice</p> <code>array_like</code> <p>array containing a batch of availables_items_by_choice</p> <code>array_like</code> <p>array containing a batch of choices</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def get_choices_batch(self, choices_indexes, features=None):\n    \"\"\"Access a chunk of data within the ChoiceDataset from choice indexes.\n\n    Parameters\n    ----------\n    choices_indexes : int or list of int or slice\n        indexes of the choices (that will be mapped to choice &amp; session indexes) to return\n    features : list of str, optional\n        list of features to return. None returns all of them, default is None.\n\n    Returns\n    -------\n    tuple of (array_like, )\n        tuple of arrays containing a batch of shared_features_by_choice\n    tuple of (array_like, )\n        tuple of arrays containing a batch of items_features_by_choice\n    array_like\n        array containing a batch of availables_items_by_choice\n    array_like\n        array containing a batch of choices\n    \"\"\"\n    _ = features\n    if isinstance(choices_indexes, list):\n        if np.array(choices_indexes).ndim &gt; 1:\n            raise ValueError(\n                \"\"\"ChoiceDataset unidimensional can only be batched along choices\n                             dimension received a list with several axis of indexing.\"\"\"\n            )\n        if self.shared_features_by_choice is None:\n            shared_features_by_choice = None\n        else:\n            shared_features_by_choice = list(\n                shared_features_by_choice[choices_indexes]\n                # .astype(self._return_types[1][i])\n                for i, shared_features_by_choice in enumerate(self.shared_features_by_choice)\n            )\n\n        if self.items_features_by_choice is None:\n            items_features_by_choice = None\n        else:\n            items_features_by_choice = list(\n                items_features_by_choice[choices_indexes]\n                # .astype(self._return_types[2][i])\n                for _, items_features_by_choice in enumerate(self.items_features_by_choice)\n            )\n        if self.available_items_by_choice is None:\n            available_items_by_choice = np.ones(\n                (len(choices_indexes), self.base_num_items)\n            ).astype(\"float32\")\n        else:\n            if isinstance(self.available_items_by_choice, tuple):\n                available_items_by_choice = self.available_items_by_choice[0].batch[\n                    self.available_items_by_choice[1][choices_indexes]\n                ]\n            else:\n                available_items_by_choice = self.available_items_by_choice[choices_indexes]\n            # .astype(self._return_types[3])\n\n        choices = self.choices[choices_indexes].astype(self._return_types[3])\n\n        if len(self.shared_features_by_choice_map) &gt; 0:\n            mapped_features = []\n            for tuple_index in range(len(shared_features_by_choice)):\n                if tuple_index in self.shared_features_by_choice_map.keys():\n                    feat_ind_min = 0\n                    unstacked_feat = []\n                    for feature_index in np.sort(\n                        list(self.shared_features_by_choice_map[tuple_index].keys())\n                    ):\n                        if feat_ind_min != feature_index:\n                            unstacked_feat.append(\n                                shared_features_by_choice[tuple_index][\n                                    :, feat_ind_min:feature_index\n                                ]\n                            )\n                        unstacked_feat.append(\n                            self.shared_features_by_choice_map[tuple_index][\n                                feature_index\n                            ].batch[shared_features_by_choice[tuple_index][:, feature_index]]\n                        )\n                        feat_ind_min = feature_index + 1\n                    if feat_ind_min != shared_features_by_choice[tuple_index].shape[1]:\n                        unstacked_feat.append(\n                            shared_features_by_choice[tuple_index][:, feat_ind_min:]\n                        )\n                    mapped_features.append(np.concatenate(unstacked_feat, axis=1))\n                else:\n                    mapped_features.append(shared_features_by_choice[tuple_index])\n\n            shared_features_by_choice = mapped_features\n\n        if len(self.items_features_by_choice_map) &gt; 0:\n            mapped_features = []\n            for tuple_index in range(len(items_features_by_choice)):\n                if tuple_index in self.items_features_by_choice_map.keys():\n                    if items_features_by_choice[tuple_index].ndim == 1:\n                        mapped_features.append(\n                            self.items_features_by_choice_map[tuple_index][0].batch[\n                                items_features_by_choice[tuple_index]\n                            ]\n                        )\n                    else:\n                        feat_ind_min = 0\n                        unstacked_feat = []\n                        for feature_index in np.sort(\n                            list(self.items_features_by_choice_map[tuple_index].keys())\n                        ):\n                            if feat_ind_min != feature_index:\n                                unstacked_feat.append(\n                                    items_features_by_choice[tuple_index][\n                                        :, :, feat_ind_min:feature_index\n                                    ]\n                                )\n                            unstacked_feat.append(\n                                self.items_features_by_choice_map[tuple_index][\n                                    feature_index\n                                ].batch[\n                                    items_features_by_choice[tuple_index][:, :, feature_index]\n                                ]\n                            )\n                            feat_ind_min = feature_index + 1\n                        if feat_ind_min != items_features_by_choice[tuple_index].shape[2]:\n                            unstacked_feat.append(\n                                shared_features_by_choice[tuple_index][:, :, feat_ind_min:]\n                            )\n                        mapped_features.append(np.concatenate(unstacked_feat, axis=2))\n                else:\n                    mapped_features.append(items_features_by_choice[tuple_index])\n\n            items_features_by_choice = mapped_features\n\n        if shared_features_by_choice is not None:\n            for i in range(len(shared_features_by_choice)):\n                shared_features_by_choice[i] = shared_features_by_choice[i].astype(\n                    self._return_types[0][i]\n                )\n            if not self._return_shared_features_by_choice_tuple:\n                shared_features_by_choice = shared_features_by_choice[0]\n            else:\n                shared_features_by_choice = tuple(shared_features_by_choice)\n\n        if items_features_by_choice is not None:\n            for i in range(len(items_features_by_choice)):\n                items_features_by_choice[i] = items_features_by_choice[i].astype(\n                    self._return_types[1][i]\n                )\n            # items_features_by_choice were not given as a tuple, so we return do not return\n            # it as a tuple\n            if not self._return_items_features_by_choice_tuple:\n                items_features_by_choice = items_features_by_choice[0]\n            else:\n                items_features_by_choice = tuple(items_features_by_choice)\n\n        return (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n\n    if isinstance(choices_indexes, slice):\n        return self.get_choices_batch(\n            list(range(*choices_indexes.indices(self.choices.shape[0])))\n        )\n\n    choices_indexes = [choices_indexes]\n    (\n        shared_features_by_choices,\n        items_features_by_choice,\n        available_items_by_choice,\n        choice,\n    ) = self.get_choices_batch(choices_indexes)\n    if shared_features_by_choices is not None:\n        if isinstance(shared_features_by_choices, tuple):\n            shared_features_by_choices = tuple(feat[0] for feat in shared_features_by_choices)\n        else:\n            shared_features_by_choices = shared_features_by_choices[0]\n    if items_features_by_choice is not None:\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tuple(feat[0] for feat in items_features_by_choice)\n        else:\n            items_features_by_choice = items_features_by_choice[0]\n\n    return (\n        shared_features_by_choices,\n        items_features_by_choice,\n        available_items_by_choice[0],\n        choice[0],\n    )\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.get_n_choices","title":"<code>get_n_choices()</code>","text":"<p>Access the total number of different choices.</p> <p>Redundant with len method.</p> <p>Returns:</p> Type Description <code>int</code> <p>total number of different choices</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def get_n_choices(self):\n    \"\"\"Access the total number of different choices.\n\n    Redundant with __len__ method.\n\n    Returns\n    -------\n    int\n        total number of different choices\n    \"\"\"\n    return len(self)\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.get_n_items","title":"<code>get_n_items()</code>","text":"<p>Access the total number of different items.</p> <p>Returns:</p> Type Description <code>int</code> <p>total number of different items</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def get_n_items(self):\n    \"\"\"Access the total number of different items.\n\n    Returns\n    -------\n    int\n        total number of different items\n    \"\"\"\n    return self.base_num_items\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.get_n_items_features","title":"<code>get_n_items_features()</code>","text":"<p>Access the number of items features.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of items features</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def get_n_items_features(self):\n    \"\"\"Access the number of items features.\n\n    Returns\n    -------\n    int\n        number of items features\n    \"\"\"\n    if self.items_features_by_choice is not None:\n        n_features = 0\n        for items_features in self.items_features_by_choice:\n            n_features += items_features.shape[2]\n        return n_features\n    return 0\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.get_n_shared_features","title":"<code>get_n_shared_features()</code>","text":"<p>Access the number of shared features.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of shared items features</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def get_n_shared_features(self):\n    \"\"\"Access the number of shared features.\n\n    Returns\n    -------\n    int\n        number of shared items features\n    \"\"\"\n    if self.shared_features_by_choice is not None:\n        n_features = 0\n        for shared_features in self.shared_features_by_choice:\n            n_features += shared_features.shape[1]\n        return n_features\n    return 0\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.iter_batch","title":"<code>iter_batch(batch_size, shuffle=False, sample_weight=None)</code>","text":"<p>Iterate over dataset return batches of length batch_size.</p> <p>Newer version.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>batch size to set</p> required <code>shuffle</code> <p>Whether or not to shuffle the dataset</p> <code>False</code> <code>sample_weight</code> <code>Iterable</code> <p>list of weights to be returned with the right indexing during the shuffling</p> <code>None</code> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def iter_batch(self, batch_size, shuffle=False, sample_weight=None):\n    \"\"\"Iterate over dataset return batches of length batch_size.\n\n    Newer version.\n\n    Parameters\n    ----------\n    batch_size : int\n        batch size to set\n    shuffle: bool\n        Whether or not to shuffle the dataset\n    sample_weight : Iterable\n        list of weights to be returned with the right indexing during the shuffling\n    \"\"\"\n    if sample_weight is not None and isinstance(sample_weight, list):\n        sample_weight = np.array(sample_weight)\n    if batch_size == -1 or batch_size == len(self):\n        yield self.indexer.get_full_dataset(sample_weight=sample_weight)\n    else:\n        # Get indexes for each choice\n        num_choices = len(self)\n        indexes = np.arange(num_choices)\n\n        # Shuffle indexes\n        if shuffle and not batch_size == -1:\n            indexes = np.random.permutation(indexes)\n\n        yielded_size = 0\n        while yielded_size &lt; num_choices:\n            # Return sample_weight if not None, for index matching\n            batch_indexes = indexes[yielded_size : yielded_size + batch_size].tolist()\n            if sample_weight is not None:\n                yield (\n                    self.batch[batch_indexes],\n                    sample_weight[batch_indexes],\n                )\n            else:\n                yield self.batch[batch_indexes]\n            yielded_size += batch_size\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.save","title":"<code>save()</code>","text":"<p>Save the dataset.</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def save(self):\n    \"\"\"Save the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"references/data/references_choice_dataset/#choice_learn.data.choice_dataset.ChoiceDataset.summary","title":"<code>summary()</code>","text":"<p>Display a summary of the dataset.</p> Source code in <code>choice_learn/data/choice_dataset.py</code> <pre><code>def summary(self):\n    \"\"\"Display a summary of the dataset.\"\"\"\n    print(\"%=====================================================================%\")\n    print(\"%%% Summary of the dataset:\")\n    print(\"%=====================================================================%\")\n    print(\"Number of items:\", self.get_n_items())\n    print(\n        \"Number of choices:\",\n        len(self),\n    )\n    print(\"%=====================================================================%\")\n\n    if self.shared_features_by_choice is not None:\n        print(\" Shared Features by Choice:\")\n        print(f\" {sum([f.shape[1] for f in self.shared_features_by_choice])} shared features\")\n        if self.shared_features_by_choice_names is not None:\n            if self.shared_features_by_choice_names[0] is not None:\n                print(f\" with names: {self.shared_features_by_choice_names}\")\n    else:\n        print(\" No Shared Features by Choice registered\")\n    print(\"\\n\")\n\n    if self.items_features_by_choice is not None:\n        if self.items_features_by_choice[0] is not None:\n            print(\" Items Features by Choice:\")\n            print(\n                f\"\"\"{\n                    sum(\n                        [\n                            f.shape[2] if f.ndim == 3 else 1\n                            for f in self.items_features_by_choice\n                        ]\n                    )\n                } items features \"\"\"\n            )\n            if self.items_features_by_choice_names is not None:\n                if self.items_features_by_choice_names[0] is not None:\n                    print(f\" with names: {self.items_features_by_choice_names}\")\n    else:\n        print(\" No Items Features by Choice registered\")\n    print(\"%=====================================================================%\")\n    return \"\"\n</code></pre>"},{"location":"references/data/references_indexer/","title":"Indexers","text":"<p>Indexer classes for data classes.</p>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.ArrayStorageIndexer","title":"<code>ArrayStorageIndexer</code>","text":"<p>             Bases: <code>StorageIndexer</code></p> <p>Class for Ilocing/Batching ArrayFeaturesStorage.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class ArrayStorageIndexer(StorageIndexer):\n    \"\"\"Class for Ilocing/Batching ArrayFeaturesStorage.\"\"\"\n\n    def __getitem__(self, sequence_keys):\n        \"\"\"Return the features appearing at the sequence_index-th position of sequence.\n\n        Parameters\n        ----------\n        sequence_keys : (int, list, slice)\n            keys of values to be retrieved\n\n        Returns\n        -------\n        array_like\n            features corresponding to the sequence_keys\n        \"\"\"\n        try:\n            return self.storage.storage[sequence_keys]\n        except IndexError as error:\n            print(\"You are using an ID that is not in the storage:\")\n            print(error)\n            raise KeyError\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.ArrayStorageIndexer.__getitem__","title":"<code>__getitem__(sequence_keys)</code>","text":"<p>Return the features appearing at the sequence_index-th position of sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_keys</code> <code>(int, list, slice)</code> <p>keys of values to be retrieved</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the sequence_keys</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __getitem__(self, sequence_keys):\n    \"\"\"Return the features appearing at the sequence_index-th position of sequence.\n\n    Parameters\n    ----------\n    sequence_keys : (int, list, slice)\n        keys of values to be retrieved\n\n    Returns\n    -------\n    array_like\n        features corresponding to the sequence_keys\n    \"\"\"\n    try:\n        return self.storage.storage[sequence_keys]\n    except IndexError as error:\n        print(\"You are using an ID that is not in the storage:\")\n        print(error)\n        raise KeyError\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.ChoiceDatasetIndexer","title":"<code>ChoiceDatasetIndexer</code>","text":"<p>             Bases: <code>Indexer</code></p> <p>Indexing class for ChoiceDataset.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class ChoiceDatasetIndexer(Indexer):\n    \"\"\"Indexing class for ChoiceDataset.\"\"\"\n\n    def __init__(self, choice_dataset):\n        \"\"\"Instanciate a ChoiceDatasetIndexer object.\n\n        Parameters\n        ----------\n        choice_dataset : choce_modeling.data.dataset.ChoiceDataset\n            Dataset to be indexed.\n        \"\"\"\n        self.choice_dataset = choice_dataset\n\n    def _get_shared_features_by_choice(self, choices_indexes):\n        \"\"\"Access sessions features of the ChoiceDataset.\n\n        Parameters\n        ----------\n        choices_indexes : list of ints or int\n            choices indexes of the shared features to return\n\n        Returns\n        -------\n        tuple of np.ndarray or np.ndarray\n            right indexed contexts_fshared_features_by_choiceeatures of the ChoiceDataset\n        \"\"\"\n        if self.choice_dataset.shared_features_by_choice is None:\n            shared_features_by_choice = None\n        else:\n            shared_features_by_choice = []\n            for i, shared_feature in enumerate(self.choice_dataset.shared_features_by_choice):\n                if hasattr(shared_feature, \"batch\"):\n                    shared_features_by_choice.append(shared_feature.batch[choices_indexes])\n                else:\n                    # shared_features_by_choice.append(\n                    #     np.stack(shared_feature[choices_indexes], axis=0)\n                    # )\n                    shared_features_by_choice.append(shared_feature[choices_indexes])\n        return shared_features_by_choice\n\n    def _get_items_features_by_choice(self, choices_indexes):\n        \"\"\"Access sessions items features of the ChoiceDataset.\n\n        Parameters\n        ----------\n        choices_indexes : list of ints or int\n            indexes of the choices for which to select the items features\n\n        Returns\n        -------\n        tuple of np.ndarray or np.ndarray\n            right indexes items_features_by_choice of the ChoiceDataset\n        \"\"\"\n        if self.choice_dataset.items_features_by_choice is None:\n            return None\n        items_features_by_choice = []\n        for i, items_feature in enumerate(self.choice_dataset.items_features_by_choice):\n            if hasattr(items_feature, \"batch\"):\n                items_features_by_choice.append(items_feature.batch[choices_indexes])\n            else:\n                # items_features_by_choice.append(np.stack(items_feature[choices_indexes], axis=0))\n                items_features_by_choice.append(items_feature[choices_indexes])\n        return items_features_by_choice\n\n    def __getitem__(self, choices_indexes):\n        \"\"\"Access data within the ChoiceDataset from its index.\n\n        One index corresponds to a choice within a session.\n        Return order:\n            - Fixed item features\n            - Contexts features\n            - Contexts item features\n            - Items availabilities\n            - Choices\n\n        Parameters\n        ----------\n        choices_indexes : int or list of int or slice\n            indexes of the choices (that will be mapped to choice &amp; session indexes) to return\n\n        Returns\n        -------\n        np.ndarray\n            shared_features at choices_indexes\n        np.ndarray\n            items_features at choices_indexes\n        np.ndarray\n            available_items_by_choice at choices_indexes\n        np.ndarray\n            choices at choices_indexes\n        \"\"\"\n        if isinstance(choices_indexes, list):\n            # Get the features\n            shared_features_by_choice = self._get_shared_features_by_choice(choices_indexes)\n            items_features_by_choice = self._get_items_features_by_choice(choices_indexes)\n\n            if self.choice_dataset.available_items_by_choice is None:\n                available_items_by_choice = np.ones(\n                    (len(choices_indexes), self.choice_dataset.base_num_items)\n                ).astype(\"float32\")\n            else:\n                if isinstance(self.choice_dataset.available_items_by_choice, tuple):\n                    available_items_by_choice = self.choice_dataset.available_items_by_choice[\n                        0\n                    ].batch[self.choice_dataset.available_items_by_choice[1][choices_indexes]]\n                else:\n                    available_items_by_choice = self.choice_dataset.available_items_by_choice[\n                        choices_indexes\n                    ]\n                available_items_by_choice = available_items_by_choice.astype(\n                    self.choice_dataset._return_types[2]\n                )\n\n            choices = self.choice_dataset.choices[choices_indexes].astype(\n                self.choice_dataset._return_types[3]\n            )\n\n            ###\n            if len(self.choice_dataset.shared_features_by_choice_map) &gt; 0:\n                mapped_features = []\n                ###\n                for tuple_index in range(len(shared_features_by_choice)):\n                    if tuple_index in self.choice_dataset.shared_features_by_choice_map.keys():\n                        feat_ind_min = 0\n                        unstacked_feat = []\n                        for feature_index in np.sort(\n                            list(\n                                self.choice_dataset.shared_features_by_choice_map[\n                                    tuple_index\n                                ].keys()\n                            )\n                        ):\n                            if feat_ind_min != feature_index:\n                                unstacked_feat.append(\n                                    shared_features_by_choice[tuple_index][\n                                        :, feat_ind_min:feature_index\n                                    ]\n                                )\n                            unstacked_feat.append(\n                                self.choice_dataset.shared_features_by_choice_map[tuple_index][\n                                    feature_index\n                                ].batch[shared_features_by_choice[tuple_index][:, feature_index]]\n                            )\n                            feat_ind_min = feature_index + 1\n                        if feat_ind_min != shared_features_by_choice[tuple_index].shape[1]:\n                            unstacked_feat.append(\n                                shared_features_by_choice[tuple_index][:, feat_ind_min:]\n                            )\n                        mapped_features.append(np.hstack(unstacked_feat))\n                    else:\n                        mapped_features.append(shared_features_by_choice[tuple_index])\n\n                shared_features_by_choice = mapped_features\n\n            if len(self.choice_dataset.items_features_by_choice_map) &gt; 0:\n                mapped_features = []\n                for tuple_index in range(len(items_features_by_choice)):\n                    if tuple_index in self.choice_dataset.items_features_by_choice_map.keys():\n                        if items_features_by_choice[tuple_index].ndim == 1:\n                            mapped_features.append(\n                                self.choice_dataset.items_features_by_choice_map[tuple_index][\n                                    0\n                                ].batch[items_features_by_choice[tuple_index]]\n                            )\n                        else:\n                            feat_ind_min = 0\n                            unstacked_feat = []\n                            for feature_index in np.sort(\n                                list(\n                                    self.choice_dataset.items_features_by_choice_map[\n                                        tuple_index\n                                    ].keys()\n                                )\n                            ):\n                                if feat_ind_min != feature_index:\n                                    unstacked_feat.append(\n                                        items_features_by_choice[tuple_index][\n                                            :, :, feat_ind_min:feature_index\n                                        ]\n                                    )\n                                unstacked_feat.append(\n                                    self.choice_dataset.items_features_by_choice_map[tuple_index][\n                                        feature_index\n                                    ].batch[\n                                        items_features_by_choice[tuple_index][:, :, feature_index]\n                                    ]\n                                )\n                                feat_ind_min = feature_index + 1\n                            if feat_ind_min != items_features_by_choice[tuple_index].shape[2]:\n                                unstacked_feat.append(\n                                    items_features_by_choice[tuple_index][:, :, feat_ind_min:]\n                                )\n                            mapped_features.append(np.concatenate(unstacked_feat, axis=2))\n                    else:\n                        mapped_features.append(items_features_by_choice[tuple_index])\n\n                items_features_by_choice = mapped_features\n\n            if shared_features_by_choice is not None:\n                for i in range(len(shared_features_by_choice)):\n                    shared_features_by_choice[i] = shared_features_by_choice[i].astype(\n                        self.choice_dataset._return_types[0][i]\n                    )\n                if not self.choice_dataset._return_shared_features_by_choice_tuple:\n                    shared_features_by_choice = shared_features_by_choice[0]\n                else:\n                    shared_features_by_choice = tuple(shared_features_by_choice)\n\n            if items_features_by_choice is not None:\n                for i in range(len(items_features_by_choice)):\n                    items_features_by_choice[i] = items_features_by_choice[i].astype(\n                        self.choice_dataset._return_types[1][i]\n                    )\n                # items_features_by_choice were not given as a tuple, so we return do not return\n                # it as a tuple\n                if not self.choice_dataset._return_items_features_by_choice_tuple:\n                    items_features_by_choice = items_features_by_choice[0]\n                else:\n                    items_features_by_choice = tuple(items_features_by_choice)\n            return (\n                shared_features_by_choice,\n                items_features_by_choice,\n                available_items_by_choice,\n                choices,\n            )\n\n        if isinstance(choices_indexes, slice):\n            return self.__getitem__(\n                list(range(*choices_indexes.indices(self.choice_dataset.choices.shape[0])))\n            )\n\n        if isinstance(choices_indexes, int):\n            choices_indexes = [choices_indexes]\n            (\n                shared_features_by_choices,\n                items_features_by_choice,\n                available_items_by_choice,\n                choice,\n            ) = self.__getitem__(choices_indexes)\n            if shared_features_by_choices is not None:\n                if isinstance(shared_features_by_choices, tuple):\n                    shared_features_by_choices = tuple(\n                        feat[0] for feat in shared_features_by_choices\n                    )\n                else:\n                    shared_features_by_choices = shared_features_by_choices[0]\n            if items_features_by_choice is not None:\n                if isinstance(items_features_by_choice, tuple):\n                    items_features_by_choice = tuple(feat[0] for feat in items_features_by_choice)\n                else:\n                    items_features_by_choice = items_features_by_choice[0]\n\n            return (\n                shared_features_by_choices,\n                items_features_by_choice,\n                available_items_by_choice[0],\n                choice[0],\n            )\n        logging.error(f\"Type{type(choices_indexes)} not handled\")\n        raise NotImplementedError(f\"Type{type(choices_indexes)} not handled\")\n\n    def get_full_dataset(self, sample_weight=None):\n        \"\"\"Return the full dataset.\n\n        This function is here to speed up iteration over dataset when batch_size\n        is -1 or length of dataset.\n\n        Parameters\n        ----------\n        sample_weight : list or np.ndarray\n            sample_weight of size (len(dataset), )\n\n        Returns\n        -------\n        np.ndarray\n            all shared_features\n        np.ndarray\n            all items_features\n        np.ndarray\n            all available_items_by_choice\n        np.ndarray\n            all choices\n        \"\"\"\n        if self.choice_dataset.shared_features_by_choice is not None:\n            shared_features_by_choice = [\n                feat for feat in self.choice_dataset.shared_features_by_choice\n            ]\n        else:\n            shared_features_by_choice = None\n\n        if self.choice_dataset.items_features_by_choice is not None:\n            items_features_by_choice = [\n                feat for feat in self.choice_dataset.items_features_by_choice\n            ]\n        else:\n            items_features_by_choice = None\n\n        if self.choice_dataset.available_items_by_choice is None:\n            available_items_by_choice = np.ones(\n                (len(self.choice_dataset), self.choice_dataset.base_num_items)\n            ).astype(\"float32\")\n        else:\n            if isinstance(self.choice_dataset.available_items_by_choice, tuple):\n                available_items_by_choice = self.choice_dataset.available_items_by_choice[0].batch[\n                    self.choice_dataset.available_items_by_choice[1]\n                ]\n            else:\n                available_items_by_choice = self.choice_dataset.available_items_by_choice\n        available_items_by_choice = available_items_by_choice.astype(\n            self.choice_dataset._return_types[2]\n        )\n\n        choices = self.choice_dataset.choices.astype(self.choice_dataset._return_types[3])\n\n        ###\n        if len(self.choice_dataset.shared_features_by_choice_map) &gt; 0:\n            mapped_features = []\n            ###\n            for tuple_index in range(len(shared_features_by_choice)):\n                if tuple_index in self.choice_dataset.shared_features_by_choice_map.keys():\n                    feat_ind_min = 0\n                    unstacked_feat = []\n                    for feature_index in np.sort(\n                        list(self.choice_dataset.shared_features_by_choice_map[tuple_index].keys())\n                    ):\n                        unstacked_feat.append(\n                            shared_features_by_choice[tuple_index][:, feat_ind_min:feature_index]\n                        )\n                        unstacked_feat.append(\n                            self.choice_dataset.shared_features_by_choice_map[tuple_index][\n                                feature_index\n                            ].batch[shared_features_by_choice[tuple_index][:, feature_index]]\n                        )\n                        feat_ind_min = feature_index + 1\n                    if feat_ind_min != shared_features_by_choice[tuple_index].shape[1]:\n                        unstacked_feat.append(\n                            shared_features_by_choice[tuple_index][:, feat_ind_min:]\n                        )\n                    mapped_features.append(np.concatenate(unstacked_feat, axis=1))\n                else:\n                    mapped_features.append(shared_features_by_choice[tuple_index])\n\n            shared_features_by_choice = mapped_features\n\n        if len(self.choice_dataset.items_features_by_choice_map) &gt; 0:\n            mapped_features = []\n            for tuple_index in range(len(items_features_by_choice)):\n                if tuple_index in self.choice_dataset.items_features_by_choice_map.keys():\n                    if items_features_by_choice[tuple_index].ndim == 1:\n                        mapped_features.append(\n                            self.choice_dataset.items_features_by_choice_map[tuple_index][0].batch[\n                                items_features_by_choice[tuple_index]\n                            ]\n                        )\n                    else:\n                        feat_ind_min = 0\n                        unstacked_feat = []\n                        for feature_index in np.sort(\n                            list(\n                                self.choice_dataset.items_features_by_choice_map[tuple_index].keys()\n                            )\n                        ):\n                            unstacked_feat.append(\n                                items_features_by_choice[tuple_index][\n                                    :, :, feat_ind_min:feature_index\n                                ]\n                            )\n                            unstacked_feat.append(\n                                self.choice_dataset.items_features_by_choice_map[tuple_index][\n                                    feature_index\n                                ].batch[items_features_by_choice[tuple_index][:, :, feature_index]]\n                            )\n                            feat_ind_min = feature_index + 1\n                        if feat_ind_min != items_features_by_choice[tuple_index].shape[2]:\n                            unstacked_feat.append(\n                                items_features_by_choice[tuple_index][:, :, feat_ind_min:]\n                            )\n                        mapped_features.append(np.concatenate(unstacked_feat, axis=2))\n                else:\n                    mapped_features.append(items_features_by_choice[tuple_index])\n\n            items_features_by_choice = mapped_features\n\n        if shared_features_by_choice is not None:\n            for i in range(len(shared_features_by_choice)):\n                shared_features_by_choice[i] = shared_features_by_choice[i].astype(\n                    self.choice_dataset._return_types[0][i]\n                )\n            if not self.choice_dataset._return_shared_features_by_choice_tuple:\n                shared_features_by_choice = shared_features_by_choice[0]\n            else:\n                shared_features_by_choice = tuple(shared_features_by_choice)\n\n        if items_features_by_choice is not None:\n            for i in range(len(items_features_by_choice)):\n                items_features_by_choice[i] = items_features_by_choice[i].astype(\n                    self.choice_dataset._return_types[1][i]\n                )\n            # items_features_by_choice were not given as a tuple, so we return do not return\n            # it as a tuple\n            if not self.choice_dataset._return_items_features_by_choice_tuple:\n                items_features_by_choice = items_features_by_choice[0]\n            else:\n                items_features_by_choice = tuple(items_features_by_choice)\n\n        if sample_weight is not None:\n            return (\n                shared_features_by_choice,\n                items_features_by_choice,\n                available_items_by_choice,\n                choices,\n            ), sample_weight\n        return (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.ChoiceDatasetIndexer.__getitem__","title":"<code>__getitem__(choices_indexes)</code>","text":"<p>Access data within the ChoiceDataset from its index.</p> <p>One index corresponds to a choice within a session. Return order:     - Fixed item features     - Contexts features     - Contexts item features     - Items availabilities     - Choices</p> <p>Parameters:</p> Name Type Description Default <code>choices_indexes</code> <code>int or list of int or slice</code> <p>indexes of the choices (that will be mapped to choice &amp; session indexes) to return</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>shared_features at choices_indexes</p> <code>ndarray</code> <p>items_features at choices_indexes</p> <code>ndarray</code> <p>available_items_by_choice at choices_indexes</p> <code>ndarray</code> <p>choices at choices_indexes</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __getitem__(self, choices_indexes):\n    \"\"\"Access data within the ChoiceDataset from its index.\n\n    One index corresponds to a choice within a session.\n    Return order:\n        - Fixed item features\n        - Contexts features\n        - Contexts item features\n        - Items availabilities\n        - Choices\n\n    Parameters\n    ----------\n    choices_indexes : int or list of int or slice\n        indexes of the choices (that will be mapped to choice &amp; session indexes) to return\n\n    Returns\n    -------\n    np.ndarray\n        shared_features at choices_indexes\n    np.ndarray\n        items_features at choices_indexes\n    np.ndarray\n        available_items_by_choice at choices_indexes\n    np.ndarray\n        choices at choices_indexes\n    \"\"\"\n    if isinstance(choices_indexes, list):\n        # Get the features\n        shared_features_by_choice = self._get_shared_features_by_choice(choices_indexes)\n        items_features_by_choice = self._get_items_features_by_choice(choices_indexes)\n\n        if self.choice_dataset.available_items_by_choice is None:\n            available_items_by_choice = np.ones(\n                (len(choices_indexes), self.choice_dataset.base_num_items)\n            ).astype(\"float32\")\n        else:\n            if isinstance(self.choice_dataset.available_items_by_choice, tuple):\n                available_items_by_choice = self.choice_dataset.available_items_by_choice[\n                    0\n                ].batch[self.choice_dataset.available_items_by_choice[1][choices_indexes]]\n            else:\n                available_items_by_choice = self.choice_dataset.available_items_by_choice[\n                    choices_indexes\n                ]\n            available_items_by_choice = available_items_by_choice.astype(\n                self.choice_dataset._return_types[2]\n            )\n\n        choices = self.choice_dataset.choices[choices_indexes].astype(\n            self.choice_dataset._return_types[3]\n        )\n\n        ###\n        if len(self.choice_dataset.shared_features_by_choice_map) &gt; 0:\n            mapped_features = []\n            ###\n            for tuple_index in range(len(shared_features_by_choice)):\n                if tuple_index in self.choice_dataset.shared_features_by_choice_map.keys():\n                    feat_ind_min = 0\n                    unstacked_feat = []\n                    for feature_index in np.sort(\n                        list(\n                            self.choice_dataset.shared_features_by_choice_map[\n                                tuple_index\n                            ].keys()\n                        )\n                    ):\n                        if feat_ind_min != feature_index:\n                            unstacked_feat.append(\n                                shared_features_by_choice[tuple_index][\n                                    :, feat_ind_min:feature_index\n                                ]\n                            )\n                        unstacked_feat.append(\n                            self.choice_dataset.shared_features_by_choice_map[tuple_index][\n                                feature_index\n                            ].batch[shared_features_by_choice[tuple_index][:, feature_index]]\n                        )\n                        feat_ind_min = feature_index + 1\n                    if feat_ind_min != shared_features_by_choice[tuple_index].shape[1]:\n                        unstacked_feat.append(\n                            shared_features_by_choice[tuple_index][:, feat_ind_min:]\n                        )\n                    mapped_features.append(np.hstack(unstacked_feat))\n                else:\n                    mapped_features.append(shared_features_by_choice[tuple_index])\n\n            shared_features_by_choice = mapped_features\n\n        if len(self.choice_dataset.items_features_by_choice_map) &gt; 0:\n            mapped_features = []\n            for tuple_index in range(len(items_features_by_choice)):\n                if tuple_index in self.choice_dataset.items_features_by_choice_map.keys():\n                    if items_features_by_choice[tuple_index].ndim == 1:\n                        mapped_features.append(\n                            self.choice_dataset.items_features_by_choice_map[tuple_index][\n                                0\n                            ].batch[items_features_by_choice[tuple_index]]\n                        )\n                    else:\n                        feat_ind_min = 0\n                        unstacked_feat = []\n                        for feature_index in np.sort(\n                            list(\n                                self.choice_dataset.items_features_by_choice_map[\n                                    tuple_index\n                                ].keys()\n                            )\n                        ):\n                            if feat_ind_min != feature_index:\n                                unstacked_feat.append(\n                                    items_features_by_choice[tuple_index][\n                                        :, :, feat_ind_min:feature_index\n                                    ]\n                                )\n                            unstacked_feat.append(\n                                self.choice_dataset.items_features_by_choice_map[tuple_index][\n                                    feature_index\n                                ].batch[\n                                    items_features_by_choice[tuple_index][:, :, feature_index]\n                                ]\n                            )\n                            feat_ind_min = feature_index + 1\n                        if feat_ind_min != items_features_by_choice[tuple_index].shape[2]:\n                            unstacked_feat.append(\n                                items_features_by_choice[tuple_index][:, :, feat_ind_min:]\n                            )\n                        mapped_features.append(np.concatenate(unstacked_feat, axis=2))\n                else:\n                    mapped_features.append(items_features_by_choice[tuple_index])\n\n            items_features_by_choice = mapped_features\n\n        if shared_features_by_choice is not None:\n            for i in range(len(shared_features_by_choice)):\n                shared_features_by_choice[i] = shared_features_by_choice[i].astype(\n                    self.choice_dataset._return_types[0][i]\n                )\n            if not self.choice_dataset._return_shared_features_by_choice_tuple:\n                shared_features_by_choice = shared_features_by_choice[0]\n            else:\n                shared_features_by_choice = tuple(shared_features_by_choice)\n\n        if items_features_by_choice is not None:\n            for i in range(len(items_features_by_choice)):\n                items_features_by_choice[i] = items_features_by_choice[i].astype(\n                    self.choice_dataset._return_types[1][i]\n                )\n            # items_features_by_choice were not given as a tuple, so we return do not return\n            # it as a tuple\n            if not self.choice_dataset._return_items_features_by_choice_tuple:\n                items_features_by_choice = items_features_by_choice[0]\n            else:\n                items_features_by_choice = tuple(items_features_by_choice)\n        return (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n\n    if isinstance(choices_indexes, slice):\n        return self.__getitem__(\n            list(range(*choices_indexes.indices(self.choice_dataset.choices.shape[0])))\n        )\n\n    if isinstance(choices_indexes, int):\n        choices_indexes = [choices_indexes]\n        (\n            shared_features_by_choices,\n            items_features_by_choice,\n            available_items_by_choice,\n            choice,\n        ) = self.__getitem__(choices_indexes)\n        if shared_features_by_choices is not None:\n            if isinstance(shared_features_by_choices, tuple):\n                shared_features_by_choices = tuple(\n                    feat[0] for feat in shared_features_by_choices\n                )\n            else:\n                shared_features_by_choices = shared_features_by_choices[0]\n        if items_features_by_choice is not None:\n            if isinstance(items_features_by_choice, tuple):\n                items_features_by_choice = tuple(feat[0] for feat in items_features_by_choice)\n            else:\n                items_features_by_choice = items_features_by_choice[0]\n\n        return (\n            shared_features_by_choices,\n            items_features_by_choice,\n            available_items_by_choice[0],\n            choice[0],\n        )\n    logging.error(f\"Type{type(choices_indexes)} not handled\")\n    raise NotImplementedError(f\"Type{type(choices_indexes)} not handled\")\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.ChoiceDatasetIndexer.__init__","title":"<code>__init__(choice_dataset)</code>","text":"<p>Instanciate a ChoiceDatasetIndexer object.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset to be indexed.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __init__(self, choice_dataset):\n    \"\"\"Instanciate a ChoiceDatasetIndexer object.\n\n    Parameters\n    ----------\n    choice_dataset : choce_modeling.data.dataset.ChoiceDataset\n        Dataset to be indexed.\n    \"\"\"\n    self.choice_dataset = choice_dataset\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.ChoiceDatasetIndexer.get_full_dataset","title":"<code>get_full_dataset(sample_weight=None)</code>","text":"<p>Return the full dataset.</p> <p>This function is here to speed up iteration over dataset when batch_size is -1 or length of dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample_weight</code> <code>list or ndarray</code> <p>sample_weight of size (len(dataset), )</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>all shared_features</p> <code>ndarray</code> <p>all items_features</p> <code>ndarray</code> <p>all available_items_by_choice</p> <code>ndarray</code> <p>all choices</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def get_full_dataset(self, sample_weight=None):\n    \"\"\"Return the full dataset.\n\n    This function is here to speed up iteration over dataset when batch_size\n    is -1 or length of dataset.\n\n    Parameters\n    ----------\n    sample_weight : list or np.ndarray\n        sample_weight of size (len(dataset), )\n\n    Returns\n    -------\n    np.ndarray\n        all shared_features\n    np.ndarray\n        all items_features\n    np.ndarray\n        all available_items_by_choice\n    np.ndarray\n        all choices\n    \"\"\"\n    if self.choice_dataset.shared_features_by_choice is not None:\n        shared_features_by_choice = [\n            feat for feat in self.choice_dataset.shared_features_by_choice\n        ]\n    else:\n        shared_features_by_choice = None\n\n    if self.choice_dataset.items_features_by_choice is not None:\n        items_features_by_choice = [\n            feat for feat in self.choice_dataset.items_features_by_choice\n        ]\n    else:\n        items_features_by_choice = None\n\n    if self.choice_dataset.available_items_by_choice is None:\n        available_items_by_choice = np.ones(\n            (len(self.choice_dataset), self.choice_dataset.base_num_items)\n        ).astype(\"float32\")\n    else:\n        if isinstance(self.choice_dataset.available_items_by_choice, tuple):\n            available_items_by_choice = self.choice_dataset.available_items_by_choice[0].batch[\n                self.choice_dataset.available_items_by_choice[1]\n            ]\n        else:\n            available_items_by_choice = self.choice_dataset.available_items_by_choice\n    available_items_by_choice = available_items_by_choice.astype(\n        self.choice_dataset._return_types[2]\n    )\n\n    choices = self.choice_dataset.choices.astype(self.choice_dataset._return_types[3])\n\n    ###\n    if len(self.choice_dataset.shared_features_by_choice_map) &gt; 0:\n        mapped_features = []\n        ###\n        for tuple_index in range(len(shared_features_by_choice)):\n            if tuple_index in self.choice_dataset.shared_features_by_choice_map.keys():\n                feat_ind_min = 0\n                unstacked_feat = []\n                for feature_index in np.sort(\n                    list(self.choice_dataset.shared_features_by_choice_map[tuple_index].keys())\n                ):\n                    unstacked_feat.append(\n                        shared_features_by_choice[tuple_index][:, feat_ind_min:feature_index]\n                    )\n                    unstacked_feat.append(\n                        self.choice_dataset.shared_features_by_choice_map[tuple_index][\n                            feature_index\n                        ].batch[shared_features_by_choice[tuple_index][:, feature_index]]\n                    )\n                    feat_ind_min = feature_index + 1\n                if feat_ind_min != shared_features_by_choice[tuple_index].shape[1]:\n                    unstacked_feat.append(\n                        shared_features_by_choice[tuple_index][:, feat_ind_min:]\n                    )\n                mapped_features.append(np.concatenate(unstacked_feat, axis=1))\n            else:\n                mapped_features.append(shared_features_by_choice[tuple_index])\n\n        shared_features_by_choice = mapped_features\n\n    if len(self.choice_dataset.items_features_by_choice_map) &gt; 0:\n        mapped_features = []\n        for tuple_index in range(len(items_features_by_choice)):\n            if tuple_index in self.choice_dataset.items_features_by_choice_map.keys():\n                if items_features_by_choice[tuple_index].ndim == 1:\n                    mapped_features.append(\n                        self.choice_dataset.items_features_by_choice_map[tuple_index][0].batch[\n                            items_features_by_choice[tuple_index]\n                        ]\n                    )\n                else:\n                    feat_ind_min = 0\n                    unstacked_feat = []\n                    for feature_index in np.sort(\n                        list(\n                            self.choice_dataset.items_features_by_choice_map[tuple_index].keys()\n                        )\n                    ):\n                        unstacked_feat.append(\n                            items_features_by_choice[tuple_index][\n                                :, :, feat_ind_min:feature_index\n                            ]\n                        )\n                        unstacked_feat.append(\n                            self.choice_dataset.items_features_by_choice_map[tuple_index][\n                                feature_index\n                            ].batch[items_features_by_choice[tuple_index][:, :, feature_index]]\n                        )\n                        feat_ind_min = feature_index + 1\n                    if feat_ind_min != items_features_by_choice[tuple_index].shape[2]:\n                        unstacked_feat.append(\n                            items_features_by_choice[tuple_index][:, :, feat_ind_min:]\n                        )\n                    mapped_features.append(np.concatenate(unstacked_feat, axis=2))\n            else:\n                mapped_features.append(items_features_by_choice[tuple_index])\n\n        items_features_by_choice = mapped_features\n\n    if shared_features_by_choice is not None:\n        for i in range(len(shared_features_by_choice)):\n            shared_features_by_choice[i] = shared_features_by_choice[i].astype(\n                self.choice_dataset._return_types[0][i]\n            )\n        if not self.choice_dataset._return_shared_features_by_choice_tuple:\n            shared_features_by_choice = shared_features_by_choice[0]\n        else:\n            shared_features_by_choice = tuple(shared_features_by_choice)\n\n    if items_features_by_choice is not None:\n        for i in range(len(items_features_by_choice)):\n            items_features_by_choice[i] = items_features_by_choice[i].astype(\n                self.choice_dataset._return_types[1][i]\n            )\n        # items_features_by_choice were not given as a tuple, so we return do not return\n        # it as a tuple\n        if not self.choice_dataset._return_items_features_by_choice_tuple:\n            items_features_by_choice = items_features_by_choice[0]\n        else:\n            items_features_by_choice = tuple(items_features_by_choice)\n\n    if sample_weight is not None:\n        return (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        ), sample_weight\n    return (\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    )\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.Indexer","title":"<code>Indexer</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for Indexer.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class Indexer(ABC):\n    \"\"\"Base class for Indexer.\"\"\"\n\n    def __init__(self, indexed_object):\n        \"\"\"Instanciate an Indexer object.\n\n        Parameters\n        ----------\n        indexed_object : object\n            object to be indexed.\n        \"\"\"\n        self.indexed_object = indexed_object\n\n    @abstractmethod\n    def __getitem__(self, index):\n        \"\"\"To be coded for children classes.\n\n        Parameters\n        ----------\n        index : int, slice, list\n            index(es) of elements of self.indexed_object to be returned.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.Indexer.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>To be coded for children classes.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>(int, slice, list)</code> <p>index(es) of elements of self.indexed_object to be returned.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, index):\n    \"\"\"To be coded for children classes.\n\n    Parameters\n    ----------\n    index : int, slice, list\n        index(es) of elements of self.indexed_object to be returned.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.Indexer.__init__","title":"<code>__init__(indexed_object)</code>","text":"<p>Instanciate an Indexer object.</p> <p>Parameters:</p> Name Type Description Default <code>indexed_object</code> <code>object</code> <p>object to be indexed.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __init__(self, indexed_object):\n    \"\"\"Instanciate an Indexer object.\n\n    Parameters\n    ----------\n    indexed_object : object\n        object to be indexed.\n    \"\"\"\n    self.indexed_object = indexed_object\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.OneHotStorageIndexer","title":"<code>OneHotStorageIndexer</code>","text":"<p>             Bases: <code>Indexer</code></p> <p>Class for Ilocing OneHotStorage.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class OneHotStorageIndexer(Indexer):\n    \"\"\"Class for Ilocing OneHotStorage.\"\"\"\n\n    def __init__(self, storage):\n        \"\"\"OneHotStorageIndexer constructor.\n\n        Parameters\n        ----------\n        storage : choice_modeling.data.store.OneHotStorage\n            OneHotStorage object to be indexed.\n        \"\"\"\n        self.storage = storage\n        self.shape = storage.shape\n        self.dtype = storage.dtype\n\n    def __getitem__(self, sequence_keys):\n        \"\"\"Get the 1 indexes corresponding to the sequence_keys and builds the OneHot matrix.\n\n        Parameters\n        ----------\n        sequence_keys : (int, list, slice)\n            keys of values to be retrieved\n\n        Returns\n        -------\n        np.ndarray\n            OneHot reconstructed vectors corresponding to sequence_keys\n        \"\"\"\n        if isinstance(sequence_keys, list) or isinstance(sequence_keys, np.ndarray):\n            # Construction of the OneHot vector from the index of the 1 value\n\n            if np.array(sequence_keys).ndim == 1:\n                one_hot = []\n                for j in sequence_keys:\n                    # one_hot.append(self[j])\n                    one_hot.append(self.storage.storage[j])\n                matrix = np.zeros((len(one_hot), self.shape[1]))\n                matrix[np.arange(len(one_hot)), one_hot] = 1\n                return matrix.astype(self.dtype)\n            one_hot = []\n            for j in sequence_keys:\n                one_hot.append(self[j])\n            return np.stack(one_hot).astype(self.dtype)\n        if isinstance(sequence_keys, slice):\n            return self[list(range(*sequence_keys.indices(self.shape[0])))]\n        one_hot = np.zeros(self.shape[1])\n        try:\n            one_hot[self.storage.storage[sequence_keys]] = 1\n        except KeyError as error:\n            print(\"You are using an ID that is not in the storage:\")\n            print(error)\n            raise\n\n        return one_hot.astype(self.dtype)\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.OneHotStorageIndexer.__getitem__","title":"<code>__getitem__(sequence_keys)</code>","text":"<p>Get the 1 indexes corresponding to the sequence_keys and builds the OneHot matrix.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_keys</code> <code>(int, list, slice)</code> <p>keys of values to be retrieved</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>OneHot reconstructed vectors corresponding to sequence_keys</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __getitem__(self, sequence_keys):\n    \"\"\"Get the 1 indexes corresponding to the sequence_keys and builds the OneHot matrix.\n\n    Parameters\n    ----------\n    sequence_keys : (int, list, slice)\n        keys of values to be retrieved\n\n    Returns\n    -------\n    np.ndarray\n        OneHot reconstructed vectors corresponding to sequence_keys\n    \"\"\"\n    if isinstance(sequence_keys, list) or isinstance(sequence_keys, np.ndarray):\n        # Construction of the OneHot vector from the index of the 1 value\n\n        if np.array(sequence_keys).ndim == 1:\n            one_hot = []\n            for j in sequence_keys:\n                # one_hot.append(self[j])\n                one_hot.append(self.storage.storage[j])\n            matrix = np.zeros((len(one_hot), self.shape[1]))\n            matrix[np.arange(len(one_hot)), one_hot] = 1\n            return matrix.astype(self.dtype)\n        one_hot = []\n        for j in sequence_keys:\n            one_hot.append(self[j])\n        return np.stack(one_hot).astype(self.dtype)\n    if isinstance(sequence_keys, slice):\n        return self[list(range(*sequence_keys.indices(self.shape[0])))]\n    one_hot = np.zeros(self.shape[1])\n    try:\n        one_hot[self.storage.storage[sequence_keys]] = 1\n    except KeyError as error:\n        print(\"You are using an ID that is not in the storage:\")\n        print(error)\n        raise\n\n    return one_hot.astype(self.dtype)\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.OneHotStorageIndexer.__init__","title":"<code>__init__(storage)</code>","text":"<p>OneHotStorageIndexer constructor.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>OneHotStorage</code> <p>OneHotStorage object to be indexed.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __init__(self, storage):\n    \"\"\"OneHotStorageIndexer constructor.\n\n    Parameters\n    ----------\n    storage : choice_modeling.data.store.OneHotStorage\n        OneHotStorage object to be indexed.\n    \"\"\"\n    self.storage = storage\n    self.shape = storage.shape\n    self.dtype = storage.dtype\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.OneHotStoreIndexer","title":"<code>OneHotStoreIndexer</code>","text":"<p>             Bases: <code>Indexer</code></p> <p>Class for Ilocing OneHotStore.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class OneHotStoreIndexer(Indexer):\n    \"\"\"Class for Ilocing OneHotStore.\"\"\"\n\n    def __init__(self, store):\n        \"\"\"OneHotStoreIndexer constructor.\n\n        Parameters\n        ----------\n        store : choice_modeling.data.store.OneHotStore\n            OneHotStore object to be indexed.\n        \"\"\"\n        self.store = store\n\n        self.shape = (len(self.store.sequence), np.max(list(self.store.store.values())) + 1)\n\n    def __getitem__(self, sequence_index):\n        \"\"\"Get an element at sequence_index-th position of self.sequence.\n\n        Parameters\n        ----------\n        sequence_index : (int, list, slice)\n            index from sequence of element to get\n\n        Returns\n        -------\n        np.ndarray\n            OneHot features corresponding to the sequence_index-th position of sequence\n        \"\"\"\n        if isinstance(sequence_index, list):\n            # Construction of the OneHot vector from the index of the 1 value\n            one_hot = np.zeros((len(sequence_index), self.shape[1]))\n            for i, j in enumerate(sequence_index):\n                one_hot[i, self.store.store[self.store.sequence[j]]] = 1\n            return one_hot.astype(self.store.dtype)\n        if isinstance(sequence_index, slice):\n            return self[list(range(*sequence_index.indices(len(self.store.sequence))))]\n        # else:\n        one_hot = np.zeros(self.shape[1])\n        one_hot[self.store.store[self.store.sequence[sequence_index]]] = 1\n        return one_hot.astype(self.store.dtype)\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.OneHotStoreIndexer.__getitem__","title":"<code>__getitem__(sequence_index)</code>","text":"<p>Get an element at sequence_index-th position of self.sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_index</code> <code>(int, list, slice)</code> <p>index from sequence of element to get</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>OneHot features corresponding to the sequence_index-th position of sequence</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __getitem__(self, sequence_index):\n    \"\"\"Get an element at sequence_index-th position of self.sequence.\n\n    Parameters\n    ----------\n    sequence_index : (int, list, slice)\n        index from sequence of element to get\n\n    Returns\n    -------\n    np.ndarray\n        OneHot features corresponding to the sequence_index-th position of sequence\n    \"\"\"\n    if isinstance(sequence_index, list):\n        # Construction of the OneHot vector from the index of the 1 value\n        one_hot = np.zeros((len(sequence_index), self.shape[1]))\n        for i, j in enumerate(sequence_index):\n            one_hot[i, self.store.store[self.store.sequence[j]]] = 1\n        return one_hot.astype(self.store.dtype)\n    if isinstance(sequence_index, slice):\n        return self[list(range(*sequence_index.indices(len(self.store.sequence))))]\n    # else:\n    one_hot = np.zeros(self.shape[1])\n    one_hot[self.store.store[self.store.sequence[sequence_index]]] = 1\n    return one_hot.astype(self.store.dtype)\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.OneHotStoreIndexer.__init__","title":"<code>__init__(store)</code>","text":"<p>OneHotStoreIndexer constructor.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>OneHotStore</code> <p>OneHotStore object to be indexed.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __init__(self, store):\n    \"\"\"OneHotStoreIndexer constructor.\n\n    Parameters\n    ----------\n    store : choice_modeling.data.store.OneHotStore\n        OneHotStore object to be indexed.\n    \"\"\"\n    self.store = store\n\n    self.shape = (len(self.store.sequence), np.max(list(self.store.store.values())) + 1)\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.StorageIndexer","title":"<code>StorageIndexer</code>","text":"<p>             Bases: <code>Indexer</code></p> <p>Class for Ilocing/Batching FeaturesStorage.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class StorageIndexer(Indexer):\n    \"\"\"Class for Ilocing/Batching FeaturesStorage.\"\"\"\n\n    def __init__(self, storage):\n        \"\"\"StoreIndexer constructor.\n\n        Parameters\n        ----------\n        storage : choice_modeling.data.store.FeaturesStorage\n            Storage object to be indexed.\n        \"\"\"\n        self.storage = storage\n\n    def __getitem__(self, sequence_keys):\n        \"\"\"Return the features appearing at the sequence_index-th position of sequence.\n\n        Parameters\n        ----------\n        sequence_keys : (int, list, slice)\n            keys of values to be retrieved\n\n        Returns\n        -------\n        array_like\n            features corresponding to the sequence_keys\n        \"\"\"\n        try:\n            if isinstance(sequence_keys, list) or isinstance(sequence_keys, np.ndarray):\n                if len(np.array(sequence_keys).shape) &gt; 1:\n                    return np.stack([self.storage.batch[key] for key in sequence_keys], axis=0)\n                return np.array([self.storage.storage[key] for key in sequence_keys])\n\n            if isinstance(sequence_keys, slice):\n                raise ValueError(\"Slicing is not supported for storage\")\n            return np.array(self.storage.storage[sequence_keys])\n        except KeyError as error:\n            print(\"You are using an ID that is not in the storage:\")\n            print(error)\n            raise\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.StorageIndexer.__getitem__","title":"<code>__getitem__(sequence_keys)</code>","text":"<p>Return the features appearing at the sequence_index-th position of sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_keys</code> <code>(int, list, slice)</code> <p>keys of values to be retrieved</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the sequence_keys</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __getitem__(self, sequence_keys):\n    \"\"\"Return the features appearing at the sequence_index-th position of sequence.\n\n    Parameters\n    ----------\n    sequence_keys : (int, list, slice)\n        keys of values to be retrieved\n\n    Returns\n    -------\n    array_like\n        features corresponding to the sequence_keys\n    \"\"\"\n    try:\n        if isinstance(sequence_keys, list) or isinstance(sequence_keys, np.ndarray):\n            if len(np.array(sequence_keys).shape) &gt; 1:\n                return np.stack([self.storage.batch[key] for key in sequence_keys], axis=0)\n            return np.array([self.storage.storage[key] for key in sequence_keys])\n\n        if isinstance(sequence_keys, slice):\n            raise ValueError(\"Slicing is not supported for storage\")\n        return np.array(self.storage.storage[sequence_keys])\n    except KeyError as error:\n        print(\"You are using an ID that is not in the storage:\")\n        print(error)\n        raise\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.StorageIndexer.__init__","title":"<code>__init__(storage)</code>","text":"<p>StoreIndexer constructor.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>FeaturesStorage</code> <p>Storage object to be indexed.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __init__(self, storage):\n    \"\"\"StoreIndexer constructor.\n\n    Parameters\n    ----------\n    storage : choice_modeling.data.store.FeaturesStorage\n        Storage object to be indexed.\n    \"\"\"\n    self.storage = storage\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.StoreIndexer","title":"<code>StoreIndexer</code>","text":"<p>             Bases: <code>Indexer</code></p> <p>Class for Ilocing/Batching FeaturesStore.</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>class StoreIndexer(Indexer):\n    \"\"\"Class for Ilocing/Batching FeaturesStore.\"\"\"\n\n    def __init__(self, store):\n        \"\"\"StoreIndexer constructor.\n\n        Parameters\n        ----------\n        store : choice_modeling.data.store.FeaturesStore\n            Store object to be indexed.\n        \"\"\"\n        self.store = store\n\n    def __getitem__(self, sequence_index):\n        \"\"\"Return the features appearing at the sequence_index-th position of sequence.\n\n        Parameters\n        ----------\n        sequence_index : (int, list, slice)\n            index position of the sequence\n\n        Returns\n        -------\n        array_like\n            features corresponding to the sequence_index-th position of sequence\n        \"\"\"\n        if isinstance(sequence_index, list):\n            return [self.store.store[self.store.sequence[i]] for i in sequence_index]\n        if isinstance(sequence_index, slice):\n            return [\n                self.store.store[self.store.sequence[i]]\n                for i in range(*sequence_index.indices(len(self.store.sequence)))\n            ]\n        return self.store.store[self.store.sequence[sequence_index]]\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.StoreIndexer.__getitem__","title":"<code>__getitem__(sequence_index)</code>","text":"<p>Return the features appearing at the sequence_index-th position of sequence.</p> <p>Parameters:</p> Name Type Description Default <code>sequence_index</code> <code>(int, list, slice)</code> <p>index position of the sequence</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the sequence_index-th position of sequence</p> Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __getitem__(self, sequence_index):\n    \"\"\"Return the features appearing at the sequence_index-th position of sequence.\n\n    Parameters\n    ----------\n    sequence_index : (int, list, slice)\n        index position of the sequence\n\n    Returns\n    -------\n    array_like\n        features corresponding to the sequence_index-th position of sequence\n    \"\"\"\n    if isinstance(sequence_index, list):\n        return [self.store.store[self.store.sequence[i]] for i in sequence_index]\n    if isinstance(sequence_index, slice):\n        return [\n            self.store.store[self.store.sequence[i]]\n            for i in range(*sequence_index.indices(len(self.store.sequence)))\n        ]\n    return self.store.store[self.store.sequence[sequence_index]]\n</code></pre>"},{"location":"references/data/references_indexer/#choice_learn.data.indexer.StoreIndexer.__init__","title":"<code>__init__(store)</code>","text":"<p>StoreIndexer constructor.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>FeaturesStore</code> <p>Store object to be indexed.</p> required Source code in <code>choice_learn/data/indexer.py</code> <pre><code>def __init__(self, store):\n    \"\"\"StoreIndexer constructor.\n\n    Parameters\n    ----------\n    store : choice_modeling.data.store.FeaturesStore\n        Store object to be indexed.\n    \"\"\"\n    self.store = store\n</code></pre>"},{"location":"references/data/references_storage/","title":"Features Storage","text":"<p>Different classes to optimize RAM usage with repeated features over time.</p>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage","title":"<code>ArrayStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Function to store features with ids as NumPy Array.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>class ArrayStorage(Storage):\n    \"\"\"Function to store features with ids as NumPy Array.\"\"\"\n\n    def __init__(self, values=None, values_names=None, name=None):\n        \"\"\"Build the store.\n\n        Parameters\n        ----------\n        values : array_like\n            list of values of features to store\n        values_names : array_like\n            Iterable of str indicating the name of the features. Must be same length as values.\n        name: string, optional\n            name of the features store\n        \"\"\"\n        if isinstance(values, list):\n            values = np.array(values)\n        elif not isinstance(values, np.ndarray):\n            raise ValueError(\"ArrayStorage Values must be a list or a numpy array\")\n\n        # Checking that FeaturesStorage increases dimensions\n        # key -&gt; features of ndim &gt;= 1\n        if not values.ndim &gt; 1:\n            raise ValueError(\"ArrayStorage Values must be a list or a numpy array of ndim &gt;= 1\")\n        # self.storage = storage\n        self.values_names = values_names\n        self.name = name\n\n        self.shape = values.shape\n        self.storage = values\n        self.indexer = ArrayStorageIndexer(self)\n\n    def get_element_from_index(self, index):\n        \"\"\"Getter method with index (int).\n\n        Returns the features stored with the index-th ID.\n\n        Parameters\n        ----------\n        index : (int, list, slice)\n            index argument of the feature\n\n        Returns\n        -------\n        array_like\n            features corresponding to the index index in self.storage\n        \"\"\"\n        return self.batch[index]\n\n    def __len__(self):\n        \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n        return self.shape[0]\n\n    def __getitem__(self, id_keys):\n        \"\"\"Subset FeaturesStorage, keeping only the features whose id is in keys.\n\n        Parameters\n        ----------\n        id_keys : Iterable\n            List of ids to keep.\n\n        Returns\n        -------\n        FeaturesStorage\n            Subset of the FeaturesStorage, with only the features whose id is in id_keys\n        \"\"\"\n        if not isinstance(id_keys, list):\n            id_keys = [id_keys]\n        return ArrayStorage(\n            values=self.batch[id_keys], values_names=self.values_names, name=self.name\n        )\n\n    def get_storage_type(self):\n        \"\"\"Functions to access stored elements dtypes.\n\n        Returns\n        -------\n        tuple\n            tuple of dtypes of the stored elements, as returned by np.dtype\n        \"\"\"\n        element = self.get_element_from_index(0)\n        return element.dtype\n\n    @property\n    def batch(self):\n        \"\"\"Indexing attribute.\"\"\"\n        return self.indexer\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Indexing attribute.</p>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage.__getitem__","title":"<code>__getitem__(id_keys)</code>","text":"<p>Subset FeaturesStorage, keeping only the features whose id is in keys.</p> <p>Parameters:</p> Name Type Description Default <code>id_keys</code> <code>Iterable</code> <p>List of ids to keep.</p> required <p>Returns:</p> Type Description <code>FeaturesStorage</code> <p>Subset of the FeaturesStorage, with only the features whose id is in id_keys</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __getitem__(self, id_keys):\n    \"\"\"Subset FeaturesStorage, keeping only the features whose id is in keys.\n\n    Parameters\n    ----------\n    id_keys : Iterable\n        List of ids to keep.\n\n    Returns\n    -------\n    FeaturesStorage\n        Subset of the FeaturesStorage, with only the features whose id is in id_keys\n    \"\"\"\n    if not isinstance(id_keys, list):\n        id_keys = [id_keys]\n    return ArrayStorage(\n        values=self.batch[id_keys], values_names=self.values_names, name=self.name\n    )\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage.__init__","title":"<code>__init__(values=None, values_names=None, name=None)</code>","text":"<p>Build the store.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>array_like</code> <p>list of values of features to store</p> <code>None</code> <code>values_names</code> <code>array_like</code> <p>Iterable of str indicating the name of the features. Must be same length as values.</p> <code>None</code> <code>name</code> <p>name of the features store</p> <code>None</code> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __init__(self, values=None, values_names=None, name=None):\n    \"\"\"Build the store.\n\n    Parameters\n    ----------\n    values : array_like\n        list of values of features to store\n    values_names : array_like\n        Iterable of str indicating the name of the features. Must be same length as values.\n    name: string, optional\n        name of the features store\n    \"\"\"\n    if isinstance(values, list):\n        values = np.array(values)\n    elif not isinstance(values, np.ndarray):\n        raise ValueError(\"ArrayStorage Values must be a list or a numpy array\")\n\n    # Checking that FeaturesStorage increases dimensions\n    # key -&gt; features of ndim &gt;= 1\n    if not values.ndim &gt; 1:\n        raise ValueError(\"ArrayStorage Values must be a list or a numpy array of ndim &gt;= 1\")\n    # self.storage = storage\n    self.values_names = values_names\n    self.name = name\n\n    self.shape = values.shape\n    self.storage = values\n    self.indexer = ArrayStorageIndexer(self)\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the sequence of apparition of the features.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n    return self.shape[0]\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage.get_element_from_index","title":"<code>get_element_from_index(index)</code>","text":"<p>Getter method with index (int).</p> <p>Returns the features stored with the index-th ID.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>(int, list, slice)</code> <p>index argument of the feature</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the index index in self.storage</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def get_element_from_index(self, index):\n    \"\"\"Getter method with index (int).\n\n    Returns the features stored with the index-th ID.\n\n    Parameters\n    ----------\n    index : (int, list, slice)\n        index argument of the feature\n\n    Returns\n    -------\n    array_like\n        features corresponding to the index index in self.storage\n    \"\"\"\n    return self.batch[index]\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.ArrayStorage.get_storage_type","title":"<code>get_storage_type()</code>","text":"<p>Functions to access stored elements dtypes.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple of dtypes of the stored elements, as returned by np.dtype</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def get_storage_type(self):\n    \"\"\"Functions to access stored elements dtypes.\n\n    Returns\n    -------\n    tuple\n        tuple of dtypes of the stored elements, as returned by np.dtype\n    \"\"\"\n    element = self.get_element_from_index(0)\n    return element.dtype\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage","title":"<code>DictStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Function to store features with ids.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>class DictStorage(Storage):\n    \"\"\"Function to store features with ids.\"\"\"\n\n    def __init__(\n        self,\n        ids=None,\n        values=None,\n        values_names=None,\n        name=None,\n        indexer=StorageIndexer,\n    ):\n        \"\"\"Build the store.\n\n        Parameters\n        ----------\n        ids : array_like or None\n            list of ids of features to store. If None is given, ids are created from\n            apparition order of values\n        values : array_like\n            list of values of features to store\n        values_names : array_like\n            Iterable of str indicating the name of the features. Must be same length as values.\n        name: string, optional\n            name of the features store\n        \"\"\"\n        if isinstance(values, dict):\n            storage = values\n            lengths = []\n            for k, v in storage.items():\n                if not isinstance(v, np.ndarray) | isinstance(v, list):\n                    raise ValueError(\"values must be a dict of np.ndarray or list\")\n                if not len(np.array(v).shape) == 1:\n                    raise ValueError(\n                        \"values (features) must be a dict of np.ndarray or list of 1D arrays\"\n                    )\n                lengths.append(len(v))\n                if isinstance(v, list):\n                    storage[k] = np.array(v)\n            if not len(set(lengths)) == 1:\n                raise ValueError(\"values (dict values) must all have same length\")\n            if ids is not None:\n                print(\"Warning: ids is ignored when values is a dict\")\n\n        elif isinstance(values, pd.DataFrame):\n            if values_names is not None:\n                print(\"Warning: values_names is ignored when values is a DataFrame\")\n            if \"id\" in values.columns:\n                values = values.set_index(\"id\")\n            values_names = values.columns\n            storage = {k: v.to_numpy() for (k, v) in values.iterrows()}\n        elif isinstance(values, list) or isinstance(values, np.ndarray):\n            if ids is None:\n                ids = list(range(len(values)))\n            storage = {k: np.array(v) for (k, v) in zip(ids, values)}\n        else:\n            raise ValueError(\"values must be a dict, a DataFrame, a list or a numpy array\")\n\n        self.storage = storage\n        self.values_names = values_names\n        self.name = name\n\n        self.shape = (len(self), len(next(iter(self.storage.values()))))\n        self.indexer = indexer(self)\n\n    def get_element_from_index(self, index):\n        \"\"\"Getter method with index (int).\n\n        Returns the features stored with the index-th ID.\n\n        Parameters\n        ----------\n        index : (int, list, slice)\n            index argument of the feature\n\n        Returns\n        -------\n        array_like\n            features corresponding to the index index in self.storage\n        \"\"\"\n        if isinstance(index, int):\n            index = [index]\n        keys = [list(self.storage.keys())[i] for i in index]\n        return self.batch[keys]\n\n    def __len__(self):\n        \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n        return len(self.storage)\n\n    def __getitem__(self, id_keys):\n        \"\"\"Subset FeaturesStorage, keeping only the features whose id is in keys.\n\n        Parameters\n        ----------\n        id_keys : Iterable\n            List of ids to keep.\n\n        Returns\n        -------\n        FeaturesStorage\n            Subset of the FeaturesStorage, with only the features whose id is in id_keys\n        \"\"\"\n        if not isinstance(id_keys, list):\n            id_keys = [id_keys]\n        sub_storage = {k: self.storage[k] for k in id_keys}\n        return FeaturesStorage(values=sub_storage, values_names=self.values_names, name=self.name)\n\n    def get_storage_type(self):\n        \"\"\"Functions to access stored elements dtypes.\n\n        Returns\n        -------\n        tuple\n            tuple of dtypes of the stored elements, as returned by np.dtype\n        \"\"\"\n        element = self.get_element_from_index(0)\n        return element.dtype\n\n    @property\n    def batch(self):\n        \"\"\"Indexing attribute.\"\"\"\n        return self.indexer\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Indexing attribute.</p>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage.__getitem__","title":"<code>__getitem__(id_keys)</code>","text":"<p>Subset FeaturesStorage, keeping only the features whose id is in keys.</p> <p>Parameters:</p> Name Type Description Default <code>id_keys</code> <code>Iterable</code> <p>List of ids to keep.</p> required <p>Returns:</p> Type Description <code>FeaturesStorage</code> <p>Subset of the FeaturesStorage, with only the features whose id is in id_keys</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __getitem__(self, id_keys):\n    \"\"\"Subset FeaturesStorage, keeping only the features whose id is in keys.\n\n    Parameters\n    ----------\n    id_keys : Iterable\n        List of ids to keep.\n\n    Returns\n    -------\n    FeaturesStorage\n        Subset of the FeaturesStorage, with only the features whose id is in id_keys\n    \"\"\"\n    if not isinstance(id_keys, list):\n        id_keys = [id_keys]\n    sub_storage = {k: self.storage[k] for k in id_keys}\n    return FeaturesStorage(values=sub_storage, values_names=self.values_names, name=self.name)\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage.__init__","title":"<code>__init__(ids=None, values=None, values_names=None, name=None, indexer=StorageIndexer)</code>","text":"<p>Build the store.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>array_like or None</code> <p>list of ids of features to store. If None is given, ids are created from apparition order of values</p> <code>None</code> <code>values</code> <code>array_like</code> <p>list of values of features to store</p> <code>None</code> <code>values_names</code> <code>array_like</code> <p>Iterable of str indicating the name of the features. Must be same length as values.</p> <code>None</code> <code>name</code> <p>name of the features store</p> <code>None</code> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __init__(\n    self,\n    ids=None,\n    values=None,\n    values_names=None,\n    name=None,\n    indexer=StorageIndexer,\n):\n    \"\"\"Build the store.\n\n    Parameters\n    ----------\n    ids : array_like or None\n        list of ids of features to store. If None is given, ids are created from\n        apparition order of values\n    values : array_like\n        list of values of features to store\n    values_names : array_like\n        Iterable of str indicating the name of the features. Must be same length as values.\n    name: string, optional\n        name of the features store\n    \"\"\"\n    if isinstance(values, dict):\n        storage = values\n        lengths = []\n        for k, v in storage.items():\n            if not isinstance(v, np.ndarray) | isinstance(v, list):\n                raise ValueError(\"values must be a dict of np.ndarray or list\")\n            if not len(np.array(v).shape) == 1:\n                raise ValueError(\n                    \"values (features) must be a dict of np.ndarray or list of 1D arrays\"\n                )\n            lengths.append(len(v))\n            if isinstance(v, list):\n                storage[k] = np.array(v)\n        if not len(set(lengths)) == 1:\n            raise ValueError(\"values (dict values) must all have same length\")\n        if ids is not None:\n            print(\"Warning: ids is ignored when values is a dict\")\n\n    elif isinstance(values, pd.DataFrame):\n        if values_names is not None:\n            print(\"Warning: values_names is ignored when values is a DataFrame\")\n        if \"id\" in values.columns:\n            values = values.set_index(\"id\")\n        values_names = values.columns\n        storage = {k: v.to_numpy() for (k, v) in values.iterrows()}\n    elif isinstance(values, list) or isinstance(values, np.ndarray):\n        if ids is None:\n            ids = list(range(len(values)))\n        storage = {k: np.array(v) for (k, v) in zip(ids, values)}\n    else:\n        raise ValueError(\"values must be a dict, a DataFrame, a list or a numpy array\")\n\n    self.storage = storage\n    self.values_names = values_names\n    self.name = name\n\n    self.shape = (len(self), len(next(iter(self.storage.values()))))\n    self.indexer = indexer(self)\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the sequence of apparition of the features.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n    return len(self.storage)\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage.get_element_from_index","title":"<code>get_element_from_index(index)</code>","text":"<p>Getter method with index (int).</p> <p>Returns the features stored with the index-th ID.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>(int, list, slice)</code> <p>index argument of the feature</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the index index in self.storage</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def get_element_from_index(self, index):\n    \"\"\"Getter method with index (int).\n\n    Returns the features stored with the index-th ID.\n\n    Parameters\n    ----------\n    index : (int, list, slice)\n        index argument of the feature\n\n    Returns\n    -------\n    array_like\n        features corresponding to the index index in self.storage\n    \"\"\"\n    if isinstance(index, int):\n        index = [index]\n    keys = [list(self.storage.keys())[i] for i in index]\n    return self.batch[keys]\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.DictStorage.get_storage_type","title":"<code>get_storage_type()</code>","text":"<p>Functions to access stored elements dtypes.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>tuple of dtypes of the stored elements, as returned by np.dtype</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def get_storage_type(self):\n    \"\"\"Functions to access stored elements dtypes.\n\n    Returns\n    -------\n    tuple\n        tuple of dtypes of the stored elements, as returned by np.dtype\n    \"\"\"\n    element = self.get_element_from_index(0)\n    return element.dtype\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.FeaturesStorage","title":"<code>FeaturesStorage</code>","text":"<p>Base FeaturesStorage class that redirects toward the right one.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>class FeaturesStorage:\n    \"\"\"Base FeaturesStorage class that redirects toward the right one.\"\"\"\n\n    def __new__(\n        cls,\n        ids=None,\n        values=None,\n        values_names=None,\n        name=None,\n        as_one_hot=False,\n    ):\n        \"\"\"Redirects toward the right object.\n\n        Parameters\n        ----------\n        ids : Iterable, optional\n            IDs to references features with, by default None\n        values : Iterable, optional\n            Features to be stored, by default None\n        values_names : list, optional\n            List of names for the features to be stored, by default None\n        name : str, optional\n            Name of the FeaturesStorage, to be matched in ChoiceDataset, by default None\n        as_one_hot: bool\n            Whether features are OneHot representations or not.\n\n        Returns\n        -------\n        FeaturesStorage\n            One of ArrayStorage, DictStorage or OneHotStorage\n        \"\"\"\n        if as_one_hot:\n            return OneHotStorage(ids=ids, values=values, name=name)\n        if ids is None and (isinstance(values, np.ndarray) or isinstance(values, list)):\n            return ArrayStorage(values=values, values_names=values_names, name=name)\n\n        if ids is not None:\n            check_ids = np.isin(ids, np.arange(len(ids))).all()\n            if check_ids:\n                values = np.array(values)[np.argsort(ids)]\n                return ArrayStorage(values=values, values_names=values_names, name=name)\n\n        return DictStorage(\n            ids=ids, values=values, values_names=values_names, name=name, indexer=StorageIndexer\n        )\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.FeaturesStorage.__new__","title":"<code>__new__(ids=None, values=None, values_names=None, name=None, as_one_hot=False)</code>","text":"<p>Redirects toward the right object.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Iterable</code> <p>IDs to references features with, by default None</p> <code>None</code> <code>values</code> <code>Iterable</code> <p>Features to be stored, by default None</p> <code>None</code> <code>values_names</code> <code>list</code> <p>List of names for the features to be stored, by default None</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the FeaturesStorage, to be matched in ChoiceDataset, by default None</p> <code>None</code> <code>as_one_hot</code> <p>Whether features are OneHot representations or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>FeaturesStorage</code> <p>One of ArrayStorage, DictStorage or OneHotStorage</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __new__(\n    cls,\n    ids=None,\n    values=None,\n    values_names=None,\n    name=None,\n    as_one_hot=False,\n):\n    \"\"\"Redirects toward the right object.\n\n    Parameters\n    ----------\n    ids : Iterable, optional\n        IDs to references features with, by default None\n    values : Iterable, optional\n        Features to be stored, by default None\n    values_names : list, optional\n        List of names for the features to be stored, by default None\n    name : str, optional\n        Name of the FeaturesStorage, to be matched in ChoiceDataset, by default None\n    as_one_hot: bool\n        Whether features are OneHot representations or not.\n\n    Returns\n    -------\n    FeaturesStorage\n        One of ArrayStorage, DictStorage or OneHotStorage\n    \"\"\"\n    if as_one_hot:\n        return OneHotStorage(ids=ids, values=values, name=name)\n    if ids is None and (isinstance(values, np.ndarray) or isinstance(values, list)):\n        return ArrayStorage(values=values, values_names=values_names, name=name)\n\n    if ids is not None:\n        check_ids = np.isin(ids, np.arange(len(ids))).all()\n        if check_ids:\n            values = np.array(values)[np.argsort(ids)]\n            return ArrayStorage(values=values, values_names=values_names, name=name)\n\n    return DictStorage(\n        ids=ids, values=values, values_names=values_names, name=name, indexer=StorageIndexer\n    )\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage","title":"<code>OneHotStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Specific Storage for one hot features storage.</p> <p>Inherits from Storage. For example can be used to store a OneHot representation of the days of week.</p> <p>Has the same attributes as FeaturesStoage, only differs whit some One-Hot optimized methods. It only stores the indexes of the features, and creates the OneHot matrix when needed, using .batch[].</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>class OneHotStorage(Storage):\n    \"\"\"Specific Storage for one hot features storage.\n\n    Inherits from Storage.\n    For example can be used to store a OneHot representation of the days of week.\n\n    Has the same attributes as FeaturesStoage, only differs whit some One-Hot optimized methods.\n    It only stores the indexes of the features, and creates the OneHot matrix\n    when needed, using .batch[].\n    \"\"\"\n\n    def __init__(\n        self, ids=None, values=None, name=None, dtype=np.uint8, indexer=OneHotStorageIndexer\n    ):\n        \"\"\"Build the store.\n\n        Parameters\n        ----------\n        ids : array_like or None\n            list of ids of features to store. If None is given, ids are created from\n            apparition order of values\n        values : array_like\n            list of values of features to store\n        dtype: type\n            type for One Hot representation, usually int or float, default is np.uint8\n        name: string, optional\n            name of the features store\n        \"\"\"\n        if isinstance(values, dict):\n            storage = values\n            for k, v in storage.items():\n                if not isinstance(v, int):\n                    raise ValueError(\n                        \"\"\"values of values dict must be int as\n                        they are indexes of the one hot vector ones.\"\"\"\n                    )\n            length = np.max(list(storage.values())) + 1\n            if ids is not None:\n                print(\"Warning: ids is ignored when values is a dict\")\n\n        elif isinstance(values, list) or isinstance(values, np.ndarray):\n            if ids is None:\n                ids = list(range(len(values)))\n            storage = {k: int(v) for (k, v) in zip(ids, values)}\n            length = np.max(values) + 1\n\n        elif values is None:\n            if ids is None:\n                raise ValueError(\"ids or values must be given, both are None\")\n            value = 0\n            storage = {}\n            for id in ids:\n                storage[id] = value\n                value += 1\n            length = value\n        else:\n            raise ValueError(\"values must be a dict, a DataFrame, a list or a numpy array\")\n\n        self.storage = storage\n        self.name = name\n\n        self.shape = (len(self), length)\n        self.dtype = dtype\n        self.indexer = indexer(self)\n\n    def __len__(self):\n        \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n        return len(self.storage)\n\n    def __getitem__(self, id_keys):\n        \"\"\"Subset FeaturesStorage, keeping only the features whose id is in keys.\n\n        Parameters\n        ----------\n        id_keys : Iterable\n            List of ids to keep.\n\n        Returns\n        -------\n        OneHotStorage\n            Subset of the OneHotStorage, with only the features whose id is in id_keys\n        \"\"\"\n        if isinstance(id_keys, int):\n            id_keys = [id_keys]\n        sub_storage = {k: self.storage[k] for k in id_keys}\n\n        subset = OneHotStorage(values=sub_storage, name=self.name, dtype=self.dtype)\n        subset.shape = (len(subset), self.shape[1])\n        return subset\n\n    def astype(self, dtype):\n        \"\"\"Change (mainly int or float) type of returned OneHot features vectors.\n\n        Parameters\n        ----------\n        dtype : type\n            Type to set the features as\n        \"\"\"\n        self.dtype = dtype\n\n    def get_element_from_index(self, index):\n        \"\"\"Getter method with index (int).\n\n        Returns the features stored with the index-th ID.\n\n        Parameters\n        ----------\n        index : (int, list, slice)\n            index argument of the feature\n\n        Returns\n        -------\n        array_like\n            features corresponding to the index index in self.storage\n        \"\"\"\n        keys = list(self.storage.keys())[index]\n        return self.storage[keys]\n\n    def get_storage_type(self):\n        \"\"\"Functions to access stored elements dtypes.\n\n        Returns\n        -------\n        type\n            tuple of dtypes of the stored elements, as returned by np.dtype\n        \"\"\"\n        return self.dtype\n\n    @property\n    def batch(self):\n        \"\"\"Indexing attribute.\"\"\"\n        return self.indexer\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Indexing attribute.</p>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.__getitem__","title":"<code>__getitem__(id_keys)</code>","text":"<p>Subset FeaturesStorage, keeping only the features whose id is in keys.</p> <p>Parameters:</p> Name Type Description Default <code>id_keys</code> <code>Iterable</code> <p>List of ids to keep.</p> required <p>Returns:</p> Type Description <code>OneHotStorage</code> <p>Subset of the OneHotStorage, with only the features whose id is in id_keys</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __getitem__(self, id_keys):\n    \"\"\"Subset FeaturesStorage, keeping only the features whose id is in keys.\n\n    Parameters\n    ----------\n    id_keys : Iterable\n        List of ids to keep.\n\n    Returns\n    -------\n    OneHotStorage\n        Subset of the OneHotStorage, with only the features whose id is in id_keys\n    \"\"\"\n    if isinstance(id_keys, int):\n        id_keys = [id_keys]\n    sub_storage = {k: self.storage[k] for k in id_keys}\n\n    subset = OneHotStorage(values=sub_storage, name=self.name, dtype=self.dtype)\n    subset.shape = (len(subset), self.shape[1])\n    return subset\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.__init__","title":"<code>__init__(ids=None, values=None, name=None, dtype=np.uint8, indexer=OneHotStorageIndexer)</code>","text":"<p>Build the store.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>array_like or None</code> <p>list of ids of features to store. If None is given, ids are created from apparition order of values</p> <code>None</code> <code>values</code> <code>array_like</code> <p>list of values of features to store</p> <code>None</code> <code>dtype</code> <p>type for One Hot representation, usually int or float, default is np.uint8</p> <code>uint8</code> <code>name</code> <p>name of the features store</p> <code>None</code> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __init__(\n    self, ids=None, values=None, name=None, dtype=np.uint8, indexer=OneHotStorageIndexer\n):\n    \"\"\"Build the store.\n\n    Parameters\n    ----------\n    ids : array_like or None\n        list of ids of features to store. If None is given, ids are created from\n        apparition order of values\n    values : array_like\n        list of values of features to store\n    dtype: type\n        type for One Hot representation, usually int or float, default is np.uint8\n    name: string, optional\n        name of the features store\n    \"\"\"\n    if isinstance(values, dict):\n        storage = values\n        for k, v in storage.items():\n            if not isinstance(v, int):\n                raise ValueError(\n                    \"\"\"values of values dict must be int as\n                    they are indexes of the one hot vector ones.\"\"\"\n                )\n        length = np.max(list(storage.values())) + 1\n        if ids is not None:\n            print(\"Warning: ids is ignored when values is a dict\")\n\n    elif isinstance(values, list) or isinstance(values, np.ndarray):\n        if ids is None:\n            ids = list(range(len(values)))\n        storage = {k: int(v) for (k, v) in zip(ids, values)}\n        length = np.max(values) + 1\n\n    elif values is None:\n        if ids is None:\n            raise ValueError(\"ids or values must be given, both are None\")\n        value = 0\n        storage = {}\n        for id in ids:\n            storage[id] = value\n            value += 1\n        length = value\n    else:\n        raise ValueError(\"values must be a dict, a DataFrame, a list or a numpy array\")\n\n    self.storage = storage\n    self.name = name\n\n    self.shape = (len(self), length)\n    self.dtype = dtype\n    self.indexer = indexer(self)\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the sequence of apparition of the features.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n    return len(self.storage)\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.astype","title":"<code>astype(dtype)</code>","text":"<p>Change (mainly int or float) type of returned OneHot features vectors.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>type</code> <p>Type to set the features as</p> required Source code in <code>choice_learn/data/storage.py</code> <pre><code>def astype(self, dtype):\n    \"\"\"Change (mainly int or float) type of returned OneHot features vectors.\n\n    Parameters\n    ----------\n    dtype : type\n        Type to set the features as\n    \"\"\"\n    self.dtype = dtype\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.get_element_from_index","title":"<code>get_element_from_index(index)</code>","text":"<p>Getter method with index (int).</p> <p>Returns the features stored with the index-th ID.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>(int, list, slice)</code> <p>index argument of the feature</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the index index in self.storage</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def get_element_from_index(self, index):\n    \"\"\"Getter method with index (int).\n\n    Returns the features stored with the index-th ID.\n\n    Parameters\n    ----------\n    index : (int, list, slice)\n        index argument of the feature\n\n    Returns\n    -------\n    array_like\n        features corresponding to the index index in self.storage\n    \"\"\"\n    keys = list(self.storage.keys())[index]\n    return self.storage[keys]\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.OneHotStorage.get_storage_type","title":"<code>get_storage_type()</code>","text":"<p>Functions to access stored elements dtypes.</p> <p>Returns:</p> Type Description <code>type</code> <p>tuple of dtypes of the stored elements, as returned by np.dtype</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def get_storage_type(self):\n    \"\"\"Functions to access stored elements dtypes.\n\n    Returns\n    -------\n    type\n        tuple of dtypes of the stored elements, as returned by np.dtype\n    \"\"\"\n    return self.dtype\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage","title":"<code>Storage</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Parent Class to have OneHotStorage and FeaturesStorage with same parent.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>class Storage(ABC):\n    \"\"\"Parent Class to have OneHotStorage and FeaturesStorage with same parent.\"\"\"\n\n    def __init__(self, features_to_store):\n        \"\"\"Instantiate the storage.\n\n        Parameters\n        ----------\n        features_to_store : object\n            Object to store\n        \"\"\"\n        self.features_to_store = features_to_store\n\n    @abstractmethod\n    def __getitem__(self, keys):\n        \"\"\"Access an element. To be implemented in children classes.\n\n        Parameters\n        ----------\n        keys : float, int, str or list of\n            values among indexes of the stiage\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_element_from_index(self, index):\n        \"\"\"Getter method with index (int).\n\n        Returns the features stored with the index-th ID.\n\n        Parameters\n        ----------\n        index : (int, list, slice)\n            index argument of the feature\n\n        Returns\n        -------\n        array_like\n            features corresponding to the index index in self.storage\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __len__(self):\n        \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n        pass\n\n    @property\n    def batch(self):\n        \"\"\"Indexing method.\"\"\"\n        pass\n\n    def __str__(self):\n        \"\"\"Return string representation method.\n\n        Returns\n        -------\n        str\n            Description of the storage.\n        \"\"\"\n        return \"FeatureStorage with name: .\"\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Indexing method.</p>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage.__getitem__","title":"<code>__getitem__(keys)</code>  <code>abstractmethod</code>","text":"<p>Access an element. To be implemented in children classes.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>float, int, str or list of</code> <p>values among indexes of the stiage</p> required Source code in <code>choice_learn/data/storage.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, keys):\n    \"\"\"Access an element. To be implemented in children classes.\n\n    Parameters\n    ----------\n    keys : float, int, str or list of\n        values among indexes of the stiage\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage.__init__","title":"<code>__init__(features_to_store)</code>","text":"<p>Instantiate the storage.</p> <p>Parameters:</p> Name Type Description Default <code>features_to_store</code> <code>object</code> <p>Object to store</p> required Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __init__(self, features_to_store):\n    \"\"\"Instantiate the storage.\n\n    Parameters\n    ----------\n    features_to_store : object\n        Object to store\n    \"\"\"\n    self.features_to_store = features_to_store\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the length of the sequence of apparition of the features.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>@abstractmethod\ndef __len__(self):\n    \"\"\"Return the length of the sequence of apparition of the features.\"\"\"\n    pass\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation method.</p> <p>Returns:</p> Type Description <code>str</code> <p>Description of the storage.</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>def __str__(self):\n    \"\"\"Return string representation method.\n\n    Returns\n    -------\n    str\n        Description of the storage.\n    \"\"\"\n    return \"FeatureStorage with name: .\"\n</code></pre>"},{"location":"references/data/references_storage/#choice_learn.data.storage.Storage.get_element_from_index","title":"<code>get_element_from_index(index)</code>  <code>abstractmethod</code>","text":"<p>Getter method with index (int).</p> <p>Returns the features stored with the index-th ID.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>(int, list, slice)</code> <p>index argument of the feature</p> required <p>Returns:</p> Type Description <code>array_like</code> <p>features corresponding to the index index in self.storage</p> Source code in <code>choice_learn/data/storage.py</code> <pre><code>@abstractmethod\ndef get_element_from_index(self, index):\n    \"\"\"Getter method with index (int).\n\n    Returns the features stored with the index-th ID.\n\n    Parameters\n    ----------\n    index : (int, list, slice)\n        index argument of the feature\n\n    Returns\n    -------\n    array_like\n        features corresponding to the index index in self.storage\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/datasets/references_base/","title":"Available Open Source Datasets","text":"<p>Datasets loader.</p>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.download_from_url","title":"<code>download_from_url(url)</code>","text":"<p>Download a dataset from an url if not already in the DATA_MODULE directory.</p> <p>The Python memory usage is restricted regardless of the size of the downloaded file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>name of the url to use for downloading the dataset</p> required <p>Returns:</p> Name Type Description <code>local_filename</code> <code>str</code> <p>local file name of the downloaded dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def download_from_url(url):\n    \"\"\"Download a dataset from an url if not already in the DATA_MODULE directory.\n\n    The Python memory usage is restricted regardless of the size of the downloaded file.\n\n    Parameters\n    ----------\n    url : str\n        name of the url to use for downloading the dataset\n\n    Returns\n    -------\n    local_filename : str\n        local file name of the downloaded dataset\n    \"\"\"\n    local_filename = url.split(\"/\")[-1]\n\n    full_path = get_path(local_filename, module=DATA_MODULE)\n\n    # Check that the file is not already downloaded in the DATA_MODULE directory\n    if not Path.is_file(Path(full_path)):\n        print(f\"Downloading {local_filename} from {url}\")\n        try:\n            with requests.get(url, stream=True, timeout=20) as r:\n                r.raise_for_status()\n                with open(local_filename, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=8192):\n                        f.write(chunk)\n        except requests.exceptions.Timeout:\n            print(f\"Couldn't download automatically the dataset from {url}\")\n\n        # Move the downloaded file to the DATA_MODULE directory\n        Path(local_filename).rename(full_path)\n        print(f\"Download completed. File saved as {local_filename} in {full_path}\")\n\n    return local_filename\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.get_path","title":"<code>get_path(data_file_name, module=DATA_MODULE)</code>","text":"<p>Get path toward data file.</p> <p>Specifically used to handled Python 3.8 and 3.9+ differences in importlib.resources handling.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <p>name of the csv file to load</p> required <code>module</code> <p>path to directory containing the data file, by default DATA_MODULE</p> <code>DATA_MODULE</code> <p>Returns:</p> Type Description <code>Path</code> <p>path to the data file</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def get_path(data_file_name, module=DATA_MODULE):\n    \"\"\"Get path toward data file.\n\n    Specifically used to handled Python 3.8 and 3.9+ differences in importlib.resources handling.\n\n    Parameters\n    ----------\n    data_file_name: str\n        name of the csv file to load\n    module: str, optional\n        path to directory containing the data file, by default DATA_MODULE\n\n    Returns\n    -------\n    Path\n        path to the data file\n    \"\"\"\n    import sys\n\n    if sys.version_info.minor &gt;= 9:\n        return resources.files(module.replace(\"/\", \".\")) / data_file_name\n    # with resources.path(module, data_file_name) as path:\n    #     return path\n    path = Path(os.path.join(\"../..\", module)).resolve() / data_file_name\n    return path.as_posix()\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_car_preferences","title":"<code>load_car_preferences(as_frame=False, return_desc=False)</code>","text":"<p>Load and return the Car dataset from  McFadden, Daniel and Kenneth Train (2000).</p> <p>\u201cMixed MNL models for discrete response\u201d, Journal of Applied Econometrics, 15(5), 447\u2013470.</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False.</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded Train dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_car_preferences(\n    as_frame=False,\n    return_desc=False,\n):\n    \"\"\"Load and return the Car dataset from  McFadden, Daniel and Kenneth Train (2000).\n\n    \u201cMixed MNL models for discrete response\u201d, Journal of Applied Econometrics, 15(5), 447\u2013470.\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False.\n    return_desc : bool, optional\n        Whether to return the description, by default False.\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded Train dataset\n    \"\"\"\n    desc = \"Stated Preferences for Car Choice.\"\n    desc += \"\"\"McFadden, Daniel and Kenneth Train (2000)\n    \u201cMixed MNL models for discrete response\u201d, Journal of Applied Econometrics, 15(5), 447\u2013470.\"\"\"\n\n    data_file_name = \"car.csv.gz\"\n\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    cars_df = pd.read_csv(full_path)\n\n    if return_desc:\n        return desc\n\n    if as_frame:\n        return cars_df\n\n    cars_df[\"choice\"] = cars_df.apply(lambda row: row.choice[-1], axis=1)\n    shared_features = [\"college\", \"hsg2\", \"coml5\"]\n    items_features = [\n        \"type\",\n        \"fuel\",\n        \"price\",\n        \"range\",\n        \"acc\",\n        \"speed\",\n        \"pollution\",\n        \"size\",\n        \"space\",\n        \"cost\",\n        \"station\",\n    ]\n    items_id = [f\"{i}\" for i in range(1, 7)]\n\n    return ChoiceDataset.from_single_wide_df(\n        df=cars_df,\n        items_id=items_id,\n        shared_features_columns=shared_features,\n        items_features_prefixes=items_features,\n        delimiter=\"\",\n        choices_column=\"choice\",\n        choice_format=\"items_id\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_csv","title":"<code>load_csv(data_file_name, data_module=DATA_MODULE, encoding='utf-8')</code>","text":"<p>Load csv files.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <code>str</code> <p>name of the csv file to load</p> required <code>data_module</code> <code>str</code> <p>path to directory containing the data file, by default DATA_MODULE</p> <code>DATA_MODULE</code> <code>encoding</code> <code>str</code> <p>encoding method of file, by default \"utf-8\"</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>list</code> <p>list of column names</p> <code>ndarray</code> <p>data contained in the csv file</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_csv(data_file_name, data_module=DATA_MODULE, encoding=\"utf-8\"):\n    \"\"\"Load csv files.\n\n    Parameters\n    ----------\n    data_file_name : str\n        name of the csv file to load\n    data_module : str, optional\n        path to directory containing the data file, by default DATA_MODULE\n    encoding : str, optional\n        encoding method of file, by default \"utf-8\"\n\n    Returns\n    -------\n    list\n        list of column names\n    np.ndarray\n        data contained in the csv file\n    \"\"\"\n    with open(os.path.join(data_module, data_file_name), encoding=encoding) as csv_file:\n        data_file = csv.reader(csv_file)\n        names = next(data_file)\n        data = []\n\n        for ir in data_file:\n            data.append(np.asarray(ir, dtype=np.float64))\n    return names, np.stack(data)\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_electricity","title":"<code>load_electricity(as_frame=False, to_wide=False, return_desc=False)</code>","text":"<p>Load and return the Electricity dataset from Kenneth Train.</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False.</p> <code>False</code> <code>to_wide</code> <code>bool</code> <p>Whether to return the dataset in wide format, by default False (an thus returned in long format).</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded Electricity dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_electricity(\n    as_frame=False,\n    to_wide=False,\n    return_desc=False,\n):\n    \"\"\"Load and return the Electricity dataset from Kenneth Train.\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False.\n    to_wide : bool, optional\n        Whether to return the dataset in wide format,\n        by default False (an thus returned in long format).\n    return_desc : bool, optional\n        Whether to return the description, by default False.\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded Electricity dataset\n    \"\"\"\n    _ = to_wide\n    data_file_name = \"electricity.csv.gz\"\n\n    description = \"\"\"A sample of 2308 households in the United States.\n    - choice: the choice of the individual, one of 1, 2, 3, 4,\n    - id: the individual index,\n    - pfi: fixed price at a stated cents per kWh, with the price varying over suppliers and\n        experiments, for scenario i=(1, 2, 3, 4),\n    - cli: the length of contract that the supplier offered, in years (such as 1 year or 5 years.)\n        During this contract period, the supplier guaranteed the prices and the buyer would have to\n        pay a penalty if he/she switched to another supplier. The supplier could offer no\n        contractin which case either side could stop the agreement at any time. This is recorded\n        as a contract length of 0,\n    - loci: is the supplier a local company,\n    - wki: is the supplier a well-known company,\n    - todi: a time-of-day rate under which the price is 11 cents per kWh from 8am to 8pm and 5 cents\n        per kWh from 8pm to 8am. These TOD prices did not vary over suppliers or experiments:\n        whenever the supplier was said to offer TOD, the prices were stated as above.\n    - seasi: a seasonal rate under which the price is 10 cents per kWh in the summer, 8 cents per\n        kWh in the winter, and 6 cents per kWh in the spring and fall. Like TOD rates, these prices\n        did not vary. Note that the price is for the electricity only, not transmission and\n        distribution, which is supplied by the local regulated utility.\n\n    Train, K.E. (2003) Discrete Choice Methods with Simulation. Cambridge University Press.\n    \"\"\"\n\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    elec_df = pd.read_csv(full_path)\n    elec_df.choice = elec_df.choice.astype(int)\n    elec_df[[\"pf\", \"cl\", \"loc\", \"wk\", \"tod\", \"seas\"]] = elec_df[\n        [\"pf\", \"cl\", \"loc\", \"wk\", \"tod\", \"seas\"]\n    ].astype(float)\n\n    if as_frame:\n        return elec_df\n    if return_desc:\n        return description\n\n    return ChoiceDataset.from_single_long_df(\n        df=elec_df,\n        items_features_columns=[\"pf\", \"cl\", \"loc\", \"wk\", \"tod\", \"seas\"],\n        items_id_column=\"alt\",\n        choices_id_column=\"chid\",\n        choice_format=\"one_zero\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_gzip","title":"<code>load_gzip(data_file_name, data_module=DATA_MODULE, encoding='utf-8')</code>","text":"<p>Load zipped .csv.gz files.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <code>str</code> <p>name of the csv.gz file to load</p> required <code>data_module</code> <code>str</code> <p>path to directory containing the data file, by default DATA_MODULE</p> <code>DATA_MODULE</code> <code>encoding</code> <code>str</code> <p>encoding method of file, by default \"utf-8\"</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>list</code> <p>list of column names</p> <code>ndarray</code> <p>data contained in the csv file</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_gzip(data_file_name, data_module=DATA_MODULE, encoding=\"utf-8\"):\n    \"\"\"Load zipped .csv.gz files.\n\n    Parameters\n    ----------\n    data_file_name : str\n        name of the csv.gz file to load\n    data_module : str, optional\n        path to directory containing the data file, by default DATA_MODULE\n    encoding : str, optional\n        encoding method of file, by default \"utf-8\"\n\n    Returns\n    -------\n    list\n        list of column names\n    np.ndarray\n        data contained in the csv file\n    \"\"\"\n    with open(os.path.join(data_module, data_file_name), \"rb\") as compressed_file:\n        compressed_file = gzip.open(compressed_file, mode=\"rt\", encoding=encoding)\n        names = next(compressed_file)\n        names = names.replace(\"\\n\", \"\")\n        data = np.loadtxt(compressed_file, delimiter=\",\", dtype=object)\n\n    return names.split(\",\"), data\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_hc","title":"<code>load_hc(as_frame=False, return_desc=False)</code>","text":"<p>Load and return the HC dataset from Kenneth Train.</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False.</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded Train dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_hc(\n    as_frame=False,\n    return_desc=False,\n):\n    \"\"\"Load and return the HC dataset from Kenneth Train.\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False.\n    return_desc : bool, optional\n        Whether to return the description, by default False.\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded Train dataset\n    \"\"\"\n    desc = \"\"\"HC contains data on the choice of heating and central cooling system for 250\n    single-family, newly built houses in California.\n\n    The alternatives are:\n\n    Gas central heat with cooling gcc,\n    Electric central resistence heat with cooling ecc,\n    Electric room resistence heat with cooling erc,\n    Electric heat pump, which provides cooling also hpc,\n    Gas central heat without cooling gc,\n    Electric central resistence heat without cooling ec,\n    Electric room resistence heat without cooling er.\n    Heat pumps necessarily provide both heating and cooling such that heat pump without cooling is\n    not an alternative.\n\n    The variables are:\n\n    depvar gives the name of the chosen alternative,\n    ich.alt are the installation cost for the heating portion of the system,\n    icca is the installation cost for cooling\n    och.alt are the operating cost for the heating portion of the system\n    occa is the operating cost for cooling\n    income is the annual income of the household\n    Note that the full installation cost of alternative gcc is ich.gcc+icca, and similarly for the\n    operating cost and for the other alternatives with cooling.\n    \"\"\"\n\n    data_file_name = \"HC.csv.gz\"\n\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    hc_df = pd.read_csv(full_path)\n\n    if return_desc:\n        return desc\n\n    if as_frame:\n        return hc_df\n\n    items_id = [\"gcc\", \"ecc\", \"erc\", \"hpc\", \"gc\", \"ec\", \"er\"]\n    return ChoiceDataset.from_single_wide_df(\n        df=hc_df,\n        shared_features_columns=[\"income\"],\n        items_features_prefixes=[\"ich\", \"och\", \"occa\", \"icca\"],\n        delimiter=\".\",\n        items_id=items_id,\n        choices_column=\"depvar\",\n        choice_format=\"items_id\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_heating","title":"<code>load_heating(as_frame=False, to_wide=False, return_desc=False)</code>","text":"<p>Load and return the Heating dataset from Kenneth Train.</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False.</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False.</p> <code>False</code> <code>to_wide</code> <code>bool</code> <p>Whether to return the dataset in wide format, by default False (an thus returned in long format).</p> <code>False</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded Heating dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_heating(\n    as_frame=False,\n    to_wide=False,\n    return_desc=False,\n):\n    \"\"\"Load and return the Heating dataset from Kenneth Train.\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False.\n    return_desc : bool, optional\n        Whether to return the description, by default False.\n    to_wide : bool, optional\n        Whether to return the dataset in wide format,\n        by default False (an thus returned in long format).\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded Heating dataset\n    \"\"\"\n    desc = \"\"\"Kenneth Train's dataset containing data on choice of heating system in California\n    houses.\n    Description can be found at: https://rdrr.io/cran/mlogitBMA/man/heating.html\n\n    Train, K.E. (2003) Discrete Choice Methods with Simulation. Cambridge University Press.\"\"\"\n    _ = to_wide\n    data_file_name = \"heating_data.csv.gz\"\n\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    heating_df = pd.read_csv(full_path)\n\n    if return_desc:\n        return desc\n\n    if as_frame:\n        return heating_df\n\n    shared_features_by_choice = [\"income\", \"agehed\", \"rooms\", \"region\"]\n    choice = [\"depvar\"]\n    items_features_by_choice = [\"ic.\", \"oc.\"]\n    items = [\"gc\", \"gr\", \"ec\", \"er\", \"hp\"]\n\n    choices = np.array([items.index(val) for val in heating_df[choice].to_numpy().ravel()])\n    contexts = heating_df[shared_features_by_choice].to_numpy()\n    contexts_items = np.stack(\n        [\n            heating_df[[feat + item for feat in items_features_by_choice]].to_numpy()\n            for item in items\n        ],\n        axis=1,\n    )\n    return ChoiceDataset(\n        shared_features_by_choice=contexts,\n        items_features_by_choice=contexts_items,\n        choices=choices,\n        shared_features_by_choice_names=shared_features_by_choice,\n        items_features_by_choice_names=items_features_by_choice,\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_londonpassenger","title":"<code>load_londonpassenger(add_items_one_hot=False, as_frame=False, return_desc=False, preprocessing=None)</code>","text":"<p>Load and return the Londer Passenger Mode Choice dataset from Hillel et al. (2018).</p> <p>Parameters:</p> Name Type Description Default <code>add_items_one_hot</code> <code>bool</code> <p>Whether to add a OneHot encoding of items as items_features, by default False</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False</p> <code>False</code> <code>preprocessing</code> <code>str</code> <p>Preprocessing to apply to the dataset, by default None Can be other than None: \"summation\"</p> <code>None</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded London Passenger Mode Choice dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_londonpassenger(\n    add_items_one_hot=False, as_frame=False, return_desc=False, preprocessing=None\n):\n    \"\"\"Load and return the Londer Passenger Mode Choice dataset from Hillel et al. (2018).\n\n    Parameters\n    ----------\n    add_items_one_hot : bool, optional\n        Whether to add a OneHot encoding of items as items_features, by default False\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False\n    return_desc : bool, optional\n        Whether to return the description, by default False\n    preprocessing : str, optional\n        Preprocessing to apply to the dataset, by default None\n        Can be other than None: \"summation\"\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded London Passenger Mode Choice dataset\n    \"\"\"\n    description = \"\"\"This case study investigates mode choice on an urban multi-modal transport\n    network. The objective was to be able to predict how people will react to changes to the\n    transport network and conditions, to allow for more efficient transport network management\n    and investment planning.This dataset is used to predict mode choice out of\n    walking, cycling, public transport, and driving.\n\n    Hillel, T., Elshafie, M. Z. E. B. and Jin, Y. (2018), \u2018Recreating passenger mode choice-sets for\n    transport simulation: A case study of London, UK\u2019, 171(1), 29\u201342.\"\"\"\n\n    # Download the dataset if it does not exist in DATA_MODULE directory\n    url = \"http://transp-or.epfl.ch/data/lpmc.dat\"\n    data_file_name = download_from_url(url)\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    london_df = pd.read_csv(full_path, delimiter=\"\\t\")\n\n    items = [\"walking\", \"cycling\", \"pt\", \"driving\"]\n    shared_features_by_choice_names = [\n        \"household_id\",\n        \"person_n\",\n        \"trip_n\",\n        \"purpose_home_to_work\",\n        \"purpose_home_to_school\",\n        \"purpose_home_to_other\",\n        \"purpose_work_travel\",\n        \"purpose_other\",\n        \"survey_year\",\n        \"travel_year\",\n        \"travel_month\",\n        \"travel_date\",\n        \"week_end\",\n        \"start_time\",\n        \"age\",\n        \"female\",\n        \"driving_license\",\n        \"car_ownership\",\n        \"distance\",\n    ]\n    items_features_by_choice_names = [\n        \"fueltype\",\n        \"faretype\",\n        \"bus_scale\",\n        \"dur_pt\",\n        \"interchanges\",\n        \"cost_pt\",\n        \"cost_driving\",\n        \"driving_traffic_percent\",\n    ]\n    choice_column = \"travel_mode\"\n\n    if return_desc:\n        return description\n\n    # Change the day of the week to a binary variable indicating whether it is a weekend or not:\n    london_df[\"week_end\"] = np.where(london_df[\"day_of_week\"] &gt;= 6, 1, 0)\n\n    # Transform the purpose column into OneHot encoding:\n    london_df = pd.get_dummies(london_df, columns=[\"purpose\"], dtype=int)\n\n    # Change the name of the purpose columns:\n    london_df = london_df.rename(columns={\"purpose_1\": \"purpose_home_to_work\"})\n    london_df = london_df.rename(columns={\"purpose_2\": \"purpose_home_to_school\"})\n    london_df = london_df.rename(columns={\"purpose_3\": \"purpose_home_to_other\"})\n    london_df = london_df.rename(columns={\"purpose_4\": \"purpose_work_travel\"})\n    london_df = london_df.rename(columns={\"purpose_5\": \"purpose_other\"})\n\n    if preprocessing == \"summation\":\n        # Compute the total public transport duration:\n        london_df[\"dur_pt\"] = (\n            london_df[\"dur_pt_access\"]\n            + london_df[\"dur_pt_rail\"]\n            + london_df[\"dur_pt_bus\"]\n            + london_df[\"dur_pt_int\"]\n        )\n\n        # Compute the total driving cost:\n        london_df[\"cost_driving\"] = (\n            london_df[\"cost_driving_fuel\"] + london_df[\"cost_driving_ccharge\"]\n        )\n\n        # Change the name of the public transport cost column:\n        london_df = london_df.rename(columns={\"cost_transit\": \"cost_pt\"})\n\n        # Drop the columns that are not needed anymore:\n        london_df = london_df.drop(\n            [\n                \"dur_pt_access\",\n                \"dur_pt_rail\",\n                \"dur_pt_bus\",\n                \"dur_pt_int\",\n                \"cost_driving_fuel\",\n                \"cost_driving_ccharge\",\n                \"day_of_week\",\n            ],\n            axis=1,\n        )\n\n    if add_items_one_hot:\n        items_features_by_choice_names += [f\"oh_{item}\" for item in items]\n        for item in items:\n            for item2 in items:\n                if item == item2:\n                    london_df[f\"{item}_oh_{item}\"] = 1\n                else:\n                    london_df[f\"{item2}_oh_{item}\"] = 0\n\n    if as_frame:\n        return london_df\n\n    # Shift the index of the travel mode to start at 0\n    london_df[\"travel_mode\"] = london_df[\"travel_mode\"] - 1\n\n    return ChoiceDataset.from_single_wide_df(\n        df=london_df,\n        items_id=items,\n        shared_features_columns=shared_features_by_choice_names,\n        items_features_suffixes=items_features_by_choice_names,\n        delimiter=\"_\",\n        choices_column=choice_column,\n        choice_format=\"items_index\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_modecanada","title":"<code>load_modecanada(add_items_one_hot=False, add_is_public=False, as_frame=False, return_desc=False, choice_format='one_zero', split_features=False, to_wide=False, preprocessing=None)</code>","text":"<p>Load and return the ModeCanada dataset from Koppleman et al. (1993).</p> <p>Parameters:</p> Name Type Description Default <code>one_hot_cat_data</code> <code>bool</code> <p>Whether to transform categorical data as OneHot, by default False.</p> required <code>add_is_public</code> <code>bool</code> <p>Whether to add the is_public feature, by default False.</p> <code>False</code> <code>add_items_one_hot</code> <code>bool</code> <p>Whether to add a OneHot encoding of items as items_features, by default False</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False.</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False.</p> <code>False</code> <code>choice_format</code> <code>(str, optional, among[one_zero, items_id])</code> <p>format indicating how the choice is encoded, by default \"one_zero\".</p> <code>'one_zero'</code> <code>split_features</code> <code>bool</code> <p>Whether to split features by type in different dataframes, by default False.</p> <code>False</code> <code>to_wide</code> <code>bool</code> <p>Whether to return the dataset in wide format, by default False (an thus returned in long format).</p> <code>False</code> <code>preprocessing</code> <code>str</code> <p>Preprocessing to apply to the dataset, by default None Can be other than None: \"tutorial\"</p> <code>None</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded ModeCanada dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_modecanada(\n    add_items_one_hot=False,\n    add_is_public=False,\n    as_frame=False,\n    return_desc=False,\n    choice_format=\"one_zero\",\n    split_features=False,\n    to_wide=False,\n    preprocessing=None,\n):\n    \"\"\"Load and return the ModeCanada dataset from Koppleman et al. (1993).\n\n    Parameters\n    ----------\n    one_hot_cat_data : bool, optional\n        Whether to transform categorical data as OneHot, by default False.\n    add_is_public : bool, optional\n        Whether to add the is_public feature, by default False.\n    add_items_one_hot : bool, optional\n        Whether to add a OneHot encoding of items as items_features, by default False\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False.\n    return_desc : bool, optional\n        Whether to return the description, by default False.\n    choice_format : str, optional, among [\"one_zero\", \"items_id\"]\n        format indicating how the choice is encoded, by default \"one_zero\".\n    split_features : bool, optional\n        Whether to split features by type in different dataframes, by default False.\n    to_wide : bool, optional\n        Whether to return the dataset in wide format,\n        by default False (an thus returned in long format).\n    preprocessing : str, optional\n        Preprocessing to apply to the dataset, by default None\n        Can be other than None: \"tutorial\"\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded ModeCanada dataset\n    \"\"\"\n    desc = \"\"\"The dataset was assembled in 1989 by VIA Rail (the Canadian national rail carrier) to\n     estimate the demand for high-speed rail in the Toronto-Montreal corridor. The main information\n     source was a Passenger Review administered to business travelers augmented by information about\n     each trip. The observations consist of a choice between four modes of transportation (train,\n     air, bus, car) with information about the travel mode and about the passenger. The posted\n     dataset has been balanced to only include cases where all four travel modes are recorded.\n\n     Christophier V. Forinash and Frank S. Koppelman (1993) \u201cApplication and interpretation of\n     nested logit models of intercity mode choice,\u201d Transportation Research Record 1413, 98-106. \"\"\"\n\n    _ = to_wide\n    data_file_name = \"ModeCanada.csv.gz\"\n    # names, data = load_gzip(data_file_name)\n    # names = [name.replace('\"', \"\") for name in names]\n    # canada_df = pd.DataFrame(data[:, 1:], index=data[:, 0].astype(int), columns=names[1:])\n\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    canada_df = pd.read_csv(full_path)\n    canada_df = canada_df.drop(columns=[\"Unnamed: 0\"])\n    canada_df[\"alt\"] = canada_df.apply(lambda row: row.alt.replace('\"', \"\"), axis=1)\n    # Just some typing\n    canada_df.income = canada_df.income.astype(\"float32\")\n\n    items = [\"air\", \"bus\", \"car\", \"train\"]\n    shared_features = [\"income\", \"dist\", \"urban\"]\n    items_features = [\"cost\", \"freq\", \"ovt\", \"ivt\"]\n    choice_column = \"choice\"\n\n    if add_items_one_hot:\n        canada_df[\"oh_air\"] = canada_df.apply(\n            lambda row: 1.0 if row.alt == items[0] else 0.0, axis=1\n        )\n        canada_df[\"oh_bus\"] = canada_df.apply(\n            lambda row: 1.0 if row.alt == items[1] else 0.0, axis=1\n        )\n        canada_df[\"oh_car\"] = canada_df.apply(\n            lambda row: 1.0 if row.alt == items[2] else 0.0, axis=1\n        )\n        canada_df[\"oh_train\"] = canada_df.apply(\n            lambda row: 1.0 if row.alt == items[3] else 0.0, axis=1\n        )\n        items_features = [\"oh_air\", \"oh_bus\", \"oh_car\", \"oh_train\"] + items_features\n\n    if add_is_public:\n        canada_df[\"is_public\"] = canada_df.apply(\n            lambda row: 0.0 if row.alt == \"car\" else 1.0, axis=1\n        )\n        items_features.append(\"is_public\")\n\n    if return_desc:\n        return desc\n\n    for col in [\"case\", \"choice\", \"dist\", \"cost\", \"ivt\", \"ovt\", \"freq\", \"income\", \"urban\", \"noalt\"]:\n        canada_df[col] = pd.to_numeric(canada_df[col])\n\n    if choice_format == \"items_id\":\n        # We need to transform how the choice is encoded to add the chosen item id\n        named_choice = [0] * len(canada_df)\n        for n_row, row in canada_df.iterrows():\n            if row.choice == 0:\n                sub_df = canada_df[canada_df.case == row.case]\n                choice = sub_df.loc[sub_df.choice == 1].alt.to_numpy()[0]\n                named_choice[n_row - 1] = choice\n\n        for n_row, row in canada_df.iterrows():\n            if row.choice == 1:\n                named_choice[n_row - 1] = row.alt\n\n        canada_df[\"named_choice\"] = named_choice\n\n    if as_frame:\n        if split_features:\n            shared_features_by_choice = canada_df[[\"case\"] + shared_features].drop_duplicates()\n            shared_features_by_choice = shared_features_by_choice.rename(\n                columns={\"case\": \"choice_id\"}\n            )\n\n            items_features_by_choice = canada_df[[\"case\", \"alt\"] + items_features]\n            items_features_by_choice = items_features_by_choice.rename(\n                columns={\"case\": \"choice_id\", \"alt\": \"item_id\"}\n            )\n\n            choices = canada_df.loc[canada_df.choice == 1][[\"case\", \"alt\"]]\n            choices = choices.rename(columns={\"case\": \"choice_id\", \"alt\": \"choice\"})\n\n            return (\n                shared_features_by_choice,\n                items_features_by_choice,\n                choices,\n            )\n        if choice_format == \"items_id\":\n            canada_df[\"choice\"] = canada_df[\"named_choice\"]\n            canada_df = canada_df.drop(\"named_choice\", axis=1)\n        return canada_df\n\n    if split_features:\n        # Order of item_id is alphabetical: air, bus, car, train\n        shared_features_by_choice = (\n            canada_df[[\"case\"] + shared_features].drop_duplicates()[shared_features].to_numpy()\n        )\n\n        cif = []\n        ci_av = []\n        for context in canada_df.case.unique():\n            context_df = canada_df.loc[canada_df.case == context]\n            # Order of item_id is alphabetical: air, bus, car, train\n            cf = []\n            cav = []\n            for item in [\"air\", \"bus\", \"car\", \"train\"]:\n                if item in context_df.alt.unique():\n                    cf.append(context_df.loc[context_df.alt == item][items_features].to_numpy()[0])\n                    cav.append(1)\n                else:\n                    cf.append([0.0 for _ in range(len(items_features))])\n                    cav.append(0)\n            cif.append(cf)\n            ci_av.append(cav)\n        items_features_by_choice = np.array(cif)\n        available_items_by_choice = np.array(ci_av)\n\n        choices = np.squeeze(canada_df.loc[canada_df.choice == 1][\"alt\"].to_numpy())\n        choices = np.array([[\"air\", \"bus\", \"car\", \"train\"].index(c) for c in choices])\n\n        return (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n\n    if len(items_features) == 0:\n        items_features = None\n\n    if preprocessing == \"tutorial\":\n        # Following torch-choice guide:\n        canada_df = canada_df.loc[canada_df.noalt == 4]\n        if add_items_one_hot:\n            preprocessing_items_features = [\"oh_air\", \"oh_car\", \"oh_bus\", \"oh_train\"] + [\n                \"cost\",\n                \"freq\",\n                \"ovt\",\n                \"ivt\",\n            ]\n        else:\n            preprocessing_items_features = [\"cost\", \"freq\", \"ovt\", \"ivt\"]\n\n        items = [\"air\", \"bus\", \"car\", \"train\"]\n        canada_df = canada_df.astype({\"income\": \"float32\"})\n        return ChoiceDataset.from_single_long_df(\n            df=canada_df,\n            shared_features_columns=[\"income\"],\n            items_features_columns=preprocessing_items_features,\n            items_id_column=\"alt\",\n            choices_id_column=\"case\",\n            choices_column=\"choice\",\n            choice_format=\"one_zero\",\n        )\n\n    return ChoiceDataset.from_single_long_df(\n        df=canada_df,\n        shared_features_columns=shared_features,\n        items_features_columns=items_features,\n        items_id_column=\"alt\",\n        choices_id_column=\"case\",\n        choices_column=choice_column,\n        choice_format=\"one_zero\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_swissmetro","title":"<code>load_swissmetro(add_items_one_hot=False, as_frame=False, return_desc=False, preprocessing=None)</code>","text":"<p>Load and return the SwissMetro dataset from Bierlaire et al. (2001).</p> <p>Parameters:</p> Name Type Description Default <code>add_items_one_hot</code> <code>bool</code> <p>Whether to add a OneHot encoding of items as items_features, by default False</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False</p> <code>False</code> <code>preprocessing</code> <code>str</code> <p>Preprocessing to apply to the dataset, by default None Can be other than None: \"long_format\", \"tastenet\", \"tutorial\", \"biogeme_nested\", \"rumnet\"</p> <code>None</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded SwissMetro dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_swissmetro(add_items_one_hot=False, as_frame=False, return_desc=False, preprocessing=None):\n    \"\"\"Load and return the SwissMetro dataset from Bierlaire et al. (2001).\n\n    Parameters\n    ----------\n    add_items_one_hot : bool, optional\n        Whether to add a OneHot encoding of items as items_features, by default False\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False\n    return_desc : bool, optional\n        Whether to return the description, by default False\n    preprocessing : str, optional\n        Preprocessing to apply to the dataset, by default None\n        Can be other than None: \"long_format\", \"tastenet\", \"tutorial\", \"biogeme_nested\", \"rumnet\"\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded SwissMetro dataset\n    \"\"\"\n    description = \"\"\"This dataset consists of survey data collected on the trains between St.Gallen\n     and Geneva, Switzerland, during March 1998. The respondents provided information in order to\n     analyze the impact of the modal innovation intransportation, represented by the Swissmetro,\n     a revolutionary mag-lev under ground system, against the usual transport modes represented by\n     car and train.\n\n    Bierlaire, M., Axhausen, K. and Abay, G. (2001), The acceptance of modal innovation:\n    The case of Swissmetro, in \u2018Proceedings of the Swiss Transport Research Conference\u2019,\n    Ascona, Switzerland.\"\"\"\n\n    data_file_name = \"swissmetro.csv.gz\"\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    swiss_df = pd.read_csv(full_path)\n    swiss_df[\"CAR_HE\"] = 0.0\n\n    items = [\"TRAIN\", \"SM\", \"CAR\"]\n    shared_features_by_choice_names = [\n        \"GROUP\",\n        \"PURPOSE\",\n        \"FIRST\",\n        \"TICKET\",\n        \"WHO\",\n        \"LUGGAGE\",\n        \"AGE\",\n        \"MALE\",\n        \"INCOME\",\n        \"GA\",\n        \"ORIGIN\",\n        \"DEST\",\n    ]\n    items_features_by_choice_names = [\"CO\", \"TT\", \"HE\", \"SEATS\"]\n    choice_column = \"CHOICE\"\n    availabilities_column = \"AV\"\n\n    if add_items_one_hot:\n        items_features_by_choice_names += [f\"oh_{item}\" for item in items]\n        for item in items:\n            for item2 in items:\n                if item == item2:\n                    swiss_df[f\"{item}_oh_{item}\"] = 1\n                else:\n                    swiss_df[f\"{item2}_oh_{item}\"] = 0\n\n    if return_desc:\n        return description\n\n    swiss_df = swiss_df.loc[swiss_df.CHOICE != 0]\n    swiss_df.CHOICE = swiss_df.CHOICE - 1\n\n    if preprocessing == \"long_format\":\n        long = []\n        for n_row, row in swiss_df.iterrows():\n            df_dict = {\n                \"PURPOSE\": [],\n                \"AGE\": [],\n                \"item_id\": [],\n                \"TT\": [],\n                \"CO\": [],\n                \"CHOICE\": [],\n                \"choice_id\": [],\n            }\n\n            for item_index, item_id in enumerate([\"TRAIN\", \"SM\", \"CAR\"]):\n                if row[f\"{item_id}_AV\"] &gt; 0:\n                    if item_index == row.CHOICE:\n                        df_dict[\"CHOICE\"].append(1)\n                    else:\n                        df_dict[\"CHOICE\"].append(0)\n\n                    df_dict[\"item_id\"].append(item_id)\n                    df_dict[\"TT\"].append(row[f\"{item_id}_TT\"])\n                    df_dict[\"CO\"].append(row[f\"{item_id}_CO\"])\n\n                    df_dict[\"PURPOSE\"].append(row[\"PURPOSE\"])\n                    df_dict[\"AGE\"].append(row[\"AGE\"])\n                    df_dict[\"choice_id\"].append(n_row)\n            long.append(pd.DataFrame(df_dict))\n        swiss_df = pd.concat(long, axis=0)\n        swiss_df = swiss_df.reset_index(drop=True)\n        as_frame = True\n    if as_frame:\n        return swiss_df\n\n    if preprocessing == \"tastenet\":\n        swiss_df = swiss_df.loc[swiss_df.AGE != 6]\n        swiss_df[\"TRAIN_ASC_TRAIN\"] = 1.0\n        swiss_df[\"SM_ASC_TRAIN\"] = 0.0\n        swiss_df[\"CAR_ASC_TRAIN\"] = 0.0\n\n        swiss_df[\"TRAIN_ASC_SM\"] = 0.0\n        swiss_df[\"SM_ASC_SM\"] = 1.0\n        swiss_df[\"CAR_ASC_SM\"] = 0.0\n\n        swiss_df[\"TRAIN_ASC_CAR\"] = 0.0\n        swiss_df[\"SM_ASC_CAR\"] = 0.0\n        swiss_df[\"CAR_ASC_CAR\"] = 1.0\n\n        swiss_df[\"FEMALE\"] = 1 - swiss_df[\"MALE\"]\n        shared_features_by_choice_names = [\"MALE\", \"FEMALE\"]\n        swiss_df[\"NOT_FIRST\"] = 1 - swiss_df[\"FIRST\"]\n        shared_features_by_choice_names += [\"FIRST\", \"NOT_FIRST\"]\n        swiss_df[\"NOT_GA\"] = 1 - swiss_df[\"GA\"]\n        shared_features_by_choice_names += [\"GA\", \"NOT_GA\"]\n\n        age_dummy = pd.get_dummies(swiss_df.AGE, prefix=\"AGE\").astype(int)\n        swiss_df = pd.concat([swiss_df, age_dummy], axis=1)\n        shared_features_by_choice_names += age_dummy.columns.to_list()\n\n        swiss_df.INCOME = swiss_df.apply(lambda row: 1 if row.INCOME == 0 else row.INCOME, axis=1)\n        income_dummy = pd.get_dummies(swiss_df.INCOME, prefix=\"INCOME\").astype(int)\n        swiss_df = pd.concat([swiss_df, income_dummy], axis=1)\n        shared_features_by_choice_names += income_dummy.columns.to_list()\n\n        swiss_df.WHO = swiss_df.apply(lambda row: 1 if row.WHO == 0 else row.WHO, axis=1)\n        who_dummy = pd.get_dummies(swiss_df.WHO, prefix=\"WHO\").astype(int)\n        swiss_df = pd.concat([swiss_df, who_dummy], axis=1)\n        shared_features_by_choice_names += who_dummy.columns.to_list()\n\n        swiss_df = swiss_df.loc[swiss_df.PURPOSE != 9]\n        purpose_dict = {\n            1: 1,\n            2: 2,\n            3: 3,\n            4: 4,\n            5: 1,\n            6: 2,\n            7: 3,\n            8: 4,\n        }\n        swiss_df.PURPOSE = swiss_df.apply(lambda row: purpose_dict[row.PURPOSE], axis=1)\n        purpose_dummy = pd.get_dummies(swiss_df.PURPOSE, prefix=\"PURPOSE\").astype(int)\n        swiss_df = pd.concat([swiss_df, purpose_dummy], axis=1)\n        shared_features_by_choice_names += purpose_dummy.columns.to_list()\n\n        luggage_dummy = pd.get_dummies(swiss_df.LUGGAGE, prefix=\"LUGGAGE\").astype(int)\n        swiss_df = pd.concat([swiss_df, luggage_dummy], axis=1)\n        shared_features_by_choice_names += luggage_dummy.columns.to_list()\n\n        swiss_df[\"SM_CO\"] = swiss_df[\"SM_CO\"] * (swiss_df[\"GA\"] == 0)\n        swiss_df[\"TRAIN_CO\"] = swiss_df[\"TRAIN_CO\"] * (swiss_df[\"GA\"] == 0)\n\n        for col in [\n            \"TRAIN_TT\",\n            \"TRAIN_HE\",\n            \"TRAIN_CO\",\n            \"SM_TT\",\n            \"SM_HE\",\n            \"SM_CO\",\n            \"CAR_TT\",\n            \"CAR_CO\",\n        ]:\n            swiss_df[col] = swiss_df[col] / 100\n\n        return swiss_df.ID.to_numpy(), ChoiceDataset.from_single_wide_df(\n            df=swiss_df,\n            items_id=items,\n            shared_features_columns=shared_features_by_choice_names,\n            items_features_suffixes=items_features_by_choice_names\n            + [\"ASC_TRAIN\", \"ASC_SM\", \"ASC_CAR\"],\n            available_items_suffix=availabilities_column,\n            choices_column=choice_column,\n            choice_format=\"items_index\",\n        )\n\n    if preprocessing == \"tutorial\":\n        # swiss_df = pd.DataFrame(data, columns=names)\n        # Removing unknown choices\n        # Keep only commute an dbusiness trips\n        swiss_df = swiss_df.loc[swiss_df.PURPOSE.isin([1, 3])]\n\n        # Normalizing values\n        swiss_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] = swiss_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] / 60.0\n        swiss_df[[\"TRAIN_HE\", \"SM_HE\"]] = swiss_df[[\"TRAIN_HE\", \"SM_HE\"]] / 60.0\n\n        swiss_df[\"train_free_ticket\"] = swiss_df.apply(\n            lambda row: ((row[\"GA\"] == 1 or row[\"WHO\"] == 2) &gt; 0).astype(int), axis=1\n        )\n        swiss_df[\"sm_free_ticket\"] = swiss_df.apply(\n            lambda row: ((row[\"GA\"] == 1 or row[\"WHO\"] == 2) &gt; 0).astype(int), axis=1\n        )\n        swiss_df[\"car_free_ticket\"] = 0\n\n        swiss_df[\"train_travel_cost\"] = swiss_df.apply(\n            lambda row: (row[\"TRAIN_CO\"] * (1 - row[\"train_free_ticket\"])) / 100, axis=1\n        )\n        swiss_df[\"sm_travel_cost\"] = swiss_df.apply(\n            lambda row: (row[\"SM_CO\"] * (1 - row[\"sm_free_ticket\"])) / 100, axis=1\n        )\n        swiss_df[\"car_travel_cost\"] = swiss_df.apply(lambda row: row[\"CAR_CO\"] / 100, axis=1)\n\n        swiss_df[\"single_luggage_piece\"] = swiss_df.apply(\n            lambda row: (row[\"LUGGAGE\"] == 1).astype(int), axis=1\n        )\n        swiss_df[\"multiple_luggage_piece\"] = swiss_df.apply(\n            lambda row: (row[\"LUGGAGE\"] == 3).astype(int), axis=1\n        )\n        swiss_df[\"regular_class\"] = swiss_df.apply(lambda row: 1 - row[\"FIRST\"], axis=1)\n        swiss_df[\"train_survey\"] = swiss_df.apply(lambda row: 1 - row[\"SURVEY\"], axis=1)\n\n        shared_features_by_choice = swiss_df[\n            [\"train_survey\", \"regular_class\", \"single_luggage_piece\", \"multiple_luggage_piece\"]\n        ].to_numpy()\n        train_features = swiss_df[[\"train_travel_cost\", \"TRAIN_TT\", \"TRAIN_HE\"]].to_numpy()\n        sm_features = swiss_df[[\"sm_travel_cost\", \"SM_TT\", \"SM_HE\", \"SM_SEATS\"]].to_numpy()\n        car_features = swiss_df[[\"car_travel_cost\", \"CAR_TT\"]].to_numpy()\n\n        # We need to have the same number of features for each item, we create dummy ones:\n        car_features = np.concatenate([car_features, np.zeros((len(car_features), 2))], axis=1)\n        train_features = np.concatenate(\n            [train_features, np.zeros((len(train_features), 1))], axis=1\n        )\n        items_features_by_choice = np.stack([train_features, sm_features, car_features], axis=1)\n\n        available_items_by_choice = swiss_df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]].to_numpy()\n        # Re-Indexing choices from 1 to 3 to 0 to 2\n        choices = swiss_df.CHOICE.to_numpy()\n\n        return ChoiceDataset(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            shared_features_by_choice_names=[\n                \"train_survey\",\n                \"regular_class\",\n                \"single_luggage_piece\",\n                \"multiple_luggage_piece\",\n            ],\n            items_features_by_choice_names=[\"cost\", \"travel_time\", \"headway\", \"seats\"],\n            choices=choices,\n        )\n    if preprocessing == \"biogeme_nested\":\n        # Keep only commute an dbusiness trips\n        swiss_df = swiss_df.loc[swiss_df.PURPOSE.isin([1, 3])]\n\n        # Normalizing values by 100\n        swiss_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] = (\n            swiss_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] / 100.0\n        )\n\n        swiss_df[\"train_free_ticket\"] = swiss_df.apply(\n            lambda row: (row[\"GA\"] == 1).astype(int), axis=1\n        )\n        swiss_df[\"sm_free_ticket\"] = swiss_df.apply(\n            lambda row: (row[\"GA\"] == 1).astype(int), axis=1\n        )\n\n        swiss_df[\"train_travel_cost\"] = swiss_df.apply(\n            lambda row: (row[\"TRAIN_CO\"] * (1 - row[\"train_free_ticket\"])) / 100, axis=1\n        )\n        swiss_df[\"sm_travel_cost\"] = swiss_df.apply(\n            lambda row: (row[\"SM_CO\"] * (1 - row[\"sm_free_ticket\"])) / 100, axis=1\n        )\n        swiss_df[\"car_travel_cost\"] = swiss_df.apply(lambda row: row[\"CAR_CO\"] / 100, axis=1)\n\n        train_features = swiss_df[[\"train_travel_cost\", \"TRAIN_TT\"]].to_numpy()\n        sm_features = swiss_df[[\"sm_travel_cost\", \"SM_TT\"]].to_numpy()\n        car_features = swiss_df[[\"car_travel_cost\", \"CAR_TT\"]].to_numpy()\n\n        items_features_by_choice = np.stack([train_features, sm_features, car_features], axis=1)\n\n        available_items_by_choice = swiss_df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]].to_numpy()\n        # Re-Indexing choices from 1 to 3 to 0 to 2\n        choices = swiss_df.CHOICE.to_numpy()\n\n        return ChoiceDataset(\n            shared_features_by_choice=None,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            shared_features_by_choice_names=None,\n            items_features_by_choice_names=[\"cost\", \"travel_time\"],\n            choices=choices,\n        )\n    if preprocessing == \"rumnet\":\n        swiss_df[\"One\"] = 1.0\n        swiss_df[\"Zero\"] = 0.0\n\n        available_items_by_choice = swiss_df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]].to_numpy()\n        items_features_by_choice = np.stack(\n            [\n                swiss_df[[\"One\", \"Zero\", \"Zero\", \"TRAIN_TT\", \"TRAIN_CO\", \"TRAIN_HE\"]].to_numpy(),\n                swiss_df[[\"Zero\", \"One\", \"Zero\", \"SM_TT\", \"SM_CO\", \"SM_HE\"]].to_numpy(),\n                swiss_df[[\"Zero\", \"Zero\", \"One\", \"CAR_TT\", \"CAR_CO\", \"CAR_HE\"]].to_numpy(),\n            ],\n            axis=1,\n        )\n\n        items_features_by_choice[:, :, 0] = items_features_by_choice[:, :, 0] / 1000\n        items_features_by_choice[:, :, 1] = items_features_by_choice[:, :, 1] / 5000\n        items_features_by_choice[:, :, 2] = items_features_by_choice[:, :, 2] / 100\n\n        long_data = pd.get_dummies(\n            swiss_df,\n            columns=[\n                \"GROUP\",\n                \"PURPOSE\",\n                \"FIRST\",\n                \"TICKET\",\n                \"WHO\",\n                \"LUGGAGE\",\n                \"AGE\",\n                \"MALE\",\n                \"INCOME\",\n                \"GA\",\n                \"ORIGIN\",\n                \"DEST\",\n            ],\n            drop_first=False,\n        )\n\n        # Transorming the category data into OneHot\n        shared_features_by_choice = []\n        for col in long_data.columns:\n            if col.startswith(\"GROUP\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"PURPOSE\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"FIRST\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"TICKET\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"WHO\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"LUGGAGE\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"AGE\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"MALE\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"INCOME\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"GA\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"ORIGIN\"):\n                shared_features_by_choice.append(col)\n            if col.startswith(\"DEST\"):\n                shared_features_by_choice.append(col)\n\n        shared_features_by_choice = long_data[shared_features_by_choice].to_numpy()\n        choices = swiss_df.CHOICE.to_numpy()\n\n        return ChoiceDataset(\n            shared_features_by_choice=shared_features_by_choice.astype(\"float32\"),\n            items_features_by_choice=items_features_by_choice.astype(\"float32\"),\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n\n    return ChoiceDataset.from_single_wide_df(\n        df=swiss_df,\n        items_id=items,\n        shared_features_columns=shared_features_by_choice_names,\n        items_features_suffixes=items_features_by_choice_names,\n        available_items_suffix=availabilities_column,\n        choices_column=choice_column,\n        choice_format=\"items_index\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.load_train","title":"<code>load_train(as_frame=False, to_wide=False, return_desc=False)</code>","text":"<p>Load and return the Train dataset from Koppleman et al. (1993).</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset, by default False.</p> <code>False</code> <code>to_wide</code> <code>bool</code> <p>Whether to return the dataset in wide format, by default False (an thus returned in long format).</p> <code>False</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ChoiceDataset</code> <p>Loaded Train dataset</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def load_train(\n    as_frame=False,\n    to_wide=False,\n    return_desc=False,\n):\n    \"\"\"Load and return the Train dataset from Koppleman et al. (1993).\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the dataset as pd.DataFrame. If not, returned as ChoiceDataset,\n        by default False.\n    to_wide : bool, optional\n        Whether to return the dataset in wide format,\n        by default False (an thus returned in long format).\n    return_desc : bool, optional\n        Whether to return the description, by default False.\n\n    Returns\n    -------\n    ChoiceDataset\n        Loaded Train dataset\n    \"\"\"\n    desc = \"A sample of 235  Dutchindividuals facing 2929 choice situations.\"\n    desc += \"\"\"Ben-Akiva M, Bolduc D, Bradley M(1993).\n    \u201cEstimation of Travel Choice Models with Randomly Distributed Values of Time.\n    \u201dPapers 9303, Laval-Recherche en Energie. https://ideas.repec.org/p/fth/lavaen/9303.html.\"\"\"\n\n    _ = to_wide\n    data_file_name = \"train_data.csv.gz\"\n\n    full_path = get_path(data_file_name, module=DATA_MODULE)\n    train_df = pd.read_csv(full_path)\n\n    if return_desc:\n        return desc\n\n    if as_frame:\n        return train_df\n    train_df[\"choice\"] = train_df.apply(lambda row: row.choice[-1], axis=1)\n\n    return ChoiceDataset.from_single_wide_df(\n        df=train_df,\n        items_id=[\"1\", \"2\"],\n        shared_features_columns=[\"id\"],\n        items_features_prefixes=[\"price\", \"time\", \"change\", \"comfort\"],\n        delimiter=\"\",\n        available_items_suffix=None,\n        choices_column=\"choice\",\n        choice_format=\"items_id\",\n    )\n</code></pre>"},{"location":"references/datasets/references_base/#choice_learn.datasets.base.slice_from_names","title":"<code>slice_from_names(array, slice_names, all_names)</code>","text":"<p>Slicing on 2nd dimension function for numpy arrays.</p> <p>Slices array in the second dimension from column names.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>array to be sliced</p> required <code>slice_names</code> <code>list</code> <p>names of columns to return</p> required <code>all_names</code> <code>list</code> <p>names of all columns</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>sliced array</p> Source code in <code>choice_learn/datasets/base.py</code> <pre><code>def slice_from_names(array, slice_names, all_names):\n    \"\"\"Slicing on 2nd dimension function for numpy arrays.\n\n    Slices array in the second dimension from column names.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        array to be sliced\n    slice_names : list\n        names of columns to return\n    all_names : list\n        names of all columns\n\n    Returns\n    -------\n    np.ndarray\n        sliced array\n    \"\"\"\n    return array[:, [all_names.index(name) for name in slice_names]]\n</code></pre>"},{"location":"references/datasets/references_expedia/","title":"Available Open Source Dataset: Expedia ICDM 2013","text":"<p>ICDM 2013 Expedia dataset.</p>"},{"location":"references/datasets/references_expedia/#choice_learn.datasets.expedia.load_expedia","title":"<code>load_expedia(as_frame=False, preprocessing='rumnet')</code>","text":"<p>Load the Expedia dataset.</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the original file as pd.DF, by default False</p> <code>False</code> <code>preprocessing</code> <code>str</code> <p>predefined pre-processing to apply, by default None</p> <code>'rumnet'</code> Source code in <code>choice_learn/datasets/expedia.py</code> <pre><code>def load_expedia(as_frame=False, preprocessing=\"rumnet\"):\n    \"\"\"Load the Expedia dataset.\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the original file as pd.DF, by default False\n    preprocessing : str, optional\n        predefined pre-processing to apply, by default None\n    \"\"\"\n    filename = \"expedia.csv\"\n    data_path = get_path(filename, module=DATA_MODULE)\n    if not Path.exists(Path(data_path)):\n        print(\"In order to use the Expedia dataset, please download it from:\")\n        print(\"https://www.kaggle.com/c/expedia-personalized-sort\")\n        print(\"and save it in the following location:\")\n        print(data_path)\n        print(\"The downloaded train.csv file should be named 'expedia.csv'\")\n        raise FileNotFoundError(f\"File {filename} not found in {data_path}\")\n\n    expedia_df = pd.read_csv(data_path, engine=\"pyarrow\")\n    logging.info(\"Expedia csv loaded\")\n    if as_frame:\n        return expedia_df\n\n    if preprocessing == \"rumnet\":\n        logging.info(\"rumnet preprocessing selected, starting preprocessing...\")\n        try:\n            expedia_df = pd.read_csv(\n                get_path(\"preprocessed_expedia_rumnet.csv\", module=CACHE_MODULE), engine=\"pyarrow\"\n            )\n            logging.info(\"Loaded cached preprocessed data.\")\n\n        except FileNotFoundError:\n            expedia_df.date_time = pd.to_datetime(expedia_df.date_time, format=\"%Y-%m-%d %H:%M:%S\")\n            expedia_df.loc[:, \"day_of_week\"] = expedia_df.loc[:, \"date_time\"].dt.dayofweek\n            expedia_df.loc[:, \"month\"] = expedia_df.loc[:, \"date_time\"].dt.month\n            expedia_df.loc[:, \"hour\"] = expedia_df.loc[:, \"date_time\"].dt.hour\n\n            logging.info(\"Filtering ids with less than 1000 occurrences\")\n            for id_col in [\n                \"site_id\",\n                \"visitor_location_country_id\",\n                \"prop_country_id\",\n                \"srch_destination_id\",\n            ]:\n                value_counts = (\n                    expedia_df[[\"srch_id\", id_col]].drop_duplicates()[id_col].value_counts()\n                )\n                kept_ids = value_counts.index[value_counts.gt(1000)]\n                for id_ in expedia_df[id_col].unique():\n                    if id_ not in kept_ids:\n                        expedia_df.loc[expedia_df[id_col] == id_, id_col] = -1\n\n            logging.info(\"Filtering DF for price, stay length, booking window, etc.\")\n            # Filtering\n            expedia_df = expedia_df[expedia_df.price_usd &lt;= 1000]\n            expedia_df = expedia_df[expedia_df.price_usd &gt;= 10]\n            expedia_df[\"log_price\"] = expedia_df.price_usd.apply(np.log)\n            expedia_df = expedia_df[expedia_df.srch_length_of_stay &lt;= 14]\n            expedia_df = expedia_df[expedia_df.srch_booking_window &lt;= 365]\n            expedia_df[\"booking_window\"] = np.log(expedia_df[\"srch_booking_window\"] + 1)\n            expedia_df = expedia_df.fillna(-1)\n\n            logging.info(\"Sorting DF columns\")\n            order_cols = [\n                \"srch_id\",\n                \"prop_id\",\n                \"prop_starrating\",\n                \"prop_review_score\",\n                \"prop_brand_bool\",\n                \"prop_location_score1\",\n                \"prop_location_score2\",\n                \"prop_log_historical_price\",\n                \"position\",\n                \"promotion_flag\",\n                \"srch_length_of_stay\",\n                \"srch_adults_count\",\n                \"srch_children_count\",\n                \"srch_room_count\",\n                \"srch_saturday_night_bool\",\n                \"orig_destination_distance\",\n                \"random_bool\",\n                \"day_of_week\",\n                \"month\",\n                \"hour\",\n                \"log_price\",\n                \"booking_window\",\n                \"site_id\",\n                \"visitor_location_country_id\",\n                \"prop_country_id\",\n                \"srch_destination_id\",\n                \"click_bool\",\n                \"booking_bool\",\n            ]\n            expedia_df = expedia_df[order_cols]\n\n            logging.info(\"Creating dummy availabilities\")\n            expedia_df[\"av\"] = 1\n            asst_size = 38  # Fixed number of items in the assortment\n\n            logging.info(\"Creating dummy products to reach assortment size\")\n            # Loop to fill the data frame with dummy products\n            # next loop creates the dummy products\n            for _ in range(asst_size):\n                dum = (\n                    expedia_df.groupby(\"srch_id\")\n                    .filter(lambda x: len(x) &lt; asst_size)\n                    .groupby(\"srch_id\")\n                    .max()\n                    .reset_index(drop=False)\n                )\n                dum.loc[:, \"booking_bool\"] = 0\n                dum.loc[:, \"av\"] = 0\n                expedia_df = pd.concat([expedia_df, dum])\n\n            # getting rid of search &amp; prop_id and the clickbool and bookingbool\n            # adding no_purchase fixed effect\n            expedia_df[\"is_no_purchase\"] = 0\n\n            logging.info(\"Creating the no purchase option\")\n            # adding the no_purchase option to the data\n            df1 = (\n                expedia_df.groupby(\"srch_id\")\n                .filter(lambda x: x.booking_bool.sum() == 1)\n                .groupby(\"srch_id\")\n                .max()\n                .reset_index(drop=False)\n            )\n            df1.loc[:, \"is_no_purchase\"] = 1\n            df1.loc[:, \"log_price\"] = 0\n            df1.loc[:, \"booking_bool\"] = 0\n\n            df2 = (\n                expedia_df.groupby(\"srch_id\")\n                .filter(lambda x: x.booking_bool.sum() == 0)\n                .groupby(\"srch_id\")\n                .max()\n                .reset_index(drop=False)\n            )\n            df2.loc[:, \"is_no_purchase\"] = 1\n            df2.loc[:, \"log_price\"] = 0\n            df2.loc[:, \"booking_bool\"] = 1\n            expedia_df = pd.concat([expedia_df, df1, df2])\n\n            logging.info(\"Sorting the data frame\")\n            expedia_df = expedia_df.sort_values(\"srch_id\")\n            expedia_df.to_csv(\n                get_path(\"preprocessed_expedia_rumnet.csv\", module=CACHE_MODULE), index=False\n            )\n\n        choices = expedia_df.groupby(\"srch_id\").apply(lambda x: x.booking_bool.argmax())\n\n        logging.info(\"Creating the Storage objects\")\n        site_id_dict = {site_id: i for i, site_id in enumerate(expedia_df.site_id.unique())}\n        expedia_df[\"site_id\"] = (\n            expedia_df[\"site_id\"].apply(lambda x: site_id_dict[x]).astype(\"uint8\")\n        )\n        site_id_storage = OneHotStorage(ids=expedia_df.site_id.unique(), name=\"site_id\")\n\n        visitor_location_country_id_dict = {\n            visitor_location_country_id: i\n            for i, visitor_location_country_id in enumerate(\n                expedia_df.visitor_location_country_id.unique()\n            )\n        }\n        expedia_df[\"visitor_location_country_id\"] = (\n            expedia_df[\"visitor_location_country_id\"]\n            .apply(lambda x: visitor_location_country_id_dict[x])\n            .astype(\"uint8\")\n        )\n        visitor_location_country_id_storage = OneHotStorage(\n            ids=expedia_df.visitor_location_country_id.unique(),\n            name=\"visitor_location_country_id\",\n        )\n\n        srch_destination_id_dict = {\n            srch_destination_id: i\n            for i, srch_destination_id in enumerate(expedia_df.srch_destination_id.unique())\n        }\n        expedia_df[\"srch_destination_id\"] = (\n            expedia_df[\"srch_destination_id\"]\n            .apply(lambda x: srch_destination_id_dict[x])\n            .astype(\"uint8\")\n        )\n        srch_destination_id_storage = OneHotStorage(\n            ids=expedia_df.srch_destination_id.unique(), name=\"srch_destination_id\"\n        )\n\n        prop_country_id_dict = {\n            prop_country_id: i\n            for i, prop_country_id in enumerate(expedia_df.prop_country_id.unique())\n        }\n        expedia_df[\"prop_country_id\"] = (\n            expedia_df[\"prop_country_id\"].apply(lambda x: prop_country_id_dict[x]).astype(\"uint8\")\n        )\n        prop_country_id_storage = OneHotStorage(\n            ids=expedia_df.prop_country_id.unique(), name=\"prop_country_id\"\n        )\n\n        logging.info(\"DF to NDarray and creating the ChoiceDataset object\")\n        contexts_features_names = [\n            \"srch_id\",\n            \"srch_length_of_stay\",\n            \"srch_adults_count\",\n            \"srch_children_count\",\n            \"srch_room_count\",\n            \"srch_saturday_night_bool\",\n            \"booking_window\",\n            \"random_bool\",\n            \"day_of_week\",\n            \"month\",\n            \"hour\",\n            \"site_id\",\n            \"visitor_location_country_id\",\n            \"srch_destination_id\",\n        ]\n\n        contexts_features = expedia_df[contexts_features_names].drop_duplicates()\n        contexts_features = contexts_features.set_index(\"srch_id\")\n        contexts_features = (\n            contexts_features[contexts_features_names[1:-3]].to_numpy(),\n            contexts_features[contexts_features_names[-3:]].to_numpy(),\n        )\n\n        contexts_items_features_names = [\n            \"srch_id\",\n            \"prop_starrating\",\n            \"prop_review_score\",\n            \"prop_brand_bool\",\n            \"prop_location_score1\",\n            \"prop_location_score2\",\n            \"prop_log_historical_price\",\n            \"position\",\n            \"promotion_flag\",\n            \"orig_destination_distance\",\n            \"log_price\",\n            \"prop_country_id\",\n        ]\n\n        contexts_items_features = (\n            expedia_df[contexts_items_features_names]\n            .groupby(\"srch_id\")\n            .apply(lambda x: x[contexts_items_features_names[1:-1]].to_numpy())\n        )\n        contexts_items_features = np.stack(contexts_items_features)\n\n        contexts_items_prop_country_id = (\n            expedia_df[contexts_items_features_names]\n            .groupby(\"srch_id\")\n            .apply(lambda x: x[contexts_items_features_names[-1:]].to_numpy())\n        )\n        contexts_items_prop_country_id = np.stack(contexts_items_prop_country_id)\n        contexts_items_features = (contexts_items_features, contexts_items_prop_country_id)\n\n        contexts_items_availabilities = (\n            expedia_df[[\"srch_id\", \"av\"]].groupby(\"srch_id\").apply(lambda x: x[\"av\"].to_numpy())\n        )\n\n        return ChoiceDataset(\n            shared_features_by_choice=contexts_features,\n            items_features_by_choice=contexts_items_features,\n            features_by_ids=[\n                site_id_storage,\n                visitor_location_country_id_storage,\n                srch_destination_id_storage,\n                prop_country_id_storage,\n            ],\n            choices=choices.to_numpy(),\n            shared_features_by_choice_names=(\n                contexts_features_names[1:-3],\n                [\n                    \"site_id\",\n                    \"visitor_location_country_id\",\n                    \"srch_destination_id\",\n                ],\n            ),\n            items_features_by_choice_names=(\n                contexts_items_features_names[1:-1],\n                [\"prop_country_id\"],\n            ),\n            available_items_by_choice=np.stack(contexts_items_availabilities.to_numpy()),\n        )\n\n    raise ValueError(\n        f\"Preprocessing {preprocessing} not recognized, only 'rumnet' currently available\"\n    )\n</code></pre>"},{"location":"references/datasets/references_tafeng/","title":"Available Open Source Datasets: The TaFeng dataset","text":"<p>Some datasets used for personal examples.</p>"},{"location":"references/datasets/references_tafeng/#choice_learn.datasets.tafeng.load_tafeng","title":"<code>load_tafeng(as_frame=False, return_desc=False, preprocessing=None)</code>","text":"<p>Load the TaFeng dataset.</p> <p>Orginal file and informations can be found here: https://www.kaggle.com/datasets/chiranjivdas09/ta-feng-grocery-dataset/</p> <p>Parameters:</p> Name Type Description Default <code>as_frame</code> <code>bool</code> <p>Whether to return the original file as pd.DF, by default False</p> <code>False</code> <code>preprocessing</code> <code>str</code> <p>predefined pre-processing to apply, by default None</p> <code>None</code> <code>return_desc</code> <code>bool</code> <p>Whether to return the description of the dataset, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DF or ChoiceDataset</code> <p>TaFeng Grocery Dataset.</p> Source code in <code>choice_learn/datasets/tafeng.py</code> <pre><code>def load_tafeng(as_frame=False, return_desc=False, preprocessing=None):\n    \"\"\"Load the TaFeng dataset.\n\n    Orginal file and informations can be found here:\n    https://www.kaggle.com/datasets/chiranjivdas09/ta-feng-grocery-dataset/\n\n    Parameters\n    ----------\n    as_frame : bool, optional\n        Whether to return the original file as pd.DF, by default False\n    preprocessing : str, optional\n        predefined pre-processing to apply, by default None\n    return_desc : bool, optional\n        Whether to return the description of the dataset, by default False\n\n    Returns\n    -------\n    pd.DF or ChoiceDataset\n        TaFeng Grocery Dataset.\n    \"\"\"\n    filename = \"ta_feng.csv.zip\"\n\n    filepath = get_path(filename, module=DATA_MODULE)\n    # url = \"https://www.kaggle.com/datasets/chiranjivdas09/ta-feng-grocery-dataset/download?datasetVersionNumber=1\"\n    # if not os.path.exists(filepath):\n    #     with urllib.request.urlopen(url) as f:\n    #         file = f.read().decode(\"utf-8\")\n\n    description = \"\"\"The dataset contains a Chinese grocery store transaction data from November\n    2000 to February 2001.\n    Details and files can be found at:\n    https://www.kaggle.com/datasets/chiranjivdas09/ta-feng-grocery-dataset/download?datasetVersionNumber=1\n    \"\"\"\n\n    tafeng_df = pd.read_csv(filepath)\n    if as_frame:\n        return tafeng_df\n    if return_desc:\n        return description\n\n    if preprocessing == \"assort_example\":\n        subdf = tafeng_df.loc[tafeng_df.PRODUCT_SUBCLASS == 100505]\n        prods = subdf.PRODUCT_ID.value_counts().index[\n            (subdf.PRODUCT_ID.value_counts() &gt; 20).to_numpy()\n        ]\n        subdf = tafeng_df.loc[tafeng_df.PRODUCT_ID.isin(prods)]\n        subdf = subdf.dropna()\n        subdf = subdf.reset_index(drop=True)\n\n        # Create Prices\n        items = list(subdf.PRODUCT_ID.unique())\n        init_prices = []\n        for item in items:\n            first_price = subdf.loc[subdf.PRODUCT_ID == item].SALES_PRICE.to_numpy()[0]\n            init_prices.append(first_price)\n\n        # Encode Age Groups\n        age_groups = {}\n        for i, j in enumerate(subdf.AGE_GROUP.unique()):\n            age_groups[j] = i\n        age_groups = {\n            \"&lt;25\": 0,\n            \"25-29\": 0,\n            \"30-34\": 0,\n            \"35-39\": 1,\n            \"40-44\": 1,\n            \"45-49\": 1,\n            \"50-54\": 2,\n            \"55-59\": 2,\n            \"60-64\": 2,\n            \"&gt;65\": 2,\n        }\n        age_groups = {\n            \"&lt;25\": [1, 0, 0],\n            \"25-29\": [0, 1, 0],\n            \"30-34\": [0, 1, 0],\n            \"35-39\": [0, 1, 0],\n            \"40-44\": [0, 1, 0],\n            \"45-49\": [0, 1, 0],\n            \"50-54\": [0, 0, 1],\n            \"55-59\": [0, 0, 1],\n            \"60-64\": [0, 0, 1],\n            \"&gt;65\": [0, 0, 1],\n        }\n\n        all_prices = []\n        customer_features = []\n        choices = []\n\n        curr_prices = [i for i in init_prices]\n\n        for n_row, row in subdf.iterrows():\n            for _ in range(int(row.AMOUNT)):\n                item = row.PRODUCT_ID\n                price = row.SALES_PRICE / row.AMOUNT\n                age = row.AGE_GROUP\n\n                item_index = items.index(item)\n\n                # customer_features.append([age_groups[age]])\n                customer_features.append(age_groups[age])\n                choices.append(item_index)\n                curr_prices[item_index] = price\n                all_prices.append([i for i in curr_prices])\n\n        all_prices = np.expand_dims(np.array(all_prices), axis=-1)\n        customer_features = np.array(customer_features).astype(\"float32\")\n        choices = np.array(choices)\n\n        # Create Dataset\n        return ChoiceDataset(\n            shared_features_by_choice=customer_features,\n            choices=choices,\n            items_features_by_choice=all_prices,\n            available_items_by_choice=np.ones((len(choices), 25)).astype(\"float32\"),\n        )\n\n    return load_tafeng(as_frame=False, preprocessing=\"assort_example\")\n</code></pre>"},{"location":"references/models/references_base_model/","title":"Base model class","text":"<p>Base class for choice models.</p>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel","title":"<code>ChoiceModel</code>","text":"<p>Base class for choice models.</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>class ChoiceModel:\n    \"\"\"Base class for choice models.\"\"\"\n\n    def __init__(\n        self,\n        label_smoothing=0.0,\n        add_exit_choice=False,\n        optimizer=\"lbfgs\",\n        lbfgs_tolerance=1e-8,\n        lbfgs_parallel_iterations=4,\n        callbacks=None,\n        lr=0.001,\n        epochs=1000,\n        batch_size=32,\n        regularization=None,\n        regularization_strength=0.0,\n    ):\n        \"\"\"Instantiate the ChoiceModel.\n\n        Parameters\n        ----------\n        label_smoothing : float, optional\n            Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing,\n        during training, by default 0.0\n            by default None. Label smoothing is applied to LogLikelihood loss.\n        add_exit_choice : bool, optional\n            Whether or not to add a normalization (then U=1) with the exit option in probabilites\n            normalization,by default True\n        callbacks : list of tf.kera callbacks, optional\n            List of callbacks to add to model.fit, by default None and only add History\n        optimizer : str, optional\n            Name of the tf.keras.optimizers to be used, by default \"lbfgs\"\n        lbfgs_tolerance : float, optional\n            Tolerance for the L-BFGS optimizer if applied, by default 1e-8\n        lbfgs_parallel_iterations : int, optional\n            Number of parallel iterations for the L-BFGS optimizer, by default 4\n        lr: float, optional\n            Learning rate for the optimizer if applied, by default 0.001\n        epochs: int, optional\n            (Max) Number of epochs to train the model, by default 1000\n        batch_size: int, optional\n            Batch size in the case of stochastic gradient descent optimizer.\n            Not used in the case of L-BFGS optimizer, by default 32\n        regularization: str\n            Type of regularization to apply: \"l1\", \"l2\" or \"l1l2\", by default None\n        regularization_strength: float or list\n            weight of regularization in loss computation. If \"l1l2\" is chosen as regularization,\n            can be given as list or tuple: [l1_strength, l2_strength], by default 0.\n        \"\"\"\n        self.is_fitted = False\n        self.add_exit_choice = add_exit_choice\n        self.label_smoothing = label_smoothing\n        self.stop_training = False\n\n        # Loss function wrapping tf.keras.losses.CategoricalCrossEntropy\n        # with smoothing and normalization options\n        self.loss = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False, label_smoothing=self.label_smoothing\n        )\n        self.exact_nll = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False,\n            label_smoothing=0.0,\n            sparse=False,\n            axis=-1,\n            epsilon=1e-35,\n            name=\"exact_categorical_crossentropy\",\n            reduction=\"sum_over_batch_size\",\n        )\n        self.callbacks = tf.keras.callbacks.CallbackList(callbacks, add_history=True, model=None)\n        self.callbacks.set_model(self)\n\n        # Was originally in BaseMNL, moved here.\n        self.optimizer_name = optimizer\n        if optimizer.lower() == \"adam\":\n            self.optimizer = tf.keras.optimizers.Adam(lr)\n        elif optimizer.lower() == \"sgd\":\n            self.optimizer = tf.keras.optimizers.SGD(lr)\n        elif optimizer.lower() == \"adamax\":\n            self.optimizer = tf.keras.optimizers.Adamax(lr)\n        elif optimizer.lower() == \"lbfgs\" or optimizer.lower() == \"l-bfgs\":\n            print(\"Using L-BFGS optimizer, setting up .fit() function\")\n            self.optimizer = \"lbfgs\"\n            self.fit = self._fit_with_lbfgs\n        else:\n            print(f\"Optimizer {optimizer} not implemented, switching for default Adam\")\n            self.optimizer = tf.keras.optimizers.Adam(lr)\n\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.lbfgs_tolerance = lbfgs_tolerance\n        self.lbfgs_parallel_iterations = lbfgs_parallel_iterations\n\n        if regularization is not None:\n            if np.sum(regularization_strength) &lt;= 0:\n                raise ValueError(\n                    \"Regularization strength must be positive if regularization is set.\"\n                )\n            if regularization.lower() == \"l1\":\n                self.regularizer = tf.keras.regularizers.L1(l1=regularization_strength)\n            elif regularization.lower() == \"l2\":\n                self.regularizer = tf.keras.regularizers.L2(l2=regularization_strength)\n            elif regularization.lower() == \"l1l2\":\n                if isinstance(regularization_strength, (list, tuple)):\n                    self.regularizer = tf.keras.regularizers.L1L2(\n                        l1=regularization_strength[0], l2=regularization_strength[1]\n                    )\n                else:\n                    self.regularizer = tf.keras.regularizers.L1L2(\n                        l1=regularization_strength, l2=regularization_strength\n                    )\n            else:\n                raise ValueError(\n                    \"Regularization type not recognized, choose among l1, l2 and l1l2.\"\n                )\n            self.regularization = regularization\n            self.regularization_strength = regularization_strength\n        else:\n            self.regularization_strength = 0.0\n            self.regularization = None\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights need to be specified in children classes.\n\n        Basically it determines which weights need to be optimized during training.\n        MUST be a list\n        \"\"\"\n        raise NotImplementedError(\n            \"\"\"Trainable_weights must be specified in children classes,\n              when you inherit from ChoiceModel.\n            See custom models documentation for more details and examples.\"\"\"\n        )\n\n    @abstractmethod\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Define how the model computes the utility of a product.\n\n        MUST be implemented in children classe !\n        For simpler use-cases this is the only method to be user-defined.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        np.ndarray\n            Utility of each product for each choice.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        # To be implemented in children classes\n        # Can be NumPy or TensorFlow based\n        return\n\n    @tf.function\n    def train_step(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor\n            Value of NegativeLogLikelihood loss for the batch\n        \"\"\"\n        with tf.GradientTape() as tape:\n            utilities = self.compute_batch_utility(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n\n            probabilities = tf_ops.softmax_with_availabilities(\n                items_logit_by_choice=utilities,\n                available_items_by_choice=available_items_by_choice,\n                normalize_exit=self.add_exit_choice,\n                axis=-1,\n            )\n            # Negative Log-Likelihood\n            neg_loglikelihood = self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            )\n            if self.regularization is not None:\n                regularization = tf.reduce_sum(\n                    [self.regularizer(w) for w in self.trainable_weights]\n                )\n                neg_loglikelihood += regularization\n\n        grads = tape.gradient(neg_loglikelihood, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return neg_loglikelihood\n\n    def fit(\n        self,\n        choice_dataset,\n        sample_weight=None,\n        val_dataset=None,\n        validation_freq=1,\n        verbose=0,\n    ):\n        \"\"\"Train the model with a ChoiceDataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Input data in the form of a ChoiceDataset\n        sample_weight : np.ndarray, optional\n            Sample weight to apply, by default None\n        val_dataset : ChoiceDataset or (ChoiceDataset, samples_weight), optional\n            Test ChoiceDataset to evaluate performances on test at each epoch, by default None\n        verbose : int, optional\n            print level, for debugging, by default 0\n        epochs : int, optional\n            Number of epochs, default is None, meaning we use self.epochs\n        batch_size : int, optional\n            Batch size, default is None, meaning we use self.batch_size\n        validation_freq: int, optional\n            Only relevant if validation data is provided. Specifies how many training epochs\n            to run before a new validation run is performed, e.g. validation_freq=2 runs validation\n            every 2 epochs.\n\n        Returns\n        -------\n        dict:\n            Different metrics values over epochs.\n        \"\"\"\n        if hasattr(self, \"instantiated\"):\n            if not self.instantiated:\n                raise ValueError(\"Model not instantiated. Please call .instantiate() first.\")\n        epochs = self.epochs\n        batch_size = self.batch_size\n\n        losses_history = {\"train_loss\": []}\n        if verbose &gt;= 0 and verbose &lt; 2:\n            t_range = tqdm.trange(epochs, position=0)\n        else:\n            t_range = range(epochs)\n\n        self.callbacks.on_train_begin()\n        # Iterate of epochs\n        for epoch_nb in t_range:\n            if verbose &gt;= 2:\n                print(f\"Start Epoch {epoch_nb}\")\n            self.callbacks.on_epoch_begin(epoch_nb)\n            t_start = time.time()\n            train_logs = {\"train_loss\": []}\n            val_logs = {\"val_loss\": []}\n            epoch_losses = []\n\n            if sample_weight is not None:\n                if verbose &gt; 0:\n                    inner_range = tqdm.tqdm(\n                        choice_dataset.iter_batch(\n                            shuffle=True, sample_weight=sample_weight, batch_size=batch_size\n                        ),\n                        total=int(len(choice_dataset) / np.max([1, batch_size])),\n                        position=1,\n                        leave=False,\n                    )\n                else:\n                    inner_range = choice_dataset.iter_batch(\n                        shuffle=True, sample_weight=sample_weight, batch_size=batch_size\n                    )\n\n                for batch_nb, (\n                    (\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                    ),\n                    weight_batch,\n                ) in enumerate(inner_range):\n                    self.callbacks.on_train_batch_begin(batch_nb)\n\n                    neg_loglikelihood = self.train_step(\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                        sample_weight=weight_batch,\n                    )\n\n                    train_logs[\"train_loss\"].append(neg_loglikelihood)\n                    temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n                    self.callbacks.on_train_batch_end(batch_nb, logs=temps_logs)\n\n                    # Optimization Steps\n                    epoch_losses.append(neg_loglikelihood)\n\n                    if verbose &gt; 0:\n                        inner_range.set_description(\n                            f\"Epoch Negative-LogLikeliHood: {np.mean(epoch_losses):.4f}\"\n                        )\n                    if self.stop_training:\n                        print(\"Training stopped with early stopping taking effect\")\n                        break\n\n            # In this case we do not need to batch the sample_weights\n            else:\n                if verbose &gt; 0:\n                    inner_range = tqdm.tqdm(\n                        choice_dataset.iter_batch(shuffle=True, batch_size=batch_size),\n                        total=int(len(choice_dataset) / np.max([batch_size, 1])),\n                        position=1,\n                        leave=False,\n                    )\n                else:\n                    inner_range = choice_dataset.iter_batch(shuffle=True, batch_size=batch_size)\n                for batch_nb, (\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                ) in enumerate(inner_range):\n                    self.callbacks.on_train_batch_begin(batch_nb)\n                    neg_loglikelihood = self.train_step(\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                    )\n                    train_logs[\"train_loss\"].append(neg_loglikelihood)\n                    temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n                    self.callbacks.on_train_batch_end(batch_nb, logs=temps_logs)\n\n                    # Optimization Steps\n                    epoch_losses.append(neg_loglikelihood)\n\n                    if verbose &gt; 0:\n                        inner_range.set_description(\n                            f\"Epoch Negative-LogLikeliHood: {np.mean(epoch_losses):.4f}\"\n                        )\n                    if self.stop_training:\n                        print(\"Training stopped with early stopping taking effect\")\n                        break\n\n            # Take into account the fact that the last batch may have a\n            # different length for the computation of the epoch loss.\n            if batch_size != -1:\n                last_batch_size = available_items_batch.shape[0]\n                coefficients = tf.concat(\n                    [tf.ones(len(epoch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n                )\n                epoch_losses = tf.multiply(epoch_losses, coefficients)\n                epoch_loss = tf.reduce_sum(epoch_losses) / len(choice_dataset)\n            else:\n                epoch_loss = tf.reduce_mean(epoch_losses)\n            losses_history[\"train_loss\"].append(epoch_loss)\n            print_loss = losses_history[\"train_loss\"][-1].numpy()\n            desc = f\"Epoch {epoch_nb} Train Loss {print_loss:.4f}\"\n            if verbose &gt; 1:\n                print(\n                    f\"Loop {epoch_nb} Time:\",\n                    f\"{time.time() - t_start:.4f}\",\n                    f\"Loss: {print_loss:.4f}\",\n                )\n\n            # Test on val_dataset if provided\n            if val_dataset is not None and ((epoch_nb + 1) % validation_freq) == 0:\n                test_losses = []\n\n                val_samples_weight = None\n                if isinstance(val_dataset, tuple):\n                    if not len(val_dataset) == 2:\n                        raise ValueError(\n                            \"\"\"if argument val_dataset is a tuple, it should be\n                            in the form (ChoiceDataset, weights)\"\"\"\n                        )\n                    validation_dataset, val_samples_weight = val_dataset\n                elif isinstance(val_dataset, ChoiceDataset):\n                    validation_dataset = val_dataset\n                else:\n                    raise ValueError(\n                        \"\"\"val_dataset should be a ChoiceDataset or\n                        a tuple of (ChoiceDataset, weights).\"\"\"\n                    )\n\n                val_iterator = validation_dataset.iter_batch(\n                    shuffle=False, sample_weight=val_samples_weight, batch_size=batch_size\n                )\n\n                for batch_nb, batch_data in enumerate(val_iterator):\n                    weight_batch = None\n                    if val_samples_weight is not None:\n                        batch_features, weight_batch = batch_data\n                    else:\n                        batch_features = batch_data\n\n                    (\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                    ) = batch_features\n\n                    self.callbacks.on_batch_begin(batch_nb)\n                    self.callbacks.on_test_batch_begin(batch_nb)\n\n                    loss = self.batch_predict(\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                        sample_weight=weight_batch,\n                    )[0][\"optimized_loss\"]\n                    test_losses.append(loss)\n\n                    val_logs[\"val_loss\"].append(test_losses[-1])\n                    temps_logs = {k: tf.reduce_mean(v) for k, v in val_logs.items()}\n                    self.callbacks.on_test_batch_end(batch_nb, logs=temps_logs)\n\n                test_loss = tf.reduce_mean(test_losses)\n                if verbose &gt; 1:\n                    print(\"Test Negative-LogLikelihood:\", test_loss.numpy())\n                    desc += f\", Test Loss {np.round(test_loss.numpy(), 4)}\"\n                losses_history[\"val_loss\"] = losses_history.get(\"val_loss\", []) + [\n                    test_loss.numpy()\n                ]\n                train_logs = {**train_logs, **val_logs}\n\n            temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n            self.callbacks.on_epoch_end(epoch_nb, logs=temps_logs)\n            if self.stop_training:\n                print(\"Training stopped with early stopping taking effect\")\n                break\n            t_range.set_description(desc)\n            t_range.refresh()\n\n        temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n        self.callbacks.on_train_end(logs=temps_logs)\n        return losses_history\n\n    @tf.function()\n    def batch_predict(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor (1, )\n            Value of NegativeLogLikelihood loss for the batch\n        tf.Tensor (batch_size, n_items)\n            Probabilities for each product to be chosen for each choice\n        \"\"\"\n        # Compute utilities from features\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n        # Compute probabilities from utilities &amp; availabilties\n        probabilities = tf_ops.softmax_with_availabilities(\n            items_logit_by_choice=utilities,\n            available_items_by_choice=available_items_by_choice,\n            normalize_exit=self.add_exit_choice,\n            axis=-1,\n        )\n\n        # Compute loss from probabilities &amp; actual choices\n        # batch_loss = self.loss(probabilities, c_batch, sample_weight=sample_weight)\n        batch_loss = {\n            \"optimized_loss\": self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n            # \"NegativeLogLikelihood\": tf.keras.losses.CategoricalCrossentropy()(\n            #     y_pred=probabilities,\n            #     y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            #     sample_weight=sample_weight,\n            # ),\n            \"Exact-NegativeLogLikelihood\": self.exact_nll(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n        }\n        return batch_loss, probabilities\n\n    def save_model(self, path, save_opt=True):\n        \"\"\"Save the different models on disk.\n\n        Parameters\n        ----------\n        path : str\n            path to the folder where to save the model\n        \"\"\"\n        if not os.path.exists(path):\n            Path(path).mkdir(parents=True)\n\n        for i, weight in enumerate(self.trainable_weights):\n            np.save(Path(path) / f\"weight_{i}.npy\", weight.numpy())\n\n        # To improve for non-string attributes\n        params = {}\n        for k, v in self.__dict__.items():\n            if isinstance(v, (int, float, str, dict, tuple)):\n                params[k] = v\n            elif isinstance(v, (list, tuple)):\n                if all(isinstance(item, (int, float, str, dict)) for item in v):\n                    params[k] = v\n                elif k != \"_trainable_weights\":\n                    logging.warning(\n                        \"\"\"Attribute '%s' is a list with non-serializable\n                         types and will not be saved.\"\"\",\n                        k,\n                    )\n        with open(Path(path) / \"params.json\", \"w\") as f:\n            json.dump(params, f)\n\n        # Save optimizer state\n        if save_opt and not isinstance(self.optimizer, str):\n            (Path(path) / \"optimizer\").mkdir(parents=True, exist_ok=True)\n            config = self.optimizer.get_config()\n            weights_store = {}\n            self.optimizer.save_own_variables(weights_store)\n            for key, value in weights_store.items():\n                if isinstance(value, tf.Variable):\n                    value = value.numpy()\n                weights_store[key] = value.tolist()\n            if \"learning_rate\" in config.keys():\n                if isinstance(config[\"learning_rate\"], tf.Variable):\n                    config[\"learning_rate\"] = config[\"learning_rate\"].numpy()\n                if isinstance(config[\"learning_rate\"], np.float32):\n                    config[\"learning_rate\"] = config[\"learning_rate\"].tolist()\n            with open(Path(path) / \"optimizer\" / \"config.json\", \"w\") as f:\n                json.dump(config, f)\n            with open(Path(path) / \"optimizer\" / \"weights_store.json\", \"w\") as f:\n                json.dump(weights_store, f)\n\n    @classmethod\n    def load_model(cls, path):\n        \"\"\"Load a ChoiceModel previously saved with save_model().\n\n        Parameters\n        ----------\n        path : str\n            path to the folder where the saved model files are\n\n        Returns\n        -------\n        ChoiceModel\n            Loaded ChoiceModel\n        \"\"\"\n        # To improve for non string attributes\n        with open(Path(path) / \"params.json\") as f:\n            params = json.load(f)\n\n        obj = cls(optimizer=params[\"optimizer_name\"])\n        obj._trainable_weights = []\n\n        i = 0\n        weight_path = f\"weight_{i}.npy\"\n        files_list = []\n        for file in Path(path).iterdir():\n            files_list.append(str(file.name))\n        while weight_path in files_list:\n            obj._trainable_weights.append(tf.Variable(np.load(Path(path) / weight_path)))\n            i += 1\n            weight_path = f\"weight_{i}.npy\"\n\n        for k, v in params.items():\n            setattr(obj, k, v)\n\n        if Path.is_dir(Path(path) / \"optimizer\"):\n            with open(Path(path) / \"optimizer\" / \"config.json\") as f:\n                config = json.load(f)\n            # obj.optimizer = tf.keras.optimizers.get(params[\"optimizer_name\"]).from_config(config)\n            obj.optimizer = obj.optimizer.from_config(config)\n            obj.optimizer.build(var_list=obj.trainable_weights)\n\n            with open(Path(path) / \"optimizer\" / \"weights_store.json\") as f:\n                store = json.load(f)\n            for key, value in store.items():\n                store[key] = np.array(value, dtype=np.float32)\n            obj.optimizer.load_own_variables(store)\n\n        # Load optimizer step\n        return obj\n\n    def predict_probas(self, choice_dataset, batch_size=-1):\n        \"\"\"Predicts the choice probabilities for each choice and each product of a ChoiceDataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset on which to apply to prediction\n        batch_size : int, optional\n            Batch size to use for the prediction, by default -1\n\n        Returns\n        -------\n        np.ndarray (n_choices, n_items)\n            Choice probabilties for each choice and each product\n        \"\"\"\n        stacked_probabilities = []\n        for (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        ) in choice_dataset.iter_batch(batch_size=batch_size):\n            _, probabilities = self.batch_predict(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n            stacked_probabilities.append(probabilities)\n\n        return tf.concat(stacked_probabilities, axis=0)\n\n    def evaluate(self, choice_dataset, sample_weight=None, batch_size=-1, mode=\"eval\"):\n        \"\"\"Evaluate the model for each choice and each product of a ChoiceDataset.\n\n        Predicts the probabilities according to the model and computes the Negative-Log-Likelihood\n        loss from the actual choices.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset on which to apply to prediction\n\n        Returns\n        -------\n        np.ndarray (n_choices, n_items)\n            Choice probabilties for each choice and each product\n        \"\"\"\n        batch_losses = []\n        if sample_weight is not None:\n            for (\n                shared_features_by_choice,\n                items_features_by_choice,\n                available_items_by_choice,\n                choices,\n            ), batch_sample_weight in choice_dataset.iter_batch(\n                batch_size=batch_size, sample_weight=sample_weight\n            ):\n                loss, _ = self.batch_predict(\n                    shared_features_by_choice=shared_features_by_choice,\n                    items_features_by_choice=items_features_by_choice,\n                    available_items_by_choice=available_items_by_choice,\n                    choices=choices,\n                    sample_weight=batch_sample_weight,\n                )\n                if mode == \"eval\":\n                    batch_losses.append(loss[\"Exact-NegativeLogLikelihood\"])\n                elif mode == \"optim\":\n                    batch_losses.append(loss[\"optimized_loss\"])\n        else:\n            for (\n                shared_features_by_choice,\n                items_features_by_choice,\n                available_items_by_choice,\n                choices,\n            ) in choice_dataset.iter_batch(batch_size=batch_size):\n                loss, _ = self.batch_predict(\n                    shared_features_by_choice=shared_features_by_choice,\n                    items_features_by_choice=items_features_by_choice,\n                    available_items_by_choice=available_items_by_choice,\n                    choices=choices,\n                )\n                if mode == \"eval\":\n                    batch_losses.append(loss[\"Exact-NegativeLogLikelihood\"])\n                elif mode == \"optim\":\n                    batch_losses.append(loss[\"optimized_loss\"])\n        if batch_size != -1:\n            last_batch_size = available_items_by_choice.shape[0]\n            coefficients = tf.concat(\n                [tf.ones(len(batch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n            )\n            batch_losses = tf.multiply(batch_losses, coefficients)\n            batch_loss = tf.reduce_sum(batch_losses) / len(choice_dataset)\n        else:\n            batch_loss = tf.reduce_mean(batch_losses)\n        return batch_loss\n\n    def _lbfgs_train_step(self, choice_dataset, sample_weight=None):\n        \"\"\"Create a function required by tfp.optimizer.lbfgs_minimize.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Dataset on which to estimate the parameters.\n        sample_weight: np.ndarray, optional\n            Sample weights to apply, by default None\n\n        Returns\n        -------\n        function\n            with the signature:\n                loss_value, gradients = f(model_parameters).\n        \"\"\"\n        # obtain the shapes of all trainable parameters in the model\n        shapes = tf.shape_n(self.trainable_weights)\n        n_tensors = len(shapes)\n\n        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n        # prepare required information first\n        count = 0\n        idx = []  # stitch indices\n        part = []  # partition indices\n\n        for i, shape in enumerate(shapes):\n            n = np.prod(shape)\n            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n            part.extend([i] * n)\n            count += n\n\n        part = tf.constant(part)\n\n        @tf.function\n        def assign_new_model_parameters(params_1d):\n            \"\"\"Update the model's parameters with a 1D tf.Tensor.\n\n            Pararmeters\n            -----------\n            params_1d: tf.Tensor\n                a 1D tf.Tensor representing the model's trainable parameters.\n            \"\"\"\n            params = tf.dynamic_partition(params_1d, part, n_tensors)\n            for i, (shape, param) in enumerate(zip(shapes, params)):\n                self.trainable_weights[i].assign(tf.reshape(param, shape))\n\n        # now create a function that will be returned by this factory\n        @tf.function\n        def f(params_1d):\n            \"\"\"Can be used by tfp.optimizer.lbfgs_minimize.\n\n            This function is created by function_factory.\n\n            Parameters\n            ----------\n            params_1d: tf.Tensor\n                a 1D tf.Tensor.\n\n            Returns\n            -------\n            tf.Tensor\n                A scalar loss and the gradients w.r.t. the `params_1d`.\n            tf.Tensor\n                A 1D tf.Tensor representing the gradients w.r.t. the `params_1d`.\n            \"\"\"\n            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n            with tf.GradientTape() as tape:\n                # update the parameters in the model\n                assign_new_model_parameters(params_1d)\n                # calculate the loss\n                loss_value = self.evaluate(\n                    choice_dataset, sample_weight=sample_weight, batch_size=-1, mode=\"eval\"\n                )\n                if self.regularization is not None:\n                    regularization = tf.reduce_sum(\n                        [self.regularizer(w) for w in self.trainable_weights]\n                    )\n                    loss_value += regularization\n\n            # calculate gradients and convert to 1D tf.Tensor\n            grads = tape.gradient(loss_value, self.trainable_weights)\n            grads = tf.dynamic_stitch(idx, grads)\n            f.iter.assign_add(1)\n\n            # store loss value so we can retrieve later\n            tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n\n            return loss_value, grads\n\n        # store these information as members so we can use them outside the scope\n        f.iter = tf.Variable(0)\n        f.idx = idx\n        f.part = part\n        f.shapes = shapes\n        f.assign_new_model_parameters = assign_new_model_parameters\n        f.history = []\n        return f\n\n    def _fit_with_lbfgs(self, choice_dataset, sample_weight=None, verbose=0):\n        \"\"\"Fit function for L-BFGS optimizer.\n\n        Replaces the .fit method when the optimizer is set to L-BFGS.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to be used for coefficients estimations\n        epochs : int\n            Maximum number of epochs allowed to reach minimum\n        sample_weight : np.ndarray, optional\n            Sample weights to apply, by default None\n        verbose : int, optional\n            print level, for debugging, by default 0\n\n        Returns\n        -------\n        dict\n            Fit history\n        \"\"\"\n        # Only import tensorflow_probability if LBFGS optimizer is used, avoid unnecessary\n        # dependency\n        import tensorflow_probability as tfp\n\n        epochs = self.epochs\n        func = self._lbfgs_train_step(choice_dataset=choice_dataset, sample_weight=sample_weight)\n\n        # convert initial model parameters to a 1D tf.Tensor\n        init_params = tf.dynamic_stitch(func.idx, self.trainable_weights)\n        # train the model with L-BFGS solver\n        results = tfp.optimizer.lbfgs_minimize(\n            value_and_gradients_function=func,\n            initial_position=init_params,\n            max_iterations=epochs,\n            tolerance=self.lbfgs_tolerance,\n            f_absolute_tolerance=-1,\n            f_relative_tolerance=-1,\n            parallel_iterations=self.lbfgs_parallel_iterations,\n        )\n\n        # after training, the final optimized parameters are still in results.position\n        # so we have to manually put them back to the model\n        func.assign_new_model_parameters(results.position)\n        if results[1].numpy():\n            logging.error(\"L-BFGS Optimization failed.\")\n        if verbose &gt; 0:\n            logging.warning(\"L-BFGS Opimization finished:\")\n            logging.warning(\"---------------------------------------------------------------\")\n            logging.warning(f\"Number of iterations: {results[2].numpy()}\")\n            logging.warning(\n                f\"Algorithm converged before reaching max iterations: {results[0].numpy()}\",\n            )\n        return {\"train_loss\": func.history}\n\n    def assign_lr(self, lr):\n        \"\"\"Change value of learning rate.\n\n        Parameters\n        ----------\n        lr : float\n            new learning rate value to be assigned\n        \"\"\"\n        if isinstance(self.optimizer, tf.keras.optimizers.Optimizer):\n            self.optimizer.lr = lr\n        else:\n            raise NotImplementedError(\n                f\"Learning rate cannot be changed for optimizer: {self.optimizer}\"\n            )\n\n    def get_weights(self) -&gt; list[np.ndarray]:\n        \"\"\"Return the values of `model.trainable_weights` as a list of NumPy arrays.\"\"\"\n        return [v.numpy() for v in self.trainable_weights]\n\n    def set_weights(self, weights: list[np.ndarray]) -&gt; None:\n        \"\"\"Set the values of `model.trainable_weights` from a list of NumPy arrays.\"\"\"\n        layer_weights = self.trainable_weights\n        if len(layer_weights) != len(weights):\n            raise ValueError(\n                f\"You called `set_weights(weights)` on a model \"\n                f\"with a weight list of length {len(weights)}, but the model \"\n                f\"was expecting {len(layer_weights)} weights.\"\n            )\n        for variable, value in zip(layer_weights, weights):\n            if variable.shape != value.shape:\n                raise ValueError(\n                    f\"Model weight shape {variable.shape} \"\n                    \"is not compatible with provided weight \"\n                    f\"shape {value.shape}.\"\n                )\n            variable.assign(value)\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights need to be specified in children classes.</p> <p>Basically it determines which weights need to be optimized during training. MUST be a list</p>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.__init__","title":"<code>__init__(label_smoothing=0.0, add_exit_choice=False, optimizer='lbfgs', lbfgs_tolerance=1e-08, lbfgs_parallel_iterations=4, callbacks=None, lr=0.001, epochs=1000, batch_size=32, regularization=None, regularization_strength=0.0)</code>","text":"<p>Instantiate the ChoiceModel.</p> <p>Parameters:</p> Name Type Description Default <code>label_smoothing</code> <code>float</code> <p>Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing,</p> <code>0.0</code> <code>during</code> <p>by default None. Label smoothing is applied to LogLikelihood loss.</p> required <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to add a normalization (then U=1) with the exit option in probabilites normalization,by default True</p> <code>False</code> <code>callbacks</code> <code>list of tf.kera callbacks</code> <p>List of callbacks to add to model.fit, by default None and only add History</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>Name of the tf.keras.optimizers to be used, by default \"lbfgs\"</p> <code>'lbfgs'</code> <code>lbfgs_tolerance</code> <code>float</code> <p>Tolerance for the L-BFGS optimizer if applied, by default 1e-8</p> <code>1e-08</code> <code>lbfgs_parallel_iterations</code> <code>int</code> <p>Number of parallel iterations for the L-BFGS optimizer, by default 4</p> <code>4</code> <code>lr</code> <p>Learning rate for the optimizer if applied, by default 0.001</p> <code>0.001</code> <code>epochs</code> <p>(Max) Number of epochs to train the model, by default 1000</p> <code>1000</code> <code>batch_size</code> <p>Batch size in the case of stochastic gradient descent optimizer. Not used in the case of L-BFGS optimizer, by default 32</p> <code>32</code> <code>regularization</code> <p>Type of regularization to apply: \"l1\", \"l2\" or \"l1l2\", by default None</p> <code>None</code> <code>regularization_strength</code> <p>weight of regularization in loss computation. If \"l1l2\" is chosen as regularization, can be given as list or tuple: [l1_strength, l2_strength], by default 0.</p> <code>0.0</code> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def __init__(\n    self,\n    label_smoothing=0.0,\n    add_exit_choice=False,\n    optimizer=\"lbfgs\",\n    lbfgs_tolerance=1e-8,\n    lbfgs_parallel_iterations=4,\n    callbacks=None,\n    lr=0.001,\n    epochs=1000,\n    batch_size=32,\n    regularization=None,\n    regularization_strength=0.0,\n):\n    \"\"\"Instantiate the ChoiceModel.\n\n    Parameters\n    ----------\n    label_smoothing : float, optional\n        Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing,\n    during training, by default 0.0\n        by default None. Label smoothing is applied to LogLikelihood loss.\n    add_exit_choice : bool, optional\n        Whether or not to add a normalization (then U=1) with the exit option in probabilites\n        normalization,by default True\n    callbacks : list of tf.kera callbacks, optional\n        List of callbacks to add to model.fit, by default None and only add History\n    optimizer : str, optional\n        Name of the tf.keras.optimizers to be used, by default \"lbfgs\"\n    lbfgs_tolerance : float, optional\n        Tolerance for the L-BFGS optimizer if applied, by default 1e-8\n    lbfgs_parallel_iterations : int, optional\n        Number of parallel iterations for the L-BFGS optimizer, by default 4\n    lr: float, optional\n        Learning rate for the optimizer if applied, by default 0.001\n    epochs: int, optional\n        (Max) Number of epochs to train the model, by default 1000\n    batch_size: int, optional\n        Batch size in the case of stochastic gradient descent optimizer.\n        Not used in the case of L-BFGS optimizer, by default 32\n    regularization: str\n        Type of regularization to apply: \"l1\", \"l2\" or \"l1l2\", by default None\n    regularization_strength: float or list\n        weight of regularization in loss computation. If \"l1l2\" is chosen as regularization,\n        can be given as list or tuple: [l1_strength, l2_strength], by default 0.\n    \"\"\"\n    self.is_fitted = False\n    self.add_exit_choice = add_exit_choice\n    self.label_smoothing = label_smoothing\n    self.stop_training = False\n\n    # Loss function wrapping tf.keras.losses.CategoricalCrossEntropy\n    # with smoothing and normalization options\n    self.loss = tf_ops.CustomCategoricalCrossEntropy(\n        from_logits=False, label_smoothing=self.label_smoothing\n    )\n    self.exact_nll = tf_ops.CustomCategoricalCrossEntropy(\n        from_logits=False,\n        label_smoothing=0.0,\n        sparse=False,\n        axis=-1,\n        epsilon=1e-35,\n        name=\"exact_categorical_crossentropy\",\n        reduction=\"sum_over_batch_size\",\n    )\n    self.callbacks = tf.keras.callbacks.CallbackList(callbacks, add_history=True, model=None)\n    self.callbacks.set_model(self)\n\n    # Was originally in BaseMNL, moved here.\n    self.optimizer_name = optimizer\n    if optimizer.lower() == \"adam\":\n        self.optimizer = tf.keras.optimizers.Adam(lr)\n    elif optimizer.lower() == \"sgd\":\n        self.optimizer = tf.keras.optimizers.SGD(lr)\n    elif optimizer.lower() == \"adamax\":\n        self.optimizer = tf.keras.optimizers.Adamax(lr)\n    elif optimizer.lower() == \"lbfgs\" or optimizer.lower() == \"l-bfgs\":\n        print(\"Using L-BFGS optimizer, setting up .fit() function\")\n        self.optimizer = \"lbfgs\"\n        self.fit = self._fit_with_lbfgs\n    else:\n        print(f\"Optimizer {optimizer} not implemented, switching for default Adam\")\n        self.optimizer = tf.keras.optimizers.Adam(lr)\n\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.lbfgs_tolerance = lbfgs_tolerance\n    self.lbfgs_parallel_iterations = lbfgs_parallel_iterations\n\n    if regularization is not None:\n        if np.sum(regularization_strength) &lt;= 0:\n            raise ValueError(\n                \"Regularization strength must be positive if regularization is set.\"\n            )\n        if regularization.lower() == \"l1\":\n            self.regularizer = tf.keras.regularizers.L1(l1=regularization_strength)\n        elif regularization.lower() == \"l2\":\n            self.regularizer = tf.keras.regularizers.L2(l2=regularization_strength)\n        elif regularization.lower() == \"l1l2\":\n            if isinstance(regularization_strength, (list, tuple)):\n                self.regularizer = tf.keras.regularizers.L1L2(\n                    l1=regularization_strength[0], l2=regularization_strength[1]\n                )\n            else:\n                self.regularizer = tf.keras.regularizers.L1L2(\n                    l1=regularization_strength, l2=regularization_strength\n                )\n        else:\n            raise ValueError(\n                \"Regularization type not recognized, choose among l1, l2 and l1l2.\"\n            )\n        self.regularization = regularization\n        self.regularization_strength = regularization_strength\n    else:\n        self.regularization_strength = 0.0\n        self.regularization = None\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.assign_lr","title":"<code>assign_lr(lr)</code>","text":"<p>Change value of learning rate.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>new learning rate value to be assigned</p> required Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def assign_lr(self, lr):\n    \"\"\"Change value of learning rate.\n\n    Parameters\n    ----------\n    lr : float\n        new learning rate value to be assigned\n    \"\"\"\n    if isinstance(self.optimizer, tf.keras.optimizers.Optimizer):\n        self.optimizer.lr = lr\n    else:\n        raise NotImplementedError(\n            f\"Learning rate cannot be changed for optimizer: {self.optimizer}\"\n        )\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.batch_predict","title":"<code>batch_predict(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor(1)</code> <p>Value of NegativeLogLikelihood loss for the batch</p> <code>Tensor(batch_size, n_items)</code> <p>Probabilities for each product to be chosen for each choice</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>@tf.function()\ndef batch_predict(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor (1, )\n        Value of NegativeLogLikelihood loss for the batch\n    tf.Tensor (batch_size, n_items)\n        Probabilities for each product to be chosen for each choice\n    \"\"\"\n    # Compute utilities from features\n    utilities = self.compute_batch_utility(\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    )\n    # Compute probabilities from utilities &amp; availabilties\n    probabilities = tf_ops.softmax_with_availabilities(\n        items_logit_by_choice=utilities,\n        available_items_by_choice=available_items_by_choice,\n        normalize_exit=self.add_exit_choice,\n        axis=-1,\n    )\n\n    # Compute loss from probabilities &amp; actual choices\n    # batch_loss = self.loss(probabilities, c_batch, sample_weight=sample_weight)\n    batch_loss = {\n        \"optimized_loss\": self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n        # \"NegativeLogLikelihood\": tf.keras.losses.CategoricalCrossentropy()(\n        #     y_pred=probabilities,\n        #     y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n        #     sample_weight=sample_weight,\n        # ),\n        \"Exact-NegativeLogLikelihood\": self.exact_nll(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n    }\n    return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>  <code>abstractmethod</code>","text":"<p>Define how the model computes the utility of a product.</p> <p>MUST be implemented in children classe ! For simpler use-cases this is the only method to be user-defined.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Utility of each product for each choice. Shape must be (n_choices, n_items)</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>@abstractmethod\ndef compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Define how the model computes the utility of a product.\n\n    MUST be implemented in children classe !\n    For simpler use-cases this is the only method to be user-defined.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    np.ndarray\n        Utility of each product for each choice.\n        Shape must be (n_choices, n_items)\n    \"\"\"\n    # To be implemented in children classes\n    # Can be NumPy or TensorFlow based\n    return\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.evaluate","title":"<code>evaluate(choice_dataset, sample_weight=None, batch_size=-1, mode='eval')</code>","text":"<p>Evaluate the model for each choice and each product of a ChoiceDataset.</p> <p>Predicts the probabilities according to the model and computes the Negative-Log-Likelihood loss from the actual choices.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset on which to apply to prediction</p> required <p>Returns:</p> Type Description <code>ndarray(n_choices, n_items)</code> <p>Choice probabilties for each choice and each product</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def evaluate(self, choice_dataset, sample_weight=None, batch_size=-1, mode=\"eval\"):\n    \"\"\"Evaluate the model for each choice and each product of a ChoiceDataset.\n\n    Predicts the probabilities according to the model and computes the Negative-Log-Likelihood\n    loss from the actual choices.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset on which to apply to prediction\n\n    Returns\n    -------\n    np.ndarray (n_choices, n_items)\n        Choice probabilties for each choice and each product\n    \"\"\"\n    batch_losses = []\n    if sample_weight is not None:\n        for (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        ), batch_sample_weight in choice_dataset.iter_batch(\n            batch_size=batch_size, sample_weight=sample_weight\n        ):\n            loss, _ = self.batch_predict(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n                sample_weight=batch_sample_weight,\n            )\n            if mode == \"eval\":\n                batch_losses.append(loss[\"Exact-NegativeLogLikelihood\"])\n            elif mode == \"optim\":\n                batch_losses.append(loss[\"optimized_loss\"])\n    else:\n        for (\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        ) in choice_dataset.iter_batch(batch_size=batch_size):\n            loss, _ = self.batch_predict(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n            if mode == \"eval\":\n                batch_losses.append(loss[\"Exact-NegativeLogLikelihood\"])\n            elif mode == \"optim\":\n                batch_losses.append(loss[\"optimized_loss\"])\n    if batch_size != -1:\n        last_batch_size = available_items_by_choice.shape[0]\n        coefficients = tf.concat(\n            [tf.ones(len(batch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n        )\n        batch_losses = tf.multiply(batch_losses, coefficients)\n        batch_loss = tf.reduce_sum(batch_losses) / len(choice_dataset)\n    else:\n        batch_loss = tf.reduce_mean(batch_losses)\n    return batch_loss\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.fit","title":"<code>fit(choice_dataset, sample_weight=None, val_dataset=None, validation_freq=1, verbose=0)</code>","text":"<p>Train the model with a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Input data in the form of a ChoiceDataset</p> required <code>sample_weight</code> <code>ndarray</code> <p>Sample weight to apply, by default None</p> <code>None</code> <code>val_dataset</code> <code>ChoiceDataset or (ChoiceDataset, samples_weight)</code> <p>Test ChoiceDataset to evaluate performances on test at each epoch, by default None</p> <code>None</code> <code>verbose</code> <code>int</code> <p>print level, for debugging, by default 0</p> <code>0</code> <code>epochs</code> <code>int</code> <p>Number of epochs, default is None, meaning we use self.epochs</p> required <code>batch_size</code> <code>int</code> <p>Batch size, default is None, meaning we use self.batch_size</p> required <code>validation_freq</code> <p>Only relevant if validation data is provided. Specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Different metrics values over epochs.</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def fit(\n    self,\n    choice_dataset,\n    sample_weight=None,\n    val_dataset=None,\n    validation_freq=1,\n    verbose=0,\n):\n    \"\"\"Train the model with a ChoiceDataset.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Input data in the form of a ChoiceDataset\n    sample_weight : np.ndarray, optional\n        Sample weight to apply, by default None\n    val_dataset : ChoiceDataset or (ChoiceDataset, samples_weight), optional\n        Test ChoiceDataset to evaluate performances on test at each epoch, by default None\n    verbose : int, optional\n        print level, for debugging, by default 0\n    epochs : int, optional\n        Number of epochs, default is None, meaning we use self.epochs\n    batch_size : int, optional\n        Batch size, default is None, meaning we use self.batch_size\n    validation_freq: int, optional\n        Only relevant if validation data is provided. Specifies how many training epochs\n        to run before a new validation run is performed, e.g. validation_freq=2 runs validation\n        every 2 epochs.\n\n    Returns\n    -------\n    dict:\n        Different metrics values over epochs.\n    \"\"\"\n    if hasattr(self, \"instantiated\"):\n        if not self.instantiated:\n            raise ValueError(\"Model not instantiated. Please call .instantiate() first.\")\n    epochs = self.epochs\n    batch_size = self.batch_size\n\n    losses_history = {\"train_loss\": []}\n    if verbose &gt;= 0 and verbose &lt; 2:\n        t_range = tqdm.trange(epochs, position=0)\n    else:\n        t_range = range(epochs)\n\n    self.callbacks.on_train_begin()\n    # Iterate of epochs\n    for epoch_nb in t_range:\n        if verbose &gt;= 2:\n            print(f\"Start Epoch {epoch_nb}\")\n        self.callbacks.on_epoch_begin(epoch_nb)\n        t_start = time.time()\n        train_logs = {\"train_loss\": []}\n        val_logs = {\"val_loss\": []}\n        epoch_losses = []\n\n        if sample_weight is not None:\n            if verbose &gt; 0:\n                inner_range = tqdm.tqdm(\n                    choice_dataset.iter_batch(\n                        shuffle=True, sample_weight=sample_weight, batch_size=batch_size\n                    ),\n                    total=int(len(choice_dataset) / np.max([1, batch_size])),\n                    position=1,\n                    leave=False,\n                )\n            else:\n                inner_range = choice_dataset.iter_batch(\n                    shuffle=True, sample_weight=sample_weight, batch_size=batch_size\n                )\n\n            for batch_nb, (\n                (\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                ),\n                weight_batch,\n            ) in enumerate(inner_range):\n                self.callbacks.on_train_batch_begin(batch_nb)\n\n                neg_loglikelihood = self.train_step(\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                    sample_weight=weight_batch,\n                )\n\n                train_logs[\"train_loss\"].append(neg_loglikelihood)\n                temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n                self.callbacks.on_train_batch_end(batch_nb, logs=temps_logs)\n\n                # Optimization Steps\n                epoch_losses.append(neg_loglikelihood)\n\n                if verbose &gt; 0:\n                    inner_range.set_description(\n                        f\"Epoch Negative-LogLikeliHood: {np.mean(epoch_losses):.4f}\"\n                    )\n                if self.stop_training:\n                    print(\"Training stopped with early stopping taking effect\")\n                    break\n\n        # In this case we do not need to batch the sample_weights\n        else:\n            if verbose &gt; 0:\n                inner_range = tqdm.tqdm(\n                    choice_dataset.iter_batch(shuffle=True, batch_size=batch_size),\n                    total=int(len(choice_dataset) / np.max([batch_size, 1])),\n                    position=1,\n                    leave=False,\n                )\n            else:\n                inner_range = choice_dataset.iter_batch(shuffle=True, batch_size=batch_size)\n            for batch_nb, (\n                shared_features_batch,\n                items_features_batch,\n                available_items_batch,\n                choices_batch,\n            ) in enumerate(inner_range):\n                self.callbacks.on_train_batch_begin(batch_nb)\n                neg_loglikelihood = self.train_step(\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                )\n                train_logs[\"train_loss\"].append(neg_loglikelihood)\n                temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n                self.callbacks.on_train_batch_end(batch_nb, logs=temps_logs)\n\n                # Optimization Steps\n                epoch_losses.append(neg_loglikelihood)\n\n                if verbose &gt; 0:\n                    inner_range.set_description(\n                        f\"Epoch Negative-LogLikeliHood: {np.mean(epoch_losses):.4f}\"\n                    )\n                if self.stop_training:\n                    print(\"Training stopped with early stopping taking effect\")\n                    break\n\n        # Take into account the fact that the last batch may have a\n        # different length for the computation of the epoch loss.\n        if batch_size != -1:\n            last_batch_size = available_items_batch.shape[0]\n            coefficients = tf.concat(\n                [tf.ones(len(epoch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n            )\n            epoch_losses = tf.multiply(epoch_losses, coefficients)\n            epoch_loss = tf.reduce_sum(epoch_losses) / len(choice_dataset)\n        else:\n            epoch_loss = tf.reduce_mean(epoch_losses)\n        losses_history[\"train_loss\"].append(epoch_loss)\n        print_loss = losses_history[\"train_loss\"][-1].numpy()\n        desc = f\"Epoch {epoch_nb} Train Loss {print_loss:.4f}\"\n        if verbose &gt; 1:\n            print(\n                f\"Loop {epoch_nb} Time:\",\n                f\"{time.time() - t_start:.4f}\",\n                f\"Loss: {print_loss:.4f}\",\n            )\n\n        # Test on val_dataset if provided\n        if val_dataset is not None and ((epoch_nb + 1) % validation_freq) == 0:\n            test_losses = []\n\n            val_samples_weight = None\n            if isinstance(val_dataset, tuple):\n                if not len(val_dataset) == 2:\n                    raise ValueError(\n                        \"\"\"if argument val_dataset is a tuple, it should be\n                        in the form (ChoiceDataset, weights)\"\"\"\n                    )\n                validation_dataset, val_samples_weight = val_dataset\n            elif isinstance(val_dataset, ChoiceDataset):\n                validation_dataset = val_dataset\n            else:\n                raise ValueError(\n                    \"\"\"val_dataset should be a ChoiceDataset or\n                    a tuple of (ChoiceDataset, weights).\"\"\"\n                )\n\n            val_iterator = validation_dataset.iter_batch(\n                shuffle=False, sample_weight=val_samples_weight, batch_size=batch_size\n            )\n\n            for batch_nb, batch_data in enumerate(val_iterator):\n                weight_batch = None\n                if val_samples_weight is not None:\n                    batch_features, weight_batch = batch_data\n                else:\n                    batch_features = batch_data\n\n                (\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                ) = batch_features\n\n                self.callbacks.on_batch_begin(batch_nb)\n                self.callbacks.on_test_batch_begin(batch_nb)\n\n                loss = self.batch_predict(\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                    sample_weight=weight_batch,\n                )[0][\"optimized_loss\"]\n                test_losses.append(loss)\n\n                val_logs[\"val_loss\"].append(test_losses[-1])\n                temps_logs = {k: tf.reduce_mean(v) for k, v in val_logs.items()}\n                self.callbacks.on_test_batch_end(batch_nb, logs=temps_logs)\n\n            test_loss = tf.reduce_mean(test_losses)\n            if verbose &gt; 1:\n                print(\"Test Negative-LogLikelihood:\", test_loss.numpy())\n                desc += f\", Test Loss {np.round(test_loss.numpy(), 4)}\"\n            losses_history[\"val_loss\"] = losses_history.get(\"val_loss\", []) + [\n                test_loss.numpy()\n            ]\n            train_logs = {**train_logs, **val_logs}\n\n        temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n        self.callbacks.on_epoch_end(epoch_nb, logs=temps_logs)\n        if self.stop_training:\n            print(\"Training stopped with early stopping taking effect\")\n            break\n        t_range.set_description(desc)\n        t_range.refresh()\n\n    temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n    self.callbacks.on_train_end(logs=temps_logs)\n    return losses_history\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.get_weights","title":"<code>get_weights()</code>","text":"<p>Return the values of <code>model.trainable_weights</code> as a list of NumPy arrays.</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def get_weights(self) -&gt; list[np.ndarray]:\n    \"\"\"Return the values of `model.trainable_weights` as a list of NumPy arrays.\"\"\"\n    return [v.numpy() for v in self.trainable_weights]\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.load_model","title":"<code>load_model(path)</code>  <code>classmethod</code>","text":"<p>Load a ChoiceModel previously saved with save_model().</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the folder where the saved model files are</p> required <p>Returns:</p> Type Description <code>ChoiceModel</code> <p>Loaded ChoiceModel</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>@classmethod\ndef load_model(cls, path):\n    \"\"\"Load a ChoiceModel previously saved with save_model().\n\n    Parameters\n    ----------\n    path : str\n        path to the folder where the saved model files are\n\n    Returns\n    -------\n    ChoiceModel\n        Loaded ChoiceModel\n    \"\"\"\n    # To improve for non string attributes\n    with open(Path(path) / \"params.json\") as f:\n        params = json.load(f)\n\n    obj = cls(optimizer=params[\"optimizer_name\"])\n    obj._trainable_weights = []\n\n    i = 0\n    weight_path = f\"weight_{i}.npy\"\n    files_list = []\n    for file in Path(path).iterdir():\n        files_list.append(str(file.name))\n    while weight_path in files_list:\n        obj._trainable_weights.append(tf.Variable(np.load(Path(path) / weight_path)))\n        i += 1\n        weight_path = f\"weight_{i}.npy\"\n\n    for k, v in params.items():\n        setattr(obj, k, v)\n\n    if Path.is_dir(Path(path) / \"optimizer\"):\n        with open(Path(path) / \"optimizer\" / \"config.json\") as f:\n            config = json.load(f)\n        # obj.optimizer = tf.keras.optimizers.get(params[\"optimizer_name\"]).from_config(config)\n        obj.optimizer = obj.optimizer.from_config(config)\n        obj.optimizer.build(var_list=obj.trainable_weights)\n\n        with open(Path(path) / \"optimizer\" / \"weights_store.json\") as f:\n            store = json.load(f)\n        for key, value in store.items():\n            store[key] = np.array(value, dtype=np.float32)\n        obj.optimizer.load_own_variables(store)\n\n    # Load optimizer step\n    return obj\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.predict_probas","title":"<code>predict_probas(choice_dataset, batch_size=-1)</code>","text":"<p>Predicts the choice probabilities for each choice and each product of a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset on which to apply to prediction</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for the prediction, by default -1</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray(n_choices, n_items)</code> <p>Choice probabilties for each choice and each product</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def predict_probas(self, choice_dataset, batch_size=-1):\n    \"\"\"Predicts the choice probabilities for each choice and each product of a ChoiceDataset.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset on which to apply to prediction\n    batch_size : int, optional\n        Batch size to use for the prediction, by default -1\n\n    Returns\n    -------\n    np.ndarray (n_choices, n_items)\n        Choice probabilties for each choice and each product\n    \"\"\"\n    stacked_probabilities = []\n    for (\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ) in choice_dataset.iter_batch(batch_size=batch_size):\n        _, probabilities = self.batch_predict(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n        stacked_probabilities.append(probabilities)\n\n    return tf.concat(stacked_probabilities, axis=0)\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.save_model","title":"<code>save_model(path, save_opt=True)</code>","text":"<p>Save the different models on disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the folder where to save the model</p> required Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def save_model(self, path, save_opt=True):\n    \"\"\"Save the different models on disk.\n\n    Parameters\n    ----------\n    path : str\n        path to the folder where to save the model\n    \"\"\"\n    if not os.path.exists(path):\n        Path(path).mkdir(parents=True)\n\n    for i, weight in enumerate(self.trainable_weights):\n        np.save(Path(path) / f\"weight_{i}.npy\", weight.numpy())\n\n    # To improve for non-string attributes\n    params = {}\n    for k, v in self.__dict__.items():\n        if isinstance(v, (int, float, str, dict, tuple)):\n            params[k] = v\n        elif isinstance(v, (list, tuple)):\n            if all(isinstance(item, (int, float, str, dict)) for item in v):\n                params[k] = v\n            elif k != \"_trainable_weights\":\n                logging.warning(\n                    \"\"\"Attribute '%s' is a list with non-serializable\n                     types and will not be saved.\"\"\",\n                    k,\n                )\n    with open(Path(path) / \"params.json\", \"w\") as f:\n        json.dump(params, f)\n\n    # Save optimizer state\n    if save_opt and not isinstance(self.optimizer, str):\n        (Path(path) / \"optimizer\").mkdir(parents=True, exist_ok=True)\n        config = self.optimizer.get_config()\n        weights_store = {}\n        self.optimizer.save_own_variables(weights_store)\n        for key, value in weights_store.items():\n            if isinstance(value, tf.Variable):\n                value = value.numpy()\n            weights_store[key] = value.tolist()\n        if \"learning_rate\" in config.keys():\n            if isinstance(config[\"learning_rate\"], tf.Variable):\n                config[\"learning_rate\"] = config[\"learning_rate\"].numpy()\n            if isinstance(config[\"learning_rate\"], np.float32):\n                config[\"learning_rate\"] = config[\"learning_rate\"].tolist()\n        with open(Path(path) / \"optimizer\" / \"config.json\", \"w\") as f:\n            json.dump(config, f)\n        with open(Path(path) / \"optimizer\" / \"weights_store.json\", \"w\") as f:\n            json.dump(weights_store, f)\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.set_weights","title":"<code>set_weights(weights)</code>","text":"<p>Set the values of <code>model.trainable_weights</code> from a list of NumPy arrays.</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>def set_weights(self, weights: list[np.ndarray]) -&gt; None:\n    \"\"\"Set the values of `model.trainable_weights` from a list of NumPy arrays.\"\"\"\n    layer_weights = self.trainable_weights\n    if len(layer_weights) != len(weights):\n        raise ValueError(\n            f\"You called `set_weights(weights)` on a model \"\n            f\"with a weight list of length {len(weights)}, but the model \"\n            f\"was expecting {len(layer_weights)} weights.\"\n        )\n    for variable, value in zip(layer_weights, weights):\n        if variable.shape != value.shape:\n            raise ValueError(\n                f\"Model weight shape {variable.shape} \"\n                \"is not compatible with provided weight \"\n                f\"shape {value.shape}.\"\n            )\n        variable.assign(value)\n</code></pre>"},{"location":"references/models/references_base_model/#choice_learn.models.base_model.ChoiceModel.train_step","title":"<code>train_step(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one training step (= one gradient descent step) of the model.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Value of NegativeLogLikelihood loss for the batch</p> Source code in <code>choice_learn/models/base_model.py</code> <pre><code>@tf.function\ndef train_step(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor\n        Value of NegativeLogLikelihood loss for the batch\n    \"\"\"\n    with tf.GradientTape() as tape:\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n\n        probabilities = tf_ops.softmax_with_availabilities(\n            items_logit_by_choice=utilities,\n            available_items_by_choice=available_items_by_choice,\n            normalize_exit=self.add_exit_choice,\n            axis=-1,\n        )\n        # Negative Log-Likelihood\n        neg_loglikelihood = self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        )\n        if self.regularization is not None:\n            regularization = tf.reduce_sum(\n                [self.regularizer(w) for w in self.trainable_weights]\n            )\n            neg_loglikelihood += regularization\n\n    grads = tape.gradient(neg_loglikelihood, self.trainable_weights)\n    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n    return neg_loglikelihood\n</code></pre>"},{"location":"references/models/references_baseline_models/","title":"Some baseline models","text":"<p>Models to be used as baselines for choice modeling. Nothing smart here.</p>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.DistribMimickingModel","title":"<code>DistribMimickingModel</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>Dumb class model that mimicks the probabilities.</p> <p>It stores the encountered in the train datasets and always returns them</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>class DistribMimickingModel(ChoiceModel):\n    \"\"\"Dumb class model that mimicks the probabilities.\n\n    It stores the encountered in the train datasets and always returns them\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize of the model.\"\"\"\n        super().__init__(**kwargs)\n        self._trainable_weights = []\n        self.is_fitted = False\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return [self._trainable_weights]\n\n    def fit(self, choice_dataset, *args, **kwargs):\n        \"\"\"Compute the choice frequency of each product and defines it as choice probabilities.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to be used for fitting\n        \"\"\"\n        _ = kwargs\n        _ = args\n        choices = choice_dataset.choices\n        for i in range(choice_dataset.get_n_items()):\n            self._trainable_weights.append(tf.reduce_sum(tf.cast(choices == i, tf.float32)))\n        self._trainable_weights = tf.stack(self._trainable_weights) / len(choices)\n        self.is_fitted = True\n\n    def _fit_with_lbfgs(self, choice_dataset, *args, **kwargs):\n        \"\"\"Compute the choice frequency of each product and defines it as choice probabilities.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to be used for fitting\n        \"\"\"\n        _ = kwargs\n        _ = args\n        choices = choice_dataset.choices\n        for i in range(choice_dataset.get_n_items()):\n            self._trainable_weights.append(tf.reduce_sum(tf.cast(choices == i, tf.float32)))\n        self._trainable_weights = tf.stack(self._trainable_weights) / len(choices)\n        self.is_fitted = True\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Return utility that is fixed. U = log(P).\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        np.ndarray (n_choices, n_items)\n            Utilities\n\n        Raises\n        ------\n        ValueError\n            If the model has not been fitted cannot evaluate the utility\n        \"\"\"\n        # In order to avoid unused arguments warnings\n        _ = items_features_by_choice, shared_features_by_choice, available_items_by_choice\n        if not self.is_fitted:\n            raise ValueError(\"Model not fitted\")\n        return tf.stack([tf.math.log(self.trainable_weights[0])] * len(choices), axis=0)\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.DistribMimickingModel.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.DistribMimickingModel.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize of the model.</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize of the model.\"\"\"\n    super().__init__(**kwargs)\n    self._trainable_weights = []\n    self.is_fitted = False\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.DistribMimickingModel.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Return utility that is fixed. U = log(P).</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>ndarray(n_choices, n_items)</code> <p>Utilities</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been fitted cannot evaluate the utility</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Return utility that is fixed. U = log(P).\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    np.ndarray (n_choices, n_items)\n        Utilities\n\n    Raises\n    ------\n    ValueError\n        If the model has not been fitted cannot evaluate the utility\n    \"\"\"\n    # In order to avoid unused arguments warnings\n    _ = items_features_by_choice, shared_features_by_choice, available_items_by_choice\n    if not self.is_fitted:\n        raise ValueError(\"Model not fitted\")\n    return tf.stack([tf.math.log(self.trainable_weights[0])] * len(choices), axis=0)\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.DistribMimickingModel.fit","title":"<code>fit(choice_dataset, *args, **kwargs)</code>","text":"<p>Compute the choice frequency of each product and defines it as choice probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset to be used for fitting</p> required Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>def fit(self, choice_dataset, *args, **kwargs):\n    \"\"\"Compute the choice frequency of each product and defines it as choice probabilities.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset to be used for fitting\n    \"\"\"\n    _ = kwargs\n    _ = args\n    choices = choice_dataset.choices\n    for i in range(choice_dataset.get_n_items()):\n        self._trainable_weights.append(tf.reduce_sum(tf.cast(choices == i, tf.float32)))\n    self._trainable_weights = tf.stack(self._trainable_weights) / len(choices)\n    self.is_fitted = True\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.RandomChoiceModel","title":"<code>RandomChoiceModel</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>Dumb model that randomly attributes utilities to products.</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>class RandomChoiceModel(ChoiceModel):\n    \"\"\"Dumb model that randomly attributes utilities to products.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize of the model.\"\"\"\n        super().__init__(**kwargs)\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Return an empty list - there is no trainable weight.\"\"\"\n        return []\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute the random utility for each product of each choice.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        tf.Tensor\n            (n_choices, n_items) matrix of random utilities\n        \"\"\"\n        # In order to avoid unused arguments warnings\n        _ = shared_features_by_choice, items_features_by_choice, choices\n        return np.squeeze(\n            np.random.uniform(size=(available_items_by_choice.shape), low=0.0, high=1.0)\n        ).astype(np.float32)\n\n    def fit(self, *args, **kwargs):\n        \"\"\"Make sure that nothing happens during .fit.\"\"\"\n        _ = kwargs\n        _ = args\n        return {}\n\n    def _fit_with_lbfgs(self, *args, **kwargs):\n        \"\"\"Make sure that nothing happens during .fit.\"\"\"\n        _ = kwargs\n        _ = args\n        return {}\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.RandomChoiceModel.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Return an empty list - there is no trainable weight.</p>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.RandomChoiceModel.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize of the model.</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Initialize of the model.\"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.RandomChoiceModel.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute the random utility for each product of each choice.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>(n_choices, n_items) matrix of random utilities</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute the random utility for each product of each choice.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    tf.Tensor\n        (n_choices, n_items) matrix of random utilities\n    \"\"\"\n    # In order to avoid unused arguments warnings\n    _ = shared_features_by_choice, items_features_by_choice, choices\n    return np.squeeze(\n        np.random.uniform(size=(available_items_by_choice.shape), low=0.0, high=1.0)\n    ).astype(np.float32)\n</code></pre>"},{"location":"references/models/references_baseline_models/#choice_learn.models.baseline_models.RandomChoiceModel.fit","title":"<code>fit(*args, **kwargs)</code>","text":"<p>Make sure that nothing happens during .fit.</p> Source code in <code>choice_learn/models/baseline_models.py</code> <pre><code>def fit(self, *args, **kwargs):\n    \"\"\"Make sure that nothing happens during .fit.\"\"\"\n    _ = kwargs\n    _ = args\n    return {}\n</code></pre>"},{"location":"references/models/references_clogit/","title":"Conditional MNL class","text":"<p>Conditional MNL model.</p>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit","title":"<code>ConditionalLogit</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>Conditional MNL that has a generic structure. It can be parametrized with a dictionnary.</p> Arguments: <p>coefficients: dict or MNLCoefficients     Specfication of the model to be estimated.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>class ConditionalLogit(ChoiceModel):\n    \"\"\"Conditional MNL that has a generic structure. It can be parametrized with a dictionnary.\n\n    Arguments:\n    ----------\n    coefficients: dict or MNLCoefficients\n        Specfication of the model to be estimated.\n    \"\"\"\n\n    def __init__(\n        self,\n        coefficients=None,\n        add_exit_choice=False,\n        optimizer=\"lbfgs\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize of Conditional-MNL.\n\n        Parameters\n        ----------\n        coefficients : dict or MNLCoefficients\n            Dictionnary containing the coefficients parametrization of the model.\n            The dictionnary must have the following structure:\n            {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n            mode must be among \"constant\", \"item\", \"item-full\" for now\n            (same specifications as torch-choice).\n        add_exit_choice : bool, optional\n            Whether or not to normalize the probabilities computation with an exit choice\n            whose utility would be 1, by default True\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n        self.coefficients = coefficients\n        self.instantiated = False\n\n    def add_coefficients(\n        self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n    ):\n        \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given in the ChoiceDataset that will be used for parameters estimation.\n        coefficient_name : str, optional\n            Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        self._add_coefficient(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n            shared=False,\n        )\n\n    def add_shared_coefficient(\n        self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n    ):\n        \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given in the ChoiceDataset that will be used for parameters estimation.\n        coefficient_name : str, optional\n            Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        self._add_coefficient(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n            shared=True,\n        )\n\n    def _add_coefficient(self, feature_name, coefficient_name, items_indexes, items_names, shared):\n        if self.coefficients is None:\n            self.coefficients = MNLCoefficients()\n        elif not isinstance(self.coefficients, MNLCoefficients):\n            raise ValueError(\"Cannot add coefficient on top of a dict instantiation.\")\n\n        coefficient_name = coefficient_name if coefficient_name else \"beta_%s\" % feature_name\n        add_method = self.coefficients.add_shared if shared else self.coefficients.add\n        add_method(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n        )\n\n    def instantiate(self, choice_dataset):\n        \"\"\"Instantiate the model using the features in the choice_dataset.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Used to match the features names with the model coefficients.\n        \"\"\"\n        if not self.instantiated:\n            if not isinstance(self.coefficients, MNLCoefficients):\n                self._build_coefficients_from_dict(n_items=choice_dataset.get_n_items())\n            self._trainable_weights = self._instantiate_tf_weights()\n\n            # Checking that no weight has been attributed to non existing feature in dataset\n            dataset_stacked_features_names = []\n            if choice_dataset.shared_features_by_choice_names is not None:\n                for feat_tuple in choice_dataset.shared_features_by_choice_names:\n                    dataset_stacked_features_names.append(feat_tuple)\n            if choice_dataset.items_features_by_choice_names is not None:\n                for feat_tuple in choice_dataset.items_features_by_choice_names:\n                    dataset_stacked_features_names.append(feat_tuple)\n            dataset_stacked_features_names = np.concatenate(dataset_stacked_features_names).ravel()\n\n            for feature_with_weight in self.coefficients.features_with_weights:\n                if feature_with_weight != \"intercept\":\n                    if feature_with_weight not in dataset_stacked_features_names:\n                        raise ValueError(\n                            f\"\"\"Feature {feature_with_weight} has an attributed coefficient\n                            but is not in dataset\"\"\"\n                        )\n            self._store_dataset_features_names(choice_dataset)\n            self.instantiated = True\n\n    def _instantiate_tf_weights(self):\n        \"\"\"Instantiate the model from MNLCoefficients object.\n\n        Returns\n        -------\n        list of tf.Tensor\n            List of the weights created coresponding to the specification.\n        \"\"\"\n        weights = []\n        for weight_nb, weight_name in enumerate(self.coefficients.names):\n            n_weights = (\n                len(self.coefficients.get(weight_name)[\"items_indexes\"])\n                if self.coefficients.get(weight_name)[\"items_indexes\"] is not None\n                else len(self.coefficients.get(weight_name)[\"items_names\"])\n            )\n            weight = tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, n_weights)),\n                name=weight_name,\n            )\n            weights.append(weight)\n            self.coefficients._add_tf_weight(weight_name, weight_nb)\n\n        self._trainable_weights = weights\n\n        return weights\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return self._trainable_weights\n\n    def _build_coefficients_from_dict(self, n_items):\n        \"\"\"Build coefficients when they are given as a dictionnay.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of different items in the assortment. Used to create the right number of weights.\n        \"\"\"\n        coefficients = MNLCoefficients()\n        for weight_counter, (feature, mode) in enumerate(self.coefficients.items()):\n            if mode == \"constant\":\n                coefficients.add_shared(\n                    feature + f\"_w_{weight_counter}\", feature, list(range(n_items))\n                )\n            elif mode == \"item\":\n                coefficients.add(feature + f\"_w_{weight_counter}\", feature, list(range(1, n_items)))\n            elif mode == \"item-full\":\n                coefficients.add(feature + f\"_w_{weight_counter}\", feature, list(range(n_items)))\n            else:\n                raise ValueError(f\"Mode {mode} not recognized.\")\n\n        self.coefficients = coefficients\n\n    def _store_dataset_features_names(self, choice_dataset):\n        \"\"\"Register the name of the features in the dataset. For later use in utility computation.\n\n        Parameters\n        ----------\n        dataset : ChoiceDataset\n            ChoiceDataset used to fit the model.\n        \"\"\"\n        self._shared_features_by_choice_names = choice_dataset.shared_features_by_choice_names\n        self._items_features_by_choice_names = choice_dataset.items_features_by_choice_names\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        verbose=1,\n    ):\n        \"\"\"Compute the utility when the model is constructed from a MNLCoefficients object.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices: np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        verbose : int, optional\n            Parametrization of the logging outputs, by default 1\n\n        Returns\n        -------\n        tf.Tensor\n            Utilities corresponding of shape (n_choices, n_items)\n        \"\"\"\n        _ = choices\n\n        n_items = available_items_by_choice.shape[1]\n        n_choices = available_items_by_choice.shape[0]\n        items_utilities_by_choice = []\n\n        if not isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = (shared_features_by_choice,)\n        if not isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = (items_features_by_choice,)\n\n        # Shared features\n        if self._shared_features_by_choice_names is not None:\n            for i, feat_tuple in enumerate(self._shared_features_by_choice_names):\n                for j, feat in enumerate(feat_tuple):\n                    if feat in self.coefficients.features_with_weights:\n                        (\n                            item_index_list,\n                            weight_index_list,\n                        ) = self.coefficients.get_weight_item_indexes(feat)\n                        for item_index, weight_index in zip(item_index_list, weight_index_list):\n                            partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n                            partial_items_utility_by_choice = [\n                                tf.zeros(n_choices) for _ in range(n_items)\n                            ]\n\n                            for q, idx in enumerate(item_index):\n                                if isinstance(idx, list):\n                                    for k in idx:\n                                        tf.cast(shared_features_by_choice[i][:, j], tf.float32)\n                                        compute = tf.multiply(\n                                            shared_features_by_choice[i][:, j],\n                                            self.trainable_weights[weight_index][:, q],\n                                        )\n                                        partial_items_utility_by_choice[k] += compute\n                                else:\n                                    compute = tf.multiply(\n                                        tf.cast(shared_features_by_choice[i][:, j], tf.float32),\n                                        self.trainable_weights[weight_index][:, q],\n                                    )\n                                    partial_items_utility_by_choice[idx] += compute\n\n                            items_utilities_by_choice.append(\n                                tf.cast(\n                                    tf.stack(partial_items_utility_by_choice, axis=1), tf.float32\n                                )\n                            )\n                    elif verbose &gt; 0:\n                        logging.info(\n                            f\"Feature {feat} is in dataset but has no weight assigned\\\n                                in utility computations\"\n                        )\n\n        # Items features\n        if self._items_features_by_choice_names is not None:\n            for i, feat_tuple in enumerate(self._items_features_by_choice_names):\n                for j, feat in enumerate(feat_tuple):\n                    if feat in self.coefficients.features_with_weights:\n                        (\n                            item_index_list,\n                            weight_index_list,\n                        ) = self.coefficients.get_weight_item_indexes(feat)\n                        for item_index, weight_index in zip(item_index_list, weight_index_list):\n                            partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n\n                            for q, idx in enumerate(item_index):\n                                if isinstance(idx, list):\n                                    for k in idx:\n                                        partial_items_utility_by_choice = tf.concat(\n                                            [\n                                                partial_items_utility_by_choice[:, :k],\n                                                tf.expand_dims(\n                                                    tf.multiply(\n                                                        tf.cast(\n                                                            items_features_by_choice[i][:, k, j],\n                                                            tf.float32,\n                                                        ),\n                                                        self.trainable_weights[weight_index][:, q],\n                                                    ),\n                                                    axis=-1,\n                                                ),\n                                                partial_items_utility_by_choice[:, k + 1 :],\n                                            ],\n                                            axis=1,\n                                        )\n                                else:\n                                    partial_items_utility_by_choice = tf.concat(\n                                        [\n                                            partial_items_utility_by_choice[:, :idx],\n                                            tf.expand_dims(\n                                                tf.multiply(\n                                                    tf.cast(\n                                                        items_features_by_choice[i][:, idx, j],\n                                                        tf.float32,\n                                                    ),\n                                                    self.trainable_weights[weight_index][:, q],\n                                                ),\n                                                axis=-1,\n                                            ),\n                                            partial_items_utility_by_choice[:, idx + 1 :],\n                                        ],\n                                        axis=1,\n                                    )\n\n                            items_utilities_by_choice.append(\n                                tf.cast(partial_items_utility_by_choice, tf.float32)\n                            )\n                    elif verbose &gt; 0:\n                        logging.info(\n                            f\"Feature {feat} is in dataset but has no weight assigned\\\n                                in utility computations\"\n                        )\n\n        if \"intercept\" in self.coefficients.features_with_weights:\n            item_index_list, weight_index_list = self.coefficients.get_weight_item_indexes(\n                \"intercept\"\n            )\n\n            for item_index, weight_index in zip(item_index_list, weight_index_list):\n                partial_items_utility_by_choice = tf.zeros((n_items,))\n                for q, idx in enumerate(item_index):\n                    partial_items_utility_by_choice = tf.concat(\n                        [\n                            partial_items_utility_by_choice[:idx],\n                            self.trainable_weights[weight_index][:, q],\n                            partial_items_utility_by_choice[idx + 1 :],\n                        ],\n                        axis=0,\n                    )\n\n                partial_items_utility_by_choice = tf.stack(\n                    [partial_items_utility_by_choice] * n_choices, axis=0\n                )\n\n                items_utilities_by_choice.append(\n                    tf.cast(partial_items_utility_by_choice, tf.float32)\n                )\n\n        return tf.reduce_sum(items_utilities_by_choice, axis=0)\n\n    def fit(self, choice_dataset, get_report=False, **kwargs):\n        \"\"\"Fit function to estimate the parameters.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        self.instantiate(choice_dataset)\n\n        fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n\n    def _fit_with_lbfgs(\n        self,\n        choice_dataset,\n        sample_weight=None,\n        get_report=False,\n        **kwargs,\n    ):\n        \"\"\"Specific fit function to estimate the parameters with LBFGS.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        sample_weight : int\n            Sample weight to use for the estimation, by default None\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        self.instantiate(choice_dataset)\n\n        fit = super()._fit_with_lbfgs(\n            choice_dataset=choice_dataset,\n            sample_weight=sample_weight,\n            **kwargs,\n        )\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n\n    def compute_report(self, choice_dataset):\n        \"\"\"Compute a report of the estimated weights.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        pandas.DataFrame\n            A DF with estimation, Std Err, z_value and p_value for each coefficient.\n        \"\"\"\n\n        def phi(x):\n            \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n            return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n        weights_std = self.get_weights_std(choice_dataset)\n\n        names = []\n        z_values = []\n        estimations = []\n        p_z = []\n        i = 0\n        for weight in self.trainable_weights:\n            for j in range(weight.shape[1]):\n                if weight.shape[1] &gt; 1:\n                    names.append(f\"{weight.name[:-2]}_{j}\")\n                else:\n                    names.append(f\"{weight.name[:-2]}\")\n                estimations.append(weight.numpy()[0][j])\n                z_values.append(weight.numpy()[0][j] / weights_std[i].numpy())\n                p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n                i += 1\n\n        return pd.DataFrame(\n            {\n                \"Coefficient Name\": names,\n                \"Coefficient Estimation\": estimations,\n                \"Std. Err\": weights_std.numpy(),\n                \"z_value\": z_values,\n                \"P(.&gt;z)\": p_z,\n            },\n        )\n\n    def get_weights_std(self, choice_dataset):\n        \"\"\"Approximates Std Err with Hessian matrix.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        tf.Tensor\n            Estimation of the Std Err for the weights.\n        \"\"\"\n        # Loops of differentiation\n        with tf.GradientTape() as tape_1:\n            with tf.GradientTape(persistent=True) as tape_2:\n                model = self.clone()\n                w = tf.concat(self.trainable_weights, axis=1)\n                tape_2.watch(w)\n                tape_1.watch(w)\n                mw = []\n                index = 0\n                for _w in self.trainable_weights:\n                    mw.append(w[:, index : index + _w.shape[1]])\n                    index += _w.shape[1]\n                model._trainable_weights = mw\n                batch = next(choice_dataset.iter_batch(batch_size=-1))\n                utilities = model.compute_batch_utility(*batch)\n                probabilities = tf.nn.softmax(utilities, axis=-1)\n                loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                    y_pred=probabilities,\n                    y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[1]),\n                )\n            # Compute the Jacobian\n            jacobian = tape_2.jacobian(loss, w)\n        # Compute the Hessian from the Jacobian\n        hessian = tape_1.batch_jacobian(jacobian, w)\n        inv_hessian = tf.linalg.inv(tf.squeeze(hessian))\n        return tf.sqrt([inv_hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n\n    def save_model(self, path):\n        \"\"\"Save the different models on disk.\n\n        Parameters\n        ----------\n        path : str\n            path to the folder where to save the model\n        \"\"\"\n        super().save_model(path)\n        self.coefficients.save_coefficient(path)\n\n    @classmethod\n    def load_model(cls, path):\n        \"\"\"Load a ChoiceModel previously saved with save_model().\n\n        Parameters\n        ----------\n        path : str\n            path to the folder where the saved model files are\n\n        Returns\n        -------\n        ChoiceModel\n            Loaded ChoiceModel\n        \"\"\"\n        obj = super().load_model(path)\n        coefficients = MNLCoefficients.load_coefficient(path)\n        obj.coefficients = coefficients\n        return obj\n\n    def clone(self):\n        \"\"\"Return a clone of the model.\"\"\"\n        clone = ConditionalLogit(\n            coefficients=self.coefficients,\n            add_exit_choice=self.add_exit_choice,\n            optimizer=self.optimizer_name,\n        )\n        if hasattr(self, \"history\"):\n            clone.history = self.history\n        if hasattr(self, \"is_fitted\"):\n            clone.is_fitted = self.is_fitted\n        if hasattr(self, \"instantiated\"):\n            clone.instantiated = self.instantiated\n        clone.loss = self.loss\n        clone.label_smoothing = self.label_smoothing\n        if hasattr(self, \"report\"):\n            clone.report = self.report\n        if hasattr(self, \"trainable_weights\"):\n            clone._trainable_weights = self.trainable_weights\n        if hasattr(self, \"lr\"):\n            clone.lr = self.lr\n        if hasattr(self, \"_shared_features_by_choice_names\"):\n            clone._shared_features_by_choice_names = self._shared_features_by_choice_names\n        if hasattr(self, \"_items_features_by_choice_names\"):\n            clone._items_features_by_choice_names = self._items_features_by_choice_names\n        return clone\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.__init__","title":"<code>__init__(coefficients=None, add_exit_choice=False, optimizer='lbfgs', lr=0.001, **kwargs)</code>","text":"<p>Initialize of Conditional-MNL.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>dict or MNLCoefficients</code> <p>Dictionnary containing the coefficients parametrization of the model. The dictionnary must have the following structure: {feature_name_1: mode_1, feature_name_2: mode_2, ...} mode must be among \"constant\", \"item\", \"item-full\" for now (same specifications as torch-choice).</p> <code>None</code> <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to normalize the probabilities computation with an exit choice whose utility would be 1, by default True</p> <code>False</code> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def __init__(\n    self,\n    coefficients=None,\n    add_exit_choice=False,\n    optimizer=\"lbfgs\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize of Conditional-MNL.\n\n    Parameters\n    ----------\n    coefficients : dict or MNLCoefficients\n        Dictionnary containing the coefficients parametrization of the model.\n        The dictionnary must have the following structure:\n        {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n        mode must be among \"constant\", \"item\", \"item-full\" for now\n        (same specifications as torch-choice).\n    add_exit_choice : bool, optional\n        Whether or not to normalize the probabilities computation with an exit choice\n        whose utility would be 1, by default True\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n    self.coefficients = coefficients\n    self.instantiated = False\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.add_coefficients","title":"<code>add_coefficients(feature_name, coefficient_name='', items_indexes=None, items_names=None)</code>","text":"<p>Add a coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given in the ChoiceDataset that will be used for parameters estimation.</p> required <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient. If not provided, name will be \"beta_feature_name\".</p> <code>''</code> <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def add_coefficients(\n    self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n):\n    \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given in the ChoiceDataset that will be used for parameters estimation.\n    coefficient_name : str, optional\n        Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    self._add_coefficient(\n        coefficient_name=coefficient_name,\n        feature_name=feature_name,\n        items_indexes=items_indexes,\n        items_names=items_names,\n        shared=False,\n    )\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.add_shared_coefficient","title":"<code>add_shared_coefficient(feature_name, coefficient_name='', items_indexes=None, items_names=None)</code>","text":"<p>Add a single, shared coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given in the ChoiceDataset that will be used for parameters estimation.</p> required <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient. If not provided, name will be \"beta_feature_name\".</p> <code>''</code> <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def add_shared_coefficient(\n    self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n):\n    \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given in the ChoiceDataset that will be used for parameters estimation.\n    coefficient_name : str, optional\n        Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    self._add_coefficient(\n        coefficient_name=coefficient_name,\n        feature_name=feature_name,\n        items_indexes=items_indexes,\n        items_names=items_names,\n        shared=True,\n    )\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.clone","title":"<code>clone()</code>","text":"<p>Return a clone of the model.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def clone(self):\n    \"\"\"Return a clone of the model.\"\"\"\n    clone = ConditionalLogit(\n        coefficients=self.coefficients,\n        add_exit_choice=self.add_exit_choice,\n        optimizer=self.optimizer_name,\n    )\n    if hasattr(self, \"history\"):\n        clone.history = self.history\n    if hasattr(self, \"is_fitted\"):\n        clone.is_fitted = self.is_fitted\n    if hasattr(self, \"instantiated\"):\n        clone.instantiated = self.instantiated\n    clone.loss = self.loss\n    clone.label_smoothing = self.label_smoothing\n    if hasattr(self, \"report\"):\n        clone.report = self.report\n    if hasattr(self, \"trainable_weights\"):\n        clone._trainable_weights = self.trainable_weights\n    if hasattr(self, \"lr\"):\n        clone.lr = self.lr\n    if hasattr(self, \"_shared_features_by_choice_names\"):\n        clone._shared_features_by_choice_names = self._shared_features_by_choice_names\n    if hasattr(self, \"_items_features_by_choice_names\"):\n        clone._items_features_by_choice_names = self._items_features_by_choice_names\n    return clone\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, verbose=1)</code>","text":"<p>Compute the utility when the model is constructed from a MNLCoefficients object.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <p>Choices Shape must be (n_choices, )</p> required <code>verbose</code> <code>int</code> <p>Parametrization of the logging outputs, by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Utilities corresponding of shape (n_choices, n_items)</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    verbose=1,\n):\n    \"\"\"Compute the utility when the model is constructed from a MNLCoefficients object.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices: np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    verbose : int, optional\n        Parametrization of the logging outputs, by default 1\n\n    Returns\n    -------\n    tf.Tensor\n        Utilities corresponding of shape (n_choices, n_items)\n    \"\"\"\n    _ = choices\n\n    n_items = available_items_by_choice.shape[1]\n    n_choices = available_items_by_choice.shape[0]\n    items_utilities_by_choice = []\n\n    if not isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = (shared_features_by_choice,)\n    if not isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = (items_features_by_choice,)\n\n    # Shared features\n    if self._shared_features_by_choice_names is not None:\n        for i, feat_tuple in enumerate(self._shared_features_by_choice_names):\n            for j, feat in enumerate(feat_tuple):\n                if feat in self.coefficients.features_with_weights:\n                    (\n                        item_index_list,\n                        weight_index_list,\n                    ) = self.coefficients.get_weight_item_indexes(feat)\n                    for item_index, weight_index in zip(item_index_list, weight_index_list):\n                        partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n                        partial_items_utility_by_choice = [\n                            tf.zeros(n_choices) for _ in range(n_items)\n                        ]\n\n                        for q, idx in enumerate(item_index):\n                            if isinstance(idx, list):\n                                for k in idx:\n                                    tf.cast(shared_features_by_choice[i][:, j], tf.float32)\n                                    compute = tf.multiply(\n                                        shared_features_by_choice[i][:, j],\n                                        self.trainable_weights[weight_index][:, q],\n                                    )\n                                    partial_items_utility_by_choice[k] += compute\n                            else:\n                                compute = tf.multiply(\n                                    tf.cast(shared_features_by_choice[i][:, j], tf.float32),\n                                    self.trainable_weights[weight_index][:, q],\n                                )\n                                partial_items_utility_by_choice[idx] += compute\n\n                        items_utilities_by_choice.append(\n                            tf.cast(\n                                tf.stack(partial_items_utility_by_choice, axis=1), tf.float32\n                            )\n                        )\n                elif verbose &gt; 0:\n                    logging.info(\n                        f\"Feature {feat} is in dataset but has no weight assigned\\\n                            in utility computations\"\n                    )\n\n    # Items features\n    if self._items_features_by_choice_names is not None:\n        for i, feat_tuple in enumerate(self._items_features_by_choice_names):\n            for j, feat in enumerate(feat_tuple):\n                if feat in self.coefficients.features_with_weights:\n                    (\n                        item_index_list,\n                        weight_index_list,\n                    ) = self.coefficients.get_weight_item_indexes(feat)\n                    for item_index, weight_index in zip(item_index_list, weight_index_list):\n                        partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n\n                        for q, idx in enumerate(item_index):\n                            if isinstance(idx, list):\n                                for k in idx:\n                                    partial_items_utility_by_choice = tf.concat(\n                                        [\n                                            partial_items_utility_by_choice[:, :k],\n                                            tf.expand_dims(\n                                                tf.multiply(\n                                                    tf.cast(\n                                                        items_features_by_choice[i][:, k, j],\n                                                        tf.float32,\n                                                    ),\n                                                    self.trainable_weights[weight_index][:, q],\n                                                ),\n                                                axis=-1,\n                                            ),\n                                            partial_items_utility_by_choice[:, k + 1 :],\n                                        ],\n                                        axis=1,\n                                    )\n                            else:\n                                partial_items_utility_by_choice = tf.concat(\n                                    [\n                                        partial_items_utility_by_choice[:, :idx],\n                                        tf.expand_dims(\n                                            tf.multiply(\n                                                tf.cast(\n                                                    items_features_by_choice[i][:, idx, j],\n                                                    tf.float32,\n                                                ),\n                                                self.trainable_weights[weight_index][:, q],\n                                            ),\n                                            axis=-1,\n                                        ),\n                                        partial_items_utility_by_choice[:, idx + 1 :],\n                                    ],\n                                    axis=1,\n                                )\n\n                        items_utilities_by_choice.append(\n                            tf.cast(partial_items_utility_by_choice, tf.float32)\n                        )\n                elif verbose &gt; 0:\n                    logging.info(\n                        f\"Feature {feat} is in dataset but has no weight assigned\\\n                            in utility computations\"\n                    )\n\n    if \"intercept\" in self.coefficients.features_with_weights:\n        item_index_list, weight_index_list = self.coefficients.get_weight_item_indexes(\n            \"intercept\"\n        )\n\n        for item_index, weight_index in zip(item_index_list, weight_index_list):\n            partial_items_utility_by_choice = tf.zeros((n_items,))\n            for q, idx in enumerate(item_index):\n                partial_items_utility_by_choice = tf.concat(\n                    [\n                        partial_items_utility_by_choice[:idx],\n                        self.trainable_weights[weight_index][:, q],\n                        partial_items_utility_by_choice[idx + 1 :],\n                    ],\n                    axis=0,\n                )\n\n            partial_items_utility_by_choice = tf.stack(\n                [partial_items_utility_by_choice] * n_choices, axis=0\n            )\n\n            items_utilities_by_choice.append(\n                tf.cast(partial_items_utility_by_choice, tf.float32)\n            )\n\n    return tf.reduce_sum(items_utilities_by_choice, axis=0)\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.compute_report","title":"<code>compute_report(choice_dataset)</code>","text":"<p>Compute a report of the estimated weights.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DF with estimation, Std Err, z_value and p_value for each coefficient.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def compute_report(self, choice_dataset):\n    \"\"\"Compute a report of the estimated weights.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DF with estimation, Std Err, z_value and p_value for each coefficient.\n    \"\"\"\n\n    def phi(x):\n        \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    weights_std = self.get_weights_std(choice_dataset)\n\n    names = []\n    z_values = []\n    estimations = []\n    p_z = []\n    i = 0\n    for weight in self.trainable_weights:\n        for j in range(weight.shape[1]):\n            if weight.shape[1] &gt; 1:\n                names.append(f\"{weight.name[:-2]}_{j}\")\n            else:\n                names.append(f\"{weight.name[:-2]}\")\n            estimations.append(weight.numpy()[0][j])\n            z_values.append(weight.numpy()[0][j] / weights_std[i].numpy())\n            p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n            i += 1\n\n    return pd.DataFrame(\n        {\n            \"Coefficient Name\": names,\n            \"Coefficient Estimation\": estimations,\n            \"Std. Err\": weights_std.numpy(),\n            \"z_value\": z_values,\n            \"P(.&gt;z)\": p_z,\n        },\n    )\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.fit","title":"<code>fit(choice_dataset, get_report=False, **kwargs)</code>","text":"<p>Fit function to estimate the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Choice dataset to use for the estimation.</p> required <code>get_report</code> <p>Whether or not to compute a report of the estimation, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict with fit history.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def fit(self, choice_dataset, get_report=False, **kwargs):\n    \"\"\"Fit function to estimate the parameters.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Choice dataset to use for the estimation.\n    get_report: bool, optional\n        Whether or not to compute a report of the estimation, by default False\n\n    Returns\n    -------\n    dict\n        dict with fit history.\n    \"\"\"\n    self.instantiate(choice_dataset)\n\n    fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n    if get_report:\n        self.report = self.compute_report(choice_dataset)\n    return fit\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.get_weights_std","title":"<code>get_weights_std(choice_dataset)</code>","text":"<p>Approximates Std Err with Hessian matrix.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Estimation of the Std Err for the weights.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def get_weights_std(self, choice_dataset):\n    \"\"\"Approximates Std Err with Hessian matrix.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    tf.Tensor\n        Estimation of the Std Err for the weights.\n    \"\"\"\n    # Loops of differentiation\n    with tf.GradientTape() as tape_1:\n        with tf.GradientTape(persistent=True) as tape_2:\n            model = self.clone()\n            w = tf.concat(self.trainable_weights, axis=1)\n            tape_2.watch(w)\n            tape_1.watch(w)\n            mw = []\n            index = 0\n            for _w in self.trainable_weights:\n                mw.append(w[:, index : index + _w.shape[1]])\n                index += _w.shape[1]\n            model._trainable_weights = mw\n            batch = next(choice_dataset.iter_batch(batch_size=-1))\n            utilities = model.compute_batch_utility(*batch)\n            probabilities = tf.nn.softmax(utilities, axis=-1)\n            loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[1]),\n            )\n        # Compute the Jacobian\n        jacobian = tape_2.jacobian(loss, w)\n    # Compute the Hessian from the Jacobian\n    hessian = tape_1.batch_jacobian(jacobian, w)\n    inv_hessian = tf.linalg.inv(tf.squeeze(hessian))\n    return tf.sqrt([inv_hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.instantiate","title":"<code>instantiate(choice_dataset)</code>","text":"<p>Instantiate the model using the features in the choice_dataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <p>Used to match the features names with the model coefficients.</p> required Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def instantiate(self, choice_dataset):\n    \"\"\"Instantiate the model using the features in the choice_dataset.\n\n    Parameters\n    ----------\n    choice_dataset: ChoiceDataset\n        Used to match the features names with the model coefficients.\n    \"\"\"\n    if not self.instantiated:\n        if not isinstance(self.coefficients, MNLCoefficients):\n            self._build_coefficients_from_dict(n_items=choice_dataset.get_n_items())\n        self._trainable_weights = self._instantiate_tf_weights()\n\n        # Checking that no weight has been attributed to non existing feature in dataset\n        dataset_stacked_features_names = []\n        if choice_dataset.shared_features_by_choice_names is not None:\n            for feat_tuple in choice_dataset.shared_features_by_choice_names:\n                dataset_stacked_features_names.append(feat_tuple)\n        if choice_dataset.items_features_by_choice_names is not None:\n            for feat_tuple in choice_dataset.items_features_by_choice_names:\n                dataset_stacked_features_names.append(feat_tuple)\n        dataset_stacked_features_names = np.concatenate(dataset_stacked_features_names).ravel()\n\n        for feature_with_weight in self.coefficients.features_with_weights:\n            if feature_with_weight != \"intercept\":\n                if feature_with_weight not in dataset_stacked_features_names:\n                    raise ValueError(\n                        f\"\"\"Feature {feature_with_weight} has an attributed coefficient\n                        but is not in dataset\"\"\"\n                    )\n        self._store_dataset_features_names(choice_dataset)\n        self.instantiated = True\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.load_model","title":"<code>load_model(path)</code>  <code>classmethod</code>","text":"<p>Load a ChoiceModel previously saved with save_model().</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the folder where the saved model files are</p> required <p>Returns:</p> Type Description <code>ChoiceModel</code> <p>Loaded ChoiceModel</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>@classmethod\ndef load_model(cls, path):\n    \"\"\"Load a ChoiceModel previously saved with save_model().\n\n    Parameters\n    ----------\n    path : str\n        path to the folder where the saved model files are\n\n    Returns\n    -------\n    ChoiceModel\n        Loaded ChoiceModel\n    \"\"\"\n    obj = super().load_model(path)\n    coefficients = MNLCoefficients.load_coefficient(path)\n    obj.coefficients = coefficients\n    return obj\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.ConditionalLogit.save_model","title":"<code>save_model(path)</code>","text":"<p>Save the different models on disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the folder where to save the model</p> required Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def save_model(self, path):\n    \"\"\"Save the different models on disk.\n\n    Parameters\n    ----------\n    path : str\n        path to the folder where to save the model\n    \"\"\"\n    super().save_model(path)\n    self.coefficients.save_coefficient(path)\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients","title":"<code>MNLCoefficients</code>","text":"<p>Base class to specify the structure of a cLogit.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>class MNLCoefficients:\n    \"\"\"Base class to specify the structure of a cLogit.\"\"\"\n\n    def __init__(self):\n        \"\"\"Instantiate a MNLCoefficients object.\"\"\"\n        # User interface\n        self.coefficients = {}\n        # Handled by the model\n        self.feature_to_weight = {}\n\n    def add(self, coefficient_name, feature_name, items_indexes=None, items_names=None):\n        \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        coefficient_name : str\n            Name given to the coefficient.\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given in the ChoiceDataset that will be used for parameters estimation.\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        if items_indexes is None and items_names is None:\n            raise ValueError(\"Either items_indexes or items_names must be specified\")\n\n        if isinstance(items_indexes, int):\n            items_indexes = [items_indexes]\n        if isinstance(items_names, str):\n            items_names = [items_names]\n\n        self.coefficients[coefficient_name] = {\n            \"feature_name\": feature_name,\n            \"items_indexes\": items_indexes,\n            \"items_names\": items_names,\n        }\n\n    def add_shared(self, coefficient_name, feature_name, items_indexes=None, items_names=None):\n        \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        coefficient_name : str\n            Name given to the coefficient.\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given in the ChoiceDataset that will be used for parameters estimation.\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        if items_indexes is None and items_names is None:\n            raise ValueError(\"Either items_indexes or items_names must be specified\")\n\n        if not coefficient_name:\n            coefficient_name = f\"beta_{feature_name}\"\n\n        if isinstance(items_indexes, int):\n            logging.warning(\n                \"You have added a single index to a shared coefficient. This is not recommended.\",\n                \"Returning to standard add_coefficients method.\",\n            )\n            self.add_coefficients(coefficient_name, feature_name, items_indexes, items_names)\n\n        if isinstance(items_names, str):\n            logging.warning(\n                \"You have added a single name to a shared coefficient. This is not recommended.\",\n                \"Returning to standard add_coefficients method.\",\n            )\n            self.add_coefficients(coefficient_name, feature_name, items_indexes, items_names)\n\n        self.coefficients[coefficient_name] = {\n            \"feature_name\": feature_name,\n            \"items_indexes\": [items_indexes] if items_indexes is not None else None,\n            \"items_names\": items_names if items_names is not None else None,\n        }\n\n    def get(self, coefficient_name):\n        \"\"\"Getter of a coefficient specification, from its name.\n\n        Parameters\n        ----------\n        coefficient_name : str\n            Name of the coefficient to get.\n\n        Returns\n        -------\n        dict\n            specification of the coefficient.\n        \"\"\"\n        return self.coefficients[coefficient_name]\n\n    def _add_tf_weight(self, weight_name, weight_index):\n        \"\"\"Create the Tensorflow weight corresponding for cLogit.\n\n        Parameters\n        ----------\n        weight_name : str\n            Name of the weight to add.\n        weight_index : int\n            Index of the weight (in the conditionalMNL) to add.\n        \"\"\"\n        if weight_name not in self.coefficients.keys():\n            raise ValueError(f\"Weight {weight_name} not in coefficients\")\n\n        if self.coefficients[weight_name][\"feature_name\"] in self.feature_to_weight.keys():\n            self.feature_to_weight[self.coefficients[weight_name][\"feature_name\"]].append(\n                (\n                    weight_name,\n                    weight_index,\n                )\n            )\n        else:\n            self.feature_to_weight[self.coefficients[weight_name][\"feature_name\"]] = [\n                (\n                    weight_name,\n                    weight_index,\n                ),\n            ]\n\n    @property\n    def features_with_weights(self):\n        \"\"\"Get a list of the features that have a weight to be estimated.\n\n        Returns\n        -------\n        dict.keys\n            List of the features that have a weight to be estimated.\n        \"\"\"\n        return self.feature_to_weight.keys()\n\n    def get_weight_item_indexes(self, feature_name):\n        \"\"\"Get the indexes of the concerned items for a given weight.\n\n        Parameters\n        ----------\n        feature_name : str\n            Features that is concerned by the weight.\n\n        Returns\n        -------\n        list\n            List of indexes of the items concerned by the weight.\n        int\n            The index of the weight in the conditionalMNL weights.\n        \"\"\"\n        weights_info = self.feature_to_weight[feature_name]\n        weight_names = [weight_info[0] for weight_info in weights_info]\n        weight_indexs = [weight_info[1] for weight_info in weights_info]\n        return [\n            self.coefficients[weight_name][\"items_indexes\"] for weight_name in weight_names\n        ], weight_indexs\n\n    @property\n    def names(self):\n        \"\"\"Returns the list of coefficients.\n\n        Returns\n        -------\n        dict keys\n            List of coefficients in the specification.\n        \"\"\"\n        return list(self.coefficients.keys())\n\n    def save_coefficient(self, directory):\n        \"\"\"Save the coefficient on disk.\n\n        Parameters\n        ----------\n        directory : str\n            path to the folder where to save the coefficient\n        \"\"\"\n        if not os.path.exists(directory):\n            Path(directory).mkdir(parents=True)\n\n        params = {}\n        for k, v in self.__dict__.items():\n            if isinstance(v, (int, float, str, dict, tuple)):\n                params[k] = v\n            elif isinstance(v, (list, tuple)):\n                if all(isinstance(item, (int, float, str, dict)) for item in v):\n                    params[k] = v\n                else:\n                    logging.warning(\n                        \"\"\"Attribute '%s' is a list with non-serializable\n                        types and will not be saved.\"\"\",\n                        k,\n                    )\n        with open(os.path.join(directory, \"coefficient_params.json\"), \"w\") as f:\n            json.dump(params, f)\n\n    @classmethod\n    def load_coefficient(cls, path):\n        \"\"\"Load a MNLCoefficient previously saved with save_coefficient().\n\n        Parameters\n        ----------\n        path : str\n            path to the folder where the saved coefficient files are\n\n        Returns\n        -------\n        MNLCoefficient\n            Loaded MNLCoefficient\n        \"\"\"\n        obj = cls()\n\n        # To improve for non string attributes\n        with open(Path(path) / \"coefficient_params.json\") as f:\n            params = json.load(f)\n        for k, v in params.items():\n            setattr(obj, k, v)\n        return obj\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.features_with_weights","title":"<code>features_with_weights</code>  <code>property</code>","text":"<p>Get a list of the features that have a weight to be estimated.</p> <p>Returns:</p> Type Description <code>keys</code> <p>List of the features that have a weight to be estimated.</p>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.names","title":"<code>names</code>  <code>property</code>","text":"<p>Returns the list of coefficients.</p> <p>Returns:</p> Type Description <code>dict keys</code> <p>List of coefficients in the specification.</p>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.__init__","title":"<code>__init__()</code>","text":"<p>Instantiate a MNLCoefficients object.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def __init__(self):\n    \"\"\"Instantiate a MNLCoefficients object.\"\"\"\n    # User interface\n    self.coefficients = {}\n    # Handled by the model\n    self.feature_to_weight = {}\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.add","title":"<code>add(coefficient_name, feature_name, items_indexes=None, items_names=None)</code>","text":"<p>Add a coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient.</p> required <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given in the ChoiceDataset that will be used for parameters estimation.</p> required <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def add(self, coefficient_name, feature_name, items_indexes=None, items_names=None):\n    \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    coefficient_name : str\n        Name given to the coefficient.\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given in the ChoiceDataset that will be used for parameters estimation.\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    if items_indexes is None and items_names is None:\n        raise ValueError(\"Either items_indexes or items_names must be specified\")\n\n    if isinstance(items_indexes, int):\n        items_indexes = [items_indexes]\n    if isinstance(items_names, str):\n        items_names = [items_names]\n\n    self.coefficients[coefficient_name] = {\n        \"feature_name\": feature_name,\n        \"items_indexes\": items_indexes,\n        \"items_names\": items_names,\n    }\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.add_shared","title":"<code>add_shared(coefficient_name, feature_name, items_indexes=None, items_names=None)</code>","text":"<p>Add a single, shared coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient.</p> required <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given in the ChoiceDataset that will be used for parameters estimation.</p> required <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def add_shared(self, coefficient_name, feature_name, items_indexes=None, items_names=None):\n    \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    coefficient_name : str\n        Name given to the coefficient.\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given in the ChoiceDataset that will be used for parameters estimation.\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    if items_indexes is None and items_names is None:\n        raise ValueError(\"Either items_indexes or items_names must be specified\")\n\n    if not coefficient_name:\n        coefficient_name = f\"beta_{feature_name}\"\n\n    if isinstance(items_indexes, int):\n        logging.warning(\n            \"You have added a single index to a shared coefficient. This is not recommended.\",\n            \"Returning to standard add_coefficients method.\",\n        )\n        self.add_coefficients(coefficient_name, feature_name, items_indexes, items_names)\n\n    if isinstance(items_names, str):\n        logging.warning(\n            \"You have added a single name to a shared coefficient. This is not recommended.\",\n            \"Returning to standard add_coefficients method.\",\n        )\n        self.add_coefficients(coefficient_name, feature_name, items_indexes, items_names)\n\n    self.coefficients[coefficient_name] = {\n        \"feature_name\": feature_name,\n        \"items_indexes\": [items_indexes] if items_indexes is not None else None,\n        \"items_names\": items_names if items_names is not None else None,\n    }\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.get","title":"<code>get(coefficient_name)</code>","text":"<p>Getter of a coefficient specification, from its name.</p> <p>Parameters:</p> Name Type Description Default <code>coefficient_name</code> <code>str</code> <p>Name of the coefficient to get.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>specification of the coefficient.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def get(self, coefficient_name):\n    \"\"\"Getter of a coefficient specification, from its name.\n\n    Parameters\n    ----------\n    coefficient_name : str\n        Name of the coefficient to get.\n\n    Returns\n    -------\n    dict\n        specification of the coefficient.\n    \"\"\"\n    return self.coefficients[coefficient_name]\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.get_weight_item_indexes","title":"<code>get_weight_item_indexes(feature_name)</code>","text":"<p>Get the indexes of the concerned items for a given weight.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Features that is concerned by the weight.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of indexes of the items concerned by the weight.</p> <code>int</code> <p>The index of the weight in the conditionalMNL weights.</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def get_weight_item_indexes(self, feature_name):\n    \"\"\"Get the indexes of the concerned items for a given weight.\n\n    Parameters\n    ----------\n    feature_name : str\n        Features that is concerned by the weight.\n\n    Returns\n    -------\n    list\n        List of indexes of the items concerned by the weight.\n    int\n        The index of the weight in the conditionalMNL weights.\n    \"\"\"\n    weights_info = self.feature_to_weight[feature_name]\n    weight_names = [weight_info[0] for weight_info in weights_info]\n    weight_indexs = [weight_info[1] for weight_info in weights_info]\n    return [\n        self.coefficients[weight_name][\"items_indexes\"] for weight_name in weight_names\n    ], weight_indexs\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.load_coefficient","title":"<code>load_coefficient(path)</code>  <code>classmethod</code>","text":"<p>Load a MNLCoefficient previously saved with save_coefficient().</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to the folder where the saved coefficient files are</p> required <p>Returns:</p> Type Description <code>MNLCoefficient</code> <p>Loaded MNLCoefficient</p> Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>@classmethod\ndef load_coefficient(cls, path):\n    \"\"\"Load a MNLCoefficient previously saved with save_coefficient().\n\n    Parameters\n    ----------\n    path : str\n        path to the folder where the saved coefficient files are\n\n    Returns\n    -------\n    MNLCoefficient\n        Loaded MNLCoefficient\n    \"\"\"\n    obj = cls()\n\n    # To improve for non string attributes\n    with open(Path(path) / \"coefficient_params.json\") as f:\n        params = json.load(f)\n    for k, v in params.items():\n        setattr(obj, k, v)\n    return obj\n</code></pre>"},{"location":"references/models/references_clogit/#choice_learn.models.conditional_logit.MNLCoefficients.save_coefficient","title":"<code>save_coefficient(directory)</code>","text":"<p>Save the coefficient on disk.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>path to the folder where to save the coefficient</p> required Source code in <code>choice_learn/models/conditional_logit.py</code> <pre><code>def save_coefficient(self, directory):\n    \"\"\"Save the coefficient on disk.\n\n    Parameters\n    ----------\n    directory : str\n        path to the folder where to save the coefficient\n    \"\"\"\n    if not os.path.exists(directory):\n        Path(directory).mkdir(parents=True)\n\n    params = {}\n    for k, v in self.__dict__.items():\n        if isinstance(v, (int, float, str, dict, tuple)):\n            params[k] = v\n        elif isinstance(v, (list, tuple)):\n            if all(isinstance(item, (int, float, str, dict)) for item in v):\n                params[k] = v\n            else:\n                logging.warning(\n                    \"\"\"Attribute '%s' is a list with non-serializable\n                    types and will not be saved.\"\"\",\n                    k,\n                )\n    with open(os.path.join(directory, \"coefficient_params.json\"), \"w\") as f:\n        json.dump(params, f)\n</code></pre>"},{"location":"references/models/references_halo_mnl/","title":"Halo MNl and Low-Rank Halo MNL","text":"<p>Halo MNL model.</p>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.HaloMNL","title":"<code>HaloMNL</code>","text":"<p>             Bases: <code>SimpleMNL</code></p> <p>Implementation of Low Rank Halo MNL model.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>class HaloMNL(SimpleMNL):\n    \"\"\"Implementation of Low Rank Halo MNL model.\"\"\"\n\n    def __init__(\n        self,\n        add_exit_choice=False,\n        intercept=None,\n        optimizer=\"lbfgs\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize of Simple-MNL.\n\n        Parameters\n        ----------\n        add_exit_choice : bool, optional\n            Whether or not to normalize the probabilities computation with an exit choice\n            whose utility would be 1, by default True\n        intercept: str, optional\n            Type of intercept to use, by default None\n        optimizer: str\n            TensorFlow optimizer to be used for estimation\n        lr: float\n            Learning Rate to be used with optimizer.\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n\n        self.instantiated = False\n        self.intercept = intercept\n\n    def instantiate(self, n_items, n_shared_features, n_items_features):\n        \"\"\"Instantiate the model from ModelSpecification object.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of items/aternatives to consider.\n        n_shared_features : int\n            Number of contexts features\n        n_items_features : int\n            Number of contexts items features\n\n        Returns\n        -------\n        list of tf.Tensor\n            List of the weights created coresponding to the specification.\n        \"\"\"\n        indexes, weights = super().instantiate(n_items, n_shared_features, n_items_features)\n\n        halo_matrix = tf.Variable((tf.random.normal((n_items, n_items))), name=\"halo_matrix\")\n        self.zero_diag = tf.zeros(n_items)\n        # halo_matrix = tf.linalg.set_diag(halo_matrix, self.zero_diag)\n        weights += [halo_matrix]\n\n        self.instantiated = True\n        self.indexes = indexes\n        self._trainable_weights = weights\n        return indexes, weights\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute the utility of the model. Selects the right method to compute.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        tf.Tensor\n            Computed utilities of shape (n_choices, n_items).\n        \"\"\"\n        items_utilities = super().compute_batch_utility(\n            shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices\n        )\n        halo = tf.linalg.matmul(\n            available_items_by_choice,\n            tf.linalg.set_diag(self.trainable_weights[-1], self.zero_diag),\n        )\n        return items_utilities + halo\n\n    def get_weights_std(self, choice_dataset):\n        \"\"\"Approximates Std Err with Hessian matrix.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        tf.Tensor\n            Estimation of the Std Err for the weights.\n        \"\"\"\n        # Loops of differentiation\n        with tf.GradientTape() as tape_1:\n            with tf.GradientTape(persistent=True) as tape_2:\n                model = self.clone()\n                w = tf.concat(self.trainable_weights[:-1], axis=0)\n                tape_2.watch(w)\n                tape_1.watch(w)\n                mw = []\n                index = 0\n                for _w in self.trainable_weights:\n                    mw.append(w[index : index + _w.shape[0]])\n                    index += _w.shape[0]\n                model._trainable_weights = mw + [\n                    self.trainable_weights[-1],\n                ]\n                for batch in choice_dataset.iter_batch(batch_size=-1):\n                    utilities = model.compute_batch_utility(*batch)\n                    probabilities = tf.nn.softmax(utilities, axis=-1)\n                    loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                        y_pred=probabilities,\n                        y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[-1]),\n                    )\n            # Compute the Jacobian\n            jacobian = tape_2.jacobian(loss, w)\n        # Compute the Hessian from the Jacobian\n        hessian = tape_1.jacobian(jacobian, w)\n        hessian = tf.linalg.inv(tf.squeeze(hessian))\n        return tf.sqrt([hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n\n    def compute_report(self, choice_dataset):\n        \"\"\"Compute a report of the estimated weights.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        pandas.DataFrame\n            A DF with estimation, Std Err, z_value and p_value for each coefficient.\n        \"\"\"\n\n        def phi(x):\n            \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n            return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n        weights_std = self.get_weights_std(choice_dataset)\n\n        names = []\n        z_values = []\n        estimations = []\n        p_z = []\n        i = 0\n        for weight in self.trainable_weights[:-1]:\n            for j in range(weight.shape[0]):\n                if weight.shape[0] &gt; 1:\n                    names.append(f\"{weight.name[:-2]}_{j}\")\n                else:\n                    names.append(f\"{weight.name[:-2]}\")\n                estimations.append(weight.numpy()[j])\n                z_values.append(weight.numpy()[j] / weights_std[i].numpy())\n                p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n                i += 1\n\n        return pd.DataFrame(\n            {\n                \"Coefficient Name\": names,\n                \"Coefficient Estimation\": estimations,\n                \"Std. Err\": weights_std.numpy(),\n                \"z_value\": z_values,\n                \"P(.&gt;z)\": p_z,\n            },\n        )\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.HaloMNL.__init__","title":"<code>__init__(add_exit_choice=False, intercept=None, optimizer='lbfgs', lr=0.001, **kwargs)</code>","text":"<p>Initialize of Simple-MNL.</p> <p>Parameters:</p> Name Type Description Default <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to normalize the probabilities computation with an exit choice whose utility would be 1, by default True</p> <code>False</code> <code>intercept</code> <p>Type of intercept to use, by default None</p> <code>None</code> <code>optimizer</code> <p>TensorFlow optimizer to be used for estimation</p> <code>'lbfgs'</code> <code>lr</code> <p>Learning Rate to be used with optimizer.</p> <code>0.001</code> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def __init__(\n    self,\n    add_exit_choice=False,\n    intercept=None,\n    optimizer=\"lbfgs\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize of Simple-MNL.\n\n    Parameters\n    ----------\n    add_exit_choice : bool, optional\n        Whether or not to normalize the probabilities computation with an exit choice\n        whose utility would be 1, by default True\n    intercept: str, optional\n        Type of intercept to use, by default None\n    optimizer: str\n        TensorFlow optimizer to be used for estimation\n    lr: float\n        Learning Rate to be used with optimizer.\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n\n    self.instantiated = False\n    self.intercept = intercept\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.HaloMNL.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute the utility of the model. Selects the right method to compute.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed utilities of shape (n_choices, n_items).</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute the utility of the model. Selects the right method to compute.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    tf.Tensor\n        Computed utilities of shape (n_choices, n_items).\n    \"\"\"\n    items_utilities = super().compute_batch_utility(\n        shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices\n    )\n    halo = tf.linalg.matmul(\n        available_items_by_choice,\n        tf.linalg.set_diag(self.trainable_weights[-1], self.zero_diag),\n    )\n    return items_utilities + halo\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.HaloMNL.compute_report","title":"<code>compute_report(choice_dataset)</code>","text":"<p>Compute a report of the estimated weights.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DF with estimation, Std Err, z_value and p_value for each coefficient.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def compute_report(self, choice_dataset):\n    \"\"\"Compute a report of the estimated weights.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DF with estimation, Std Err, z_value and p_value for each coefficient.\n    \"\"\"\n\n    def phi(x):\n        \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    weights_std = self.get_weights_std(choice_dataset)\n\n    names = []\n    z_values = []\n    estimations = []\n    p_z = []\n    i = 0\n    for weight in self.trainable_weights[:-1]:\n        for j in range(weight.shape[0]):\n            if weight.shape[0] &gt; 1:\n                names.append(f\"{weight.name[:-2]}_{j}\")\n            else:\n                names.append(f\"{weight.name[:-2]}\")\n            estimations.append(weight.numpy()[j])\n            z_values.append(weight.numpy()[j] / weights_std[i].numpy())\n            p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n            i += 1\n\n    return pd.DataFrame(\n        {\n            \"Coefficient Name\": names,\n            \"Coefficient Estimation\": estimations,\n            \"Std. Err\": weights_std.numpy(),\n            \"z_value\": z_values,\n            \"P(.&gt;z)\": p_z,\n        },\n    )\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.HaloMNL.get_weights_std","title":"<code>get_weights_std(choice_dataset)</code>","text":"<p>Approximates Std Err with Hessian matrix.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Estimation of the Std Err for the weights.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def get_weights_std(self, choice_dataset):\n    \"\"\"Approximates Std Err with Hessian matrix.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    tf.Tensor\n        Estimation of the Std Err for the weights.\n    \"\"\"\n    # Loops of differentiation\n    with tf.GradientTape() as tape_1:\n        with tf.GradientTape(persistent=True) as tape_2:\n            model = self.clone()\n            w = tf.concat(self.trainable_weights[:-1], axis=0)\n            tape_2.watch(w)\n            tape_1.watch(w)\n            mw = []\n            index = 0\n            for _w in self.trainable_weights:\n                mw.append(w[index : index + _w.shape[0]])\n                index += _w.shape[0]\n            model._trainable_weights = mw + [\n                self.trainable_weights[-1],\n            ]\n            for batch in choice_dataset.iter_batch(batch_size=-1):\n                utilities = model.compute_batch_utility(*batch)\n                probabilities = tf.nn.softmax(utilities, axis=-1)\n                loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                    y_pred=probabilities,\n                    y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[-1]),\n                )\n        # Compute the Jacobian\n        jacobian = tape_2.jacobian(loss, w)\n    # Compute the Hessian from the Jacobian\n    hessian = tape_1.jacobian(jacobian, w)\n    hessian = tf.linalg.inv(tf.squeeze(hessian))\n    return tf.sqrt([hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.HaloMNL.instantiate","title":"<code>instantiate(n_items, n_shared_features, n_items_features)</code>","text":"<p>Instantiate the model from ModelSpecification object.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items/aternatives to consider.</p> required <code>n_shared_features</code> <code>int</code> <p>Number of contexts features</p> required <code>n_items_features</code> <code>int</code> <p>Number of contexts items features</p> required <p>Returns:</p> Type Description <code>list of tf.Tensor</code> <p>List of the weights created coresponding to the specification.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def instantiate(self, n_items, n_shared_features, n_items_features):\n    \"\"\"Instantiate the model from ModelSpecification object.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of items/aternatives to consider.\n    n_shared_features : int\n        Number of contexts features\n    n_items_features : int\n        Number of contexts items features\n\n    Returns\n    -------\n    list of tf.Tensor\n        List of the weights created coresponding to the specification.\n    \"\"\"\n    indexes, weights = super().instantiate(n_items, n_shared_features, n_items_features)\n\n    halo_matrix = tf.Variable((tf.random.normal((n_items, n_items))), name=\"halo_matrix\")\n    self.zero_diag = tf.zeros(n_items)\n    # halo_matrix = tf.linalg.set_diag(halo_matrix, self.zero_diag)\n    weights += [halo_matrix]\n\n    self.instantiated = True\n    self.indexes = indexes\n    self._trainable_weights = weights\n    return indexes, weights\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.LowRankHaloMNL","title":"<code>LowRankHaloMNL</code>","text":"<p>             Bases: <code>SimpleMNL</code></p> <p>Implementation of Low Rank Halo MNL model.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>class LowRankHaloMNL(SimpleMNL):\n    \"\"\"Implementation of Low Rank Halo MNL model.\"\"\"\n\n    def __init__(\n        self,\n        halo_latent_dim,\n        add_exit_choice=False,\n        intercept=None,\n        optimizer=\"lbfgs\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize of Simple-MNL.\n\n        Parameters\n        ----------\n        add_exit_choice : bool, optional\n            Whether or not to normalize the probabilities computation with an exit choice\n            whose utility would be 1, by default True\n        intercept: str, optional\n            Type of intercept to use, by default None\n        optimizer: str\n            TensorFlow optimizer to be used for estimation\n        lr: float\n            Learning Rate to be used with optimizer.\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n\n        self.halo_latent_dim = halo_latent_dim\n        self.instantiated = False\n        self.intercept = intercept\n\n    def instantiate(self, n_items, n_shared_features, n_items_features):\n        \"\"\"Instantiate the model from ModelSpecification object.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of items/aternatives to consider.\n        n_shared_features : int\n            Number of contexts features\n        n_items_features : int\n            Number of contexts items features\n\n        Returns\n        -------\n        list of tf.Tensor\n            List of the weights created coresponding to the specification.\n        \"\"\"\n        indexes, weights = super().instantiate(n_items, n_shared_features, n_items_features)\n\n        u_mat = tf.Variable((tf.random.normal((n_items, self.halo_latent_dim))), name=\"U\")\n        v_mat = tf.Variable((tf.random.normal((self.halo_latent_dim, n_items))), name=\"V\")\n        weights += [u_mat, v_mat]\n\n        self.zero_diag = tf.zeros(n_items)\n        self.instantiated = True\n        self.indexes = indexes\n        self._trainable_weights = weights\n        return indexes, weights\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute the utility of the model. Selects the right method to compute.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        tf.Tensor\n            Computed utilities of shape (n_choices, n_items).\n        \"\"\"\n        items_utilities = super().compute_batch_utility(\n            shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices\n        )\n\n        halo = tf.linalg.matmul(self.trainable_weights[-2], self.trainable_weights[-1])\n        halo = tf.linalg.set_diag(halo, self.zero_diag)\n        halo = tf.linalg.matmul(available_items_by_choice, halo)\n        return items_utilities + halo\n\n    def get_weights_std(self, choice_dataset):\n        \"\"\"Approximates Std Err with Hessian matrix.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        tf.Tensor\n            Estimation of the Std Err for the weights.\n        \"\"\"\n        # Loops of differentiation\n        with tf.GradientTape() as tape_1:\n            with tf.GradientTape(persistent=True) as tape_2:\n                model = self.clone()\n                w = tf.concat(self.trainable_weights[:-2], axis=0)\n                tape_2.watch(w)\n                tape_1.watch(w)\n                mw = []\n                index = 0\n                for _w in self.trainable_weights:\n                    mw.append(w[index : index + _w.shape[0]])\n                    index += _w.shape[0]\n                model._trainable_weights = mw + [\n                    self.trainable_weights[-2],\n                    self.trainable_weights[-1],\n                ]\n                for batch in choice_dataset.iter_batch(batch_size=-1):\n                    utilities = model.compute_batch_utility(*batch)\n                    probabilities = tf.nn.softmax(utilities, axis=-1)\n                    loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                        y_pred=probabilities,\n                        y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[-1]),\n                    )\n            # Compute the Jacobian\n            jacobian = tape_2.jacobian(loss, w)\n        # Compute the Hessian from the Jacobian\n        hessian = tape_1.jacobian(jacobian, w)\n        hessian = tf.linalg.inv(tf.squeeze(hessian))\n        return tf.sqrt([hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n\n    def compute_report(self, choice_dataset):\n        \"\"\"Compute a report of the estimated weights.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        pandas.DataFrame\n            A DF with estimation, Std Err, z_value and p_value for each coefficient.\n        \"\"\"\n\n        def phi(x):\n            \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n            return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n        weights_std = self.get_weights_std(choice_dataset)\n\n        names = []\n        z_values = []\n        estimations = []\n        p_z = []\n        i = 0\n        for weight in self.trainable_weights[:-2]:\n            for j in range(weight.shape[0]):\n                if weight.shape[0] &gt; 1:\n                    names.append(f\"{weight.name[:-2]}_{j}\")\n                else:\n                    names.append(f\"{weight.name[:-2]}\")\n                estimations.append(weight.numpy()[j])\n                z_values.append(weight.numpy()[j] / weights_std[i].numpy())\n                p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n                i += 1\n\n        return pd.DataFrame(\n            {\n                \"Coefficient Name\": names,\n                \"Coefficient Estimation\": estimations,\n                \"Std. Err\": weights_std.numpy(),\n                \"z_value\": z_values,\n                \"P(.&gt;z)\": p_z,\n            },\n        )\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.LowRankHaloMNL.__init__","title":"<code>__init__(halo_latent_dim, add_exit_choice=False, intercept=None, optimizer='lbfgs', lr=0.001, **kwargs)</code>","text":"<p>Initialize of Simple-MNL.</p> <p>Parameters:</p> Name Type Description Default <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to normalize the probabilities computation with an exit choice whose utility would be 1, by default True</p> <code>False</code> <code>intercept</code> <p>Type of intercept to use, by default None</p> <code>None</code> <code>optimizer</code> <p>TensorFlow optimizer to be used for estimation</p> <code>'lbfgs'</code> <code>lr</code> <p>Learning Rate to be used with optimizer.</p> <code>0.001</code> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def __init__(\n    self,\n    halo_latent_dim,\n    add_exit_choice=False,\n    intercept=None,\n    optimizer=\"lbfgs\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize of Simple-MNL.\n\n    Parameters\n    ----------\n    add_exit_choice : bool, optional\n        Whether or not to normalize the probabilities computation with an exit choice\n        whose utility would be 1, by default True\n    intercept: str, optional\n        Type of intercept to use, by default None\n    optimizer: str\n        TensorFlow optimizer to be used for estimation\n    lr: float\n        Learning Rate to be used with optimizer.\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n\n    self.halo_latent_dim = halo_latent_dim\n    self.instantiated = False\n    self.intercept = intercept\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.LowRankHaloMNL.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute the utility of the model. Selects the right method to compute.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed utilities of shape (n_choices, n_items).</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute the utility of the model. Selects the right method to compute.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    tf.Tensor\n        Computed utilities of shape (n_choices, n_items).\n    \"\"\"\n    items_utilities = super().compute_batch_utility(\n        shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices\n    )\n\n    halo = tf.linalg.matmul(self.trainable_weights[-2], self.trainable_weights[-1])\n    halo = tf.linalg.set_diag(halo, self.zero_diag)\n    halo = tf.linalg.matmul(available_items_by_choice, halo)\n    return items_utilities + halo\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.LowRankHaloMNL.compute_report","title":"<code>compute_report(choice_dataset)</code>","text":"<p>Compute a report of the estimated weights.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DF with estimation, Std Err, z_value and p_value for each coefficient.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def compute_report(self, choice_dataset):\n    \"\"\"Compute a report of the estimated weights.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DF with estimation, Std Err, z_value and p_value for each coefficient.\n    \"\"\"\n\n    def phi(x):\n        \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    weights_std = self.get_weights_std(choice_dataset)\n\n    names = []\n    z_values = []\n    estimations = []\n    p_z = []\n    i = 0\n    for weight in self.trainable_weights[:-2]:\n        for j in range(weight.shape[0]):\n            if weight.shape[0] &gt; 1:\n                names.append(f\"{weight.name[:-2]}_{j}\")\n            else:\n                names.append(f\"{weight.name[:-2]}\")\n            estimations.append(weight.numpy()[j])\n            z_values.append(weight.numpy()[j] / weights_std[i].numpy())\n            p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n            i += 1\n\n    return pd.DataFrame(\n        {\n            \"Coefficient Name\": names,\n            \"Coefficient Estimation\": estimations,\n            \"Std. Err\": weights_std.numpy(),\n            \"z_value\": z_values,\n            \"P(.&gt;z)\": p_z,\n        },\n    )\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.LowRankHaloMNL.get_weights_std","title":"<code>get_weights_std(choice_dataset)</code>","text":"<p>Approximates Std Err with Hessian matrix.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Estimation of the Std Err for the weights.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def get_weights_std(self, choice_dataset):\n    \"\"\"Approximates Std Err with Hessian matrix.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    tf.Tensor\n        Estimation of the Std Err for the weights.\n    \"\"\"\n    # Loops of differentiation\n    with tf.GradientTape() as tape_1:\n        with tf.GradientTape(persistent=True) as tape_2:\n            model = self.clone()\n            w = tf.concat(self.trainable_weights[:-2], axis=0)\n            tape_2.watch(w)\n            tape_1.watch(w)\n            mw = []\n            index = 0\n            for _w in self.trainable_weights:\n                mw.append(w[index : index + _w.shape[0]])\n                index += _w.shape[0]\n            model._trainable_weights = mw + [\n                self.trainable_weights[-2],\n                self.trainable_weights[-1],\n            ]\n            for batch in choice_dataset.iter_batch(batch_size=-1):\n                utilities = model.compute_batch_utility(*batch)\n                probabilities = tf.nn.softmax(utilities, axis=-1)\n                loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                    y_pred=probabilities,\n                    y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[-1]),\n                )\n        # Compute the Jacobian\n        jacobian = tape_2.jacobian(loss, w)\n    # Compute the Hessian from the Jacobian\n    hessian = tape_1.jacobian(jacobian, w)\n    hessian = tf.linalg.inv(tf.squeeze(hessian))\n    return tf.sqrt([hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n</code></pre>"},{"location":"references/models/references_halo_mnl/#choice_learn.models.halo_mnl.LowRankHaloMNL.instantiate","title":"<code>instantiate(n_items, n_shared_features, n_items_features)</code>","text":"<p>Instantiate the model from ModelSpecification object.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items/aternatives to consider.</p> required <code>n_shared_features</code> <code>int</code> <p>Number of contexts features</p> required <code>n_items_features</code> <code>int</code> <p>Number of contexts items features</p> required <p>Returns:</p> Type Description <code>list of tf.Tensor</code> <p>List of the weights created coresponding to the specification.</p> Source code in <code>choice_learn/models/halo_mnl.py</code> <pre><code>def instantiate(self, n_items, n_shared_features, n_items_features):\n    \"\"\"Instantiate the model from ModelSpecification object.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of items/aternatives to consider.\n    n_shared_features : int\n        Number of contexts features\n    n_items_features : int\n        Number of contexts items features\n\n    Returns\n    -------\n    list of tf.Tensor\n        List of the weights created coresponding to the specification.\n    \"\"\"\n    indexes, weights = super().instantiate(n_items, n_shared_features, n_items_features)\n\n    u_mat = tf.Variable((tf.random.normal((n_items, self.halo_latent_dim))), name=\"U\")\n    v_mat = tf.Variable((tf.random.normal((self.halo_latent_dim, n_items))), name=\"V\")\n    weights += [u_mat, v_mat]\n\n    self.zero_diag = tf.zeros(n_items)\n    self.instantiated = True\n    self.indexes = indexes\n    self._trainable_weights = weights\n    return indexes, weights\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/","title":"Base latent model class","text":"<p>Base class for latent class choice models.</p>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel","title":"<code>BaseLatentClassModel</code>","text":"<p>Base Class to work with Mixtures of models.</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>class BaseLatentClassModel:\n    \"\"\"Base Class to work with Mixtures of models.\"\"\"\n\n    def __init__(\n        self,\n        n_latent_classes,\n        model_class,\n        model_parameters,\n        fit_method,\n        epochs,\n        batch_size=128,\n        optimizer=None,\n        add_exit_choice=False,\n        lbfgs_tolerance=1e-6,\n        lr=0.001,\n    ):\n        \"\"\"Instantiate of the model mixture.\n\n        Parameters\n        ----------\n        n_latent_classes : int\n            Number of latent classes\n        model_class : BaseModel\n            class of models to get a mixture of\n        model_parameters : dict\n            hyper-parameters of the models\n        fit_method : str\n            Method to estimate the parameters: \"EM\", \"MLE\".\n            \"EM\" for Expectation-Maximization, \"MLE\" for Maximum Likelihood Estimation\n        epochs : int\n            Number of epochs to train the model.\n        optimizer: str, optional\n            Name of the tf.keras.optimizers to be used if one is used, by default None\n        add_exit_choice : bool, optional\n            Whether or not to add an exit choice, by default False\n        lbfgs_tolerance: float, optional\n            Tolerance for the L-BFGS optimizer if applied, by default 1e-6\n        lr: float, optional\n            Learning rate for the optimizer if applied, by default 0.001\n        \"\"\"\n        self.n_latent_classes = n_latent_classes\n        if isinstance(model_parameters, list):\n            if not len(model_parameters) == n_latent_classes:\n                raise ValueError(\n                    \"\"\"If you specify a list of hyper-parameters, it means that you want to use\\\n                    different hyper-parameters for each latent class. In this case, the length\\\n                        of the list must be equal to the number of latent classes.\"\"\"\n                )\n            self.model_parameters = model_parameters\n        else:\n            self.model_parameters = [model_parameters] * n_latent_classes\n        self.model_class = model_class\n        self.fit_method = fit_method\n\n        self.epochs = epochs\n        self.add_exit_choice = add_exit_choice\n        self.lbfgs_tolerance = lbfgs_tolerance\n        self.optimizer = optimizer\n        self.lr = lr\n        self.batch_size = batch_size\n\n        self.loss = tf_ops.CustomCategoricalCrossEntropy(from_logits=False, label_smoothing=0.0)\n        self.exact_nll = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False,\n            label_smoothing=0.0,\n            sparse=False,\n            axis=-1,\n            epsilon=1e-10,\n            name=\"exact_categorical_crossentropy\",\n            reduction=\"sum_over_batch_size\",\n        )\n        self.instantiated = False\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Return trainable weights.\n\n        Returns\n        -------\n        list\n           list of trainable weights.\n        \"\"\"\n        weights = [self.latent_logits]\n        for model in self.models:\n            weights += model.trainable_weights\n        return weights\n\n    def instantiate(self, **kwargs):\n        \"\"\"Instantiate the model.\"\"\"\n        init_logit = tf.Variable(\n            tf.random_normal_initializer(0.0, 0.08, seed=42)(shape=(self.n_latent_classes - 1,)),\n            name=\"Latent-Logits\",\n        )\n        self.latent_logits = init_logit\n\n        self.models = self.instantiate_latent_models(**kwargs)\n        self.instantiated = True\n\n    def instantiate_latent_models(self, **kwargs):\n        \"\"\"Instantiate latent models.\"\"\"\n        models = [self.model_class(**mp) for mp in self.model_parameters]\n        for model in models:\n            model.instantiate(**kwargs)\n\n        return models\n\n    # @tf.function\n    def batch_predict(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices: np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor (1, )\n            Value of NegativeLogLikelihood loss for the batch\n        tf.Tensor (batch_size, n_items)\n            Probabilities for each product to be chosen for each choice\n        \"\"\"\n        # Compute utilities from features\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n\n        latent_probabilities = self.get_latent_classes_weights()\n        # Compute probabilities from utilities &amp; availabilties\n        probabilities = []\n        for i, class_utilities in enumerate(utilities):\n            class_probabilities = tf_ops.softmax_with_availabilities(\n                items_logit_by_choice=class_utilities,\n                available_items_by_choice=available_items_by_choice,\n                normalize_exit=self.add_exit_choice,\n                axis=-1,\n            )\n            probabilities.append(class_probabilities * latent_probabilities[i])\n        # Summing over the latent classes\n        probabilities = tf.reduce_sum(probabilities, axis=0)\n\n        # Compute loss from probabilities &amp; actual choices\n        # batch_loss = self.loss(probabilities, c_batch, sample_weight=sample_weight)\n        batch_loss = {\n            \"optimized_loss\": self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n            \"NegativeLogLikelihood\": self.exact_nll(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n        }\n        return batch_loss, probabilities\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Latent class computation of utility.\n\n        It computes the utility for each of the latent models and stores them in a list.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        list of np.ndarray\n            List of:\n                Utility of each product for each choice.\n                Shape must be (n_choices, n_items)\n            for each of the latent models.\n        \"\"\"\n        utilities = []\n        # Iterates over latent models\n        for model in self.models:\n            model_utilities = model.compute_batch_utility(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n            utilities.append(model_utilities)\n        return utilities\n\n    def fit(self, choice_dataset, sample_weight=None, val_dataset=None, verbose=0):\n        \"\"\"Fit the model on a ChoiceDataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to be used for coefficients estimations\n        sample_weight : np.ndarray, optional\n            sample weights to apply, by default None\n        val_dataset: ChoiceDataset\n            Validation dataset for MLE Gradient Descent Optimization\n        verbose : int, optional\n            print level, for debugging, by default 0\n\n        Returns\n        -------\n        dict\n            Fit history\n        \"\"\"\n        if self.fit_method.lower() == \"em\":\n            self.minf = np.log(1e-3)\n            return self._em_fit(\n                choice_dataset=choice_dataset, sample_weight=sample_weight, verbose=verbose\n            )\n\n        if self.fit_method.lower() == \"mle\":\n            if isinstance(self.optimizer, str):\n                if self.optimizer.lower() == \"lbfgs\" or self.optimizer.lower() == \"l-bfgs\":\n                    return self._fit_with_lbfgs(\n                        choice_dataset=choice_dataset, sample_weight=sample_weight, verbose=verbose\n                    )\n\n                if self.optimizer.lower() == \"adam\":\n                    self.optimizer = tf.keras.optimizers.Adam(self.lr)\n                elif self.optimizer.lower() == \"sgd\":\n                    self.optimizer = tf.keras.optimizers.SGD(self.lr)\n                elif self.optimizer.lower() == \"adamax\":\n                    self.optimizer = tf.keras.optimizers.Adamax(self.lr)\n                else:\n                    print(f\"Optimizer {self.optimizer} not implemnted, switching for default Adam\")\n                    self.optimizer = tf.keras.optimizers.Adam(self.lr)\n\n            return self._fit_with_gd(\n                choice_dataset=choice_dataset,\n                sample_weight=sample_weight,\n                verbose=verbose,\n                val_dataset=val_dataset,\n            )\n\n        raise ValueError(f\"Fit method not implemented: {self.fit_method}\")\n\n    def evaluate(self, choice_dataset, sample_weight=None, batch_size=-1, mode=\"eval\"):\n        \"\"\"Evaluate the model for each choice and each product of a ChoiceDataset.\n\n        Predicts the probabilities according to the model and computes the Negative-Log-Likelihood\n        loss from the actual choices.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset on which to apply to prediction\n\n        Returns\n        -------\n        np.ndarray (n_choices, n_items)\n            Choice probabilties for each choice and each product\n        \"\"\"\n        batch_losses = []\n        for (\n            shared_features,\n            items_features,\n            available_items,\n            choices,\n        ) in choice_dataset.iter_batch(batch_size=batch_size):\n            loss, _ = self.batch_predict(\n                shared_features_by_choice=shared_features,\n                items_features_by_choice=items_features,\n                available_items_by_choice=available_items,\n                choices=choices,\n                sample_weight=sample_weight,\n            )\n            if mode == \"eval\":\n                batch_losses.append(loss[\"NegativeLogLikelihood\"])\n            elif mode == \"optim\":\n                batch_losses.append(loss[\"optimized_loss\"])\n        if batch_size != -1:\n            last_batch_size = available_items.shape[0]\n            coefficients = tf.concat(\n                [tf.ones(len(batch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n            )\n            batch_losses = tf.multiply(batch_losses, coefficients)\n            batch_loss = tf.reduce_sum(batch_losses) / len(choice_dataset)\n        else:\n            batch_loss = tf.reduce_mean(batch_losses)\n        return batch_loss\n\n    def _lbfgs_train_step(self, choice_dataset, sample_weight=None):\n        \"\"\"Create a function required by tfp.optimizer.lbfgs_minimize.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Dataset on which to estimate the paramters.\n        sample_weight: np.ndarray, optional\n            Sample weights to apply, by default None\n\n        Returns\n        -------\n        function\n            with the signature:\n                loss_value, gradients = f(model_parameters).\n        \"\"\"\n        # obtain the shapes of all trainable parameters in the model\n        trainable_weights = []\n        w_to_model = []\n        w_to_model_indexes = []\n        for i, model in enumerate(self.models):\n            for j, w in enumerate(model.trainable_weights):\n                trainable_weights.append(w)\n                w_to_model.append(i)\n                w_to_model_indexes.append(j)\n        trainable_weights.append(self.latent_logits)\n        w_to_model.append(-1)\n        w_to_model_indexes.append(-1)\n        shapes = tf.shape_n(trainable_weights)\n        n_tensors = len(shapes)\n\n        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n        # prepare required information first\n        count = 0\n        idx = []  # stitch indices\n        part = []  # partition indices\n\n        for i, shape in enumerate(shapes):\n            n = np.prod(shape)\n            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n            part.extend([i] * n)\n            count += n\n\n        part = tf.constant(part)\n\n        @tf.function\n        def assign_new_model_parameters(params_1d):\n            \"\"\"Update the model's parameters with a 1D tf.Tensor.\n\n            Pararmeters\n            -----------\n            params_1d: tf.Tensor\n                a 1D tf.Tensor representing the model's trainable parameters.\n            \"\"\"\n            params = tf.dynamic_partition(params_1d, part, n_tensors)\n            for i, (shape, param) in enumerate(zip(shapes, params)):\n                if w_to_model[i] != -1:\n                    self.models[w_to_model[i]].trainable_weights[w_to_model_indexes[i]].assign(\n                        tf.reshape(param, shape)\n                    )\n                else:\n                    self.latent_logits.assign(tf.reshape(param, shape))\n\n        # now create a function that will be returned by this factory\n        @tf.function\n        def f(params_1d):\n            \"\"\"To be used by tfp.optimizer.lbfgs_minimize.\n\n            This function is created by function_factory.\n\n            Parameters\n            ----------\n            params_1d: tf.Tensor\n                a 1D tf.Tensor.\n\n            Returns\n            -------\n            tf.Tensor\n                A scalar loss and the gradients w.r.t. the `params_1d`.\n            tf.Tensor\n                A 1D tf.Tensor representing the gradients w.r.t. the `params_1d`.\n            \"\"\"\n            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n            with tf.GradientTape() as tape:\n                # update the parameters in the model\n                assign_new_model_parameters(params_1d)\n                # calculate the loss\n                loss_value = self.evaluate(\n                    choice_dataset, sample_weight=sample_weight, batch_size=-1, mode=\"optim\"\n                )\n            # calculate gradients and convert to 1D tf.Tensor\n            grads = tape.gradient(loss_value, trainable_weights)\n            grads = tf.dynamic_stitch(idx, grads)\n\n            # print out iteration &amp; loss\n            f.iter.assign_add(1)\n\n            # store loss value so we can retrieve later\n            tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n\n            return loss_value, grads\n\n        # store these information as members so we can use them outside the scope\n        f.iter = tf.Variable(0)\n        f.idx = idx\n        f.part = part\n        f.shapes = shapes\n        f.assign_new_model_parameters = assign_new_model_parameters\n        f.history = []\n        return f\n\n    def _fit_with_lbfgs(self, choice_dataset, sample_weight=None, verbose=0):\n        \"\"\"Fit function for L-BFGS optimizer.\n\n        Replaces the .fit method when the optimizer is set to L-BFGS.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to be used for coefficients estimations\n        epochs : int\n            Maximum number of epochs allowed to reach minimum\n        sample_weight : np.ndarray, optional\n            Sample weights to apply, by default None\n        verbose : int, optional\n            print level, for debugging, by default 0\n\n        Returns\n        -------\n        dict\n            Fit history\n        \"\"\"\n        # Only import tensorflow_probability if LBFGS optimizer is used, avoid unnecessary\n        # dependency\n        import tensorflow_probability as tfp\n\n        epochs = self.epochs\n        func = self._lbfgs_train_step(choice_dataset, sample_weight=sample_weight)\n\n        # convert initial model parameters to a 1D tf.Tensor\n        init = []\n        for model in self.models:\n            for w in model.trainable_weights:\n                init.append(w)\n        init.append(self.latent_logits)\n        init_params = tf.dynamic_stitch(func.idx, init)\n\n        # train the model with L-BFGS solver\n        results = tfp.optimizer.lbfgs_minimize(\n            value_and_gradients_function=func,\n            initial_position=init_params,\n            max_iterations=epochs,\n            tolerance=-1,\n            f_absolute_tolerance=self.lbfgs_tolerance,\n            f_relative_tolerance=-1,\n            x_tolerance=-1,\n        )\n\n        # after training, the final optimized parameters are still in results.position\n        # so we have to manually put them back to the model\n        func.assign_new_model_parameters(results.position)\n        if verbose &gt; 0:\n            print(\"L-BFGS Opimization finished:\")\n            print(\"---------------------------------------------------------------\")\n            print(\"Number of iterations:\", results[2].numpy())\n            print(\"Algorithm converged before reaching max iterations:\", results[0].numpy())\n        return func.history, results\n\n    # @tf.function\n    def train_step(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor\n            Value of NegativeLogLikelihood loss for the batch\n        \"\"\"\n        with tf.GradientTape() as tape:\n            utilities = self.compute_batch_utility(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n\n            latent_probabilities = self.get_latent_classes_weights()\n            # Compute probabilities from utilities &amp; availabilties\n            probabilities = []\n            for i, class_utilities in enumerate(utilities):\n                class_probabilities = tf_ops.softmax_with_availabilities(\n                    items_logit_by_choice=class_utilities,\n                    available_items_by_choice=available_items_by_choice,\n                    normalize_exit=self.add_exit_choice,\n                    axis=-1,\n                )\n                probabilities.append(class_probabilities * latent_probabilities[i])\n            # Summing over the latent classes\n            probabilities = tf.reduce_sum(probabilities, axis=0)\n            # Negative Log-Likelihood\n            neg_loglikelihood = self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            )\n            # if self.regularization is not None:\n            #     regularization = tf.reduce_sum(\n            #         [self.regularizer(w) for w in self.trainable_weights]\n            #     )\n            #     neg_loglikelihood += regularization\n\n        grads = tape.gradient(neg_loglikelihood, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return neg_loglikelihood\n\n    def _fit_with_gd(\n        self,\n        choice_dataset,\n        sample_weight=None,\n        val_dataset=None,\n        verbose=0,\n    ):\n        \"\"\"Train the model with a ChoiceDataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Input data in the form of a ChoiceDataset\n        sample_weight : np.ndarray, optional\n            Sample weight to apply, by default None\n        val_dataset : ChoiceDataset, optional\n            Test ChoiceDataset to evaluate performances on test at each epoch, by default None\n        verbose : int, optional\n            print level, for debugging, by default 0\n        epochs : int, optional\n            Number of epochs, default is None, meaning we use self.epochs\n        batch_size : int, optional\n            Batch size, default is None, meaning we use self.batch_size\n\n        Returns\n        -------\n        dict:\n            Different metrics values over epochs.\n        \"\"\"\n        if hasattr(self, \"instantiated\"):\n            if not self.instantiated:\n                raise ValueError(\"Model not instantiated. Please call .instantiate() first.\")\n        epochs = self.epochs\n        batch_size = self.batch_size\n\n        losses_history = {\"train_loss\": []}\n        t_range = tqdm.trange(epochs, position=0)\n\n        # self.callbacks.on_train_begin()\n\n        # Iterate of epochs\n        for epoch_nb in t_range:\n            # self.callbacks.on_epoch_begin(epoch_nb)\n            t_start = time.time()\n            train_logs = {\"train_loss\": []}\n            val_logs = {\"val_loss\": []}\n            epoch_losses = []\n\n            if sample_weight is not None:\n                if verbose &gt; 0:\n                    inner_range = tqdm.tqdm(\n                        choice_dataset.iter_batch(\n                            shuffle=True, sample_weight=sample_weight, batch_size=batch_size\n                        ),\n                        total=int(len(choice_dataset) / np.max([1, batch_size])),\n                        position=1,\n                        leave=False,\n                    )\n                else:\n                    inner_range = choice_dataset.iter_batch(\n                        shuffle=True, sample_weight=sample_weight, batch_size=batch_size\n                    )\n\n                for batch_nb, (\n                    (\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                    ),\n                    weight_batch,\n                ) in enumerate(inner_range):\n                    # self.callbacks.on_train_batch_begin(batch_nb)\n\n                    neg_loglikelihood = self.train_step(\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                        sample_weight=weight_batch,\n                    )\n\n                    train_logs[\"train_loss\"].append(neg_loglikelihood)\n\n                    # temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n                    # self.callbacks.on_train_batch_end(batch_nb, logs=temps_logs)\n\n                    # Optimization Steps\n                    epoch_losses.append(neg_loglikelihood)\n\n                    if verbose &gt; 0:\n                        inner_range.set_description(\n                            f\"Epoch Negative-LogLikeliHood: {np.sum(epoch_losses):.4f}\"\n                        )\n\n            # In this case we do not need to batch the sample_weights\n            else:\n                if verbose &gt; 0:\n                    inner_range = tqdm.tqdm(\n                        choice_dataset.iter_batch(shuffle=True, batch_size=batch_size),\n                        total=int(len(choice_dataset) / np.max([batch_size, 1])),\n                        position=1,\n                        leave=False,\n                    )\n                else:\n                    inner_range = choice_dataset.iter_batch(shuffle=True, batch_size=batch_size)\n                for batch_nb, (\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                ) in enumerate(inner_range):\n                    # self.callbacks.on_train_batch_begin(batch_nb)\n                    neg_loglikelihood = self.train_step(\n                        shared_features_batch,\n                        items_features_batch,\n                        available_items_batch,\n                        choices_batch,\n                    )\n                    train_logs[\"train_loss\"].append(neg_loglikelihood)\n                    # temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n                    # self.callbacks.on_train_batch_end(batch_nb, logs=temps_logs)\n\n                    # Optimization Steps\n                    epoch_losses.append(neg_loglikelihood)\n\n                    if verbose &gt; 0:\n                        inner_range.set_description(\n                            f\"Epoch Negative-LogLikeliHood: {np.sum(epoch_losses):.4f}\"\n                        )\n\n            # Take into account last batch that may have a differnt length into account for\n            # the computation of the epoch loss.\n            if batch_size != -1:\n                last_batch_size = available_items_batch.shape[0]\n                coefficients = tf.concat(\n                    [tf.ones(len(epoch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n                )\n                epoch_lossses = tf.multiply(epoch_losses, coefficients)\n                epoch_loss = tf.reduce_sum(epoch_lossses) / len(choice_dataset)\n            else:\n                epoch_loss = tf.reduce_mean(epoch_losses)\n            losses_history[\"train_loss\"].append(epoch_loss)\n            print_loss = losses_history[\"train_loss\"][-1].numpy()\n            desc = f\"Epoch {epoch_nb} Train Loss {print_loss:.4f}\"\n            if verbose &gt; 1:\n                print(\n                    f\"Loop {epoch_nb} Time:\",\n                    f\"{time.time() - t_start:.4f}\",\n                    f\"Loss: {print_loss:.4f}\",\n                )\n\n            # Test on val_dataset if provided\n            if val_dataset is not None:\n                test_losses = []\n                for batch_nb, (\n                    shared_features_batch,\n                    items_features_batch,\n                    available_items_batch,\n                    choices_batch,\n                ) in enumerate(val_dataset.iter_batch(shuffle=False, batch_size=batch_size)):\n                    # self.callbacks.on_batch_begin(batch_nb)\n                    # self.callbacks.on_test_batch_begin(batch_nb)\n                    test_losses.append(\n                        self.batch_predict(\n                            shared_features_batch,\n                            items_features_batch,\n                            available_items_batch,\n                            choices_batch,\n                        )[0][\"optimized_loss\"]\n                    )\n                    val_logs[\"val_loss\"].append(test_losses[-1])\n                    # temps_logs = {k: tf.reduce_mean(v) for k, v in val_logs.items()}\n                    # self.callbacks.on_test_batch_end(batch_nb, logs=temps_logs)\n\n                test_loss = tf.reduce_mean(test_losses)\n                if verbose &gt; 1:\n                    print(\"Test Negative-LogLikelihood:\", test_loss.numpy())\n                    desc += f\", Test Loss {np.round(test_loss.numpy(), 4)}\"\n                losses_history[\"test_loss\"] = losses_history.get(\"test_loss\", []) + [\n                    test_loss.numpy()\n                ]\n                train_logs = {**train_logs, **val_logs}\n\n            # temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n            # self.callbacks.on_epoch_end(epoch_nb, logs=temps_logs)\n            # if self.stop_training:\n            #     print(\"Early Stopping taking effect\")\n            #     break\n            t_range.set_description(desc)\n            t_range.refresh()\n\n        # temps_logs = {k: tf.reduce_mean(v) for k, v in train_logs.items()}\n        # self.callbacks.on_train_end(logs=temps_logs)\n        return losses_history\n\n    def _expectation(self, choice_dataset):\n        predicted_probas = [model.predict_probas(choice_dataset) for model in self.models]\n        latent_probabilities = self.get_latent_classes_weights()\n        if np.sum(np.isnan(predicted_probas)) &gt; 0:\n            print(\"A NaN values has been found. You should try again to fit with\")\n            print(\"smaller tolerance value (for l-bfgs) and epsilon value (in loss computation)\")\n\n        latent_model_probas = [\n            latent * proba for latent, proba in zip(latent_probabilities, predicted_probas)\n        ]\n        latent_model_probas = tf.reduce_sum(latent_model_probas, axis=0)\n        predicted_probas = [\n            latent\n            * tf.gather_nd(\n                params=proba,\n                indices=tf.stack(\n                    [tf.range(0, len(choice_dataset), 1), choice_dataset.choices], axis=1\n                ),\n            )\n            for latent, proba in zip(latent_probabilities, predicted_probas)\n        ]\n        predicted_probas = np.stack(predicted_probas, axis=1)\n        loss = self.loss(\n            y_pred=latent_model_probas,\n            y_true=tf.one_hot(choice_dataset.choices, depth=latent_model_probas.shape[1]),\n        )\n\n        return tf.clip_by_value(\n            predicted_probas / np.sum(predicted_probas, axis=1, keepdims=True), 1e-6, 1\n        ), loss\n\n    def _maximization(self, choice_dataset, verbose=0):\n        \"\"\"Maximize step.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            dataset to be fitted\n        verbose : int, optional\n            print level, for debugging, by default 0\n\n        Returns\n        -------\n        np.ndarray\n            latent probabilities resulting of maximization step\n        \"\"\"\n        # models = [self.model_class(**mp) for mp in self.model_parameters]\n        # for i in range(len(models)):\n        #     for j, var in enumerate(self.models[i].trainable_weights):\n        #         models[i]._trainable_weights[j] = var\n        # self.instantiate_latent_models(choice_dataset)\n\n        # M-step: MNL estimation\n        for q in range(self.n_latent_classes):\n            self.models[q].fit(\n                choice_dataset, sample_weight=self.weights[:, q].numpy(), verbose=verbose\n            )\n\n        # M-step: latent probability estimation\n        latent_probas = np.sum(self.weights, axis=0)\n        return tf.math.log((latent_probas / latent_probas[0])[1:])\n\n    def _em_fit(self, choice_dataset, sample_weight=None, verbose=0):\n        \"\"\"Fit with Expectation-Maximization Algorithm.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Dataset to be used for coefficients estimations\n        sample_weight : np.ndarray, optional\n            sample weights to apply, by default None\n        verbose : int, optional\n            print level, for debugging, by default 0\n\n        Returns\n        -------\n        list\n            List of logits for each latent class\n        list\n            List of losses at each epoch\n        \"\"\"\n        hist_logits = []\n        hist_loss = []\n        _ = sample_weight\n\n        # Initialization\n        init_sample_weight = np.random.rand(self.n_latent_classes, len(choice_dataset))\n        init_sample_weight = np.clip(\n            init_sample_weight / np.sum(init_sample_weight, axis=0, keepdims=True), 1e-6, 1\n        )\n        for i, model in enumerate(self.models):\n            # model.instantiate()\n            model.fit(choice_dataset, sample_weight=init_sample_weight[i], verbose=verbose)\n        for i in tqdm.trange(self.epochs):\n            self.weights, loss = self._expectation(choice_dataset)\n            self.latent_logits = self._maximization(choice_dataset, verbose=verbose)\n            hist_logits.append(self.latent_logits)\n            hist_loss.append(loss)\n            if np.sum(np.isnan(self.latent_logits)) &gt; 0:\n                print(\"Nan in logits\")\n                break\n        return hist_loss, hist_logits\n\n    def predict_probas(self, choice_dataset, batch_size=-1):\n        \"\"\"Predicts the choice probabilities for each choice and each product of a ChoiceDataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset on which to apply to prediction\n        batch_size : int, optional\n            Batch size to use for the prediction, by default -1\n\n        Returns\n        -------\n        np.ndarray (n_choices, n_items)\n            Choice probabilties for each choice and each product\n        \"\"\"\n        stacked_probabilities = []\n        for (\n            shared_features,\n            items_features,\n            available_items,\n            choices,\n        ) in choice_dataset.iter_batch(batch_size=batch_size):\n            _, probabilities = self.batch_predict(\n                shared_features_by_choice=shared_features,\n                items_features_by_choice=items_features,\n                available_items_by_choice=available_items,\n                choices=choices,\n            )\n            stacked_probabilities.append(probabilities)\n\n        return tf.concat(stacked_probabilities, axis=0)\n\n    def predict_modelwise_probas(self, choice_dataset, batch_size=-1):\n        \"\"\"Predicts the choice probabilities for each choice and each product of a ChoiceDataset.\n\n        Stacks each model probability.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset on which to apply to prediction\n        batch_size : int, optional\n            Batch size to use for the prediction, by default -1\n\n        Returns\n        -------\n        np.ndarray (n_choices, n_items)\n            Choice probabilties for each choice and each product\n        \"\"\"\n        modelwise_probabilities = []\n        for model in self.models:\n            stacked_probabilities = []\n            for (\n                shared_features,\n                items_features,\n                available_items,\n                choices,\n            ) in choice_dataset.iter_batch(batch_size=batch_size):\n                _, probabilities = model.batch_predict(\n                    shared_features_by_choice=shared_features,\n                    items_features_by_choice=items_features,\n                    available_items_by_choice=available_items,\n                    choices=choices,\n                )\n                stacked_probabilities.append(probabilities)\n            modelwise_probabilities.append(tf.concat(stacked_probabilities, axis=0))\n\n        return tf.stack(modelwise_probabilities, axis=0)\n\n    def get_latent_classes_weights(self):\n        \"\"\"Return the latent classes weights / probabilities from logits.\n\n        Returns\n        -------\n        np.ndarray (n_latent_classes, )\n            Latent classes weights/probabilities\n        \"\"\"\n        return tf.nn.softmax(tf.concat([[tf.constant(0.0)], self.latent_logits], axis=0))\n\n    def compute_report(self, choice_dataset):\n        \"\"\"Compute a report of the estimated weights.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        pandas.DataFrame\n            A DF with estimation, Std Err, z_value and p_value for each coefficient.\n        \"\"\"\n        reports = []\n        for i, model in enumerate(self.models):\n            compute = getattr(model, \"compute_report\", None)\n            if callable(compute):\n                report = model.compute_report(choice_dataset)\n                report[\"Latent Class\"] = i\n                reports.append(report)\n            else:\n                raise ValueError(f\"{i}-th model {model} does not have a compute_report method.\")\n        return pd.concat(reports, axis=0, ignore_index=True)[\n            [\n                \"Latent Class\",\n                \"Coefficient Name\",\n                \"Coefficient Estimation\",\n                \"Std. Err\",\n                \"z_value\",\n                \"P(.&gt;z)\",\n            ]\n        ]\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Return trainable weights.</p> <p>Returns:</p> Type Description <code>list</code> <p>list of trainable weights.</p>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.__init__","title":"<code>__init__(n_latent_classes, model_class, model_parameters, fit_method, epochs, batch_size=128, optimizer=None, add_exit_choice=False, lbfgs_tolerance=1e-06, lr=0.001)</code>","text":"<p>Instantiate of the model mixture.</p> <p>Parameters:</p> Name Type Description Default <code>n_latent_classes</code> <code>int</code> <p>Number of latent classes</p> required <code>model_class</code> <code>BaseModel</code> <p>class of models to get a mixture of</p> required <code>model_parameters</code> <code>dict</code> <p>hyper-parameters of the models</p> required <code>fit_method</code> <code>str</code> <p>Method to estimate the parameters: \"EM\", \"MLE\". \"EM\" for Expectation-Maximization, \"MLE\" for Maximum Likelihood Estimation</p> required <code>epochs</code> <code>int</code> <p>Number of epochs to train the model.</p> required <code>optimizer</code> <p>Name of the tf.keras.optimizers to be used if one is used, by default None</p> <code>None</code> <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to add an exit choice, by default False</p> <code>False</code> <code>lbfgs_tolerance</code> <p>Tolerance for the L-BFGS optimizer if applied, by default 1e-6</p> <code>1e-06</code> <code>lr</code> <p>Learning rate for the optimizer if applied, by default 0.001</p> <code>0.001</code> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def __init__(\n    self,\n    n_latent_classes,\n    model_class,\n    model_parameters,\n    fit_method,\n    epochs,\n    batch_size=128,\n    optimizer=None,\n    add_exit_choice=False,\n    lbfgs_tolerance=1e-6,\n    lr=0.001,\n):\n    \"\"\"Instantiate of the model mixture.\n\n    Parameters\n    ----------\n    n_latent_classes : int\n        Number of latent classes\n    model_class : BaseModel\n        class of models to get a mixture of\n    model_parameters : dict\n        hyper-parameters of the models\n    fit_method : str\n        Method to estimate the parameters: \"EM\", \"MLE\".\n        \"EM\" for Expectation-Maximization, \"MLE\" for Maximum Likelihood Estimation\n    epochs : int\n        Number of epochs to train the model.\n    optimizer: str, optional\n        Name of the tf.keras.optimizers to be used if one is used, by default None\n    add_exit_choice : bool, optional\n        Whether or not to add an exit choice, by default False\n    lbfgs_tolerance: float, optional\n        Tolerance for the L-BFGS optimizer if applied, by default 1e-6\n    lr: float, optional\n        Learning rate for the optimizer if applied, by default 0.001\n    \"\"\"\n    self.n_latent_classes = n_latent_classes\n    if isinstance(model_parameters, list):\n        if not len(model_parameters) == n_latent_classes:\n            raise ValueError(\n                \"\"\"If you specify a list of hyper-parameters, it means that you want to use\\\n                different hyper-parameters for each latent class. In this case, the length\\\n                    of the list must be equal to the number of latent classes.\"\"\"\n            )\n        self.model_parameters = model_parameters\n    else:\n        self.model_parameters = [model_parameters] * n_latent_classes\n    self.model_class = model_class\n    self.fit_method = fit_method\n\n    self.epochs = epochs\n    self.add_exit_choice = add_exit_choice\n    self.lbfgs_tolerance = lbfgs_tolerance\n    self.optimizer = optimizer\n    self.lr = lr\n    self.batch_size = batch_size\n\n    self.loss = tf_ops.CustomCategoricalCrossEntropy(from_logits=False, label_smoothing=0.0)\n    self.exact_nll = tf_ops.CustomCategoricalCrossEntropy(\n        from_logits=False,\n        label_smoothing=0.0,\n        sparse=False,\n        axis=-1,\n        epsilon=1e-10,\n        name=\"exact_categorical_crossentropy\",\n        reduction=\"sum_over_batch_size\",\n    )\n    self.instantiated = False\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.batch_predict","title":"<code>batch_predict(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor(1)</code> <p>Value of NegativeLogLikelihood loss for the batch</p> <code>Tensor(batch_size, n_items)</code> <p>Probabilities for each product to be chosen for each choice</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def batch_predict(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices: np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor (1, )\n        Value of NegativeLogLikelihood loss for the batch\n    tf.Tensor (batch_size, n_items)\n        Probabilities for each product to be chosen for each choice\n    \"\"\"\n    # Compute utilities from features\n    utilities = self.compute_batch_utility(\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    )\n\n    latent_probabilities = self.get_latent_classes_weights()\n    # Compute probabilities from utilities &amp; availabilties\n    probabilities = []\n    for i, class_utilities in enumerate(utilities):\n        class_probabilities = tf_ops.softmax_with_availabilities(\n            items_logit_by_choice=class_utilities,\n            available_items_by_choice=available_items_by_choice,\n            normalize_exit=self.add_exit_choice,\n            axis=-1,\n        )\n        probabilities.append(class_probabilities * latent_probabilities[i])\n    # Summing over the latent classes\n    probabilities = tf.reduce_sum(probabilities, axis=0)\n\n    # Compute loss from probabilities &amp; actual choices\n    # batch_loss = self.loss(probabilities, c_batch, sample_weight=sample_weight)\n    batch_loss = {\n        \"optimized_loss\": self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n        \"NegativeLogLikelihood\": self.exact_nll(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n    }\n    return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Latent class computation of utility.</p> <p>It computes the utility for each of the latent models and stores them in a list.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>list of np.ndarray</code> <p>List of:     Utility of each product for each choice.     Shape must be (n_choices, n_items) for each of the latent models.</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Latent class computation of utility.\n\n    It computes the utility for each of the latent models and stores them in a list.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    list of np.ndarray\n        List of:\n            Utility of each product for each choice.\n            Shape must be (n_choices, n_items)\n        for each of the latent models.\n    \"\"\"\n    utilities = []\n    # Iterates over latent models\n    for model in self.models:\n        model_utilities = model.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n        utilities.append(model_utilities)\n    return utilities\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.compute_report","title":"<code>compute_report(choice_dataset)</code>","text":"<p>Compute a report of the estimated weights.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DF with estimation, Std Err, z_value and p_value for each coefficient.</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def compute_report(self, choice_dataset):\n    \"\"\"Compute a report of the estimated weights.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DF with estimation, Std Err, z_value and p_value for each coefficient.\n    \"\"\"\n    reports = []\n    for i, model in enumerate(self.models):\n        compute = getattr(model, \"compute_report\", None)\n        if callable(compute):\n            report = model.compute_report(choice_dataset)\n            report[\"Latent Class\"] = i\n            reports.append(report)\n        else:\n            raise ValueError(f\"{i}-th model {model} does not have a compute_report method.\")\n    return pd.concat(reports, axis=0, ignore_index=True)[\n        [\n            \"Latent Class\",\n            \"Coefficient Name\",\n            \"Coefficient Estimation\",\n            \"Std. Err\",\n            \"z_value\",\n            \"P(.&gt;z)\",\n        ]\n    ]\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.evaluate","title":"<code>evaluate(choice_dataset, sample_weight=None, batch_size=-1, mode='eval')</code>","text":"<p>Evaluate the model for each choice and each product of a ChoiceDataset.</p> <p>Predicts the probabilities according to the model and computes the Negative-Log-Likelihood loss from the actual choices.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset on which to apply to prediction</p> required <p>Returns:</p> Type Description <code>ndarray(n_choices, n_items)</code> <p>Choice probabilties for each choice and each product</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def evaluate(self, choice_dataset, sample_weight=None, batch_size=-1, mode=\"eval\"):\n    \"\"\"Evaluate the model for each choice and each product of a ChoiceDataset.\n\n    Predicts the probabilities according to the model and computes the Negative-Log-Likelihood\n    loss from the actual choices.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset on which to apply to prediction\n\n    Returns\n    -------\n    np.ndarray (n_choices, n_items)\n        Choice probabilties for each choice and each product\n    \"\"\"\n    batch_losses = []\n    for (\n        shared_features,\n        items_features,\n        available_items,\n        choices,\n    ) in choice_dataset.iter_batch(batch_size=batch_size):\n        loss, _ = self.batch_predict(\n            shared_features_by_choice=shared_features,\n            items_features_by_choice=items_features,\n            available_items_by_choice=available_items,\n            choices=choices,\n            sample_weight=sample_weight,\n        )\n        if mode == \"eval\":\n            batch_losses.append(loss[\"NegativeLogLikelihood\"])\n        elif mode == \"optim\":\n            batch_losses.append(loss[\"optimized_loss\"])\n    if batch_size != -1:\n        last_batch_size = available_items.shape[0]\n        coefficients = tf.concat(\n            [tf.ones(len(batch_losses) - 1) * batch_size, [last_batch_size]], axis=0\n        )\n        batch_losses = tf.multiply(batch_losses, coefficients)\n        batch_loss = tf.reduce_sum(batch_losses) / len(choice_dataset)\n    else:\n        batch_loss = tf.reduce_mean(batch_losses)\n    return batch_loss\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.fit","title":"<code>fit(choice_dataset, sample_weight=None, val_dataset=None, verbose=0)</code>","text":"<p>Fit the model on a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset to be used for coefficients estimations</p> required <code>sample_weight</code> <code>ndarray</code> <p>sample weights to apply, by default None</p> <code>None</code> <code>val_dataset</code> <p>Validation dataset for MLE Gradient Descent Optimization</p> <code>None</code> <code>verbose</code> <code>int</code> <p>print level, for debugging, by default 0</p> <code>0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Fit history</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def fit(self, choice_dataset, sample_weight=None, val_dataset=None, verbose=0):\n    \"\"\"Fit the model on a ChoiceDataset.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset to be used for coefficients estimations\n    sample_weight : np.ndarray, optional\n        sample weights to apply, by default None\n    val_dataset: ChoiceDataset\n        Validation dataset for MLE Gradient Descent Optimization\n    verbose : int, optional\n        print level, for debugging, by default 0\n\n    Returns\n    -------\n    dict\n        Fit history\n    \"\"\"\n    if self.fit_method.lower() == \"em\":\n        self.minf = np.log(1e-3)\n        return self._em_fit(\n            choice_dataset=choice_dataset, sample_weight=sample_weight, verbose=verbose\n        )\n\n    if self.fit_method.lower() == \"mle\":\n        if isinstance(self.optimizer, str):\n            if self.optimizer.lower() == \"lbfgs\" or self.optimizer.lower() == \"l-bfgs\":\n                return self._fit_with_lbfgs(\n                    choice_dataset=choice_dataset, sample_weight=sample_weight, verbose=verbose\n                )\n\n            if self.optimizer.lower() == \"adam\":\n                self.optimizer = tf.keras.optimizers.Adam(self.lr)\n            elif self.optimizer.lower() == \"sgd\":\n                self.optimizer = tf.keras.optimizers.SGD(self.lr)\n            elif self.optimizer.lower() == \"adamax\":\n                self.optimizer = tf.keras.optimizers.Adamax(self.lr)\n            else:\n                print(f\"Optimizer {self.optimizer} not implemnted, switching for default Adam\")\n                self.optimizer = tf.keras.optimizers.Adam(self.lr)\n\n        return self._fit_with_gd(\n            choice_dataset=choice_dataset,\n            sample_weight=sample_weight,\n            verbose=verbose,\n            val_dataset=val_dataset,\n        )\n\n    raise ValueError(f\"Fit method not implemented: {self.fit_method}\")\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.get_latent_classes_weights","title":"<code>get_latent_classes_weights()</code>","text":"<p>Return the latent classes weights / probabilities from logits.</p> <p>Returns:</p> Type Description <code>ndarray(n_latent_classes)</code> <p>Latent classes weights/probabilities</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def get_latent_classes_weights(self):\n    \"\"\"Return the latent classes weights / probabilities from logits.\n\n    Returns\n    -------\n    np.ndarray (n_latent_classes, )\n        Latent classes weights/probabilities\n    \"\"\"\n    return tf.nn.softmax(tf.concat([[tf.constant(0.0)], self.latent_logits], axis=0))\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.instantiate","title":"<code>instantiate(**kwargs)</code>","text":"<p>Instantiate the model.</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def instantiate(self, **kwargs):\n    \"\"\"Instantiate the model.\"\"\"\n    init_logit = tf.Variable(\n        tf.random_normal_initializer(0.0, 0.08, seed=42)(shape=(self.n_latent_classes - 1,)),\n        name=\"Latent-Logits\",\n    )\n    self.latent_logits = init_logit\n\n    self.models = self.instantiate_latent_models(**kwargs)\n    self.instantiated = True\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.instantiate_latent_models","title":"<code>instantiate_latent_models(**kwargs)</code>","text":"<p>Instantiate latent models.</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def instantiate_latent_models(self, **kwargs):\n    \"\"\"Instantiate latent models.\"\"\"\n    models = [self.model_class(**mp) for mp in self.model_parameters]\n    for model in models:\n        model.instantiate(**kwargs)\n\n    return models\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.predict_modelwise_probas","title":"<code>predict_modelwise_probas(choice_dataset, batch_size=-1)</code>","text":"<p>Predicts the choice probabilities for each choice and each product of a ChoiceDataset.</p> <p>Stacks each model probability.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset on which to apply to prediction</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for the prediction, by default -1</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray(n_choices, n_items)</code> <p>Choice probabilties for each choice and each product</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def predict_modelwise_probas(self, choice_dataset, batch_size=-1):\n    \"\"\"Predicts the choice probabilities for each choice and each product of a ChoiceDataset.\n\n    Stacks each model probability.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset on which to apply to prediction\n    batch_size : int, optional\n        Batch size to use for the prediction, by default -1\n\n    Returns\n    -------\n    np.ndarray (n_choices, n_items)\n        Choice probabilties for each choice and each product\n    \"\"\"\n    modelwise_probabilities = []\n    for model in self.models:\n        stacked_probabilities = []\n        for (\n            shared_features,\n            items_features,\n            available_items,\n            choices,\n        ) in choice_dataset.iter_batch(batch_size=batch_size):\n            _, probabilities = model.batch_predict(\n                shared_features_by_choice=shared_features,\n                items_features_by_choice=items_features,\n                available_items_by_choice=available_items,\n                choices=choices,\n            )\n            stacked_probabilities.append(probabilities)\n        modelwise_probabilities.append(tf.concat(stacked_probabilities, axis=0))\n\n    return tf.stack(modelwise_probabilities, axis=0)\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.predict_probas","title":"<code>predict_probas(choice_dataset, batch_size=-1)</code>","text":"<p>Predicts the choice probabilities for each choice and each product of a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset on which to apply to prediction</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for the prediction, by default -1</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray(n_choices, n_items)</code> <p>Choice probabilties for each choice and each product</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def predict_probas(self, choice_dataset, batch_size=-1):\n    \"\"\"Predicts the choice probabilities for each choice and each product of a ChoiceDataset.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset on which to apply to prediction\n    batch_size : int, optional\n        Batch size to use for the prediction, by default -1\n\n    Returns\n    -------\n    np.ndarray (n_choices, n_items)\n        Choice probabilties for each choice and each product\n    \"\"\"\n    stacked_probabilities = []\n    for (\n        shared_features,\n        items_features,\n        available_items,\n        choices,\n    ) in choice_dataset.iter_batch(batch_size=batch_size):\n        _, probabilities = self.batch_predict(\n            shared_features_by_choice=shared_features,\n            items_features_by_choice=items_features,\n            available_items_by_choice=available_items,\n            choices=choices,\n        )\n        stacked_probabilities.append(probabilities)\n\n    return tf.concat(stacked_probabilities, axis=0)\n</code></pre>"},{"location":"references/models/references_latent_class_base_model/#choice_learn.models.latent_class_base_model.BaseLatentClassModel.train_step","title":"<code>train_step(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one training step (= one gradient descent step) of the model.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Value of NegativeLogLikelihood loss for the batch</p> Source code in <code>choice_learn/models/latent_class_base_model.py</code> <pre><code>def train_step(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor\n        Value of NegativeLogLikelihood loss for the batch\n    \"\"\"\n    with tf.GradientTape() as tape:\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n\n        latent_probabilities = self.get_latent_classes_weights()\n        # Compute probabilities from utilities &amp; availabilties\n        probabilities = []\n        for i, class_utilities in enumerate(utilities):\n            class_probabilities = tf_ops.softmax_with_availabilities(\n                items_logit_by_choice=class_utilities,\n                available_items_by_choice=available_items_by_choice,\n                normalize_exit=self.add_exit_choice,\n                axis=-1,\n            )\n            probabilities.append(class_probabilities * latent_probabilities[i])\n        # Summing over the latent classes\n        probabilities = tf.reduce_sum(probabilities, axis=0)\n        # Negative Log-Likelihood\n        neg_loglikelihood = self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        )\n        # if self.regularization is not None:\n        #     regularization = tf.reduce_sum(\n        #         [self.regularizer(w) for w in self.trainable_weights]\n        #     )\n        #     neg_loglikelihood += regularization\n\n    grads = tape.gradient(neg_loglikelihood, self.trainable_weights)\n    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n    return neg_loglikelihood\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/","title":"Latent Class SimpleMNL and ConditionalMNL","text":"<p>Latent Class MNL models.</p>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit","title":"<code>LatentClassConditionalLogit</code>","text":"<p>             Bases: <code>BaseLatentClassModel</code></p> <p>Latent Class for ConditionalLogit.</p> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>class LatentClassConditionalLogit(BaseLatentClassModel):\n    \"\"\"Latent Class for ConditionalLogit.\"\"\"\n\n    def __init__(\n        self,\n        n_latent_classes,\n        fit_method,\n        coefficients=None,\n        epochs=100,\n        add_exit_choice=False,\n        lbfgs_tolerance=1e-6,\n        optimizer=\"Adam\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize model.\n\n        Parameters\n        ----------\n        n_latent_classes : int\n            Number of latent classes.\n        fit_method : str\n            Method to be used to estimate the model.\n        coefficients : dict or MNLCoefficients\n            Dictionnary containing the parametrization of the model.\n            The dictionnary must have the following structure:\n            {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n            mode must be among \"constant\", \"item\", \"item-full\" for now\n            (same specifications as torch-choice).\n        epochs : int\n            Number of epochs\n        add_exit_choice : bool, optional\n            Whether to normalize probabilities with exit choice, by default False\n        lbfgs_tolerance : float, optional\n            LBFG-S tolerance, by default 1e-6\n        optimizer : str, optional\n            tf.keras.optimizers to be used, by default \"Adam\"\n        lr : float, optional\n            Learning rate to use for optimizer if relevant, by default 0.001\n        \"\"\"\n        self.n_latent_classes = n_latent_classes\n        self.fit_method = fit_method\n        self.coefficients = coefficients\n        self.epochs = epochs\n        self.add_exit_choice = add_exit_choice\n        self.lbfgs_tolerance = lbfgs_tolerance\n        self.optimizer = optimizer\n        self.lr = lr\n\n        model_coefficients = {\n            \"coefficients\": self.coefficients,\n            \"add_exit_choice\": self.add_exit_choice,\n            \"optimizer\": self.optimizer,\n            \"lbfgs_tolerance\": self.lbfgs_tolerance,\n            \"lr\": self.lr,\n            \"epochs\": self.epochs,\n        }\n\n        super().__init__(\n            model_class=ConditionalLogit,\n            model_parameters=model_coefficients,\n            n_latent_classes=n_latent_classes,\n            fit_method=fit_method,\n            epochs=epochs,\n            add_exit_choice=add_exit_choice,\n            lbfgs_tolerance=lbfgs_tolerance,\n            optimizer=optimizer,\n            lr=lr,\n            **kwargs,\n        )\n\n    def instantiate_latent_models(self, choice_dataset):\n        \"\"\"Instantiate of the Latent Models that are SimpleMNLs.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Used to match the features names with the model coefficients.\n        \"\"\"\n        for model in self.models:\n            model.coefficients = copy.deepcopy(self.coefficients)\n            model.instantiate(choice_dataset)\n\n    def instantiate(self, choice_dataset):\n        \"\"\"Instantiate of the Latent Class MNL model.\"\"\"\n        self.latent_logits = tf.Variable(\n            tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(self.n_latent_classes - 1,)),\n            name=\"Latent-Logits\",\n        )\n\n        self.models = [self.model_class(**mp) for mp in self.model_parameters]\n\n        self.instantiate_latent_models(choice_dataset)\n\n    def add_coefficients(\n        self, coefficient_name, feature_name, items_indexes=None, items_names=None\n    ):\n        \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        coefficient_name : str\n            Name given to the coefficient.\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given.\n            in the ChoiceDataset that will be used for parameters estimation.\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        if self.coefficients is None:\n            self.coefficients = MNLCoefficients()\n        elif not isinstance(self.coefficients, MNLCoefficients):\n            raise ValueError(\"Cannot add coefficient on top of a dict instantiation.\")\n        self.coefficients.add(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n        )\n\n    def add_shared_coefficient(\n        self, coefficient_name, feature_name, items_indexes=None, items_names=None\n    ):\n        \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        coefficient_name : str\n            Name given to the coefficient.\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given.\n            in the ChoiceDataset that will be used for parameters estimation.\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        if self.coefficients is None:\n            self.coefficients = MNLCoefficients()\n        elif not isinstance(self.coefficients, MNLCoefficients):\n            raise ValueError(\"Cannot add shared coefficient on top of a dict instantiation.\")\n        self.coefficients.add_shared(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n        )\n\n    def fit(self, choice_dataset, **kwargs):\n        \"\"\"Fit the model to the choice_dataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to fit the model to.\n        \"\"\"\n        if not self.instantiated:\n            self.instantiate(choice_dataset=choice_dataset)\n        return super().fit(choice_dataset, **kwargs)\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit.__init__","title":"<code>__init__(n_latent_classes, fit_method, coefficients=None, epochs=100, add_exit_choice=False, lbfgs_tolerance=1e-06, optimizer='Adam', lr=0.001, **kwargs)</code>","text":"<p>Initialize model.</p> <p>Parameters:</p> Name Type Description Default <code>n_latent_classes</code> <code>int</code> <p>Number of latent classes.</p> required <code>fit_method</code> <code>str</code> <p>Method to be used to estimate the model.</p> required <code>coefficients</code> <code>dict or MNLCoefficients</code> <p>Dictionnary containing the parametrization of the model. The dictionnary must have the following structure: {feature_name_1: mode_1, feature_name_2: mode_2, ...} mode must be among \"constant\", \"item\", \"item-full\" for now (same specifications as torch-choice).</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Number of epochs</p> <code>100</code> <code>add_exit_choice</code> <code>bool</code> <p>Whether to normalize probabilities with exit choice, by default False</p> <code>False</code> <code>lbfgs_tolerance</code> <code>float</code> <p>LBFG-S tolerance, by default 1e-6</p> <code>1e-06</code> <code>optimizer</code> <code>str</code> <p>tf.keras.optimizers to be used, by default \"Adam\"</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>Learning rate to use for optimizer if relevant, by default 0.001</p> <code>0.001</code> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def __init__(\n    self,\n    n_latent_classes,\n    fit_method,\n    coefficients=None,\n    epochs=100,\n    add_exit_choice=False,\n    lbfgs_tolerance=1e-6,\n    optimizer=\"Adam\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize model.\n\n    Parameters\n    ----------\n    n_latent_classes : int\n        Number of latent classes.\n    fit_method : str\n        Method to be used to estimate the model.\n    coefficients : dict or MNLCoefficients\n        Dictionnary containing the parametrization of the model.\n        The dictionnary must have the following structure:\n        {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n        mode must be among \"constant\", \"item\", \"item-full\" for now\n        (same specifications as torch-choice).\n    epochs : int\n        Number of epochs\n    add_exit_choice : bool, optional\n        Whether to normalize probabilities with exit choice, by default False\n    lbfgs_tolerance : float, optional\n        LBFG-S tolerance, by default 1e-6\n    optimizer : str, optional\n        tf.keras.optimizers to be used, by default \"Adam\"\n    lr : float, optional\n        Learning rate to use for optimizer if relevant, by default 0.001\n    \"\"\"\n    self.n_latent_classes = n_latent_classes\n    self.fit_method = fit_method\n    self.coefficients = coefficients\n    self.epochs = epochs\n    self.add_exit_choice = add_exit_choice\n    self.lbfgs_tolerance = lbfgs_tolerance\n    self.optimizer = optimizer\n    self.lr = lr\n\n    model_coefficients = {\n        \"coefficients\": self.coefficients,\n        \"add_exit_choice\": self.add_exit_choice,\n        \"optimizer\": self.optimizer,\n        \"lbfgs_tolerance\": self.lbfgs_tolerance,\n        \"lr\": self.lr,\n        \"epochs\": self.epochs,\n    }\n\n    super().__init__(\n        model_class=ConditionalLogit,\n        model_parameters=model_coefficients,\n        n_latent_classes=n_latent_classes,\n        fit_method=fit_method,\n        epochs=epochs,\n        add_exit_choice=add_exit_choice,\n        lbfgs_tolerance=lbfgs_tolerance,\n        optimizer=optimizer,\n        lr=lr,\n        **kwargs,\n    )\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit.add_coefficients","title":"<code>add_coefficients(coefficient_name, feature_name, items_indexes=None, items_names=None)</code>","text":"<p>Add a coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient.</p> required <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given. in the ChoiceDataset that will be used for parameters estimation.</p> required <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def add_coefficients(\n    self, coefficient_name, feature_name, items_indexes=None, items_names=None\n):\n    \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    coefficient_name : str\n        Name given to the coefficient.\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given.\n        in the ChoiceDataset that will be used for parameters estimation.\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    if self.coefficients is None:\n        self.coefficients = MNLCoefficients()\n    elif not isinstance(self.coefficients, MNLCoefficients):\n        raise ValueError(\"Cannot add coefficient on top of a dict instantiation.\")\n    self.coefficients.add(\n        coefficient_name=coefficient_name,\n        feature_name=feature_name,\n        items_indexes=items_indexes,\n        items_names=items_names,\n    )\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit.add_shared_coefficient","title":"<code>add_shared_coefficient(coefficient_name, feature_name, items_indexes=None, items_names=None)</code>","text":"<p>Add a single, shared coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient.</p> required <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given. in the ChoiceDataset that will be used for parameters estimation.</p> required <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def add_shared_coefficient(\n    self, coefficient_name, feature_name, items_indexes=None, items_names=None\n):\n    \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    coefficient_name : str\n        Name given to the coefficient.\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given.\n        in the ChoiceDataset that will be used for parameters estimation.\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    if self.coefficients is None:\n        self.coefficients = MNLCoefficients()\n    elif not isinstance(self.coefficients, MNLCoefficients):\n        raise ValueError(\"Cannot add shared coefficient on top of a dict instantiation.\")\n    self.coefficients.add_shared(\n        coefficient_name=coefficient_name,\n        feature_name=feature_name,\n        items_indexes=items_indexes,\n        items_names=items_names,\n    )\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit.fit","title":"<code>fit(choice_dataset, **kwargs)</code>","text":"<p>Fit the model to the choice_dataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset to fit the model to.</p> required Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def fit(self, choice_dataset, **kwargs):\n    \"\"\"Fit the model to the choice_dataset.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset to fit the model to.\n    \"\"\"\n    if not self.instantiated:\n        self.instantiate(choice_dataset=choice_dataset)\n    return super().fit(choice_dataset, **kwargs)\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit.instantiate","title":"<code>instantiate(choice_dataset)</code>","text":"<p>Instantiate of the Latent Class MNL model.</p> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def instantiate(self, choice_dataset):\n    \"\"\"Instantiate of the Latent Class MNL model.\"\"\"\n    self.latent_logits = tf.Variable(\n        tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(self.n_latent_classes - 1,)),\n        name=\"Latent-Logits\",\n    )\n\n    self.models = [self.model_class(**mp) for mp in self.model_parameters]\n\n    self.instantiate_latent_models(choice_dataset)\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassConditionalLogit.instantiate_latent_models","title":"<code>instantiate_latent_models(choice_dataset)</code>","text":"<p>Instantiate of the Latent Models that are SimpleMNLs.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <p>Used to match the features names with the model coefficients.</p> required Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def instantiate_latent_models(self, choice_dataset):\n    \"\"\"Instantiate of the Latent Models that are SimpleMNLs.\n\n    Parameters\n    ----------\n    choice_dataset: ChoiceDataset\n        Used to match the features names with the model coefficients.\n    \"\"\"\n    for model in self.models:\n        model.coefficients = copy.deepcopy(self.coefficients)\n        model.instantiate(choice_dataset)\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassSimpleMNL","title":"<code>LatentClassSimpleMNL</code>","text":"<p>             Bases: <code>BaseLatentClassModel</code></p> <p>Latent Class for SimpleMNL.</p> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>class LatentClassSimpleMNL(BaseLatentClassModel):\n    \"\"\"Latent Class for SimpleMNL.\"\"\"\n\n    def __init__(\n        self,\n        n_latent_classes,\n        fit_method,\n        epochs=100,\n        batch_size=128,\n        add_exit_choice=False,\n        lbfgs_tolerance=1e-6,\n        intercept=None,\n        optimizer=\"Adam\",\n        lr=0.001,\n        epochs_maximization=1000,\n        **kwargs,\n    ):\n        \"\"\"Initialize model.\n\n        Parameters\n        ----------\n        n_latent_classes : int\n            Number of latent classes.\n        fit_method : str\n            Method to be used to estimate the model.\n        epochs : int\n            Number of epochs\n        add_exit_choice : bool, optional\n            Whether to normalize probabilities with exit choice, by default False\n        lbfgs_tolerance : float, optional\n            LBFG-S tolerance, by default 1e-6\n        intercept : str, optional\n            Type of intercept to include in the SimpleMNL.\n            Must be in (None, 'item', 'item-full', 'constant'), by default None\n        optimizer : str, optional\n            tf.keras.optimizers to be used, by default \"Adam\"\n        lr : float, optional\n            Learning rate to use for optimizer if relevant, by default 0.001\n        \"\"\"\n        self.n_latent_classes = n_latent_classes\n        self.intercept = intercept\n        model_parameters = {\n            \"add_exit_choice\": add_exit_choice,\n            \"intercept\": intercept,\n            \"optimizer\": optimizer,\n            \"batch_size\": batch_size,\n            \"lbfgs_tolerance\": lbfgs_tolerance,\n            \"lr\": lr,\n            \"epochs\": epochs_maximization,\n        }\n\n        super().__init__(\n            model_class=SimpleMNL,\n            model_parameters=model_parameters,\n            n_latent_classes=n_latent_classes,\n            fit_method=fit_method,\n            epochs=epochs,\n            add_exit_choice=add_exit_choice,\n            lbfgs_tolerance=lbfgs_tolerance,\n            optimizer=optimizer,\n            lr=lr,\n            **kwargs,\n        )\n\n    def instantiate_latent_models(self, n_items, n_shared_features, n_items_features):\n        \"\"\"Instantiate the Latent Models that are SimpleMNLs.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of items/aternatives to consider.\n        n_shared_features : int\n            Number of shared features\n        n_items_features : int\n            Number of items features\n        \"\"\"\n        for model in self.models:\n            model.indexes, model.weights = model.instantiate(\n                n_items, n_shared_features, n_items_features\n            )\n            model.exact_nll = tf_ops.CustomCategoricalCrossEntropy(\n                from_logits=False,\n                label_smoothing=0.0,\n                sparse=False,\n                axis=-1,\n                epsilon=1e-25,\n                name=\"exact_categorical_crossentropy\",\n                reduction=\"sum_over_batch_size\",\n            )\n            model.instantiated = True\n\n    def instantiate(self, n_items, n_shared_features, n_items_features):\n        \"\"\"Instantiate the Latent Class MNL model.\"\"\"\n        self.latent_logits = tf.Variable(\n            tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(self.n_latent_classes - 1,)),\n            name=\"Latent-Logits\",\n        )\n\n        self.models = [self.model_class(**mp) for mp in self.model_parameters]\n\n        self.instantiate_latent_models(\n            n_items=n_items,\n            n_shared_features=n_shared_features,\n            n_items_features=n_items_features,\n        )\n        self.instantiated = True\n\n    def fit(self, choice_dataset, **kwargs):\n        \"\"\"Fit the model to the choice_dataset.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Dataset to fit the model to.\n        \"\"\"\n        if not self.instantiated:\n            self.instantiate(\n                n_items=choice_dataset.get_n_items(),\n                n_shared_features=choice_dataset.get_n_shared_features(),\n                n_items_features=choice_dataset.get_n_items_features(),\n            )\n        return super().fit(choice_dataset, **kwargs)\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassSimpleMNL.__init__","title":"<code>__init__(n_latent_classes, fit_method, epochs=100, batch_size=128, add_exit_choice=False, lbfgs_tolerance=1e-06, intercept=None, optimizer='Adam', lr=0.001, epochs_maximization=1000, **kwargs)</code>","text":"<p>Initialize model.</p> <p>Parameters:</p> Name Type Description Default <code>n_latent_classes</code> <code>int</code> <p>Number of latent classes.</p> required <code>fit_method</code> <code>str</code> <p>Method to be used to estimate the model.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs</p> <code>100</code> <code>add_exit_choice</code> <code>bool</code> <p>Whether to normalize probabilities with exit choice, by default False</p> <code>False</code> <code>lbfgs_tolerance</code> <code>float</code> <p>LBFG-S tolerance, by default 1e-6</p> <code>1e-06</code> <code>intercept</code> <code>str</code> <p>Type of intercept to include in the SimpleMNL. Must be in (None, 'item', 'item-full', 'constant'), by default None</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>tf.keras.optimizers to be used, by default \"Adam\"</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>Learning rate to use for optimizer if relevant, by default 0.001</p> <code>0.001</code> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def __init__(\n    self,\n    n_latent_classes,\n    fit_method,\n    epochs=100,\n    batch_size=128,\n    add_exit_choice=False,\n    lbfgs_tolerance=1e-6,\n    intercept=None,\n    optimizer=\"Adam\",\n    lr=0.001,\n    epochs_maximization=1000,\n    **kwargs,\n):\n    \"\"\"Initialize model.\n\n    Parameters\n    ----------\n    n_latent_classes : int\n        Number of latent classes.\n    fit_method : str\n        Method to be used to estimate the model.\n    epochs : int\n        Number of epochs\n    add_exit_choice : bool, optional\n        Whether to normalize probabilities with exit choice, by default False\n    lbfgs_tolerance : float, optional\n        LBFG-S tolerance, by default 1e-6\n    intercept : str, optional\n        Type of intercept to include in the SimpleMNL.\n        Must be in (None, 'item', 'item-full', 'constant'), by default None\n    optimizer : str, optional\n        tf.keras.optimizers to be used, by default \"Adam\"\n    lr : float, optional\n        Learning rate to use for optimizer if relevant, by default 0.001\n    \"\"\"\n    self.n_latent_classes = n_latent_classes\n    self.intercept = intercept\n    model_parameters = {\n        \"add_exit_choice\": add_exit_choice,\n        \"intercept\": intercept,\n        \"optimizer\": optimizer,\n        \"batch_size\": batch_size,\n        \"lbfgs_tolerance\": lbfgs_tolerance,\n        \"lr\": lr,\n        \"epochs\": epochs_maximization,\n    }\n\n    super().__init__(\n        model_class=SimpleMNL,\n        model_parameters=model_parameters,\n        n_latent_classes=n_latent_classes,\n        fit_method=fit_method,\n        epochs=epochs,\n        add_exit_choice=add_exit_choice,\n        lbfgs_tolerance=lbfgs_tolerance,\n        optimizer=optimizer,\n        lr=lr,\n        **kwargs,\n    )\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassSimpleMNL.fit","title":"<code>fit(choice_dataset, **kwargs)</code>","text":"<p>Fit the model to the choice_dataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Dataset to fit the model to.</p> required Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def fit(self, choice_dataset, **kwargs):\n    \"\"\"Fit the model to the choice_dataset.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Dataset to fit the model to.\n    \"\"\"\n    if not self.instantiated:\n        self.instantiate(\n            n_items=choice_dataset.get_n_items(),\n            n_shared_features=choice_dataset.get_n_shared_features(),\n            n_items_features=choice_dataset.get_n_items_features(),\n        )\n    return super().fit(choice_dataset, **kwargs)\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassSimpleMNL.instantiate","title":"<code>instantiate(n_items, n_shared_features, n_items_features)</code>","text":"<p>Instantiate the Latent Class MNL model.</p> Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def instantiate(self, n_items, n_shared_features, n_items_features):\n    \"\"\"Instantiate the Latent Class MNL model.\"\"\"\n    self.latent_logits = tf.Variable(\n        tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(self.n_latent_classes - 1,)),\n        name=\"Latent-Logits\",\n    )\n\n    self.models = [self.model_class(**mp) for mp in self.model_parameters]\n\n    self.instantiate_latent_models(\n        n_items=n_items,\n        n_shared_features=n_shared_features,\n        n_items_features=n_items_features,\n    )\n    self.instantiated = True\n</code></pre>"},{"location":"references/models/references_latent_class_mnl/#choice_learn.models.latent_class_mnl.LatentClassSimpleMNL.instantiate_latent_models","title":"<code>instantiate_latent_models(n_items, n_shared_features, n_items_features)</code>","text":"<p>Instantiate the Latent Models that are SimpleMNLs.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items/aternatives to consider.</p> required <code>n_shared_features</code> <code>int</code> <p>Number of shared features</p> required <code>n_items_features</code> <code>int</code> <p>Number of items features</p> required Source code in <code>choice_learn/models/latent_class_mnl.py</code> <pre><code>def instantiate_latent_models(self, n_items, n_shared_features, n_items_features):\n    \"\"\"Instantiate the Latent Models that are SimpleMNLs.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of items/aternatives to consider.\n    n_shared_features : int\n        Number of shared features\n    n_items_features : int\n        Number of items features\n    \"\"\"\n    for model in self.models:\n        model.indexes, model.weights = model.instantiate(\n            n_items, n_shared_features, n_items_features\n        )\n        model.exact_nll = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False,\n            label_smoothing=0.0,\n            sparse=False,\n            axis=-1,\n            epsilon=1e-25,\n            name=\"exact_categorical_crossentropy\",\n            reduction=\"sum_over_batch_size\",\n        )\n        model.instantiated = True\n</code></pre>"},{"location":"references/models/references_learning_mnl/","title":"Learning MNL Model","text":"<p>Implementation of Enhancing Discrete Choice Models with Representation Learning.</p> <p>https://arxiv.org/abs/1812.09747 .</p>"},{"location":"references/models/references_learning_mnl/#choice_learn.models.learning_mnl.LearningMNL","title":"<code>LearningMNL</code>","text":"<p>             Bases: <code>ConditionalLogit</code></p> <p>Learning MNL from paper https://arxiv.org/abs/1812.09747 .</p> Arguments: <p>coefficients: dict or MNLCoefficients     Specfication of the model to be estimated.</p> Source code in <code>choice_learn/models/learning_mnl.py</code> <pre><code>class LearningMNL(ConditionalLogit):\n    \"\"\"Learning MNL from paper https://arxiv.org/abs/1812.09747 .\n\n    Arguments:\n    ----------\n    coefficients: dict or MNLCoefficients\n        Specfication of the model to be estimated.\n    \"\"\"\n\n    def __init__(\n        self,\n        coefficients=None,\n        nn_features=[],\n        nn_layers_widths=[10],\n        nn_activation=\"relu\",\n        add_exit_choice=False,\n        optimizer=\"Adam\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize of Conditional-MNL.\n\n        Parameters\n        ----------\n        coefficients : dict or MNLCoefficients\n            Dictionnary containing the coefficients parametrization of the model.\n            The dictionnary must have the following structure:\n            {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n            mode must be among \"constant\", \"item\", \"item-full\" for now\n            (same specifications as torch-choice).\n        nn_features: list of str\n            List of features names that will be used in the neural network.\n            Features used as NN inputs MUST BE shared_features !\n        nn_layers_widths: list of int\n            List of integers representing the width of each hidden layer in the neural network.\n        add_exit_choice : bool, optional\n            Whether or not to normalize the probabilities computation with an exit choice\n            whose utility would be 1, by default True\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n        self.coefficients = coefficients\n        self.nn_features = nn_features\n        self.nn_layers_widths = nn_layers_widths\n        self.nn_activation = nn_activation\n        self.instantiated = False\n\n    def instantiate(self, choice_dataset):\n        \"\"\"Instantiate the model using the features in the choice_dataset.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Used to match the features names with the model coefficients.\n        \"\"\"\n        if not self.instantiated:\n            # Instantiate NN\n            nn_input = tf.keras.Input(shape=(len(self.nn_features), 1, 1))\n            nn_output = tf.keras.layers.Conv2D(\n                filters=self.nn_layers_widths[0],\n                kernel_size=[len(self.nn_features), 1],\n                activation=\"relu\",\n                padding=\"valid\",\n                name=\"Dense_NN_per_frame\",\n            )(nn_input)\n            nn_output = tf.keras.layers.Dropout(0.2, name=\"Regularizer\")(nn_output)\n            # nn_output = tf.reshape(nn_output, (-1, self.nn_layers_widths[0]))\n            nn_output = tf.keras.layers.Reshape((self.nn_layers_widths[0],))(nn_output)\n\n            for i in range(len(self.nn_layers_widths) - 1):\n                nn_output = tf.keras.layers.Dense(\n                    units=self.nn_layers_widths[i + 1], activation=\"relu\", name=f\"Dense{i}\"\n                )(nn_output)\n                nn_output = tf.keras.layers.Dropout(0.2, name=f\"Drop{i}\")(nn_output)\n            nn_output = tf.keras.layers.Dense(\n                units=choice_dataset.get_n_items(), name=\"Output_new_feature\"\n            )(nn_output)\n\n            # nn_input = tf.keras.Input(shape=(len(self.nn_features), ))\n            # x = nn_input\n            # for width in self.nn_layers_widths:\n            #     x = tf.keras.layers.Dense(width, activation=self.nn_activation)(x)\n            #     x = tf.keras.layers.Dropout(0.2, name=\"Regularizer\")(x)\n            # nn_output = tf.keras.layers.Dense(choice_dataset.get_n_items())(x)\n            self.nn_model = tf.keras.Model(inputs=nn_input, outputs=nn_output)\n\n            super().instantiate(choice_dataset)\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return self._trainable_weights + self.nn_model.trainable_variables\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        verbose=1,\n    ):\n        \"\"\"Compute the utility when the model is constructed from a MNLCoefficients object.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices: np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        verbose : int, optional\n            Parametrization of the logging outputs, by default 1\n\n        Returns\n        -------\n        tf.Tensor\n            Utilities corresponding of shape (n_choices, n_items)\n        \"\"\"\n        if not isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = (shared_features_by_choice,)\n        if not isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = (items_features_by_choice,)\n        knowledge_driven_utilities = super().compute_batch_utility(\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n            verbose=verbose,\n        )\n        data_driven_inputs = []\n        if self._shared_features_by_choice_names is not None:\n            for nn_feature in self.nn_features:\n                for i, feat_tuple in enumerate(self._shared_features_by_choice_names):\n                    for j, feat in enumerate(feat_tuple):\n                        if feat == nn_feature:\n                            data_driven_inputs.append(shared_features_by_choice[i][:, j])\n        else:\n            logging.warn(\"No shared features found in the dataset.\")\n        data_driven_utilities = self.nn_model(\n            tf.expand_dims(tf.expand_dims(tf.stack(data_driven_inputs, axis=1), axis=-1), axis=-1)\n        )\n        return knowledge_driven_utilities + data_driven_utilities\n\n    def clone(self):\n        \"\"\"Return a clone of the model.\"\"\"\n        clone = LearningMNL(\n            coefficients=self.coefficients,\n            add_exit_choice=self.add_exit_choice,\n            optimizer=self.optimizer_name,\n            nn_features=self.nn_features,\n            nn_layers_widths=self.nn_layers_widths,\n            nn_activation=self.nn_activation,\n        )\n        if hasattr(self, \"history\"):\n            clone.history = self.history\n        if hasattr(self, \"is_fitted\"):\n            clone.is_fitted = self.is_fitted\n        if hasattr(self, \"instantiated\"):\n            clone.instantiated = self.instantiated\n        clone.loss = self.loss\n        clone.label_smoothing = self.label_smoothing\n        if hasattr(self, \"report\"):\n            clone.report = self.report\n        if hasattr(self, \"trainable_weights\"):\n            clone._trainable_weights = self.trainable_weights\n        if hasattr(self, \"nn_model\"):\n            clone.nn_model = self.nn_model\n        if hasattr(self, \"lr\"):\n            clone.lr = self.lr\n        if hasattr(self, \"_shared_features_by_choice_names\"):\n            clone._shared_features_by_choice_names = self._shared_features_by_choice_names\n        if hasattr(self, \"_items_features_by_choice_names\"):\n            clone._items_features_by_choice_names = self._items_features_by_choice_names\n        if hasattr(self, \"_items_features_by_choice_names\"):\n            clone._items_features_by_choice_names = self._items_features_by_choice_names\n        return clone\n</code></pre>"},{"location":"references/models/references_learning_mnl/#choice_learn.models.learning_mnl.LearningMNL.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_learning_mnl/#choice_learn.models.learning_mnl.LearningMNL.__init__","title":"<code>__init__(coefficients=None, nn_features=[], nn_layers_widths=[10], nn_activation='relu', add_exit_choice=False, optimizer='Adam', lr=0.001, **kwargs)</code>","text":"<p>Initialize of Conditional-MNL.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>dict or MNLCoefficients</code> <p>Dictionnary containing the coefficients parametrization of the model. The dictionnary must have the following structure: {feature_name_1: mode_1, feature_name_2: mode_2, ...} mode must be among \"constant\", \"item\", \"item-full\" for now (same specifications as torch-choice).</p> <code>None</code> <code>nn_features</code> <p>List of features names that will be used in the neural network. Features used as NN inputs MUST BE shared_features !</p> <code>[]</code> <code>nn_layers_widths</code> <p>List of integers representing the width of each hidden layer in the neural network.</p> <code>[10]</code> <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to normalize the probabilities computation with an exit choice whose utility would be 1, by default True</p> <code>False</code> Source code in <code>choice_learn/models/learning_mnl.py</code> <pre><code>def __init__(\n    self,\n    coefficients=None,\n    nn_features=[],\n    nn_layers_widths=[10],\n    nn_activation=\"relu\",\n    add_exit_choice=False,\n    optimizer=\"Adam\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize of Conditional-MNL.\n\n    Parameters\n    ----------\n    coefficients : dict or MNLCoefficients\n        Dictionnary containing the coefficients parametrization of the model.\n        The dictionnary must have the following structure:\n        {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n        mode must be among \"constant\", \"item\", \"item-full\" for now\n        (same specifications as torch-choice).\n    nn_features: list of str\n        List of features names that will be used in the neural network.\n        Features used as NN inputs MUST BE shared_features !\n    nn_layers_widths: list of int\n        List of integers representing the width of each hidden layer in the neural network.\n    add_exit_choice : bool, optional\n        Whether or not to normalize the probabilities computation with an exit choice\n        whose utility would be 1, by default True\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n    self.coefficients = coefficients\n    self.nn_features = nn_features\n    self.nn_layers_widths = nn_layers_widths\n    self.nn_activation = nn_activation\n    self.instantiated = False\n</code></pre>"},{"location":"references/models/references_learning_mnl/#choice_learn.models.learning_mnl.LearningMNL.clone","title":"<code>clone()</code>","text":"<p>Return a clone of the model.</p> Source code in <code>choice_learn/models/learning_mnl.py</code> <pre><code>def clone(self):\n    \"\"\"Return a clone of the model.\"\"\"\n    clone = LearningMNL(\n        coefficients=self.coefficients,\n        add_exit_choice=self.add_exit_choice,\n        optimizer=self.optimizer_name,\n        nn_features=self.nn_features,\n        nn_layers_widths=self.nn_layers_widths,\n        nn_activation=self.nn_activation,\n    )\n    if hasattr(self, \"history\"):\n        clone.history = self.history\n    if hasattr(self, \"is_fitted\"):\n        clone.is_fitted = self.is_fitted\n    if hasattr(self, \"instantiated\"):\n        clone.instantiated = self.instantiated\n    clone.loss = self.loss\n    clone.label_smoothing = self.label_smoothing\n    if hasattr(self, \"report\"):\n        clone.report = self.report\n    if hasattr(self, \"trainable_weights\"):\n        clone._trainable_weights = self.trainable_weights\n    if hasattr(self, \"nn_model\"):\n        clone.nn_model = self.nn_model\n    if hasattr(self, \"lr\"):\n        clone.lr = self.lr\n    if hasattr(self, \"_shared_features_by_choice_names\"):\n        clone._shared_features_by_choice_names = self._shared_features_by_choice_names\n    if hasattr(self, \"_items_features_by_choice_names\"):\n        clone._items_features_by_choice_names = self._items_features_by_choice_names\n    if hasattr(self, \"_items_features_by_choice_names\"):\n        clone._items_features_by_choice_names = self._items_features_by_choice_names\n    return clone\n</code></pre>"},{"location":"references/models/references_learning_mnl/#choice_learn.models.learning_mnl.LearningMNL.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, verbose=1)</code>","text":"<p>Compute the utility when the model is constructed from a MNLCoefficients object.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <p>Choices Shape must be (n_choices, )</p> required <code>verbose</code> <code>int</code> <p>Parametrization of the logging outputs, by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Utilities corresponding of shape (n_choices, n_items)</p> Source code in <code>choice_learn/models/learning_mnl.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    verbose=1,\n):\n    \"\"\"Compute the utility when the model is constructed from a MNLCoefficients object.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices: np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    verbose : int, optional\n        Parametrization of the logging outputs, by default 1\n\n    Returns\n    -------\n    tf.Tensor\n        Utilities corresponding of shape (n_choices, n_items)\n    \"\"\"\n    if not isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = (shared_features_by_choice,)\n    if not isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = (items_features_by_choice,)\n    knowledge_driven_utilities = super().compute_batch_utility(\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        verbose=verbose,\n    )\n    data_driven_inputs = []\n    if self._shared_features_by_choice_names is not None:\n        for nn_feature in self.nn_features:\n            for i, feat_tuple in enumerate(self._shared_features_by_choice_names):\n                for j, feat in enumerate(feat_tuple):\n                    if feat == nn_feature:\n                        data_driven_inputs.append(shared_features_by_choice[i][:, j])\n    else:\n        logging.warn(\"No shared features found in the dataset.\")\n    data_driven_utilities = self.nn_model(\n        tf.expand_dims(tf.expand_dims(tf.stack(data_driven_inputs, axis=1), axis=-1), axis=-1)\n    )\n    return knowledge_driven_utilities + data_driven_utilities\n</code></pre>"},{"location":"references/models/references_learning_mnl/#choice_learn.models.learning_mnl.LearningMNL.instantiate","title":"<code>instantiate(choice_dataset)</code>","text":"<p>Instantiate the model using the features in the choice_dataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <p>Used to match the features names with the model coefficients.</p> required Source code in <code>choice_learn/models/learning_mnl.py</code> <pre><code>def instantiate(self, choice_dataset):\n    \"\"\"Instantiate the model using the features in the choice_dataset.\n\n    Parameters\n    ----------\n    choice_dataset: ChoiceDataset\n        Used to match the features names with the model coefficients.\n    \"\"\"\n    if not self.instantiated:\n        # Instantiate NN\n        nn_input = tf.keras.Input(shape=(len(self.nn_features), 1, 1))\n        nn_output = tf.keras.layers.Conv2D(\n            filters=self.nn_layers_widths[0],\n            kernel_size=[len(self.nn_features), 1],\n            activation=\"relu\",\n            padding=\"valid\",\n            name=\"Dense_NN_per_frame\",\n        )(nn_input)\n        nn_output = tf.keras.layers.Dropout(0.2, name=\"Regularizer\")(nn_output)\n        # nn_output = tf.reshape(nn_output, (-1, self.nn_layers_widths[0]))\n        nn_output = tf.keras.layers.Reshape((self.nn_layers_widths[0],))(nn_output)\n\n        for i in range(len(self.nn_layers_widths) - 1):\n            nn_output = tf.keras.layers.Dense(\n                units=self.nn_layers_widths[i + 1], activation=\"relu\", name=f\"Dense{i}\"\n            )(nn_output)\n            nn_output = tf.keras.layers.Dropout(0.2, name=f\"Drop{i}\")(nn_output)\n        nn_output = tf.keras.layers.Dense(\n            units=choice_dataset.get_n_items(), name=\"Output_new_feature\"\n        )(nn_output)\n\n        # nn_input = tf.keras.Input(shape=(len(self.nn_features), ))\n        # x = nn_input\n        # for width in self.nn_layers_widths:\n        #     x = tf.keras.layers.Dense(width, activation=self.nn_activation)(x)\n        #     x = tf.keras.layers.Dropout(0.2, name=\"Regularizer\")(x)\n        # nn_output = tf.keras.layers.Dense(choice_dataset.get_n_items())(x)\n        self.nn_model = tf.keras.Model(inputs=nn_input, outputs=nn_output)\n\n        super().instantiate(choice_dataset)\n</code></pre>"},{"location":"references/models/references_nested_logit/","title":"Nested Logit Model","text":"<p>Implementation of the Nested Logit model.</p>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit","title":"<code>NestedLogit</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>Nested Logit Model class.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>class NestedLogit(ChoiceModel):\n    \"\"\"Nested Logit Model class.\"\"\"\n\n    def __init__(\n        self,\n        items_nests,\n        shared_gammas_over_nests=False,\n        coefficients=None,\n        add_exit_choice=False,\n        optimizer=\"lbfgs\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize the Nested Logit model.\n\n        Parameters\n        ----------\n        items_nest: list\n            list of nests lists, each containing the items indexes in the nest.\n        shared_gammas_over_nests : bool, optional\n            Whether or not to share the gammas over the nests, by default False.\n            If True it means that only one gamma value is estimated, and used for\n            all the nests.\n        coefficients : dict or MNLCoefficients\n            Dictionnary containing the coefficients parametrization of the model.\n            The dictionnary must have the following structure:\n            {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n            mode must be among \"constant\", \"item\", \"item-full\" and \"nest\" for now\n            (same specifications as torch-choice).\n        add_exit_choice : bool, optional\n            Whether or not to normalize the probabilities computation with an exit choice\n            whose utility would be 1, by default True\n        optimizer: str, optional\n            Optimizer to use for the estimation, by default \"lbfgs\"\n        lr: float, optional\n            Learning rate for the optimizer, by default 0.001\n        **kwargs\n            Additional arguments to pass to the ChoiceModel base class.\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n        self.coefficients = coefficients\n        self.instantiated = False\n\n        # Checking the items_nests format:\n        if len(items_nests) &lt; 2:\n            raise ValueError(f\"At least two nests should be given, got {len(items_nests)}\")\n        for i_nest, nest in enumerate(items_nests):\n            if len(nest) &lt; 1:\n                raise ValueError(f\"Nest {i_nest} is empty.\")\n            logging.info(\n                f\"\"\"Checking nest specification,\n                         got nest nb {i_nest + 1} / {len(items_nests)}\n                         with {len(nest)} items within.\"\"\"\n            )\n        flat_items = np.concatenate(items_nests).flatten()\n        if np.max(flat_items) &gt;= len(flat_items):\n            raise ValueError(\n                f\"\"\"{len(flat_items)} have been given,\\\n                             cannot have an item index greater than this.\"\"\"\n            )\n        if len(np.unique(flat_items)) != len(flat_items):\n            raise ValueError(\"Got at least one items in several nests, which is not possible.\")\n\n        # create mapping items -&gt; nests\n        self.items_nests = items_nests\n        items_to_nest = []\n        for item_index in range(len(np.unique(flat_items))):\n            for i_nest, nest in enumerate(items_nests):\n                if item_index in nest:\n                    if len(nest) &gt; 1:\n                        items_to_nest.append(i_nest)\n                    else:\n                        items_to_nest.append(-1)\n        for i in range(np.max(items_to_nest)):\n            if i not in items_to_nest:\n                items_to_nest = [j - 1 if j &gt; i else j for j in items_to_nest]\n        self.items_to_nest = items_to_nest\n        self.shared_gammas_over_nests = shared_gammas_over_nests\n\n    def add_coefficients(\n        self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n    ):\n        \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given in the ChoiceDataset that will be used for parameters estimation.\n        coefficient_name : str, optional\n            Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        self._add_coefficient(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n            shared=False,\n        )\n\n    def add_shared_coefficient(\n        self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n    ):\n        \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n        Parameters\n        ----------\n        feature_name : str\n            features name to which the coefficient is associated. It should work with\n            the names given in the ChoiceDataset that will be used for parameters estimation.\n        coefficient_name : str, optional\n            Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n        items_indexes : list of int, optional\n            list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n        items_names : list of str, optional\n            list of items names (in the ChoiceDataset) for which the coefficient will be used,\n            by default None\n\n        Raises\n        ------\n        ValueError\n            When names or indexes are both not specified.\n        \"\"\"\n        self._add_coefficient(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n            shared=True,\n        )\n\n    def _add_coefficient(self, feature_name, coefficient_name, items_indexes, items_names, shared):\n        if self.coefficients is None:\n            self.coefficients = MNLCoefficients()\n        elif not isinstance(self.coefficients, MNLCoefficients):\n            raise ValueError(\"Cannot add shared coefficient on top of a dict instantiation.\")\n\n        coefficient_name = coefficient_name if coefficient_name else \"beta_%s\" % feature_name\n        add_method = self.coefficients.add_shared if shared else self.coefficients.add\n        add_method(\n            coefficient_name=coefficient_name,\n            feature_name=feature_name,\n            items_indexes=items_indexes,\n            items_names=items_names,\n        )\n\n    def instantiate(self, choice_dataset):\n        \"\"\"Instantiate the model using the features in the choice_dataset.\n\n        Parameters\n        ----------\n        choice_dataset: ChoiceDataset\n            Used to match the features names with the model coefficients.\n        \"\"\"\n        if not self.instantiated:\n            if not isinstance(self.coefficients, MNLCoefficients):\n                self._build_coefficients_from_dict(n_items=choice_dataset.get_n_items())\n            self._trainable_weights = self._instantiate_tf_weights()\n\n            # Checking that no weight has been attributed to non existing feature in dataset\n            dataset_stacked_features_names = []\n            if choice_dataset.shared_features_by_choice_names is not None:\n                for i, feat_tuple in enumerate(choice_dataset.shared_features_by_choice_names):\n                    dataset_stacked_features_names.append(feat_tuple)\n            if choice_dataset.items_features_by_choice_names is not None:\n                for i, feat_tuple in enumerate(choice_dataset.items_features_by_choice_names):\n                    dataset_stacked_features_names.append(feat_tuple)\n            dataset_stacked_features_names = np.concatenate(dataset_stacked_features_names).ravel()\n\n            for feature_with_weight in self.coefficients.features_with_weights:\n                if feature_with_weight != \"intercept\":\n                    if feature_with_weight not in dataset_stacked_features_names:\n                        raise ValueError(\n                            f\"\"\"Feature {feature_with_weight} has an attributed coefficient\n                            but is not in dataset\"\"\"\n                        )\n            self._store_dataset_features_names(choice_dataset)\n            self.instantiated = True\n\n    def _instantiate_tf_weights(self):\n        \"\"\"Instantiate the model from MNLCoefficients object.\n\n        Returns\n        -------\n        list of tf.Tensor\n            List of the weights created coresponding to the specification.\n        \"\"\"\n        weights = []\n        for weight_nb, weight_name in enumerate(self.coefficients.names):\n            n_weights = (\n                len(self.coefficients.get(weight_name)[\"items_indexes\"])\n                if self.coefficients.get(weight_name)[\"items_indexes\"] is not None\n                else len(self.coefficients.get(weight_name)[\"items_names\"])\n            )\n            weight = tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, n_weights)),\n                name=weight_name,\n            )\n            weights.append(weight)\n            self.coefficients._add_tf_weight(weight_name, weight_nb)\n\n        # Initialization of gammas a bit different, it's a sensible variable\n        # which should be in [eps, 1] -&gt; initialized at 0.5\n        if self.shared_gammas_over_nests:\n            weights.append(\n                tf.Variable(\n                    [[0.5]],\n                    name=\"gamma_nests\",\n                )\n            )\n        else:\n            weights.append(\n                tf.Variable(\n                    [[0.5] * np.sum([1 if len(nest) &gt; 1 else 0 for nest in self.items_nests])],\n                    name=\"gammas_nests\",\n                )\n            )\n\n        self._trainable_weights = weights\n\n        return weights\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return self._trainable_weights\n\n    def _build_coefficients_from_dict(self, n_items):\n        \"\"\"Build coefficients when they are given as a dictionnay.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of different items in the assortment. Used to create the right number of weights.\n        \"\"\"\n        coefficients = MNLCoefficients()\n        for weight_counter, (feature, mode) in enumerate(self.coefficients.items()):\n            if mode == \"constant\":\n                coefficients.add_shared(\n                    feature + f\"_w_{weight_counter}\", feature, list(range(n_items))\n                )\n            elif mode == \"item\":\n                coefficients.add(feature + f\"_w_{weight_counter}\", feature, list(range(1, n_items)))\n            elif mode == \"item-full\":\n                coefficients.add(feature + f\"_w_{weight_counter}\", feature, list(range(n_items)))\n\n            # Additional mode compared to Conditional Logit\n            elif mode == \"nest\":\n                for nest in self.items_nests:\n                    items_in_nest = [i for (i, j) in enumerate(nest) if j == nest]\n                    coefficients.add_shared(\n                        feature + f\"_w_{weight_counter}\", feature, items_in_nest\n                    )\n            else:\n                raise ValueError(f\"Mode {mode} for coefficients not recognized.\")\n\n        self.coefficients = coefficients\n\n    def _store_dataset_features_names(self, choice_dataset):\n        \"\"\"Register the name of the features in the dataset. For later use in utility computation.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used to fit the model.\n        \"\"\"\n        self._shared_features_by_choice_names = choice_dataset.shared_features_by_choice_names\n        self._items_features_by_choice_names = choice_dataset.items_features_by_choice_names\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        verbose=1,\n    ):\n        \"\"\"Compute the utility when the model is constructed from a MNLCoefficients object.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices: np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        verbose : int, optional\n            Parametrization of the logging outputs, by default 1\n\n        Returns\n        -------\n        tf.Tensor\n            Utilities corresponding of shape (n_choices, n_items)\n        \"\"\"\n        _ = choices\n\n        n_items = available_items_by_choice.shape[1]\n        n_choices = available_items_by_choice.shape[0]\n        items_utilities_by_choice = []\n\n        if not isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = (shared_features_by_choice,)\n        if not isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = (items_features_by_choice,)\n\n        # Shared features\n        if self._shared_features_by_choice_names is not None:\n            for i, feat_tuple in enumerate(self._shared_features_by_choice_names):\n                for j, feat in enumerate(feat_tuple):\n                    if feat in self.coefficients.features_with_weights:\n                        (\n                            item_index_list,\n                            weight_index_list,\n                        ) = self.coefficients.get_weight_item_indexes(feat)\n                        for item_index, weight_index in zip(item_index_list, weight_index_list):\n                            partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n                            partial_items_utility_by_choice = [\n                                tf.zeros(n_choices) for _ in range(n_items)\n                            ]\n\n                            for q, idx in enumerate(item_index):\n                                if isinstance(idx, list):\n                                    for k in idx:\n                                        tf.cast(shared_features_by_choice[i][:, j], tf.float32)\n                                        compute = tf.multiply(\n                                            shared_features_by_choice[i][:, j],\n                                            self.trainable_weights[weight_index][:, q],\n                                        )\n                                        partial_items_utility_by_choice[k] += compute\n                                else:\n                                    compute = tf.multiply(\n                                        tf.cast(shared_features_by_choice[i][:, j], tf.float32),\n                                        self.trainable_weights[weight_index][:, q],\n                                    )\n                                    partial_items_utility_by_choice[idx] += compute\n\n                            items_utilities_by_choice.append(\n                                tf.cast(\n                                    tf.stack(partial_items_utility_by_choice, axis=1), tf.float32\n                                )\n                            )\n                    elif verbose &gt; 0:\n                        logging.info(\n                            f\"Feature {feat} is in dataset but has no weight assigned\\\n                                in utility computations\"\n                        )\n\n        # Items features\n        if self._items_features_by_choice_names is not None:\n            for i, feat_tuple in enumerate(self._items_features_by_choice_names):\n                for j, feat in enumerate(feat_tuple):\n                    if feat in self.coefficients.features_with_weights:\n                        (\n                            item_index_list,\n                            weight_index_list,\n                        ) = self.coefficients.get_weight_item_indexes(feat)\n                        for item_index, weight_index in zip(item_index_list, weight_index_list):\n                            partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n\n                            for q, idx in enumerate(item_index):\n                                if isinstance(idx, list):\n                                    for k in idx:\n                                        partial_items_utility_by_choice = tf.concat(\n                                            [\n                                                partial_items_utility_by_choice[:, :k],\n                                                tf.expand_dims(\n                                                    tf.multiply(\n                                                        tf.cast(\n                                                            items_features_by_choice[i][:, k, j],\n                                                            tf.float32,\n                                                        ),\n                                                        self.trainable_weights[weight_index][:, q],\n                                                    ),\n                                                    axis=-1,\n                                                ),\n                                                partial_items_utility_by_choice[:, k + 1 :],\n                                            ],\n                                            axis=1,\n                                        )\n                                else:\n                                    partial_items_utility_by_choice = tf.concat(\n                                        [\n                                            partial_items_utility_by_choice[:, :idx],\n                                            tf.expand_dims(\n                                                tf.multiply(\n                                                    tf.cast(\n                                                        items_features_by_choice[i][:, idx, j],\n                                                        tf.float32,\n                                                    ),\n                                                    self.trainable_weights[weight_index][:, q],\n                                                ),\n                                                axis=-1,\n                                            ),\n                                            partial_items_utility_by_choice[:, idx + 1 :],\n                                        ],\n                                        axis=1,\n                                    )\n\n                            items_utilities_by_choice.append(\n                                tf.cast(partial_items_utility_by_choice, tf.float32)\n                            )\n                    elif verbose &gt; 0:\n                        logging.info(\n                            f\"Feature {feat} is in dataset but has no weight assigned\\\n                                in utility computations\"\n                        )\n\n        if \"intercept\" in self.coefficients.features_with_weights:\n            item_index_list, weight_index_list = self.coefficients.get_weight_item_indexes(\n                \"intercept\"\n            )\n\n            for item_index, weight_index in zip(item_index_list, weight_index_list):\n                partial_items_utility_by_choice = tf.zeros((n_items,))\n                for q, idx in enumerate(item_index):\n                    if isinstance(idx, list):\n                        for idx_idx in idx:\n                            partial_items_utility_by_choice = tf.concat(\n                                [\n                                    partial_items_utility_by_choice[:idx_idx],\n                                    self.trainable_weights[weight_index][:, q],\n                                    partial_items_utility_by_choice[idx_idx + 1 :],\n                                ],\n                                axis=0,\n                            )\n                    else:\n                        partial_items_utility_by_choice = tf.concat(\n                            [\n                                partial_items_utility_by_choice[:idx],\n                                self.trainable_weights[weight_index][:, q],\n                                partial_items_utility_by_choice[idx + 1 :],\n                            ],\n                            axis=0,\n                        )\n\n                partial_items_utility_by_choice = tf.stack(\n                    [partial_items_utility_by_choice] * n_choices, axis=0\n                )\n\n                items_utilities_by_choice.append(\n                    tf.cast(partial_items_utility_by_choice, tf.float32)\n                )\n\n        return tf.reduce_sum(items_utilities_by_choice, axis=0)\n\n    @tf.function\n    def batch_predict(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor (1, )\n            Value of NegativeLogLikelihood loss for the batch\n        tf.Tensor (batch_size, n_items)\n            Probabilities for each product to be chosen for each choice\n        \"\"\"\n        # Compute utilities from features\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice,\n            items_features_by_choice,\n            available_items_by_choice,\n            choices,\n        )\n\n        batch_size = utilities.shape[0]\n        batch_gammas = []\n        if self.shared_gammas_over_nests:\n            batch_gammas = self.trainable_weights[-1][0, 0] * tf.ones_like(utilities)\n        else:\n            for i in range(len(self.items_to_nest)):\n                if self.items_to_nest[i] == -1:\n                    batch_gammas.append([tf.constant(1.0)] * batch_size)\n                else:\n                    batch_gammas.append(\n                        [self.trainable_weights[-1][0, self.items_to_nest[i]]] * batch_size\n                    )\n            batch_gammas = tf.stack(batch_gammas, axis=-1)\n\n        probabilities = nested_softmax_with_availabilities(\n            items_logit_by_choice=utilities,\n            available_items_by_choice=available_items_by_choice,\n            items_nests=tf.constant(self.items_to_nest),\n            gammas=batch_gammas,\n            normalize_exit=self.add_exit_choice,\n        )\n\n        # Compute loss from probabilities &amp; actual choices\n        batch_loss = {\n            \"optimized_loss\": self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n            \"Exact-NegativeLogLikelihood\": self.exact_nll(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n        }\n        return batch_loss, probabilities\n\n    @tf.function\n    def train_step(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor\n            Value of NegativeLogLikelihood loss for the batch\n        \"\"\"\n        with tf.GradientTape() as tape:\n            utilities = self.compute_batch_utility(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n\n            batch_size = utilities.shape[0]\n            batch_gammas = []\n            if self.shared_gammas_over_nests:\n                batch_gammas = self.trainable_weights[-1][0, 0] * tf.ones_like(utilities)\n            else:\n                for i in range(len(self.items_to_nest)):\n                    if self.items_to_nest[i] == -1:\n                        batch_gammas.append([tf.constant(1.0)] * batch_size)\n                    else:\n                        batch_gammas.append(\n                            [self.trainable_weights[-1][0, self.items_to_nest[i]]] * batch_size\n                        )\n                batch_gammas = tf.stack(batch_gammas, axis=-1)\n\n            probabilities = nested_softmax_with_availabilities(\n                items_logit_by_choice=utilities,\n                available_items_by_choice=available_items_by_choice,\n                items_nests=tf.constant(self.items_to_nest),\n                gammas=batch_gammas,\n                normalize_exit=self.add_exit_choice,\n            )\n            # Negative Log-Likelihood\n            neg_loglikelihood = self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            )\n            if self.regularization is not None:\n                regularization = tf.reduce_sum(\n                    [self.regularizer(w) for w in self.trainable_weights]\n                )\n                neg_loglikelihood += regularization\n\n        grads = tape.gradient(neg_loglikelihood, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return neg_loglikelihood\n\n    def fit(self, choice_dataset, get_report=False, **kwargs):\n        \"\"\"Fit function to estimate the paramters.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        self.instantiate(choice_dataset)\n\n        fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n\n    def _fit_with_lbfgs(\n        self,\n        choice_dataset,\n        sample_weight=None,\n        get_report=False,\n        **kwargs,\n    ):\n        \"\"\"Specific fit function to estimate the paramters with LBFGS.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        n_epochs : int\n            Number of epochs to run.\n        tolerance : float, optional\n            Tolerance in the research of minimum, by default 1e-8\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        self.instantiate(choice_dataset)\n\n        fit = super()._fit_with_lbfgs(\n            choice_dataset=choice_dataset,\n            sample_weight=sample_weight,\n            **kwargs,\n        )\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n\n    def compute_report(self, choice_dataset):\n        \"\"\"Compute a report of the estimated weights.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        pandas.DataFrame\n            A DF with estimation, Std Err, z_value and p_value for each coefficient.\n        \"\"\"\n        import tensorflow_probability as tfp\n\n        weights_std = self.get_weights_std(choice_dataset)\n        dist = tfp.distributions.Normal(loc=0.0, scale=1.0)\n\n        names = []\n        z_values = []\n        estimations = []\n        p_z = []\n        i = 0\n        for weight in self.trainable_weights:\n            for j in range(weight.shape[1]):\n                if weight.shape[1] &gt; 1:\n                    names.append(f\"{weight.name[:-2]}_{j}\")\n                else:\n                    names.append(f\"{weight.name[:-2]}\")\n                estimations.append(weight.numpy()[0][j])\n                z_values.append(weight.numpy()[0][j] / weights_std[i].numpy())\n                p_z.append(2 * (1 - dist.cdf(tf.math.abs(z_values[-1])).numpy()))\n                i += 1\n\n        return pd.DataFrame(\n            {\n                \"Coefficient Name\": names,\n                \"Coefficient Estimation\": estimations,\n                \"Std. Err\": weights_std.numpy(),\n                \"z_value\": z_values,\n                \"P(.&gt;z)\": p_z,\n            },\n        )\n\n    def get_weights_std(self, choice_dataset):\n        \"\"\"Approximates Std Err with Hessian matrix.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        tf.Tensor\n            Estimation of the Std Err for the weights.\n        \"\"\"\n        # Loops of differentiation\n        with tf.GradientTape() as tape_1:\n            with tf.GradientTape(persistent=True) as tape_2:\n                model = self.clone()\n                w = tf.concat(self.trainable_weights, axis=1)\n                tape_2.watch(w)\n                tape_1.watch(w)\n                mw = []\n                index = 0\n                for _w in self.trainable_weights:\n                    mw.append(w[:, index : index + _w.shape[1]])\n                    index += _w.shape[1]\n                model._trainable_weights = mw\n                batch = next(choice_dataset.iter_batch(batch_size=-1))\n                utilities = model.compute_batch_utility(*batch)\n\n                batch_gammas = []\n                if model.shared_gammas_over_nests:\n                    batch_gammas = model.trainable_weights[-1][0, 0] * tf.ones_like(utilities)\n                else:\n                    for i in range(len(self.items_to_nest)):\n                        if model.items_to_nest[i] == -1:\n                            batch_gammas.append([tf.constant(1.0)] * len(choice_dataset))\n                        else:\n                            batch_gammas.append(\n                                [model.trainable_weights[-1][0, model.items_to_nest[i]]]\n                                * len(choice_dataset)\n                            )\n                    batch_gammas = tf.stack(batch_gammas, axis=-1)\n\n                probabilities = nested_softmax_with_availabilities(\n                    items_logit_by_choice=utilities,\n                    available_items_by_choice=batch[2],\n                    items_nests=tf.constant(model.items_to_nest),\n                    gammas=batch_gammas,\n                    normalize_exit=self.add_exit_choice,\n                    eps=1e-15,\n                )\n                loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                    y_pred=probabilities,\n                    y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[1]),\n                )\n\n            # Compute the Jacobian\n            jacobian = tape_2.jacobian(loss, w)\n        # Compute the Hessian from the Jacobian\n        hessian = tape_1.batch_jacobian(jacobian, w)\n        inv_hessian = tf.linalg.inv(tf.squeeze(hessian))\n        return tf.sqrt(\n            [\n                tf.clip_by_value(inv_hessian[i][i], 0.0, tf.float32.max)\n                for i in range(len(tf.squeeze(hessian)))\n            ]\n        )\n\n    def clone(self):\n        \"\"\"Return a clone/deepcopy of the model.\"\"\"\n        clone = NestedLogit(\n            coefficients=self.coefficients,\n            add_exit_choice=self.add_exit_choice,\n            optimizer=self.optimizer_name,\n            items_nests=self.items_nests,\n        )\n        if hasattr(self, \"history\"):\n            clone.history = self.history\n        if hasattr(self, \"is_fitted\"):\n            clone.is_fitted = self.is_fitted\n        if hasattr(self, \"instantiated\"):\n            clone.instantiated = self.instantiated\n        if hasattr(self, \"shared_gammas_over_nests\"):\n            clone.shared_gammas_over_nests = self.shared_gammas_over_nests\n        clone.loss = self.loss\n        clone.label_smoothing = self.label_smoothing\n        if hasattr(self, \"report\"):\n            clone.report = self.report\n        if hasattr(self, \"_trainable_weights\"):\n            clone._trainable_weights = self._trainable_weights\n        if hasattr(self, \"lr\"):\n            clone.lr = self.lr\n        if hasattr(self, \"_shared_features_by_choice_names\"):\n            clone._shared_features_by_choice_names = self._shared_features_by_choice_names\n        if hasattr(self, \"_items_features_by_choice_names\"):\n            clone._items_features_by_choice_names = self._items_features_by_choice_names\n        return clone\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.__init__","title":"<code>__init__(items_nests, shared_gammas_over_nests=False, coefficients=None, add_exit_choice=False, optimizer='lbfgs', lr=0.001, **kwargs)</code>","text":"<p>Initialize the Nested Logit model.</p> <p>Parameters:</p> Name Type Description Default <code>items_nest</code> <p>list of nests lists, each containing the items indexes in the nest.</p> required <code>shared_gammas_over_nests</code> <code>bool</code> <p>Whether or not to share the gammas over the nests, by default False. If True it means that only one gamma value is estimated, and used for all the nests.</p> <code>False</code> <code>coefficients</code> <code>dict or MNLCoefficients</code> <p>Dictionnary containing the coefficients parametrization of the model. The dictionnary must have the following structure: {feature_name_1: mode_1, feature_name_2: mode_2, ...} mode must be among \"constant\", \"item\", \"item-full\" and \"nest\" for now (same specifications as torch-choice).</p> <code>None</code> <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to normalize the probabilities computation with an exit choice whose utility would be 1, by default True</p> <code>False</code> <code>optimizer</code> <p>Optimizer to use for the estimation, by default \"lbfgs\"</p> <code>'lbfgs'</code> <code>lr</code> <p>Learning rate for the optimizer, by default 0.001</p> <code>0.001</code> <code>**kwargs</code> <p>Additional arguments to pass to the ChoiceModel base class.</p> <code>{}</code> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def __init__(\n    self,\n    items_nests,\n    shared_gammas_over_nests=False,\n    coefficients=None,\n    add_exit_choice=False,\n    optimizer=\"lbfgs\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize the Nested Logit model.\n\n    Parameters\n    ----------\n    items_nest: list\n        list of nests lists, each containing the items indexes in the nest.\n    shared_gammas_over_nests : bool, optional\n        Whether or not to share the gammas over the nests, by default False.\n        If True it means that only one gamma value is estimated, and used for\n        all the nests.\n    coefficients : dict or MNLCoefficients\n        Dictionnary containing the coefficients parametrization of the model.\n        The dictionnary must have the following structure:\n        {feature_name_1: mode_1, feature_name_2: mode_2, ...}\n        mode must be among \"constant\", \"item\", \"item-full\" and \"nest\" for now\n        (same specifications as torch-choice).\n    add_exit_choice : bool, optional\n        Whether or not to normalize the probabilities computation with an exit choice\n        whose utility would be 1, by default True\n    optimizer: str, optional\n        Optimizer to use for the estimation, by default \"lbfgs\"\n    lr: float, optional\n        Learning rate for the optimizer, by default 0.001\n    **kwargs\n        Additional arguments to pass to the ChoiceModel base class.\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n    self.coefficients = coefficients\n    self.instantiated = False\n\n    # Checking the items_nests format:\n    if len(items_nests) &lt; 2:\n        raise ValueError(f\"At least two nests should be given, got {len(items_nests)}\")\n    for i_nest, nest in enumerate(items_nests):\n        if len(nest) &lt; 1:\n            raise ValueError(f\"Nest {i_nest} is empty.\")\n        logging.info(\n            f\"\"\"Checking nest specification,\n                     got nest nb {i_nest + 1} / {len(items_nests)}\n                     with {len(nest)} items within.\"\"\"\n        )\n    flat_items = np.concatenate(items_nests).flatten()\n    if np.max(flat_items) &gt;= len(flat_items):\n        raise ValueError(\n            f\"\"\"{len(flat_items)} have been given,\\\n                         cannot have an item index greater than this.\"\"\"\n        )\n    if len(np.unique(flat_items)) != len(flat_items):\n        raise ValueError(\"Got at least one items in several nests, which is not possible.\")\n\n    # create mapping items -&gt; nests\n    self.items_nests = items_nests\n    items_to_nest = []\n    for item_index in range(len(np.unique(flat_items))):\n        for i_nest, nest in enumerate(items_nests):\n            if item_index in nest:\n                if len(nest) &gt; 1:\n                    items_to_nest.append(i_nest)\n                else:\n                    items_to_nest.append(-1)\n    for i in range(np.max(items_to_nest)):\n        if i not in items_to_nest:\n            items_to_nest = [j - 1 if j &gt; i else j for j in items_to_nest]\n    self.items_to_nest = items_to_nest\n    self.shared_gammas_over_nests = shared_gammas_over_nests\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.add_coefficients","title":"<code>add_coefficients(feature_name, coefficient_name='', items_indexes=None, items_names=None)</code>","text":"<p>Add a coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given in the ChoiceDataset that will be used for parameters estimation.</p> required <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient. If not provided, name will be \"beta_feature_name\".</p> <code>''</code> <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which we need to add a coefficient, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def add_coefficients(\n    self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n):\n    \"\"\"Add a coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given in the ChoiceDataset that will be used for parameters estimation.\n    coefficient_name : str, optional\n        Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which we need to add a coefficient,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    self._add_coefficient(\n        coefficient_name=coefficient_name,\n        feature_name=feature_name,\n        items_indexes=items_indexes,\n        items_names=items_names,\n        shared=False,\n    )\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.add_shared_coefficient","title":"<code>add_shared_coefficient(feature_name, coefficient_name='', items_indexes=None, items_names=None)</code>","text":"<p>Add a single, shared coefficient to the model throught the specification of the utility.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>features name to which the coefficient is associated. It should work with the names given in the ChoiceDataset that will be used for parameters estimation.</p> required <code>coefficient_name</code> <code>str</code> <p>Name given to the coefficient. If not provided, name will be \"beta_feature_name\".</p> <code>''</code> <code>items_indexes</code> <code>list of int</code> <p>list of items indexes (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <code>items_names</code> <code>list of str</code> <p>list of items names (in the ChoiceDataset) for which the coefficient will be used, by default None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When names or indexes are both not specified.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def add_shared_coefficient(\n    self, feature_name, coefficient_name=\"\", items_indexes=None, items_names=None\n):\n    \"\"\"Add a single, shared coefficient to the model throught the specification of the utility.\n\n    Parameters\n    ----------\n    feature_name : str\n        features name to which the coefficient is associated. It should work with\n        the names given in the ChoiceDataset that will be used for parameters estimation.\n    coefficient_name : str, optional\n        Name given to the coefficient. If not provided, name will be \"beta_feature_name\".\n    items_indexes : list of int, optional\n        list of items indexes (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n    items_names : list of str, optional\n        list of items names (in the ChoiceDataset) for which the coefficient will be used,\n        by default None\n\n    Raises\n    ------\n    ValueError\n        When names or indexes are both not specified.\n    \"\"\"\n    self._add_coefficient(\n        coefficient_name=coefficient_name,\n        feature_name=feature_name,\n        items_indexes=items_indexes,\n        items_names=items_names,\n        shared=True,\n    )\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.batch_predict","title":"<code>batch_predict(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor(1)</code> <p>Value of NegativeLogLikelihood loss for the batch</p> <code>Tensor(batch_size, n_items)</code> <p>Probabilities for each product to be chosen for each choice</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>@tf.function\ndef batch_predict(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor (1, )\n        Value of NegativeLogLikelihood loss for the batch\n    tf.Tensor (batch_size, n_items)\n        Probabilities for each product to be chosen for each choice\n    \"\"\"\n    # Compute utilities from features\n    utilities = self.compute_batch_utility(\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    )\n\n    batch_size = utilities.shape[0]\n    batch_gammas = []\n    if self.shared_gammas_over_nests:\n        batch_gammas = self.trainable_weights[-1][0, 0] * tf.ones_like(utilities)\n    else:\n        for i in range(len(self.items_to_nest)):\n            if self.items_to_nest[i] == -1:\n                batch_gammas.append([tf.constant(1.0)] * batch_size)\n            else:\n                batch_gammas.append(\n                    [self.trainable_weights[-1][0, self.items_to_nest[i]]] * batch_size\n                )\n        batch_gammas = tf.stack(batch_gammas, axis=-1)\n\n    probabilities = nested_softmax_with_availabilities(\n        items_logit_by_choice=utilities,\n        available_items_by_choice=available_items_by_choice,\n        items_nests=tf.constant(self.items_to_nest),\n        gammas=batch_gammas,\n        normalize_exit=self.add_exit_choice,\n    )\n\n    # Compute loss from probabilities &amp; actual choices\n    batch_loss = {\n        \"optimized_loss\": self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n        \"Exact-NegativeLogLikelihood\": self.exact_nll(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n    }\n    return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.clone","title":"<code>clone()</code>","text":"<p>Return a clone/deepcopy of the model.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def clone(self):\n    \"\"\"Return a clone/deepcopy of the model.\"\"\"\n    clone = NestedLogit(\n        coefficients=self.coefficients,\n        add_exit_choice=self.add_exit_choice,\n        optimizer=self.optimizer_name,\n        items_nests=self.items_nests,\n    )\n    if hasattr(self, \"history\"):\n        clone.history = self.history\n    if hasattr(self, \"is_fitted\"):\n        clone.is_fitted = self.is_fitted\n    if hasattr(self, \"instantiated\"):\n        clone.instantiated = self.instantiated\n    if hasattr(self, \"shared_gammas_over_nests\"):\n        clone.shared_gammas_over_nests = self.shared_gammas_over_nests\n    clone.loss = self.loss\n    clone.label_smoothing = self.label_smoothing\n    if hasattr(self, \"report\"):\n        clone.report = self.report\n    if hasattr(self, \"_trainable_weights\"):\n        clone._trainable_weights = self._trainable_weights\n    if hasattr(self, \"lr\"):\n        clone.lr = self.lr\n    if hasattr(self, \"_shared_features_by_choice_names\"):\n        clone._shared_features_by_choice_names = self._shared_features_by_choice_names\n    if hasattr(self, \"_items_features_by_choice_names\"):\n        clone._items_features_by_choice_names = self._items_features_by_choice_names\n    return clone\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, verbose=1)</code>","text":"<p>Compute the utility when the model is constructed from a MNLCoefficients object.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <p>Choices Shape must be (n_choices, )</p> required <code>verbose</code> <code>int</code> <p>Parametrization of the logging outputs, by default 1</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Utilities corresponding of shape (n_choices, n_items)</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    verbose=1,\n):\n    \"\"\"Compute the utility when the model is constructed from a MNLCoefficients object.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices: np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    verbose : int, optional\n        Parametrization of the logging outputs, by default 1\n\n    Returns\n    -------\n    tf.Tensor\n        Utilities corresponding of shape (n_choices, n_items)\n    \"\"\"\n    _ = choices\n\n    n_items = available_items_by_choice.shape[1]\n    n_choices = available_items_by_choice.shape[0]\n    items_utilities_by_choice = []\n\n    if not isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = (shared_features_by_choice,)\n    if not isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = (items_features_by_choice,)\n\n    # Shared features\n    if self._shared_features_by_choice_names is not None:\n        for i, feat_tuple in enumerate(self._shared_features_by_choice_names):\n            for j, feat in enumerate(feat_tuple):\n                if feat in self.coefficients.features_with_weights:\n                    (\n                        item_index_list,\n                        weight_index_list,\n                    ) = self.coefficients.get_weight_item_indexes(feat)\n                    for item_index, weight_index in zip(item_index_list, weight_index_list):\n                        partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n                        partial_items_utility_by_choice = [\n                            tf.zeros(n_choices) for _ in range(n_items)\n                        ]\n\n                        for q, idx in enumerate(item_index):\n                            if isinstance(idx, list):\n                                for k in idx:\n                                    tf.cast(shared_features_by_choice[i][:, j], tf.float32)\n                                    compute = tf.multiply(\n                                        shared_features_by_choice[i][:, j],\n                                        self.trainable_weights[weight_index][:, q],\n                                    )\n                                    partial_items_utility_by_choice[k] += compute\n                            else:\n                                compute = tf.multiply(\n                                    tf.cast(shared_features_by_choice[i][:, j], tf.float32),\n                                    self.trainable_weights[weight_index][:, q],\n                                )\n                                partial_items_utility_by_choice[idx] += compute\n\n                        items_utilities_by_choice.append(\n                            tf.cast(\n                                tf.stack(partial_items_utility_by_choice, axis=1), tf.float32\n                            )\n                        )\n                elif verbose &gt; 0:\n                    logging.info(\n                        f\"Feature {feat} is in dataset but has no weight assigned\\\n                            in utility computations\"\n                    )\n\n    # Items features\n    if self._items_features_by_choice_names is not None:\n        for i, feat_tuple in enumerate(self._items_features_by_choice_names):\n            for j, feat in enumerate(feat_tuple):\n                if feat in self.coefficients.features_with_weights:\n                    (\n                        item_index_list,\n                        weight_index_list,\n                    ) = self.coefficients.get_weight_item_indexes(feat)\n                    for item_index, weight_index in zip(item_index_list, weight_index_list):\n                        partial_items_utility_by_choice = tf.zeros((n_choices, n_items))\n\n                        for q, idx in enumerate(item_index):\n                            if isinstance(idx, list):\n                                for k in idx:\n                                    partial_items_utility_by_choice = tf.concat(\n                                        [\n                                            partial_items_utility_by_choice[:, :k],\n                                            tf.expand_dims(\n                                                tf.multiply(\n                                                    tf.cast(\n                                                        items_features_by_choice[i][:, k, j],\n                                                        tf.float32,\n                                                    ),\n                                                    self.trainable_weights[weight_index][:, q],\n                                                ),\n                                                axis=-1,\n                                            ),\n                                            partial_items_utility_by_choice[:, k + 1 :],\n                                        ],\n                                        axis=1,\n                                    )\n                            else:\n                                partial_items_utility_by_choice = tf.concat(\n                                    [\n                                        partial_items_utility_by_choice[:, :idx],\n                                        tf.expand_dims(\n                                            tf.multiply(\n                                                tf.cast(\n                                                    items_features_by_choice[i][:, idx, j],\n                                                    tf.float32,\n                                                ),\n                                                self.trainable_weights[weight_index][:, q],\n                                            ),\n                                            axis=-1,\n                                        ),\n                                        partial_items_utility_by_choice[:, idx + 1 :],\n                                    ],\n                                    axis=1,\n                                )\n\n                        items_utilities_by_choice.append(\n                            tf.cast(partial_items_utility_by_choice, tf.float32)\n                        )\n                elif verbose &gt; 0:\n                    logging.info(\n                        f\"Feature {feat} is in dataset but has no weight assigned\\\n                            in utility computations\"\n                    )\n\n    if \"intercept\" in self.coefficients.features_with_weights:\n        item_index_list, weight_index_list = self.coefficients.get_weight_item_indexes(\n            \"intercept\"\n        )\n\n        for item_index, weight_index in zip(item_index_list, weight_index_list):\n            partial_items_utility_by_choice = tf.zeros((n_items,))\n            for q, idx in enumerate(item_index):\n                if isinstance(idx, list):\n                    for idx_idx in idx:\n                        partial_items_utility_by_choice = tf.concat(\n                            [\n                                partial_items_utility_by_choice[:idx_idx],\n                                self.trainable_weights[weight_index][:, q],\n                                partial_items_utility_by_choice[idx_idx + 1 :],\n                            ],\n                            axis=0,\n                        )\n                else:\n                    partial_items_utility_by_choice = tf.concat(\n                        [\n                            partial_items_utility_by_choice[:idx],\n                            self.trainable_weights[weight_index][:, q],\n                            partial_items_utility_by_choice[idx + 1 :],\n                        ],\n                        axis=0,\n                    )\n\n            partial_items_utility_by_choice = tf.stack(\n                [partial_items_utility_by_choice] * n_choices, axis=0\n            )\n\n            items_utilities_by_choice.append(\n                tf.cast(partial_items_utility_by_choice, tf.float32)\n            )\n\n    return tf.reduce_sum(items_utilities_by_choice, axis=0)\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.compute_report","title":"<code>compute_report(choice_dataset)</code>","text":"<p>Compute a report of the estimated weights.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DF with estimation, Std Err, z_value and p_value for each coefficient.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def compute_report(self, choice_dataset):\n    \"\"\"Compute a report of the estimated weights.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DF with estimation, Std Err, z_value and p_value for each coefficient.\n    \"\"\"\n    import tensorflow_probability as tfp\n\n    weights_std = self.get_weights_std(choice_dataset)\n    dist = tfp.distributions.Normal(loc=0.0, scale=1.0)\n\n    names = []\n    z_values = []\n    estimations = []\n    p_z = []\n    i = 0\n    for weight in self.trainable_weights:\n        for j in range(weight.shape[1]):\n            if weight.shape[1] &gt; 1:\n                names.append(f\"{weight.name[:-2]}_{j}\")\n            else:\n                names.append(f\"{weight.name[:-2]}\")\n            estimations.append(weight.numpy()[0][j])\n            z_values.append(weight.numpy()[0][j] / weights_std[i].numpy())\n            p_z.append(2 * (1 - dist.cdf(tf.math.abs(z_values[-1])).numpy()))\n            i += 1\n\n    return pd.DataFrame(\n        {\n            \"Coefficient Name\": names,\n            \"Coefficient Estimation\": estimations,\n            \"Std. Err\": weights_std.numpy(),\n            \"z_value\": z_values,\n            \"P(.&gt;z)\": p_z,\n        },\n    )\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.fit","title":"<code>fit(choice_dataset, get_report=False, **kwargs)</code>","text":"<p>Fit function to estimate the paramters.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Choice dataset to use for the estimation.</p> required <code>get_report</code> <p>Whether or not to compute a report of the estimation, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict with fit history.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def fit(self, choice_dataset, get_report=False, **kwargs):\n    \"\"\"Fit function to estimate the paramters.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Choice dataset to use for the estimation.\n    get_report: bool, optional\n        Whether or not to compute a report of the estimation, by default False\n\n    Returns\n    -------\n    dict\n        dict with fit history.\n    \"\"\"\n    self.instantiate(choice_dataset)\n\n    fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n    if get_report:\n        self.report = self.compute_report(choice_dataset)\n    return fit\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.get_weights_std","title":"<code>get_weights_std(choice_dataset)</code>","text":"<p>Approximates Std Err with Hessian matrix.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Estimation of the Std Err for the weights.</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def get_weights_std(self, choice_dataset):\n    \"\"\"Approximates Std Err with Hessian matrix.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    tf.Tensor\n        Estimation of the Std Err for the weights.\n    \"\"\"\n    # Loops of differentiation\n    with tf.GradientTape() as tape_1:\n        with tf.GradientTape(persistent=True) as tape_2:\n            model = self.clone()\n            w = tf.concat(self.trainable_weights, axis=1)\n            tape_2.watch(w)\n            tape_1.watch(w)\n            mw = []\n            index = 0\n            for _w in self.trainable_weights:\n                mw.append(w[:, index : index + _w.shape[1]])\n                index += _w.shape[1]\n            model._trainable_weights = mw\n            batch = next(choice_dataset.iter_batch(batch_size=-1))\n            utilities = model.compute_batch_utility(*batch)\n\n            batch_gammas = []\n            if model.shared_gammas_over_nests:\n                batch_gammas = model.trainable_weights[-1][0, 0] * tf.ones_like(utilities)\n            else:\n                for i in range(len(self.items_to_nest)):\n                    if model.items_to_nest[i] == -1:\n                        batch_gammas.append([tf.constant(1.0)] * len(choice_dataset))\n                    else:\n                        batch_gammas.append(\n                            [model.trainable_weights[-1][0, model.items_to_nest[i]]]\n                            * len(choice_dataset)\n                        )\n                batch_gammas = tf.stack(batch_gammas, axis=-1)\n\n            probabilities = nested_softmax_with_availabilities(\n                items_logit_by_choice=utilities,\n                available_items_by_choice=batch[2],\n                items_nests=tf.constant(model.items_to_nest),\n                gammas=batch_gammas,\n                normalize_exit=self.add_exit_choice,\n                eps=1e-15,\n            )\n            loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[1]),\n            )\n\n        # Compute the Jacobian\n        jacobian = tape_2.jacobian(loss, w)\n    # Compute the Hessian from the Jacobian\n    hessian = tape_1.batch_jacobian(jacobian, w)\n    inv_hessian = tf.linalg.inv(tf.squeeze(hessian))\n    return tf.sqrt(\n        [\n            tf.clip_by_value(inv_hessian[i][i], 0.0, tf.float32.max)\n            for i in range(len(tf.squeeze(hessian)))\n        ]\n    )\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.instantiate","title":"<code>instantiate(choice_dataset)</code>","text":"<p>Instantiate the model using the features in the choice_dataset.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <p>Used to match the features names with the model coefficients.</p> required Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def instantiate(self, choice_dataset):\n    \"\"\"Instantiate the model using the features in the choice_dataset.\n\n    Parameters\n    ----------\n    choice_dataset: ChoiceDataset\n        Used to match the features names with the model coefficients.\n    \"\"\"\n    if not self.instantiated:\n        if not isinstance(self.coefficients, MNLCoefficients):\n            self._build_coefficients_from_dict(n_items=choice_dataset.get_n_items())\n        self._trainable_weights = self._instantiate_tf_weights()\n\n        # Checking that no weight has been attributed to non existing feature in dataset\n        dataset_stacked_features_names = []\n        if choice_dataset.shared_features_by_choice_names is not None:\n            for i, feat_tuple in enumerate(choice_dataset.shared_features_by_choice_names):\n                dataset_stacked_features_names.append(feat_tuple)\n        if choice_dataset.items_features_by_choice_names is not None:\n            for i, feat_tuple in enumerate(choice_dataset.items_features_by_choice_names):\n                dataset_stacked_features_names.append(feat_tuple)\n        dataset_stacked_features_names = np.concatenate(dataset_stacked_features_names).ravel()\n\n        for feature_with_weight in self.coefficients.features_with_weights:\n            if feature_with_weight != \"intercept\":\n                if feature_with_weight not in dataset_stacked_features_names:\n                    raise ValueError(\n                        f\"\"\"Feature {feature_with_weight} has an attributed coefficient\n                        but is not in dataset\"\"\"\n                    )\n        self._store_dataset_features_names(choice_dataset)\n        self.instantiated = True\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.NestedLogit.train_step","title":"<code>train_step(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one training step (= one gradient descent step) of the model.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Value of NegativeLogLikelihood loss for the batch</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>@tf.function\ndef train_step(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor\n        Value of NegativeLogLikelihood loss for the batch\n    \"\"\"\n    with tf.GradientTape() as tape:\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n\n        batch_size = utilities.shape[0]\n        batch_gammas = []\n        if self.shared_gammas_over_nests:\n            batch_gammas = self.trainable_weights[-1][0, 0] * tf.ones_like(utilities)\n        else:\n            for i in range(len(self.items_to_nest)):\n                if self.items_to_nest[i] == -1:\n                    batch_gammas.append([tf.constant(1.0)] * batch_size)\n                else:\n                    batch_gammas.append(\n                        [self.trainable_weights[-1][0, self.items_to_nest[i]]] * batch_size\n                    )\n            batch_gammas = tf.stack(batch_gammas, axis=-1)\n\n        probabilities = nested_softmax_with_availabilities(\n            items_logit_by_choice=utilities,\n            available_items_by_choice=available_items_by_choice,\n            items_nests=tf.constant(self.items_to_nest),\n            gammas=batch_gammas,\n            normalize_exit=self.add_exit_choice,\n        )\n        # Negative Log-Likelihood\n        neg_loglikelihood = self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        )\n        if self.regularization is not None:\n            regularization = tf.reduce_sum(\n                [self.regularizer(w) for w in self.trainable_weights]\n            )\n            neg_loglikelihood += regularization\n\n    grads = tape.gradient(neg_loglikelihood, self.trainable_weights)\n    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n    return neg_loglikelihood\n</code></pre>"},{"location":"references/models/references_nested_logit/#choice_learn.models.nested_logit.nested_softmax_with_availabilities","title":"<code>nested_softmax_with_availabilities(items_logit_by_choice, available_items_by_choice, items_nests, gammas, normalize_exit=False, eps=1e-17)</code>","text":"<p>Compute softmax probabilities from utilities and items repartition within nests.</p> <p>Takes into account availabilties (1 if the product is available, 0 otherwise) to set probabilities to 0 for unavailable products and to renormalize the probabilities of available products. Takes also into account Items nest to compute a two step probability: first, probability to choose a given nest then probability to choose a product within this nest. See Nested Logit formulation for more details.</p> <p>Parameters:</p> Name Type Description Default <code>items_logit_by_choice</code> <code>ndarray(n_choices, n_items)</code> <p>Utilities / Logits on which to compute the softmax</p> required <code>available_items_by_choice</code> <code>ndarray(n_choices, n_items)</code> <p>Matrix indicating the availabitily (1) or not (0) of the products</p> required <code>items_nests</code> <code>ndarray(n_items)</code> <p>Nest index for each item  # Beware that nest index matches well gammas, it is not verified.</p> required <code>gammas</code> <code>np.ndarray of shape (n_choices, n_items)</code> <p>Nest gammas value that must be reshaped so that it matches items_logit_by_choice items_gammas_by_choice ?</p> required <code>normalize_exit</code> <code>bool</code> <p>Whether to normalize the probabilities of available products with an exit choice of utility 1, by default False</p> <code>False</code> <code>eps</code> <code>float</code> <p>Value to avoid division by 0 when a product with probability almost 1 is unavailable, by default 1e-5</p> <code>1e-17</code> <p>Returns:</p> Type Description <code>Tensor(n_choices, n_items)</code> <p>Probabilities of each product for each choice computed from Logits</p> Source code in <code>choice_learn/models/nested_logit.py</code> <pre><code>def nested_softmax_with_availabilities(\n    items_logit_by_choice,\n    available_items_by_choice,\n    items_nests,\n    gammas,\n    normalize_exit=False,\n    eps=1e-17,\n):\n    \"\"\"Compute softmax probabilities from utilities and items repartition within nests.\n\n    Takes into account availabilties (1 if the product is available, 0 otherwise) to set\n    probabilities to 0 for unavailable products and to renormalize the probabilities of\n    available products.\n    Takes also into account Items nest to compute a two step probability: first, probability\n    to choose a given nest then probability to choose a product within this nest.\n    See Nested Logit formulation for more details.\n\n    Parameters\n    ----------\n    items_logit_by_choice : np.ndarray (n_choices, n_items)\n        Utilities / Logits on which to compute the softmax\n    available_items_by_choice : np.ndarray (n_choices, n_items)\n        Matrix indicating the availabitily (1) or not (0) of the products\n    items_nests : np.ndarray (n_items)\n        Nest index for each item  # Beware that nest index matches well gammas,\n        it is not verified.\n    gammas : np.ndarray of shape (n_choices, n_items)\n        Nest gammas value that must be reshaped so that it matches items_logit_by_choice\n        items_gammas_by_choice ?\n    normalize_exit : bool, optional\n        Whether to normalize the probabilities of available products with an exit choice of\n        utility 1, by default False\n    eps : float, optional\n        Value to avoid division by 0 when a product with probability almost 1 is unavailable,\n        by default 1e-5\n\n    Returns\n    -------\n    tf.Tensor (n_choices, n_items)\n        Probabilities of each product for each choice computed from Logits\n    \"\"\"\n    if tf.reduce_any(gammas &lt; 0.05):\n        logging.warning(\n            \"\"\"At least one gamma value for nests is below 0.05 and is\n        clipped to 0.05 for numeric optimization purposes.\"\"\"\n        )\n    gammas = tf.clip_by_value(gammas, 0.05, tf.float32.max)\n    numerator = tf.exp(tf.clip_by_value(items_logit_by_choice / gammas, tf.float32.min, 50))\n    # Set unavailable products utility to 0\n    numerator = tf.multiply(numerator, available_items_by_choice)\n    items_nest_utility = tf.zeros_like(numerator) + eps\n    for nest_index in tf.unique(items_nests)[0]:\n        stack = tf.boolean_mask(numerator, items_nests == nest_index, axis=1)\n        nest_utility = tf.reduce_sum(\n            stack,\n            axis=-1,\n            keepdims=True,\n        )\n        items_nest_utility += nest_utility * tf.cast(items_nests == nest_index, tf.float32)\n    numerator = numerator * (items_nest_utility ** (gammas - 1))\n    # Sum of total available utilities\n    denominator = tf.reduce_sum(numerator, axis=-1, keepdims=True)\n    # Add 1 to the denominator to take into account the exit choice\n    if normalize_exit:\n        denominator += 1\n    # Avoir division by 0 when only unavailable items have highest utilities\n    elif eps:\n        denominator += eps\n    # Compute softmax\n    return numerator / denominator\n</code></pre>"},{"location":"references/models/references_reslogit/","title":"ResLogit Model","text":"<p>Implementation of ResLogit for easy use.</p>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLayer","title":"<code>ResLayer</code>","text":"<p>             Bases: <code>Layer</code></p> <p>The residual layer class.</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>class ResLayer(tf.keras.layers.Layer):\n    \"\"\"The residual layer class.\"\"\"\n\n    def get_activation_function(self, name):\n        \"\"\"Get an activation function from its str name.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function to apply.\n\n        Returns\n        -------\n        function\n            Tensorflow function to apply.\n        \"\"\"\n        if name == \"linear\":\n            return lambda x: x\n        if name == \"relu\":\n            return tf.nn.relu\n        if name == \"-relu\":\n            return lambda x: -tf.nn.relu(-x)\n        if name == \"tanh\":\n            return tf.nn.tanh\n        if name == \"sigmoid\":\n            return tf.nn.sigmoid\n        if name == \"softplus\":\n            return tf.math.softplus\n        raise ValueError(f\"Activation function {name} not supported.\")\n\n    def __init__(self, layer_width=None, activation=\"softplus\"):\n        \"\"\"Initialize the ResLayer class.\n\n        Parameters\n        ----------\n        layer_width : int, optional\n            Width of the layer, by default None\n            If None, the width of the layer is the same as the input shape\n        activation : str, optional\n            Activation function to use in the layer, by default \"softplus\"\n        \"\"\"\n        self.layer_width = layer_width\n        self.activation = self.get_activation_function(activation)\n\n        super().__init__()\n\n    def build(self, input_shape):\n        \"\"\"Create the state of the layer (weights).\n\n        The build() method is automatically invoked by the first __call__() to the layer.\n\n        Parameters\n        ----------\n        input_shape : tuple\n            Shape of the input of the layer. Typically (batch_size, num_features)\n            Batch_size (None) is ignored, but num_features is the shape of the input\n        \"\"\"\n        self.num_features = input_shape[-1]\n\n        # If None, the width of the layer is the same as the input shape\n        if self.layer_width is None:\n            self.layer_width = input_shape[-1]\n\n        # Random normal initialization of the weights\n        # Shape of the weights: (num_features, layer_width)\n        self.add_weight(\n            shape=(self.num_features, self.layer_width),\n            initializer=\"random_normal\",\n            trainable=True,\n            name=\"res_weight\",\n        )\n\n    def call(self, input):\n        \"\"\"Return the output of the residual layer.\n\n        Parameters\n        ----------\n        inputs : tf.Variable\n            Input of the residual layer\n\n        Returns\n        -------\n        tf.Variable\n            Output of the residual layer\n        \"\"\"\n        lin_output = tf.matmul(input, self.trainable_variables[0])\n\n        # Ensure the dimensions are compatible for subtraction\n        if input.shape != lin_output.shape:\n            # Then perform a linear projection to match the dimensions\n            input = tf.matmul(input, tf.ones((self.num_features, self.layer_width)))\n\n        # Softplus: smooth approximation of ReLU\n        return input - self.activation(tf.cast(lin_output, tf.float32))\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"Compute the output shape of the layer.\n\n        Automatically used when calling ResLayer.call() to infer the shape of the output.\n\n        Parameters\n        ----------\n        input_shape : tuple\n            Shape of the input of the layer. Typically (batch_size, num_features)\n            Batch_size (None) is ignored, but num_features is the shape of the input\n\n        Returns\n        -------\n        tuple\n            Shape of the output of the layer\n        \"\"\"\n        return (input_shape[0], self.layer_width)\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLayer.__init__","title":"<code>__init__(layer_width=None, activation='softplus')</code>","text":"<p>Initialize the ResLayer class.</p> <p>Parameters:</p> Name Type Description Default <code>layer_width</code> <code>int</code> <p>Width of the layer, by default None If None, the width of the layer is the same as the input shape</p> <code>None</code> <code>activation</code> <code>str</code> <p>Activation function to use in the layer, by default \"softplus\"</p> <code>'softplus'</code> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def __init__(self, layer_width=None, activation=\"softplus\"):\n    \"\"\"Initialize the ResLayer class.\n\n    Parameters\n    ----------\n    layer_width : int, optional\n        Width of the layer, by default None\n        If None, the width of the layer is the same as the input shape\n    activation : str, optional\n        Activation function to use in the layer, by default \"softplus\"\n    \"\"\"\n    self.layer_width = layer_width\n    self.activation = self.get_activation_function(activation)\n\n    super().__init__()\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLayer.build","title":"<code>build(input_shape)</code>","text":"<p>Create the state of the layer (weights).</p> <p>The build() method is automatically invoked by the first call() to the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input of the layer. Typically (batch_size, num_features) Batch_size (None) is ignored, but num_features is the shape of the input</p> required Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def build(self, input_shape):\n    \"\"\"Create the state of the layer (weights).\n\n    The build() method is automatically invoked by the first __call__() to the layer.\n\n    Parameters\n    ----------\n    input_shape : tuple\n        Shape of the input of the layer. Typically (batch_size, num_features)\n        Batch_size (None) is ignored, but num_features is the shape of the input\n    \"\"\"\n    self.num_features = input_shape[-1]\n\n    # If None, the width of the layer is the same as the input shape\n    if self.layer_width is None:\n        self.layer_width = input_shape[-1]\n\n    # Random normal initialization of the weights\n    # Shape of the weights: (num_features, layer_width)\n    self.add_weight(\n        shape=(self.num_features, self.layer_width),\n        initializer=\"random_normal\",\n        trainable=True,\n        name=\"res_weight\",\n    )\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLayer.call","title":"<code>call(input)</code>","text":"<p>Return the output of the residual layer.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Variable</code> <p>Input of the residual layer</p> required <p>Returns:</p> Type Description <code>Variable</code> <p>Output of the residual layer</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def call(self, input):\n    \"\"\"Return the output of the residual layer.\n\n    Parameters\n    ----------\n    inputs : tf.Variable\n        Input of the residual layer\n\n    Returns\n    -------\n    tf.Variable\n        Output of the residual layer\n    \"\"\"\n    lin_output = tf.matmul(input, self.trainable_variables[0])\n\n    # Ensure the dimensions are compatible for subtraction\n    if input.shape != lin_output.shape:\n        # Then perform a linear projection to match the dimensions\n        input = tf.matmul(input, tf.ones((self.num_features, self.layer_width)))\n\n    # Softplus: smooth approximation of ReLU\n    return input - self.activation(tf.cast(lin_output, tf.float32))\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLayer.compute_output_shape","title":"<code>compute_output_shape(input_shape)</code>","text":"<p>Compute the output shape of the layer.</p> <p>Automatically used when calling ResLayer.call() to infer the shape of the output.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input of the layer. Typically (batch_size, num_features) Batch_size (None) is ignored, but num_features is the shape of the input</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Shape of the output of the layer</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def compute_output_shape(self, input_shape):\n    \"\"\"Compute the output shape of the layer.\n\n    Automatically used when calling ResLayer.call() to infer the shape of the output.\n\n    Parameters\n    ----------\n    input_shape : tuple\n        Shape of the input of the layer. Typically (batch_size, num_features)\n        Batch_size (None) is ignored, but num_features is the shape of the input\n\n    Returns\n    -------\n    tuple\n        Shape of the output of the layer\n    \"\"\"\n    return (input_shape[0], self.layer_width)\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLayer.get_activation_function","title":"<code>get_activation_function(name)</code>","text":"<p>Get an activation function from its str name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the function to apply.</p> required <p>Returns:</p> Type Description <code>function</code> <p>Tensorflow function to apply.</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def get_activation_function(self, name):\n    \"\"\"Get an activation function from its str name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the function to apply.\n\n    Returns\n    -------\n    function\n        Tensorflow function to apply.\n    \"\"\"\n    if name == \"linear\":\n        return lambda x: x\n    if name == \"relu\":\n        return tf.nn.relu\n    if name == \"-relu\":\n        return lambda x: -tf.nn.relu(-x)\n    if name == \"tanh\":\n        return tf.nn.tanh\n    if name == \"sigmoid\":\n        return tf.nn.sigmoid\n    if name == \"softplus\":\n        return tf.math.softplus\n    raise ValueError(f\"Activation function {name} not supported.\")\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLogit","title":"<code>ResLogit</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>The ResLogit class.</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>class ResLogit(ChoiceModel):\n    \"\"\"The ResLogit class.\"\"\"\n\n    def __init__(\n        self,\n        intercept=\"item\",\n        n_layers=16,\n        res_layers_width=None,\n        activation=\"softplus\",\n        label_smoothing=0.0,\n        optimizer=\"SGD\",\n        tolerance=1e-8,\n        lr=0.001,\n        epochs=1000,\n        batch_size=32,\n        logmin=1e-5,\n        **kwargs,\n    ):\n        \"\"\"Initialize the ResLogit class.\n\n        Parameters\n        ----------\n        intercept: str, optional\n            Type of intercept to use, by default None\n        n_layers : int\n            Number of residual layers.\n        res_layers_width : list of int, optional\n            Width of the *hidden* residual layers, by default None\n            If None, all the residual layers have the same width (n_items)\n            The length of the list should be equal to n_layers - 1\n            The last element of the list should be equal to n_items\n        activation : str, optional\n            Activation function to use in the residual layers, by default \"softplus\"\n        label_smoothing : float, optional\n            Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing\n        optimizer: str\n            String representation of the TensorFlow optimizer to be used for estimation,\n            by default \"SGD\"\n            Should be within tf.keras.optimizers\n        tolerance : float, optional\n            Tolerance for the L-BFGS optimizer if applied, by default 1e-8\n        lr: float, optional\n            Learning rate for the optimizer if applied, by default 0.001\n        epochs: int, optional\n            (Max) Number of epochs to train the model, by default 1000\n        batch_size: int, optional\n            Batch size in the case of stochastic gradient descent optimizer\n            Not used in the case of L-BFGS optimizer, by default 32\n        logmin : float, optional\n            Value to be added within log computation to avoid infinity, by default 1e-5\n        \"\"\"\n        super().__init__(\n            self,\n            optimizer=optimizer,\n            **kwargs,\n        )\n        self.intercept = intercept\n        self.n_layers = n_layers\n        self.res_layers_width = res_layers_width\n        self.activation = activation\n\n        # Optimization parameters\n        self.label_smoothing = label_smoothing\n        self.tolerance = tolerance\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.logmin = logmin\n\n        self.instantiated = False\n\n    def instantiate(self, n_items, n_shared_features, n_items_features):\n        \"\"\"Instantiate the model from ModelSpecification object.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of items/aternatives to consider\n        n_shared_features : int\n            Number of contexts features\n        n_items_features : int\n            Number of contexts items features\n\n        Returns\n        -------\n        indexes : dict\n            Dictionary of the indexes of the weights created\n        weights : list of tf.Variable\n            List of the weights created coresponding to the specification\n        \"\"\"\n        # Instantiate the loss function\n        self.loss = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False,\n            label_smoothing=self.label_smoothing,\n            epsilon=self.logmin,\n        )\n\n        # Instantiate the weights\n        mnl_weights = []\n        indexes = {}\n\n        # Create the betas parameters for the shared and items features\n        for n_feat, feat_name in zip(\n            [n_shared_features, n_items_features],\n            [\"shared_features\", \"items_features\"],\n        ):\n            if n_feat &gt; 0:\n                mnl_weights += [\n                    tf.Variable(\n                        tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_feat,)),\n                        name=f\"Betas_{feat_name}\",\n                    )\n                ]\n                indexes[feat_name] = len(mnl_weights) - 1\n\n        # Create the alphas parameters\n        if self.intercept is None:\n            logging.info(\"No intercept in the model\")\n        elif self.intercept == \"item\":\n            mnl_weights.append(\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items - 1,)),\n                    name=\"Intercept\",\n                )\n            )\n            indexes[\"intercept\"] = len(mnl_weights) - 1\n        elif self.intercept == \"item-full\":\n            logging.info(\"Simple MNL intercept is not normalized to 0!\")\n            mnl_weights.append(\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items,)),\n                    name=\"Intercept\",\n                )\n            )\n            indexes[\"intercept\"] = len(mnl_weights) - 1\n        else:\n            mnl_weights.append(\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1,)),\n                    name=\"Intercept\",\n                )\n            )\n            indexes[\"intercept\"] = len(mnl_weights) - 1\n\n        # Create the residual layer\n        resnet_input = tf.keras.layers.Input(shape=(n_items,))\n        resnet_output = tf.keras.layers.Identity()(resnet_input)\n\n        if self.res_layers_width is None:\n            # Common width by default for all the residual layers: n_items\n            # (Like in the original paper of ResLogit)\n            layers = [ResLayer(activation=self.activation) for _ in range(self.n_layers)]\n\n        else:\n            # Different width for each *hidden* residual layer\n            if self.n_layers &gt; 0 and len(self.res_layers_width) != self.n_layers - 1:\n                raise ValueError(\n                    \"The length of the res_layers_width list should be equal to n_layers - 1\"\n                )\n            if self.n_layers &gt; 1 and self.res_layers_width[-1] != n_items:\n                raise ValueError(\"The width of the last residual layer should be equal to n_items\")\n\n            # Initialize the residual layers\n            if self.n_layers == 0:\n                layers = []\n            else:\n                # The first layer has the same width as the input\n                layers = [ResLayer(activation=self.activation)]\n                for i in range(1, self.n_layers):\n                    # The other layers have a width defined by the res_layers_width parameter\n                    layers.append(\n                        ResLayer(\n                            layer_width=self.res_layers_width[i - 1], activation=self.activation\n                        )\n                    )\n\n        # Build the residual layers\n        for layer in layers:\n            resnet_output = layer(resnet_output)\n\n        resnet_model = tf.keras.Model(\n            inputs=resnet_input, outputs=resnet_output, name=f\"resnet_with_{self.n_layers}_layers\"\n        )\n\n        self.instantiated = True\n        self.resnet_model = resnet_model\n        self.indexes = indexes\n        self.mnl_weights = mnl_weights\n        # Concatenation of all the trainable weights\n        self._trainable_weights = self.mnl_weights + self.resnet_model.trainable_variables\n\n        return self.indexes, self._trainable_weights\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return self.mnl_weights + self.resnet_model.trainable_variables\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute utility from a batch of ChoiceDataset.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        tf.Tensor\n            Computed utilities of shape (n_choices, n_items)\n        \"\"\"\n        (_, _) = available_items_by_choice, choices  # Avoid unused variable warning\n\n        batch_size = shared_features_by_choice.shape[0]  # Other name: n_choices\n        n_items = items_features_by_choice.shape[1]\n\n        # Deterministic component of the utility\n        if \"shared_features\" in self.indexes.keys():\n            if isinstance(shared_features_by_choice, tuple):\n                shared_features_by_choice = tf.concat(*shared_features_by_choice, axis=1)\n            shared_features_by_choice = tf.cast(shared_features_by_choice, tf.float32)\n            shared_features_utilities = tf.tensordot(\n                shared_features_by_choice,\n                self.trainable_weights[self.indexes[\"shared_features\"]],\n                axes=1,\n            )\n            shared_features_utilities = tf.expand_dims(shared_features_utilities, axis=-1)\n        else:\n            shared_features_utilities = 0\n        shared_features_utilities = tf.squeeze(shared_features_utilities)\n\n        if \"items_features\" in self.indexes.keys():\n            if isinstance(items_features_by_choice, tuple):\n                items_features_by_choice = tf.concat([*items_features_by_choice], axis=2)\n            items_features_by_choice = tf.cast(items_features_by_choice, tf.float32)\n            items_features_utilities = tf.tensordot(\n                items_features_by_choice,\n                self.trainable_weights[self.indexes[\"items_features\"]],\n                axes=1,\n            )\n        else:\n            items_features_utilities = tf.zeros((batch_size, n_items))\n\n        if \"intercept\" in self.indexes.keys():\n            intercept = self.trainable_weights[self.indexes[\"intercept\"]]\n            if self.intercept == \"item\":\n                intercept = tf.concat([tf.constant([0.0]), intercept], axis=0)\n            if self.intercept in [\"item\", \"item-full\"]:\n                intercept = tf.expand_dims(intercept, axis=0)\n        else:\n            intercept = 0\n\n        shared_features_utilities = tf.tile(\n            tf.expand_dims(shared_features_utilities, axis=-1), [1, n_items]\n        )\n        deterministic_utilities_without_intercept = (\n            shared_features_utilities + items_features_utilities\n        )\n        deterministic_utilities = deterministic_utilities_without_intercept + intercept\n\n        # Residual component of the utility\n        residual_utilities = self.resnet_model(deterministic_utilities_without_intercept)\n        residual_utilities = tf.cast(residual_utilities, tf.float32)\n\n        return deterministic_utilities + residual_utilities\n\n    def fit(self, choice_dataset, get_report=False, **kwargs):\n        \"\"\"Fit to estimate the parameters.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False\n\n        Returns\n        -------\n        fit : dict\n            dict with fit history\n        \"\"\"\n        if not self.instantiated:\n            # Lazy Instantiation\n            self.indexes, self._trainable_weights = self.instantiate(\n                n_items=choice_dataset.get_n_items(),\n                n_shared_features=choice_dataset.get_n_shared_features(),\n                n_items_features=choice_dataset.get_n_items_features(),\n            )\n            self.instantiated = True\n        fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLogit.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLogit.__init__","title":"<code>__init__(intercept='item', n_layers=16, res_layers_width=None, activation='softplus', label_smoothing=0.0, optimizer='SGD', tolerance=1e-08, lr=0.001, epochs=1000, batch_size=32, logmin=1e-05, **kwargs)</code>","text":"<p>Initialize the ResLogit class.</p> <p>Parameters:</p> Name Type Description Default <code>intercept</code> <p>Type of intercept to use, by default None</p> <code>'item'</code> <code>n_layers</code> <code>int</code> <p>Number of residual layers.</p> <code>16</code> <code>res_layers_width</code> <code>list of int</code> <p>Width of the hidden residual layers, by default None If None, all the residual layers have the same width (n_items) The length of the list should be equal to n_layers - 1 The last element of the list should be equal to n_items</p> <code>None</code> <code>activation</code> <code>str</code> <p>Activation function to use in the residual layers, by default \"softplus\"</p> <code>'softplus'</code> <code>label_smoothing</code> <code>float</code> <p>Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing</p> <code>0.0</code> <code>optimizer</code> <p>String representation of the TensorFlow optimizer to be used for estimation, by default \"SGD\" Should be within tf.keras.optimizers</p> <code>'SGD'</code> <code>tolerance</code> <code>float</code> <p>Tolerance for the L-BFGS optimizer if applied, by default 1e-8</p> <code>1e-08</code> <code>lr</code> <p>Learning rate for the optimizer if applied, by default 0.001</p> <code>0.001</code> <code>epochs</code> <p>(Max) Number of epochs to train the model, by default 1000</p> <code>1000</code> <code>batch_size</code> <p>Batch size in the case of stochastic gradient descent optimizer Not used in the case of L-BFGS optimizer, by default 32</p> <code>32</code> <code>logmin</code> <code>float</code> <p>Value to be added within log computation to avoid infinity, by default 1e-5</p> <code>1e-05</code> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def __init__(\n    self,\n    intercept=\"item\",\n    n_layers=16,\n    res_layers_width=None,\n    activation=\"softplus\",\n    label_smoothing=0.0,\n    optimizer=\"SGD\",\n    tolerance=1e-8,\n    lr=0.001,\n    epochs=1000,\n    batch_size=32,\n    logmin=1e-5,\n    **kwargs,\n):\n    \"\"\"Initialize the ResLogit class.\n\n    Parameters\n    ----------\n    intercept: str, optional\n        Type of intercept to use, by default None\n    n_layers : int\n        Number of residual layers.\n    res_layers_width : list of int, optional\n        Width of the *hidden* residual layers, by default None\n        If None, all the residual layers have the same width (n_items)\n        The length of the list should be equal to n_layers - 1\n        The last element of the list should be equal to n_items\n    activation : str, optional\n        Activation function to use in the residual layers, by default \"softplus\"\n    label_smoothing : float, optional\n        Whether (then is ]O, 1[ value) or not (then can be None or 0) to use label smoothing\n    optimizer: str\n        String representation of the TensorFlow optimizer to be used for estimation,\n        by default \"SGD\"\n        Should be within tf.keras.optimizers\n    tolerance : float, optional\n        Tolerance for the L-BFGS optimizer if applied, by default 1e-8\n    lr: float, optional\n        Learning rate for the optimizer if applied, by default 0.001\n    epochs: int, optional\n        (Max) Number of epochs to train the model, by default 1000\n    batch_size: int, optional\n        Batch size in the case of stochastic gradient descent optimizer\n        Not used in the case of L-BFGS optimizer, by default 32\n    logmin : float, optional\n        Value to be added within log computation to avoid infinity, by default 1e-5\n    \"\"\"\n    super().__init__(\n        self,\n        optimizer=optimizer,\n        **kwargs,\n    )\n    self.intercept = intercept\n    self.n_layers = n_layers\n    self.res_layers_width = res_layers_width\n    self.activation = activation\n\n    # Optimization parameters\n    self.label_smoothing = label_smoothing\n    self.tolerance = tolerance\n    self.lr = lr\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.logmin = logmin\n\n    self.instantiated = False\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLogit.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute utility from a batch of ChoiceDataset.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed utilities of shape (n_choices, n_items)</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute utility from a batch of ChoiceDataset.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    tf.Tensor\n        Computed utilities of shape (n_choices, n_items)\n    \"\"\"\n    (_, _) = available_items_by_choice, choices  # Avoid unused variable warning\n\n    batch_size = shared_features_by_choice.shape[0]  # Other name: n_choices\n    n_items = items_features_by_choice.shape[1]\n\n    # Deterministic component of the utility\n    if \"shared_features\" in self.indexes.keys():\n        if isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = tf.concat(*shared_features_by_choice, axis=1)\n        shared_features_by_choice = tf.cast(shared_features_by_choice, tf.float32)\n        shared_features_utilities = tf.tensordot(\n            shared_features_by_choice,\n            self.trainable_weights[self.indexes[\"shared_features\"]],\n            axes=1,\n        )\n        shared_features_utilities = tf.expand_dims(shared_features_utilities, axis=-1)\n    else:\n        shared_features_utilities = 0\n    shared_features_utilities = tf.squeeze(shared_features_utilities)\n\n    if \"items_features\" in self.indexes.keys():\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tf.concat([*items_features_by_choice], axis=2)\n        items_features_by_choice = tf.cast(items_features_by_choice, tf.float32)\n        items_features_utilities = tf.tensordot(\n            items_features_by_choice,\n            self.trainable_weights[self.indexes[\"items_features\"]],\n            axes=1,\n        )\n    else:\n        items_features_utilities = tf.zeros((batch_size, n_items))\n\n    if \"intercept\" in self.indexes.keys():\n        intercept = self.trainable_weights[self.indexes[\"intercept\"]]\n        if self.intercept == \"item\":\n            intercept = tf.concat([tf.constant([0.0]), intercept], axis=0)\n        if self.intercept in [\"item\", \"item-full\"]:\n            intercept = tf.expand_dims(intercept, axis=0)\n    else:\n        intercept = 0\n\n    shared_features_utilities = tf.tile(\n        tf.expand_dims(shared_features_utilities, axis=-1), [1, n_items]\n    )\n    deterministic_utilities_without_intercept = (\n        shared_features_utilities + items_features_utilities\n    )\n    deterministic_utilities = deterministic_utilities_without_intercept + intercept\n\n    # Residual component of the utility\n    residual_utilities = self.resnet_model(deterministic_utilities_without_intercept)\n    residual_utilities = tf.cast(residual_utilities, tf.float32)\n\n    return deterministic_utilities + residual_utilities\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLogit.fit","title":"<code>fit(choice_dataset, get_report=False, **kwargs)</code>","text":"<p>Fit to estimate the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Choice dataset to use for the estimation.</p> required <code>get_report</code> <p>Whether or not to compute a report of the estimation, by default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fit</code> <code>dict</code> <p>dict with fit history</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def fit(self, choice_dataset, get_report=False, **kwargs):\n    \"\"\"Fit to estimate the parameters.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Choice dataset to use for the estimation.\n    get_report: bool, optional\n        Whether or not to compute a report of the estimation, by default False\n\n    Returns\n    -------\n    fit : dict\n        dict with fit history\n    \"\"\"\n    if not self.instantiated:\n        # Lazy Instantiation\n        self.indexes, self._trainable_weights = self.instantiate(\n            n_items=choice_dataset.get_n_items(),\n            n_shared_features=choice_dataset.get_n_shared_features(),\n            n_items_features=choice_dataset.get_n_items_features(),\n        )\n        self.instantiated = True\n    fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n    if get_report:\n        self.report = self.compute_report(choice_dataset)\n    return fit\n</code></pre>"},{"location":"references/models/references_reslogit/#choice_learn.models.reslogit.ResLogit.instantiate","title":"<code>instantiate(n_items, n_shared_features, n_items_features)</code>","text":"<p>Instantiate the model from ModelSpecification object.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items/aternatives to consider</p> required <code>n_shared_features</code> <code>int</code> <p>Number of contexts features</p> required <code>n_items_features</code> <code>int</code> <p>Number of contexts items features</p> required <p>Returns:</p> Name Type Description <code>indexes</code> <code>dict</code> <p>Dictionary of the indexes of the weights created</p> <code>weights</code> <code>list of tf.Variable</code> <p>List of the weights created coresponding to the specification</p> Source code in <code>choice_learn/models/reslogit.py</code> <pre><code>def instantiate(self, n_items, n_shared_features, n_items_features):\n    \"\"\"Instantiate the model from ModelSpecification object.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of items/aternatives to consider\n    n_shared_features : int\n        Number of contexts features\n    n_items_features : int\n        Number of contexts items features\n\n    Returns\n    -------\n    indexes : dict\n        Dictionary of the indexes of the weights created\n    weights : list of tf.Variable\n        List of the weights created coresponding to the specification\n    \"\"\"\n    # Instantiate the loss function\n    self.loss = tf_ops.CustomCategoricalCrossEntropy(\n        from_logits=False,\n        label_smoothing=self.label_smoothing,\n        epsilon=self.logmin,\n    )\n\n    # Instantiate the weights\n    mnl_weights = []\n    indexes = {}\n\n    # Create the betas parameters for the shared and items features\n    for n_feat, feat_name in zip(\n        [n_shared_features, n_items_features],\n        [\"shared_features\", \"items_features\"],\n    ):\n        if n_feat &gt; 0:\n            mnl_weights += [\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_feat,)),\n                    name=f\"Betas_{feat_name}\",\n                )\n            ]\n            indexes[feat_name] = len(mnl_weights) - 1\n\n    # Create the alphas parameters\n    if self.intercept is None:\n        logging.info(\"No intercept in the model\")\n    elif self.intercept == \"item\":\n        mnl_weights.append(\n            tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items - 1,)),\n                name=\"Intercept\",\n            )\n        )\n        indexes[\"intercept\"] = len(mnl_weights) - 1\n    elif self.intercept == \"item-full\":\n        logging.info(\"Simple MNL intercept is not normalized to 0!\")\n        mnl_weights.append(\n            tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items,)),\n                name=\"Intercept\",\n            )\n        )\n        indexes[\"intercept\"] = len(mnl_weights) - 1\n    else:\n        mnl_weights.append(\n            tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1,)),\n                name=\"Intercept\",\n            )\n        )\n        indexes[\"intercept\"] = len(mnl_weights) - 1\n\n    # Create the residual layer\n    resnet_input = tf.keras.layers.Input(shape=(n_items,))\n    resnet_output = tf.keras.layers.Identity()(resnet_input)\n\n    if self.res_layers_width is None:\n        # Common width by default for all the residual layers: n_items\n        # (Like in the original paper of ResLogit)\n        layers = [ResLayer(activation=self.activation) for _ in range(self.n_layers)]\n\n    else:\n        # Different width for each *hidden* residual layer\n        if self.n_layers &gt; 0 and len(self.res_layers_width) != self.n_layers - 1:\n            raise ValueError(\n                \"The length of the res_layers_width list should be equal to n_layers - 1\"\n            )\n        if self.n_layers &gt; 1 and self.res_layers_width[-1] != n_items:\n            raise ValueError(\"The width of the last residual layer should be equal to n_items\")\n\n        # Initialize the residual layers\n        if self.n_layers == 0:\n            layers = []\n        else:\n            # The first layer has the same width as the input\n            layers = [ResLayer(activation=self.activation)]\n            for i in range(1, self.n_layers):\n                # The other layers have a width defined by the res_layers_width parameter\n                layers.append(\n                    ResLayer(\n                        layer_width=self.res_layers_width[i - 1], activation=self.activation\n                    )\n                )\n\n    # Build the residual layers\n    for layer in layers:\n        resnet_output = layer(resnet_output)\n\n    resnet_model = tf.keras.Model(\n        inputs=resnet_input, outputs=resnet_output, name=f\"resnet_with_{self.n_layers}_layers\"\n    )\n\n    self.instantiated = True\n    self.resnet_model = resnet_model\n    self.indexes = indexes\n    self.mnl_weights = mnl_weights\n    # Concatenation of all the trainable weights\n    self._trainable_weights = self.mnl_weights + self.resnet_model.trainable_variables\n\n    return self.indexes, self._trainable_weights\n</code></pre>"},{"location":"references/models/references_rumnet/","title":"RUMnet Model","text":"<p>Implementation of RUMnet for easy use.</p>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentParallelDense","title":"<code>AssortmentParallelDense</code>","text":"<p>             Bases: <code>Layer</code></p> <p>Several Dense layers in Parallel applied to an Assortment.</p> <p>Parallel means that they have the same input, but then are not intricated and are totally independant from each other. The layer applies the same Dense layers to an assortment of items.</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>class AssortmentParallelDense(tf.keras.layers.Layer):\n    \"\"\"Several Dense layers in Parallel applied to an Assortment.\n\n    Parallel means that they have the same input, but then are not intricated and\n    are totally independant from each other. The layer applies the same Dense layers\n    to an assortment of items.\n    \"\"\"\n\n    def __init__(self, width, depth, heterogeneity, activation=\"relu\", **kwargs):\n        \"\"\"Inialization of the layer.\n\n        Parameters\n        ----------\n        width : int\n            Number of neurons of each dense layer.\n        depth : int\n            Number of dense layers\n        heterogeneity : int\n            Number of dense networks in parallel.\n        activation : str, optional\n            Activation function of each dense, by default \"relu\"\n        \"\"\"\n        super().__init__(**kwargs)\n        self.width = width\n        self.depth = depth\n        self.heterogeneity = heterogeneity\n        self.activation = tf.keras.layers.Activation(activation)\n\n    def build(self, input_shape):\n        \"\"\"Lazy build of the layer.\n\n        Follows tf.keras API.\n\n        Parameters\n        ----------\n        input_shape : tuple\n            Shape of the input of the layer.\n            Typically (batch_size, num_items, num_features).\n        \"\"\"\n        super().build(input_shape)\n\n        weights = [\n            (\n                self.add_weight(\n                    shape=(input_shape[-1], self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n                self.add_weight(\n                    shape=(self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n            )\n        ]\n        for i in range(self.depth - 1):\n            weights.append(\n                (\n                    self.add_weight(\n                        shape=(self.width, self.width, self.heterogeneity),\n                        initializer=\"glorot_normal\",\n                        trainable=True,\n                    ),\n                    self.add_weight(\n                        shape=(self.width, self.heterogeneity),\n                        initializer=\"glorot_normal\",\n                        trainable=True,\n                    ),\n                )\n            )\n\n        self.w = weights\n\n    def call(self, inputs):\n        \"\"\"Predict of the layer.\n\n        Follows tf.keras.Layer API.\n\n        Parameters\n        ----------\n        inputs : tf.Tensor, np.ndarray\n            Tensor of shape (batch_size, n_items, n_features) as input of the model.\n\n        Returns\n        -------\n        tf.Tensor\n            Embeddings of shape (batch_size, n_items, width, heterogeneity)\n        \"\"\"\n        outputs = tf.tensordot(inputs, self.w[0][0], axes=[[2], [0]]) + self.w[0][1]\n        outputs = self.activation(outputs)\n\n        for w, b in self.w[1:]:\n            outputs = tf.einsum(\"imjk,jlk-&gt;imlk\", outputs, w) + b\n            outputs = self.activation(outputs)\n\n        return outputs\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentParallelDense.__init__","title":"<code>__init__(width, depth, heterogeneity, activation='relu', **kwargs)</code>","text":"<p>Inialization of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Number of neurons of each dense layer.</p> required <code>depth</code> <code>int</code> <p>Number of dense layers</p> required <code>heterogeneity</code> <code>int</code> <p>Number of dense networks in parallel.</p> required <code>activation</code> <code>str</code> <p>Activation function of each dense, by default \"relu\"</p> <code>'relu'</code> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def __init__(self, width, depth, heterogeneity, activation=\"relu\", **kwargs):\n    \"\"\"Inialization of the layer.\n\n    Parameters\n    ----------\n    width : int\n        Number of neurons of each dense layer.\n    depth : int\n        Number of dense layers\n    heterogeneity : int\n        Number of dense networks in parallel.\n    activation : str, optional\n        Activation function of each dense, by default \"relu\"\n    \"\"\"\n    super().__init__(**kwargs)\n    self.width = width\n    self.depth = depth\n    self.heterogeneity = heterogeneity\n    self.activation = tf.keras.layers.Activation(activation)\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentParallelDense.build","title":"<code>build(input_shape)</code>","text":"<p>Lazy build of the layer.</p> <p>Follows tf.keras API.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input of the layer. Typically (batch_size, num_items, num_features).</p> required Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def build(self, input_shape):\n    \"\"\"Lazy build of the layer.\n\n    Follows tf.keras API.\n\n    Parameters\n    ----------\n    input_shape : tuple\n        Shape of the input of the layer.\n        Typically (batch_size, num_items, num_features).\n    \"\"\"\n    super().build(input_shape)\n\n    weights = [\n        (\n            self.add_weight(\n                shape=(input_shape[-1], self.width, self.heterogeneity),\n                initializer=\"glorot_normal\",\n                trainable=True,\n            ),\n            self.add_weight(\n                shape=(self.width, self.heterogeneity),\n                initializer=\"glorot_normal\",\n                trainable=True,\n            ),\n        )\n    ]\n    for i in range(self.depth - 1):\n        weights.append(\n            (\n                self.add_weight(\n                    shape=(self.width, self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n                self.add_weight(\n                    shape=(self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n            )\n        )\n\n    self.w = weights\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentParallelDense.call","title":"<code>call(inputs)</code>","text":"<p>Predict of the layer.</p> <p>Follows tf.keras.Layer API.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>(Tensor, ndarray)</code> <p>Tensor of shape (batch_size, n_items, n_features) as input of the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Embeddings of shape (batch_size, n_items, width, heterogeneity)</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def call(self, inputs):\n    \"\"\"Predict of the layer.\n\n    Follows tf.keras.Layer API.\n\n    Parameters\n    ----------\n    inputs : tf.Tensor, np.ndarray\n        Tensor of shape (batch_size, n_items, n_features) as input of the model.\n\n    Returns\n    -------\n    tf.Tensor\n        Embeddings of shape (batch_size, n_items, width, heterogeneity)\n    \"\"\"\n    outputs = tf.tensordot(inputs, self.w[0][0], axes=[[2], [0]]) + self.w[0][1]\n    outputs = self.activation(outputs)\n\n    for w, b in self.w[1:]:\n        outputs = tf.einsum(\"imjk,jlk-&gt;imlk\", outputs, w) + b\n        outputs = self.activation(outputs)\n\n    return outputs\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentUtilityDenseNetwork","title":"<code>AssortmentUtilityDenseNetwork</code>","text":"<p>             Bases: <code>Layer</code></p> <p>Dense Network that is applied to an assortment of items.</p> <p>We apply to the same network over several items and several heterogeneitites.</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>class AssortmentUtilityDenseNetwork(tf.keras.layers.Layer):\n    \"\"\"Dense Network that is applied to an assortment of items.\n\n    We apply to the same network over several items and several heterogeneitites.\n    \"\"\"\n\n    def __init__(self, width, depth, activation=\"relu\", add_last=True, **kwargs):\n        \"\"\"Initialize the layer.\n\n        Parameters\n        ----------\n        width : int\n            Nnumber of neurons of each dense layer.\n        depth : int\n            Number of dense layers.\n        activation : str, optional\n            Activation function for each layer, by default \"relu\"\n        add_last : bool, optional\n            Whether to add a final dense layer with 1 neuron, by default True\n        \"\"\"\n        super().__init__(**kwargs)\n        self.width = width\n        self.depth = depth\n        self.activation = tf.keras.layers.Activation(activation)\n        self.add_last = add_last\n\n    def build(self, input_shape):\n        \"\"\"Lazy build of the layer.\n\n        Follows tf.keras.Layer API.\n\n        Parameters\n        ----------\n        input_shape : tuple\n            Shape of the input of the layer.\n            Typically (batch_size, num_items, width, heterogeneity).\n        \"\"\"\n        super().build(input_shape)\n\n        weights = [\n            (\n                self.add_weight(\n                    shape=(input_shape[-2], self.width),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n                self.add_weight(\n                    shape=(self.width, 1),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n            )\n        ]\n        for i in range(self.depth - 1):\n            weights.append(\n                (\n                    self.add_weight(\n                        shape=(self.width, self.width),\n                        initializer=\"glorot_normal\",\n                        trainable=True,\n                    ),\n                    self.add_weight(\n                        shape=(self.width, 1),\n                        initializer=\"glorot_normal\",\n                        trainable=True,\n                    ),\n                )\n            )\n        if self.add_last:\n            self.last = self.add_weight(\n                shape=(self.width, 1), initializer=\"glorot_normal\", trainable=True\n            )\n\n        self.w = weights\n\n    def call(self, inputs):\n        \"\"\"Predict of the layer.\n\n        Parameters\n        ----------\n        inputs : tf.Tensor, np.ndarray\n            Input Tensor of shape (batch_size, num_items, width, heterogeneity)\n\n        Returns\n        -------\n        tf.Tensor\n            Utilities of shape (batch_size, num_items, heterogeneity)\n        \"\"\"\n        outputs = inputs\n\n        for w, b in self.w:\n            # bs, items, features, heterogeneities\n            outputs = tf.einsum(\"ijlk, lm-&gt;ijmk\", outputs, w) + b\n            outputs = self.activation(outputs)\n\n        if self.add_last:\n            outputs = tf.einsum(\"ijlk, lm-&gt;ijmk\", outputs, self.last)\n\n        return outputs\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentUtilityDenseNetwork.__init__","title":"<code>__init__(width, depth, activation='relu', add_last=True, **kwargs)</code>","text":"<p>Initialize the layer.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Nnumber of neurons of each dense layer.</p> required <code>depth</code> <code>int</code> <p>Number of dense layers.</p> required <code>activation</code> <code>str</code> <p>Activation function for each layer, by default \"relu\"</p> <code>'relu'</code> <code>add_last</code> <code>bool</code> <p>Whether to add a final dense layer with 1 neuron, by default True</p> <code>True</code> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def __init__(self, width, depth, activation=\"relu\", add_last=True, **kwargs):\n    \"\"\"Initialize the layer.\n\n    Parameters\n    ----------\n    width : int\n        Nnumber of neurons of each dense layer.\n    depth : int\n        Number of dense layers.\n    activation : str, optional\n        Activation function for each layer, by default \"relu\"\n    add_last : bool, optional\n        Whether to add a final dense layer with 1 neuron, by default True\n    \"\"\"\n    super().__init__(**kwargs)\n    self.width = width\n    self.depth = depth\n    self.activation = tf.keras.layers.Activation(activation)\n    self.add_last = add_last\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentUtilityDenseNetwork.build","title":"<code>build(input_shape)</code>","text":"<p>Lazy build of the layer.</p> <p>Follows tf.keras.Layer API.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input of the layer. Typically (batch_size, num_items, width, heterogeneity).</p> required Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def build(self, input_shape):\n    \"\"\"Lazy build of the layer.\n\n    Follows tf.keras.Layer API.\n\n    Parameters\n    ----------\n    input_shape : tuple\n        Shape of the input of the layer.\n        Typically (batch_size, num_items, width, heterogeneity).\n    \"\"\"\n    super().build(input_shape)\n\n    weights = [\n        (\n            self.add_weight(\n                shape=(input_shape[-2], self.width),\n                initializer=\"glorot_normal\",\n                trainable=True,\n            ),\n            self.add_weight(\n                shape=(self.width, 1),\n                initializer=\"glorot_normal\",\n                trainable=True,\n            ),\n        )\n    ]\n    for i in range(self.depth - 1):\n        weights.append(\n            (\n                self.add_weight(\n                    shape=(self.width, self.width),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n                self.add_weight(\n                    shape=(self.width, 1),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n            )\n        )\n    if self.add_last:\n        self.last = self.add_weight(\n            shape=(self.width, 1), initializer=\"glorot_normal\", trainable=True\n        )\n\n    self.w = weights\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.AssortmentUtilityDenseNetwork.call","title":"<code>call(inputs)</code>","text":"<p>Predict of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>(Tensor, ndarray)</code> <p>Input Tensor of shape (batch_size, num_items, width, heterogeneity)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Utilities of shape (batch_size, num_items, heterogeneity)</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def call(self, inputs):\n    \"\"\"Predict of the layer.\n\n    Parameters\n    ----------\n    inputs : tf.Tensor, np.ndarray\n        Input Tensor of shape (batch_size, num_items, width, heterogeneity)\n\n    Returns\n    -------\n    tf.Tensor\n        Utilities of shape (batch_size, num_items, heterogeneity)\n    \"\"\"\n    outputs = inputs\n\n    for w, b in self.w:\n        # bs, items, features, heterogeneities\n        outputs = tf.einsum(\"ijlk, lm-&gt;ijmk\", outputs, w) + b\n        outputs = self.activation(outputs)\n\n    if self.add_last:\n        outputs = tf.einsum(\"ijlk, lm-&gt;ijmk\", outputs, self.last)\n\n    return outputs\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.CPURUMnet","title":"<code>CPURUMnet</code>","text":"<p>             Bases: <code>PaperRUMnet</code></p> <p>CPU-optimized Re-Implementation of the RUMnet model.</p> <p>Re-implemented from the paper: Representing Random Utility Choice Models with Neural Networks from Ali Aouad and Antoine D\u00e9sir. https://arxiv.org/abs/2207.12877</p> <p>--- Attention: --- Note that the model uses two type of features that are treated differently:     - customer features     - product features</p> <p>In this implementation, please make sure that the features are correctly formatted:     - customer features: (n_choices, n_features) are given as 'shared_features_by_choice'     in the ChoiceDataset used to fit the model     - product features: (n_choices, n_items, n_features) are given as     'items_features_by_choice' in the ChoiceDataset used to fit the model</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>class CPURUMnet(PaperRUMnet):\n    \"\"\"CPU-optimized Re-Implementation of the RUMnet model.\n\n    Re-implemented from the paper:\n    Representing Random Utility Choice Models with Neural Networks from Ali Aouad and Antoine D\u00e9sir.\n    https://arxiv.org/abs/2207.12877\n\n    --- Attention: ---\n    Note that the model uses two type of features that are treated differently:\n        - customer features\n        - product features\n    &gt;&gt;&gt; In this implementation, please make sure that the features are correctly formatted:\n        - customer features: (n_choices, n_features) are given as 'shared_features_by_choice'\n        in the ChoiceDataset used to fit the model\n        - product features: (n_choices, n_items, n_features) are given as\n        'items_features_by_choice' in the ChoiceDataset used to fit the model\n    \"\"\"\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute utility from a batch of ChoiceDataset.\n\n        Here we asssume that: item features = {fixed item features + contexts item features}\n                                user features = {contexts features}\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        np.ndarray\n            Utility of each product for each contexts.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        (_, _) = available_items_by_choice, choices\n        # Restacking and dtyping of the item features\n        if isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = tf.concat(\n                [\n                    tf.cast(shared_feature, tf.float32)\n                    for shared_feature in shared_features_by_choice\n                ],\n                axis=-1,\n            )\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tf.concat(\n                [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n                axis=-1,\n            )\n\n        # Computation of utilities\n        utilities = []\n        batch_size = shared_features_by_choice.shape[0]\n\n        # Computation of the customer features embeddings\n        z_embeddings = self.z_model(shared_features_by_choice)\n\n        # Iterate over items in assortment\n        for item_i in range(items_features_by_choice.shape[1]):\n            # Computation of item features embeddings\n            x_embeddings = self.x_model(items_features_by_choice[:, item_i, :])\n\n            stacked_heterogeneities = []\n            # Computation of utilites from embeddings, iteration over heterogeneities\n            # eps_x * eps_z\n            for _x in x_embeddings:\n                for _z in z_embeddings:\n                    full_embedding = tf.keras.layers.Concatenate()(\n                        [\n                            items_features_by_choice[:, item_i, :],\n                            _x,\n                            shared_features_by_choice,\n                            _z,\n                        ]\n                    )\n                    stacked_heterogeneities.append(full_embedding)\n            item_utilities = self.u_model(tf.concat(stacked_heterogeneities, axis=0))\n            item_utilities = tf.stack(\n                [\n                    item_utilities[batch_size * i : batch_size * (i + 1)]\n                    for i in range(len(x_embeddings) * len(z_embeddings))\n                ],\n                axis=1,\n            )\n            utilities.append(item_utilities)\n        # Reshape utilities: (batch_size, num_items, heterogeneity)\n        return tf.squeeze(tf.stack(utilities, axis=1), -1)\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.CPURUMnet.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute utility from a batch of ChoiceDataset.</p> <p>Here we asssume that: item features = {fixed item features + contexts item features}                         user features = {contexts features}</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Utility of each product for each contexts. Shape must be (n_choices, n_items)</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute utility from a batch of ChoiceDataset.\n\n    Here we asssume that: item features = {fixed item features + contexts item features}\n                            user features = {contexts features}\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    np.ndarray\n        Utility of each product for each contexts.\n        Shape must be (n_choices, n_items)\n    \"\"\"\n    (_, _) = available_items_by_choice, choices\n    # Restacking and dtyping of the item features\n    if isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = tf.concat(\n            [\n                tf.cast(shared_feature, tf.float32)\n                for shared_feature in shared_features_by_choice\n            ],\n            axis=-1,\n        )\n    if isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = tf.concat(\n            [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n            axis=-1,\n        )\n\n    # Computation of utilities\n    utilities = []\n    batch_size = shared_features_by_choice.shape[0]\n\n    # Computation of the customer features embeddings\n    z_embeddings = self.z_model(shared_features_by_choice)\n\n    # Iterate over items in assortment\n    for item_i in range(items_features_by_choice.shape[1]):\n        # Computation of item features embeddings\n        x_embeddings = self.x_model(items_features_by_choice[:, item_i, :])\n\n        stacked_heterogeneities = []\n        # Computation of utilites from embeddings, iteration over heterogeneities\n        # eps_x * eps_z\n        for _x in x_embeddings:\n            for _z in z_embeddings:\n                full_embedding = tf.keras.layers.Concatenate()(\n                    [\n                        items_features_by_choice[:, item_i, :],\n                        _x,\n                        shared_features_by_choice,\n                        _z,\n                    ]\n                )\n                stacked_heterogeneities.append(full_embedding)\n        item_utilities = self.u_model(tf.concat(stacked_heterogeneities, axis=0))\n        item_utilities = tf.stack(\n            [\n                item_utilities[batch_size * i : batch_size * (i + 1)]\n                for i in range(len(x_embeddings) * len(z_embeddings))\n            ],\n            axis=1,\n        )\n        utilities.append(item_utilities)\n    # Reshape utilities: (batch_size, num_items, heterogeneity)\n    return tf.squeeze(tf.stack(utilities, axis=1), -1)\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet","title":"<code>GPURUMnet</code>","text":"<p>             Bases: <code>PaperRUMnet</code></p> <p>GPU-optimized Re-Implementation of the RUMnet model.</p> <p>Re-implemented from the paper: Representing Random Utility Choice Models with Neural Networks from Ali Aouad and Antoine D\u00e9sir. https://arxiv.org/abs/2207.12877</p> <p>--- Attention: --- Note that the model uses two type of features that are treated differently:     - customer features     - product features</p> <p>In this implementation, please make sure that the features are correctly formatted:     - customer features: (n_choices, n_features) are given as 'shared_features_by_choice'     in the ChoiceDataset used to fit the model     - product features: (n_choices, n_items, n_features) are given as     'items_features_by_choice' in the ChoiceDataset used to fit the model</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>class GPURUMnet(PaperRUMnet):\n    \"\"\"GPU-optimized Re-Implementation of the RUMnet model.\n\n    Re-implemented from the paper:\n    Representing Random Utility Choice Models with Neural Networks from Ali Aouad and Antoine D\u00e9sir.\n    https://arxiv.org/abs/2207.12877\n\n    --- Attention: ---\n    Note that the model uses two type of features that are treated differently:\n        - customer features\n        - product features\n    &gt;&gt;&gt; In this implementation, please make sure that the features are correctly formatted:\n        - customer features: (n_choices, n_features) are given as 'shared_features_by_choice'\n        in the ChoiceDataset used to fit the model\n        - product features: (n_choices, n_items, n_features) are given as\n        'items_features_by_choice' in the ChoiceDataset used to fit the model\n    \"\"\"\n\n    def instantiate(self):\n        \"\"\"Instatiation of the RUMnet model.\n\n        Instantiation of the three nets:\n            - x_model encoding products features,\n            - z_model encoding customers features,\n            - u_model computing utilities from product, customer features and their embeddings\n        \"\"\"\n        # Instatiation of the different nets\n        self.x_model = AssortmentParallelDense(\n            width=self.width_eps_x,\n            depth=self.depth_eps_x,\n            heterogeneity=self.heterogeneity_x,\n        )\n        self.z_model = ParallelDense(\n            width=self.width_eps_z,\n            depth=self.depth_eps_z,\n            heterogeneity=self.heterogeneity_z,\n        )\n        self.u_model = AssortmentUtilityDenseNetwork(\n            width=self.width_u, depth=self.depth_u, add_last=True\n        )\n\n        self.loss = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False, label_smoothing=self.label_smoothing\n        )\n        self.time_dict = {}\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return (\n            self.x_model.trainable_variables\n            + self.z_model.trainable_variables\n            + self.u_model.trainable_variables\n        )\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute utility from a batch of ChoiceDataset.\n\n        Here we asssume that: item features = {fixed item features + contexts item features}\n                                user features = {contexts features}\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        np.ndarray\n            Utility of each product for each contexts.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        (_, _) = available_items_by_choice, choices\n\n        # Restacking and dtyping of the item features\n        if isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = tf.concat(\n                [\n                    tf.cast(shared_feature, tf.float32)\n                    for shared_feature in shared_features_by_choice\n                ],\n                axis=-1,\n            )\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tf.concat(\n                [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n                axis=-1,\n            )\n\n        item_utility_by_choice = []\n\n        # Computation of the customer features embeddings\n        z_embeddings = self.z_model(shared_features_by_choice)\n        x_embeddings = self.x_model(items_features_by_choice)\n        # Reshaping\n        big_z = tf.tile(\n            tf.expand_dims(shared_features_by_choice, axis=2),\n            multiples=[1, 1, self.heterogeneity_z],\n        )\n        big_z = tf.repeat(\n            tf.concat([big_z, z_embeddings], axis=1),\n            repeats=self.heterogeneity_x,\n            axis=2,\n        )\n\n        # Iterate over items in assortment\n        for item_i in range(items_features_by_choice.shape[1]):\n            # Computation of item features embeddings\n            # utilities.append([])\n\n            # Computation of utilites from embeddings, iteration over heterogeneities\n            # (eps_x * eps_z)\n            x_fixed_features = tf.repeat(\n                tf.expand_dims(items_features_by_choice[:, item_i, :], axis=2),\n                repeats=self.heterogeneity_x * self.heterogeneity_z,\n                axis=2,\n            )\n            big_x = tf.repeat(x_embeddings[:, item_i], repeats=self.heterogeneity_z, axis=2)\n\n            item_utility_by_choice.append(tf.concat([big_z, x_fixed_features, big_x], axis=1))\n\n        # Computing resulting utilitiies\n        utilities = self.u_model(tf.stack(item_utility_by_choice, axis=1))\n        utilities = tf.squeeze(utilities, -2)\n\n        # Reshape &amp; return\n        return tf.transpose(utilities, perm=[0, 2, 1])\n\n    @tf.function\n    def train_step(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n        Recoded because heterogeneities generate different shapes of tensors.\n        # TODO: verify that it is indeed different than PaperRUMnet\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor\n            Value of NegativeLogLikelihood loss for the batch\n        \"\"\"\n        with tf.GradientTape() as tape:\n            # Computation of utilities\n            utilities = self.compute_batch_utility(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n            eps_probabilities = tf.nn.softmax(utilities, axis=2)\n            # Average probabilities over heterogeneities\n            probabilities = tf.reduce_mean(eps_probabilities, axis=1)\n\n            # Availability normalization\n            probabilities = tf.multiply(probabilities, available_items_by_choice)\n            probabilities = tf.divide(\n                probabilities,\n                tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5,\n            )\n            if self.tol &gt; 0:\n                probabilities = (1 - self.tol) * probabilities + self.tol * tf.ones_like(\n                    probabilities\n                ) / probabilities.shape[-1]\n\n            # Negative Log-Likelihood\n            batch_nll = self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            )\n\n        grads = tape.gradient(\n            batch_nll,\n            self.x_model.trainable_variables\n            + self.z_model.trainable_variables\n            + self.u_model.trainable_variables,\n        )\n        self.optimizer.apply_gradients(\n            zip(\n                grads,\n                self.x_model.trainable_variables\n                + self.z_model.trainable_variables\n                + self.u_model.trainable_variables,\n            )\n        )\n        return batch_nll\n\n    @tf.function\n    def batch_predict(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"RUMnet batch_predict.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor (1, )\n            Value of NegativeLogLikelihood loss for the batch\n        tf.Tensor (batch_size, n_items)\n            Probabilities for each product to be chosen for each contexts\n        \"\"\"\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n        probabilities = tf.nn.softmax(utilities, axis=2)\n        probabilities = tf.reduce_mean(probabilities, axis=1)\n\n        # Test with availability normalization\n        probabilities = tf.multiply(probabilities, available_items_by_choice)\n        probabilities = tf.divide(\n            probabilities, tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5\n        )\n\n        batch_loss = {\n            \"optimized_loss\": self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n            \"Exact-NegativeLogLikelihood\": self.exact_nll(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n        }\n        return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet.batch_predict","title":"<code>batch_predict(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>RUMnet batch_predict.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor(1)</code> <p>Value of NegativeLogLikelihood loss for the batch</p> <code>Tensor(batch_size, n_items)</code> <p>Probabilities for each product to be chosen for each contexts</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>@tf.function\ndef batch_predict(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"RUMnet batch_predict.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor (1, )\n        Value of NegativeLogLikelihood loss for the batch\n    tf.Tensor (batch_size, n_items)\n        Probabilities for each product to be chosen for each contexts\n    \"\"\"\n    utilities = self.compute_batch_utility(\n        shared_features_by_choice=shared_features_by_choice,\n        items_features_by_choice=items_features_by_choice,\n        available_items_by_choice=available_items_by_choice,\n        choices=choices,\n    )\n    probabilities = tf.nn.softmax(utilities, axis=2)\n    probabilities = tf.reduce_mean(probabilities, axis=1)\n\n    # Test with availability normalization\n    probabilities = tf.multiply(probabilities, available_items_by_choice)\n    probabilities = tf.divide(\n        probabilities, tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5\n    )\n\n    batch_loss = {\n        \"optimized_loss\": self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n        \"Exact-NegativeLogLikelihood\": self.exact_nll(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n    }\n    return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute utility from a batch of ChoiceDataset.</p> <p>Here we asssume that: item features = {fixed item features + contexts item features}                         user features = {contexts features}</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Utility of each product for each contexts. Shape must be (n_choices, n_items)</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute utility from a batch of ChoiceDataset.\n\n    Here we asssume that: item features = {fixed item features + contexts item features}\n                            user features = {contexts features}\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    np.ndarray\n        Utility of each product for each contexts.\n        Shape must be (n_choices, n_items)\n    \"\"\"\n    (_, _) = available_items_by_choice, choices\n\n    # Restacking and dtyping of the item features\n    if isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = tf.concat(\n            [\n                tf.cast(shared_feature, tf.float32)\n                for shared_feature in shared_features_by_choice\n            ],\n            axis=-1,\n        )\n    if isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = tf.concat(\n            [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n            axis=-1,\n        )\n\n    item_utility_by_choice = []\n\n    # Computation of the customer features embeddings\n    z_embeddings = self.z_model(shared_features_by_choice)\n    x_embeddings = self.x_model(items_features_by_choice)\n    # Reshaping\n    big_z = tf.tile(\n        tf.expand_dims(shared_features_by_choice, axis=2),\n        multiples=[1, 1, self.heterogeneity_z],\n    )\n    big_z = tf.repeat(\n        tf.concat([big_z, z_embeddings], axis=1),\n        repeats=self.heterogeneity_x,\n        axis=2,\n    )\n\n    # Iterate over items in assortment\n    for item_i in range(items_features_by_choice.shape[1]):\n        # Computation of item features embeddings\n        # utilities.append([])\n\n        # Computation of utilites from embeddings, iteration over heterogeneities\n        # (eps_x * eps_z)\n        x_fixed_features = tf.repeat(\n            tf.expand_dims(items_features_by_choice[:, item_i, :], axis=2),\n            repeats=self.heterogeneity_x * self.heterogeneity_z,\n            axis=2,\n        )\n        big_x = tf.repeat(x_embeddings[:, item_i], repeats=self.heterogeneity_z, axis=2)\n\n        item_utility_by_choice.append(tf.concat([big_z, x_fixed_features, big_x], axis=1))\n\n    # Computing resulting utilitiies\n    utilities = self.u_model(tf.stack(item_utility_by_choice, axis=1))\n    utilities = tf.squeeze(utilities, -2)\n\n    # Reshape &amp; return\n    return tf.transpose(utilities, perm=[0, 2, 1])\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet.instantiate","title":"<code>instantiate()</code>","text":"<p>Instatiation of the RUMnet model.</p> <p>Instantiation of the three nets:     - x_model encoding products features,     - z_model encoding customers features,     - u_model computing utilities from product, customer features and their embeddings</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def instantiate(self):\n    \"\"\"Instatiation of the RUMnet model.\n\n    Instantiation of the three nets:\n        - x_model encoding products features,\n        - z_model encoding customers features,\n        - u_model computing utilities from product, customer features and their embeddings\n    \"\"\"\n    # Instatiation of the different nets\n    self.x_model = AssortmentParallelDense(\n        width=self.width_eps_x,\n        depth=self.depth_eps_x,\n        heterogeneity=self.heterogeneity_x,\n    )\n    self.z_model = ParallelDense(\n        width=self.width_eps_z,\n        depth=self.depth_eps_z,\n        heterogeneity=self.heterogeneity_z,\n    )\n    self.u_model = AssortmentUtilityDenseNetwork(\n        width=self.width_u, depth=self.depth_u, add_last=True\n    )\n\n    self.loss = tf_ops.CustomCategoricalCrossEntropy(\n        from_logits=False, label_smoothing=self.label_smoothing\n    )\n    self.time_dict = {}\n    self.instantiated = True\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet.train_step","title":"<code>train_step(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one training step (= one gradient descent step) of the model.</p> <p>Recoded because heterogeneities generate different shapes of tensors.</p>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.GPURUMnet.train_step--todo-verify-that-it-is-indeed-different-than-paperrumnet","title":"TODO: verify that it is indeed different than PaperRUMnet","text":"<p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Value of NegativeLogLikelihood loss for the batch</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>@tf.function\ndef train_step(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one training step (= one gradient descent step) of the model.\n\n    Recoded because heterogeneities generate different shapes of tensors.\n    # TODO: verify that it is indeed different than PaperRUMnet\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor\n        Value of NegativeLogLikelihood loss for the batch\n    \"\"\"\n    with tf.GradientTape() as tape:\n        # Computation of utilities\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n        eps_probabilities = tf.nn.softmax(utilities, axis=2)\n        # Average probabilities over heterogeneities\n        probabilities = tf.reduce_mean(eps_probabilities, axis=1)\n\n        # Availability normalization\n        probabilities = tf.multiply(probabilities, available_items_by_choice)\n        probabilities = tf.divide(\n            probabilities,\n            tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5,\n        )\n        if self.tol &gt; 0:\n            probabilities = (1 - self.tol) * probabilities + self.tol * tf.ones_like(\n                probabilities\n            ) / probabilities.shape[-1]\n\n        # Negative Log-Likelihood\n        batch_nll = self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        )\n\n    grads = tape.gradient(\n        batch_nll,\n        self.x_model.trainable_variables\n        + self.z_model.trainable_variables\n        + self.u_model.trainable_variables,\n    )\n    self.optimizer.apply_gradients(\n        zip(\n            grads,\n            self.x_model.trainable_variables\n            + self.z_model.trainable_variables\n            + self.u_model.trainable_variables,\n        )\n    )\n    return batch_nll\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet","title":"<code>PaperRUMnet</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>Re-Implementation of the RUMnet model.</p> <p>Re-implemented from the paper: Representing Random Utility Choice Models with Neural Networks from Ali Aouad and Antoine D\u00e9sir. https://arxiv.org/abs/2207.12877</p> <p>--- Attention: --- Note that the model uses two type of features that are treated differently:     - customer features     - product features</p> <p>In this implementation, please make sure that the features are correctly formatted:     - customer features: (n_choices, n_features) are given as 'shared_features_by_choice'     in the ChoiceDataset used to fit the model     - product features: (n_choices, n_items, n_features) are given as</p> <pre><code>'items_features_by_choice' in the ChoiceDataset used to fit the model\n</code></pre> <p>Inherits from base_model.ChoiceModel</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>class PaperRUMnet(ChoiceModel):\n    \"\"\"Re-Implementation of the RUMnet model.\n\n    Re-implemented from the paper:\n    Representing Random Utility Choice Models with Neural Networks from Ali Aouad and Antoine D\u00e9sir.\n    https://arxiv.org/abs/2207.12877\n\n    --- Attention: ---\n    Note that the model uses two type of features that are treated differently:\n        - customer features\n        - product features\n    &gt;&gt;&gt; In this implementation, please make sure that the features are correctly formatted:\n        - customer features: (n_choices, n_features) are given as 'shared_features_by_choice'\n        in the ChoiceDataset used to fit the model\n        - product features: (n_choices, n_items, n_features) are given as\n        'items_features_by_choice' in the ChoiceDataset used to fit the model\n    ---\n    Inherits from base_model.ChoiceModel\n    \"\"\"\n\n    def __init__(\n        self,\n        num_products_features,\n        num_customer_features,\n        width_eps_x,\n        depth_eps_x,\n        heterogeneity_x,\n        width_eps_z,\n        depth_eps_z,\n        heterogeneity_z,\n        width_u,\n        depth_u,\n        tol,\n        optimizer,\n        lr,\n        add_exit_choice=False,\n        logmin=1e-5,\n        l2_regularization_coef=0.0,\n        label_smoothing=0.0,\n        **kwargs,\n    ):\n        \"\"\"Initialize the RUMnet Model.\n\n        Parameters\n        ----------\n        num_products_features : int\n            Number of features each product will be described with.\n            In terms of ChoiceDataset it is the number of\n            { items_features + contexts_items_features } for one product.\n        num_customer_features : int\n            Number of features each customer will be described with.\n            In terms of ChoiceDataset it is the number of contexts_features.\n        width_eps_x : int\n            Number of neurons for each dense layer for the products encoding net.\n        depth_eps_x : int\n            Number of dense layers for the products encoding net.\n        heterogeneity_x : int\n            Number of nets of products features encoding.\n        width_eps_z : int\n            Number of neurons for each dense layer for the customers encoding net.\n        depth_eps_z : int\n            Number of dense layers for the customers encoding net.\n        heterogeneity_z : int\n            Number of nets of customers features encoding.\n        width_u : int\n            Number of neurons for each dense layer for the utility net.\n        depth_u : int\n            Number of dense layers for the utility net.\n        tol : float\n            # To be Implemented\n        optimizer : str\n            String representation of the optimizer to use. By default is Adam if not specified.\n            Should be within tf.keras.optimizers.\n        lr : float\n            Starting learning rate to associate with optimizer.\n        add_exit_choice : bool, optional\n            Whether or not to add exit option with utility 1, by default True\n        logmin : float, optional\n            Value to be added within log computation to avoid infinity, by default 1e-5\n        l2_regularization_coef : float, optional\n            Value of dense layers weights regulariation to apply during training, by default 0.0\n        label_smoothing : float, optional\n            Value of smoothing to apply in CrossEntropy loss computation, by default 0.0\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n        # Number of features\n        if num_customer_features &lt;= 0:\n            raise ValueError(\"Number of customer features must be at least 1.\")\n        if num_products_features &lt;= 0:\n            raise ValueError(\"Number of product features must be at least 1.\")\n        self.num_products_features = num_products_features\n        self.num_customer_features = num_customer_features\n\n        # Dimension of encoding nets\n        self.width_eps_x = width_eps_x\n        self.depth_eps_x = depth_eps_x\n        self.heterogeneity_x = heterogeneity_x\n\n        self.width_eps_z = width_eps_z\n        self.depth_eps_z = depth_eps_z\n        self.heterogeneity_z = heterogeneity_z\n\n        # Dimension of utility net\n        self.width_u = width_u\n        self.depth_u = depth_u\n\n        # Optimization parameters\n        self.logmin = logmin\n        self.tol = tol\n        self.lr = lr\n        self.add_exit_choice = add_exit_choice\n        self.l2_regularization_coef = l2_regularization_coef\n        self.label_smoothing = label_smoothing\n\n        self.instantiated = False\n\n    def instantiate(self):\n        \"\"\"Instatiation of the RUMnet model.\n\n        Creation of :\n            - x_model encoding products features,\n            - z_model encoding customers features,\n            - u_model computing utilities from product, customer features and their embeddings\n        \"\"\"\n        # Instatiation of the different nets\n        self.x_model, self.z_model, self.u_model = recreate_official_nets(\n            num_products_features=self.num_products_features,\n            num_customer_features=self.num_customer_features,\n            x_width=self.width_eps_x,\n            x_depth=self.depth_eps_x,\n            x_eps=self.heterogeneity_x,\n            z_width=self.width_eps_z,\n            z_depth=self.depth_eps_z,\n            z_eps=self.heterogeneity_z,\n            width_u=self.width_u,\n            depth_u=self.depth_u,\n            l2_regularization_coeff=self.l2_regularization_coef,\n        )\n\n        self.loss = tf_ops.CustomCategoricalCrossEntropy(\n            from_logits=False,\n            label_smoothing=self.label_smoothing,\n            epsilon=self.logmin,\n        )\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return self.x_model.weights + self.z_model.weights + self.u_model.weights\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute utility from a batch of ChoiceDataset.\n\n        Here we asssume that: item features = {fixed item features + contexts item features}\n                              user features = {contexts features}\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        np.ndarray\n            Utility of each product for each choice.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        (_, _) = available_items_by_choice, choices\n        # Restacking and dtyping of the item features\n        if isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = tf.concat(\n                [\n                    tf.cast(shared_feature, tf.float32)\n                    for shared_feature in shared_features_by_choice\n                ],\n                axis=-1,\n            )\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tf.concat(\n                [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n                axis=-1,\n            )\n\n        # Computation of utilities\n        utilities = []\n\n        # Computation of the customer features embeddings\n        z_embeddings = self.z_model(shared_features_by_choice)\n\n        # Iterate over items in assortment\n        for item_i in range(items_features_by_choice.shape[1]):\n            # Computation of item features embeddings\n            x_embeddings = self.x_model(items_features_by_choice[:, item_i, :])\n\n            utilities.append([])\n\n            # Computation of utilites from embeddings, iteration over heterogeneities\n            # (eps_x * eps_z)\n            for _x in x_embeddings:\n                for _z in z_embeddings:\n                    _u = tf.keras.layers.Concatenate()(\n                        [\n                            items_features_by_choice[:, item_i, :],\n                            _x,\n                            shared_features_by_choice,\n                            _z,\n                        ]\n                    )\n                    utilities[-1].append(self.u_model(_u))\n        # Reshape utilities: (batch_size, num_items, heterogeneity)\n        return tf.squeeze(tf.transpose(tf.stack(utilities, axis=1)), 0)\n\n    @tf.function\n    def train_step(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Update model's weight with a step of gradient descent.\n\n        Function that represents one training step (= one gradient descent step) of the model.\n        Handles a batch of data of size n_contexts = n_choices = batch_size\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor\n            Value of NegativeLogLikelihood loss for the batch\n        \"\"\"\n        with tf.GradientTape() as tape:\n            # Computation of utilities\n            all_u = self.compute_batch_utility(\n                shared_features_by_choice=shared_features_by_choice,\n                items_features_by_choice=items_features_by_choice,\n                available_items_by_choice=available_items_by_choice,\n                choices=choices,\n            )\n            # Iterate over heterogeneities\n            eps_probabilities = tf.nn.softmax(all_u, axis=1)\n\n            # Average probabilities over heterogeneities\n            probabilities = tf.reduce_mean(eps_probabilities, axis=-1)\n            # It is not in the paper, but let's normalize with availabilities\n            probabilities = tf.multiply(probabilities, available_items_by_choice)\n            probabilities = tf.divide(\n                probabilities,\n                tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5,\n            )\n            if self.tol &gt; 0:\n                probabilities = (1 - self.tol) * probabilities + self.tol * tf.ones_like(\n                    probabilities\n                ) / probabilities.shape[-1]\n\n            # Probabilities of selected products\n\n            # Negative Log-Likelihood\n            batch_nll = self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            )\n\n            if self.regularization is not None:\n                regularization = tf.reduce_sum(\n                    [self.regularizer(w) for w in self.trainable_weights]\n                )\n                batch_nll += regularization\n\n        grads = tape.gradient(batch_nll, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return batch_nll\n\n    @tf.function\n    def batch_predict(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n        sample_weight=None,\n    ):\n        \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n        Specifically recoded for RUMnet because it is needed to average probabilities over\n        heterogeneities.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            A batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            A batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n        sample_weight : np.ndarray, optional\n            List samples weights to apply during the gradient descent to the batch elements,\n            by default None\n\n        Returns\n        -------\n        tf.Tensor (1, )\n            Value of NegativeLogLikelihood loss for the batch\n        tf.Tensor (batch_size, n_items)\n            Probabilities for each product to be chosen for each contexts\n        \"\"\"\n        utilities = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n        probabilities = tf.nn.softmax(utilities, axis=1)\n        probabilities = tf.reduce_mean(probabilities, axis=-1)\n\n        # Normalization with availabilties\n        probabilities = tf.multiply(probabilities, available_items_by_choice)\n        probabilities = tf.divide(\n            probabilities, tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5\n        )\n\n        batch_loss = {\n            \"optimized_loss\": self.loss(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n            \"Exact-NegativeLogLikelihood\": self.exact_nll(\n                y_pred=probabilities,\n                y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n                sample_weight=sample_weight,\n            ),\n        }\n        return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.__init__","title":"<code>__init__(num_products_features, num_customer_features, width_eps_x, depth_eps_x, heterogeneity_x, width_eps_z, depth_eps_z, heterogeneity_z, width_u, depth_u, tol, optimizer, lr, add_exit_choice=False, logmin=1e-05, l2_regularization_coef=0.0, label_smoothing=0.0, **kwargs)</code>","text":"<p>Initialize the RUMnet Model.</p> <p>Parameters:</p> Name Type Description Default <code>num_products_features</code> <code>int</code> <p>Number of features each product will be described with. In terms of ChoiceDataset it is the number of { items_features + contexts_items_features } for one product.</p> required <code>num_customer_features</code> <code>int</code> <p>Number of features each customer will be described with. In terms of ChoiceDataset it is the number of contexts_features.</p> required <code>width_eps_x</code> <code>int</code> <p>Number of neurons for each dense layer for the products encoding net.</p> required <code>depth_eps_x</code> <code>int</code> <p>Number of dense layers for the products encoding net.</p> required <code>heterogeneity_x</code> <code>int</code> <p>Number of nets of products features encoding.</p> required <code>width_eps_z</code> <code>int</code> <p>Number of neurons for each dense layer for the customers encoding net.</p> required <code>depth_eps_z</code> <code>int</code> <p>Number of dense layers for the customers encoding net.</p> required <code>heterogeneity_z</code> <code>int</code> <p>Number of nets of customers features encoding.</p> required <code>width_u</code> <code>int</code> <p>Number of neurons for each dense layer for the utility net.</p> required <code>depth_u</code> <code>int</code> <p>Number of dense layers for the utility net.</p> required <code>tol</code> <code>float</code> required <code>optimizer</code> <code>str</code> <p>String representation of the optimizer to use. By default is Adam if not specified. Should be within tf.keras.optimizers.</p> required <code>lr</code> <code>float</code> <p>Starting learning rate to associate with optimizer.</p> required <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to add exit option with utility 1, by default True</p> <code>False</code> <code>logmin</code> <code>float</code> <p>Value to be added within log computation to avoid infinity, by default 1e-5</p> <code>1e-05</code> <code>l2_regularization_coef</code> <code>float</code> <p>Value of dense layers weights regulariation to apply during training, by default 0.0</p> <code>0.0</code> <code>label_smoothing</code> <code>float</code> <p>Value of smoothing to apply in CrossEntropy loss computation, by default 0.0</p> <code>0.0</code> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def __init__(\n    self,\n    num_products_features,\n    num_customer_features,\n    width_eps_x,\n    depth_eps_x,\n    heterogeneity_x,\n    width_eps_z,\n    depth_eps_z,\n    heterogeneity_z,\n    width_u,\n    depth_u,\n    tol,\n    optimizer,\n    lr,\n    add_exit_choice=False,\n    logmin=1e-5,\n    l2_regularization_coef=0.0,\n    label_smoothing=0.0,\n    **kwargs,\n):\n    \"\"\"Initialize the RUMnet Model.\n\n    Parameters\n    ----------\n    num_products_features : int\n        Number of features each product will be described with.\n        In terms of ChoiceDataset it is the number of\n        { items_features + contexts_items_features } for one product.\n    num_customer_features : int\n        Number of features each customer will be described with.\n        In terms of ChoiceDataset it is the number of contexts_features.\n    width_eps_x : int\n        Number of neurons for each dense layer for the products encoding net.\n    depth_eps_x : int\n        Number of dense layers for the products encoding net.\n    heterogeneity_x : int\n        Number of nets of products features encoding.\n    width_eps_z : int\n        Number of neurons for each dense layer for the customers encoding net.\n    depth_eps_z : int\n        Number of dense layers for the customers encoding net.\n    heterogeneity_z : int\n        Number of nets of customers features encoding.\n    width_u : int\n        Number of neurons for each dense layer for the utility net.\n    depth_u : int\n        Number of dense layers for the utility net.\n    tol : float\n        # To be Implemented\n    optimizer : str\n        String representation of the optimizer to use. By default is Adam if not specified.\n        Should be within tf.keras.optimizers.\n    lr : float\n        Starting learning rate to associate with optimizer.\n    add_exit_choice : bool, optional\n        Whether or not to add exit option with utility 1, by default True\n    logmin : float, optional\n        Value to be added within log computation to avoid infinity, by default 1e-5\n    l2_regularization_coef : float, optional\n        Value of dense layers weights regulariation to apply during training, by default 0.0\n    label_smoothing : float, optional\n        Value of smoothing to apply in CrossEntropy loss computation, by default 0.0\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n    # Number of features\n    if num_customer_features &lt;= 0:\n        raise ValueError(\"Number of customer features must be at least 1.\")\n    if num_products_features &lt;= 0:\n        raise ValueError(\"Number of product features must be at least 1.\")\n    self.num_products_features = num_products_features\n    self.num_customer_features = num_customer_features\n\n    # Dimension of encoding nets\n    self.width_eps_x = width_eps_x\n    self.depth_eps_x = depth_eps_x\n    self.heterogeneity_x = heterogeneity_x\n\n    self.width_eps_z = width_eps_z\n    self.depth_eps_z = depth_eps_z\n    self.heterogeneity_z = heterogeneity_z\n\n    # Dimension of utility net\n    self.width_u = width_u\n    self.depth_u = depth_u\n\n    # Optimization parameters\n    self.logmin = logmin\n    self.tol = tol\n    self.lr = lr\n    self.add_exit_choice = add_exit_choice\n    self.l2_regularization_coef = l2_regularization_coef\n    self.label_smoothing = label_smoothing\n\n    self.instantiated = False\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.__init__--to-be-implemented","title":"To be Implemented","text":""},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.batch_predict","title":"<code>batch_predict(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.</p> <p>Specifically recoded for RUMnet because it is needed to average probabilities over heterogeneities.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor(1)</code> <p>Value of NegativeLogLikelihood loss for the batch</p> <code>Tensor(batch_size, n_items)</code> <p>Probabilities for each product to be chosen for each contexts</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>@tf.function\ndef batch_predict(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Represent one prediction (Probas + Loss) for one batch of a ChoiceDataset.\n\n    Specifically recoded for RUMnet because it is needed to average probabilities over\n    heterogeneities.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor (1, )\n        Value of NegativeLogLikelihood loss for the batch\n    tf.Tensor (batch_size, n_items)\n        Probabilities for each product to be chosen for each contexts\n    \"\"\"\n    utilities = self.compute_batch_utility(\n        shared_features_by_choice=shared_features_by_choice,\n        items_features_by_choice=items_features_by_choice,\n        available_items_by_choice=available_items_by_choice,\n        choices=choices,\n    )\n    probabilities = tf.nn.softmax(utilities, axis=1)\n    probabilities = tf.reduce_mean(probabilities, axis=-1)\n\n    # Normalization with availabilties\n    probabilities = tf.multiply(probabilities, available_items_by_choice)\n    probabilities = tf.divide(\n        probabilities, tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5\n    )\n\n    batch_loss = {\n        \"optimized_loss\": self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n        \"Exact-NegativeLogLikelihood\": self.exact_nll(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        ),\n    }\n    return batch_loss, probabilities\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute utility from a batch of ChoiceDataset.</p> <p>Here we asssume that: item features = {fixed item features + contexts item features}                       user features = {contexts features}</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Utility of each product for each choice. Shape must be (n_choices, n_items)</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute utility from a batch of ChoiceDataset.\n\n    Here we asssume that: item features = {fixed item features + contexts item features}\n                          user features = {contexts features}\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    np.ndarray\n        Utility of each product for each choice.\n        Shape must be (n_choices, n_items)\n    \"\"\"\n    (_, _) = available_items_by_choice, choices\n    # Restacking and dtyping of the item features\n    if isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = tf.concat(\n            [\n                tf.cast(shared_feature, tf.float32)\n                for shared_feature in shared_features_by_choice\n            ],\n            axis=-1,\n        )\n    if isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = tf.concat(\n            [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n            axis=-1,\n        )\n\n    # Computation of utilities\n    utilities = []\n\n    # Computation of the customer features embeddings\n    z_embeddings = self.z_model(shared_features_by_choice)\n\n    # Iterate over items in assortment\n    for item_i in range(items_features_by_choice.shape[1]):\n        # Computation of item features embeddings\n        x_embeddings = self.x_model(items_features_by_choice[:, item_i, :])\n\n        utilities.append([])\n\n        # Computation of utilites from embeddings, iteration over heterogeneities\n        # (eps_x * eps_z)\n        for _x in x_embeddings:\n            for _z in z_embeddings:\n                _u = tf.keras.layers.Concatenate()(\n                    [\n                        items_features_by_choice[:, item_i, :],\n                        _x,\n                        shared_features_by_choice,\n                        _z,\n                    ]\n                )\n                utilities[-1].append(self.u_model(_u))\n    # Reshape utilities: (batch_size, num_items, heterogeneity)\n    return tf.squeeze(tf.transpose(tf.stack(utilities, axis=1)), 0)\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.instantiate","title":"<code>instantiate()</code>","text":"<p>Instatiation of the RUMnet model.</p> <p>Creation of :     - x_model encoding products features,     - z_model encoding customers features,     - u_model computing utilities from product, customer features and their embeddings</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def instantiate(self):\n    \"\"\"Instatiation of the RUMnet model.\n\n    Creation of :\n        - x_model encoding products features,\n        - z_model encoding customers features,\n        - u_model computing utilities from product, customer features and their embeddings\n    \"\"\"\n    # Instatiation of the different nets\n    self.x_model, self.z_model, self.u_model = recreate_official_nets(\n        num_products_features=self.num_products_features,\n        num_customer_features=self.num_customer_features,\n        x_width=self.width_eps_x,\n        x_depth=self.depth_eps_x,\n        x_eps=self.heterogeneity_x,\n        z_width=self.width_eps_z,\n        z_depth=self.depth_eps_z,\n        z_eps=self.heterogeneity_z,\n        width_u=self.width_u,\n        depth_u=self.depth_u,\n        l2_regularization_coeff=self.l2_regularization_coef,\n    )\n\n    self.loss = tf_ops.CustomCategoricalCrossEntropy(\n        from_logits=False,\n        label_smoothing=self.label_smoothing,\n        epsilon=self.logmin,\n    )\n    self.instantiated = True\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.PaperRUMnet.train_step","title":"<code>train_step(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices, sample_weight=None)</code>","text":"<p>Update model's weight with a step of gradient descent.</p> <p>Function that represents one training step (= one gradient descent step) of the model. Handles a batch of data of size n_contexts = n_choices = batch_size</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>A batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>A batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <code>sample_weight</code> <code>ndarray</code> <p>List samples weights to apply during the gradient descent to the batch elements, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Value of NegativeLogLikelihood loss for the batch</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>@tf.function\ndef train_step(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n    sample_weight=None,\n):\n    \"\"\"Update model's weight with a step of gradient descent.\n\n    Function that represents one training step (= one gradient descent step) of the model.\n    Handles a batch of data of size n_contexts = n_choices = batch_size\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        A batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        A batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n    sample_weight : np.ndarray, optional\n        List samples weights to apply during the gradient descent to the batch elements,\n        by default None\n\n    Returns\n    -------\n    tf.Tensor\n        Value of NegativeLogLikelihood loss for the batch\n    \"\"\"\n    with tf.GradientTape() as tape:\n        # Computation of utilities\n        all_u = self.compute_batch_utility(\n            shared_features_by_choice=shared_features_by_choice,\n            items_features_by_choice=items_features_by_choice,\n            available_items_by_choice=available_items_by_choice,\n            choices=choices,\n        )\n        # Iterate over heterogeneities\n        eps_probabilities = tf.nn.softmax(all_u, axis=1)\n\n        # Average probabilities over heterogeneities\n        probabilities = tf.reduce_mean(eps_probabilities, axis=-1)\n        # It is not in the paper, but let's normalize with availabilities\n        probabilities = tf.multiply(probabilities, available_items_by_choice)\n        probabilities = tf.divide(\n            probabilities,\n            tf.reduce_sum(probabilities, axis=1, keepdims=True) + 1e-5,\n        )\n        if self.tol &gt; 0:\n            probabilities = (1 - self.tol) * probabilities + self.tol * tf.ones_like(\n                probabilities\n            ) / probabilities.shape[-1]\n\n        # Probabilities of selected products\n\n        # Negative Log-Likelihood\n        batch_nll = self.loss(\n            y_pred=probabilities,\n            y_true=tf.one_hot(choices, depth=probabilities.shape[1]),\n            sample_weight=sample_weight,\n        )\n\n        if self.regularization is not None:\n            regularization = tf.reduce_sum(\n                [self.regularizer(w) for w in self.trainable_weights]\n            )\n            batch_nll += regularization\n\n    grads = tape.gradient(batch_nll, self.trainable_weights)\n    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n    return batch_nll\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.ParallelDense","title":"<code>ParallelDense</code>","text":"<p>             Bases: <code>Layer</code></p> <p>Layer that represents several Dense layers in Parallel.</p> <p>Parallel means that they have the same input, but then are not intricated and are totally independant from each other.</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>class ParallelDense(tf.keras.layers.Layer):\n    \"\"\"Layer that represents several Dense layers in Parallel.\n\n    Parallel means that they have the same input, but then are not intricated and\n    are totally independant from each other.\n    \"\"\"\n\n    def __init__(self, width, depth, heterogeneity, activation=\"relu\", **kwargs):\n        \"\"\"Instantiate the layer.\n\n        Following tf.keras.Layer API. Note that there will be width * depth * heterogeneity\n        number of neurons in the layer.\n\n        Parameters\n        ----------\n        width : int\n            Number of neurons for each dense layer.\n        depth : int\n            Number of neuron layers.\n        heterogeneity : int\n            Number of dense layers that are in parallel\n        activation : str, optional\n            Activation function at the end of each layer, by default \"relu\"\n        \"\"\"\n        super().__init__(**kwargs)\n        self.width = width\n        self.depth = depth\n        self.heterogeneity = heterogeneity\n        self.activation = tf.keras.layers.Activation(activation)\n\n    def build(self, input_shape):\n        \"\"\"Lazy build of the layer.\n\n        Parameters\n        ----------\n        input_shape : tuple\n            Shape of the input of the layer. Typically (batch_size, num_features).\n            Batch_size (None) is ignored, but num_features is the shape of the input.\n        \"\"\"\n        super().build(input_shape)\n\n        weights = [\n            (\n                self.add_weight(\n                    shape=(input_shape[-1], self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n                self.add_weight(\n                    shape=(self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n            )\n        ]\n        for i in range(self.depth - 1):\n            weights.append(\n                (\n                    self.add_weight(\n                        shape=(self.width, self.width, self.heterogeneity),\n                        initializer=\"glorot_normal\",\n                        trainable=True,\n                    ),\n                    self.add_weight(\n                        shape=(self.width, self.heterogeneity),\n                        initializer=\"glorot_normal\",\n                        trainable=True,\n                    ),\n                )\n            )\n\n        self.w = weights\n\n    def call(self, inputs):\n        \"\"\"Predict of the layer.\n\n        Follows tf.keras.Layer API.\n\n        Parameters\n        ----------\n        inputs : tf.Tensor, np.ndarray\n            Tensor of shape (batch_size, n_features) as input of the model.\n\n        Returns\n        -------\n        outputs\n            Tensor of shape (batch_size, width, heterogeneity)\n        \"\"\"\n        outputs = tf.tensordot(inputs, self.w[0][0], axes=1) + self.w[0][1]\n        outputs = self.activation(outputs)\n        # tf.nn.bias_add(y, weights[0][1], data_format=\"NC...\")\n\n        for w, b in self.w[1:]:\n            outputs = tf.einsum(\"ijk,jlk-&gt;ilk\", outputs, w) + b\n            outputs = self.activation(outputs)\n\n        return outputs\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.ParallelDense.__init__","title":"<code>__init__(width, depth, heterogeneity, activation='relu', **kwargs)</code>","text":"<p>Instantiate the layer.</p> <p>Following tf.keras.Layer API. Note that there will be width * depth * heterogeneity number of neurons in the layer.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Number of neurons for each dense layer.</p> required <code>depth</code> <code>int</code> <p>Number of neuron layers.</p> required <code>heterogeneity</code> <code>int</code> <p>Number of dense layers that are in parallel</p> required <code>activation</code> <code>str</code> <p>Activation function at the end of each layer, by default \"relu\"</p> <code>'relu'</code> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def __init__(self, width, depth, heterogeneity, activation=\"relu\", **kwargs):\n    \"\"\"Instantiate the layer.\n\n    Following tf.keras.Layer API. Note that there will be width * depth * heterogeneity\n    number of neurons in the layer.\n\n    Parameters\n    ----------\n    width : int\n        Number of neurons for each dense layer.\n    depth : int\n        Number of neuron layers.\n    heterogeneity : int\n        Number of dense layers that are in parallel\n    activation : str, optional\n        Activation function at the end of each layer, by default \"relu\"\n    \"\"\"\n    super().__init__(**kwargs)\n    self.width = width\n    self.depth = depth\n    self.heterogeneity = heterogeneity\n    self.activation = tf.keras.layers.Activation(activation)\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.ParallelDense.build","title":"<code>build(input_shape)</code>","text":"<p>Lazy build of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple</code> <p>Shape of the input of the layer. Typically (batch_size, num_features). Batch_size (None) is ignored, but num_features is the shape of the input.</p> required Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def build(self, input_shape):\n    \"\"\"Lazy build of the layer.\n\n    Parameters\n    ----------\n    input_shape : tuple\n        Shape of the input of the layer. Typically (batch_size, num_features).\n        Batch_size (None) is ignored, but num_features is the shape of the input.\n    \"\"\"\n    super().build(input_shape)\n\n    weights = [\n        (\n            self.add_weight(\n                shape=(input_shape[-1], self.width, self.heterogeneity),\n                initializer=\"glorot_normal\",\n                trainable=True,\n            ),\n            self.add_weight(\n                shape=(self.width, self.heterogeneity),\n                initializer=\"glorot_normal\",\n                trainable=True,\n            ),\n        )\n    ]\n    for i in range(self.depth - 1):\n        weights.append(\n            (\n                self.add_weight(\n                    shape=(self.width, self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n                self.add_weight(\n                    shape=(self.width, self.heterogeneity),\n                    initializer=\"glorot_normal\",\n                    trainable=True,\n                ),\n            )\n        )\n\n    self.w = weights\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.ParallelDense.call","title":"<code>call(inputs)</code>","text":"<p>Predict of the layer.</p> <p>Follows tf.keras.Layer API.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>(Tensor, ndarray)</code> <p>Tensor of shape (batch_size, n_features) as input of the model.</p> required <p>Returns:</p> Type Description <code>outputs</code> <p>Tensor of shape (batch_size, width, heterogeneity)</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def call(self, inputs):\n    \"\"\"Predict of the layer.\n\n    Follows tf.keras.Layer API.\n\n    Parameters\n    ----------\n    inputs : tf.Tensor, np.ndarray\n        Tensor of shape (batch_size, n_features) as input of the model.\n\n    Returns\n    -------\n    outputs\n        Tensor of shape (batch_size, width, heterogeneity)\n    \"\"\"\n    outputs = tf.tensordot(inputs, self.w[0][0], axes=1) + self.w[0][1]\n    outputs = self.activation(outputs)\n    # tf.nn.bias_add(y, weights[0][1], data_format=\"NC...\")\n\n    for w, b in self.w[1:]:\n        outputs = tf.einsum(\"ijk,jlk-&gt;ilk\", outputs, w) + b\n        outputs = self.activation(outputs)\n\n    return outputs\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.create_ff_network","title":"<code>create_ff_network(input_shape, depth, width, activation='elu', add_last=False, l2_regularization_coeff=0.0)</code>","text":"<p>Create a simple fully connected (Dense) network.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple of int</code> <p>shape of the input of the network. Typically (num_features, )</p> required <code>depth</code> <code>int</code> <p>Number of dense/fully-connected of the network to create.</p> required <code>width</code> <code>int</code> <p>Neurons number for all dense layers.</p> required <code>activation</code> <code>str</code> <p>Activation function to use at the end of each layer except the last one, by default \"elu\"</p> <code>'elu'</code> <code>add_last</code> <code>bool</code> <p>Whether to add a Dense layer with a single output at the end, by default False Typically to be used when creating the utility network, that outputs a single number: the utility.</p> <code>False</code> <code>l2_regularization_coeff</code> <code>float</code> <p>Regularization coefficient for Dense layers weights during training, by default 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Model</code> <p>Dense Neural Network with tensorflow backend.</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def create_ff_network(\n    input_shape,\n    depth,\n    width,\n    activation=\"elu\",\n    add_last=False,\n    l2_regularization_coeff=0.0,\n):\n    \"\"\"Create a simple fully connected (Dense) network.\n\n    Parameters\n    ----------\n    input_shape : tuple of int\n        shape of the input of the network. Typically (num_features, )\n    depth : int\n        Number of dense/fully-connected of the network to create.\n    width : int\n        Neurons number for all dense layers.\n    activation : str, optional\n        Activation function to use at the end of each layer except the last one,\n        by default \"elu\"\n    add_last : bool, optional\n        Whether to add a Dense layer with a single output at the end, by default False\n        Typically to be used when creating the utility network, that outputs a single number:\n        the utility.\n    l2_regularization_coeff : float, optional\n        Regularization coefficient for Dense layers weights during training, by default 0.0\n\n    Returns\n    -------\n    tf.keras.Model\n        Dense Neural Network with tensorflow backend.\n    \"\"\"\n    input = tf.keras.layers.Input(shape=input_shape)\n    regularizer = tf.keras.regularizers.L2(l2_regularization_coeff)\n    out = input\n    for _ in range(depth):\n        out = tf.keras.layers.Dense(\n            width, activation=activation, kernel_regularizer=regularizer, use_bias=True\n        )(out)\n    if add_last:\n        out = tf.keras.layers.Dense(1, activation=\"linear\", use_bias=False)(out)\n    return tf.keras.Model(inputs=input, outputs=out)\n</code></pre>"},{"location":"references/models/references_rumnet/#choice_learn.models.rumnet.recreate_official_nets","title":"<code>recreate_official_nets(num_products_features, x_width, x_depth, x_eps, num_customer_features, z_width, z_depth, z_eps, width_u, depth_u, l2_regularization_coeff=0.0)</code>","text":"<p>Create the three nets used in RUMnet: X_net, Z_net and U_net.</p> <p>Parameters:</p> Name Type Description Default <code>num_products_features</code> <code>int</code> <p>Number of features each product will be described with. In terms of ChoiceDataset it is the number of { items_features + contexts_items_features } for one product.</p> required <code>x_width</code> <code>int</code> <p>Number of neurons for each dense layer for the products encoding net.</p> required <code>x_depth</code> <code>int</code> <p>Number of dense layers for the products encoding net.</p> required <code>x_eps</code> <code>int</code> <p>Number of nets of products features encoding.</p> required <code>num_customer_features</code> <code>int</code> <p>Number of features each customer will be described with. In terms of ChoiceDataset it is the number of contexts_features.</p> required <code>z_width</code> <code>int</code> <p>Number of neurons for each dense layer for the customers encoding net.</p> required <code>z_depth</code> <code>int</code> <p>Number of dense layers for the customers encoding net.</p> required <code>z_eps</code> <code>int</code> <p>Number of nets of customers features encoding.</p> required <code>width_u</code> <code>int</code> <p>Number of neurons for each dense layer for the utility net.</p> required <code>depth_u</code> <code>int</code> <p>Number of dense layers for the utility net.</p> required <code>l2_regularization_coef</code> <code>float</code> <p>Value of dense layers weights regulariation to apply during training, by default 0.0</p> required <p>Returns:</p> Type Description <code>Model</code> <p>Product features encoding network</p> <code>Model</code> <p>Customer features encoding network</p> <code>Model</code> <p>Features and encoding to utility computation network</p> Source code in <code>choice_learn/models/rumnet.py</code> <pre><code>def recreate_official_nets(\n    num_products_features,\n    x_width,\n    x_depth,\n    x_eps,\n    num_customer_features,\n    z_width,\n    z_depth,\n    z_eps,\n    width_u,\n    depth_u,\n    l2_regularization_coeff=0.0,\n):\n    \"\"\"Create the three nets used in RUMnet: X_net, Z_net and U_net.\n\n    Parameters\n    ----------\n    num_products_features : int\n        Number of features each product will be described with.\n        In terms of ChoiceDataset it is the number of { items_features + contexts_items_features }\n        for one product.\n    x_width : int\n        Number of neurons for each dense layer for the products encoding net.\n    x_depth : int\n        Number of dense layers for the products encoding net.\n    x_eps : int\n        Number of nets of products features encoding.\n    num_customer_features : int\n        Number of features each customer will be described with.\n        In terms of ChoiceDataset it is the number of contexts_features.\n    z_width : int\n        Number of neurons for each dense layer for the customers encoding net.\n    z_depth : int\n        Number of dense layers for the customers encoding net.\n    z_eps : int\n        Number of nets of customers features encoding.\n    width_u : int\n        Number of neurons for each dense layer for the utility net.\n    depth_u : int\n        Number of dense layers for the utility net.\n    l2_regularization_coef : float, optional\n        Value of dense layers weights regulariation to apply during training, by default 0.0\n\n    Returns\n    -------\n    tf.keras.Model\n        Product features encoding network\n    tf.keras.Model\n        Customer features encoding network\n    tf.keras.Model\n        Features and encoding to utility computation network\n    \"\"\"\n    # Products and Customers embeddings nets, quiet symmetrical\n    products_input = tf.keras.layers.Input(shape=(num_products_features,))\n    customer_input = tf.keras.layers.Input(shape=(num_customer_features,))\n    x_embeddings = []\n    z_embeddings = []\n\n    # Creating independant nets for each heterogeneity\n    for _ in range(x_eps):\n        x_embedding = create_ff_network(\n            input_shape=(num_products_features,),\n            depth=x_depth,\n            width=x_width,\n            l2_regularization_coeff=l2_regularization_coeff,\n        )(products_input)\n        x_embeddings.append(x_embedding)\n\n    # Creating independant nets for each heterogeneity\n    for _ in range(z_eps):\n        z_embedding = create_ff_network(\n            input_shape=(num_customer_features,),\n            depth=z_depth,\n            width=z_width,\n            l2_regularization_coeff=l2_regularization_coeff,\n        )(customer_input)\n\n        z_embeddings.append(z_embedding)\n\n    x_net = tf.keras.Model(inputs=products_input, outputs=x_embeddings, name=\"X_embedding\")\n    z_net = tf.keras.Model(inputs=customer_input, outputs=z_embeddings, name=\"Z_embedding\")\n\n    # Utility network\n    u_net = create_ff_network(\n        input_shape=(\n            x_width + z_width + num_products_features + num_customer_features,\n        ),  # Input shape from previous nets\n        width=width_u,\n        depth=depth_u,\n        add_last=True,  # Add last for utility\n        l2_regularization_coeff=l2_regularization_coeff,\n    )\n\n    return x_net, z_net, u_net\n</code></pre>"},{"location":"references/models/references_simple_mnl/","title":"SimpleMNL model","text":"<p>Implementation of the simple linear multinomial logit model.</p> <p>It is a multi output logistic regression.</p>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL","title":"<code>SimpleMNL</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>Simple MNL with one linear coefficient to estimate by feature.</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>class SimpleMNL(ChoiceModel):\n    \"\"\"Simple MNL with one linear coefficient to estimate by feature.\"\"\"\n\n    def __init__(\n        self,\n        add_exit_choice=False,\n        intercept=None,\n        optimizer=\"lbfgs\",\n        lr=0.001,\n        **kwargs,\n    ):\n        \"\"\"Initialize of Simple-MNL.\n\n        Parameters\n        ----------\n        add_exit_choice : bool, optional\n            Whether or not to normalize the probabilities computation with an exit choice\n            whose utility would be 1, by default True\n        intercept: str, optional\n            Type of intercept to use, by default None\n        optimizer: str\n            TensorFlow optimizer to be used for estimation\n        lr: float\n            Learning Rate to be used with optimizer.\n        \"\"\"\n        super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n        self.instantiated = False\n        self.intercept = intercept\n\n    def instantiate(self, n_items, n_shared_features, n_items_features):\n        \"\"\"Instantiate the model from ModelSpecification object.\n\n        Parameters\n        ----------\n        n_items : int\n            Number of items/aternatives to consider.\n        n_shared_features : int\n            Number of contexts features\n        n_items_features : int\n            Number of contexts items features\n\n        Returns\n        -------\n        list of tf.Tensor\n            List of the weights created coresponding to the specification.\n        \"\"\"\n        weights = []\n        indexes = {}\n        for n_feat, feat_name in zip(\n            [n_shared_features, n_items_features],\n            [\"shared_features\", \"items_features\"],\n        ):\n            if n_feat &gt; 0:\n                weights += [\n                    tf.Variable(\n                        tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_feat,)),\n                        name=f\"Weights_{feat_name}\",\n                    )\n                ]\n                indexes[feat_name] = len(weights) - 1\n        if self.intercept is None:\n            logging.info(\"No intercept in the model\")\n        elif self.intercept == \"item\":\n            weights.append(\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items - 1,)),\n                    name=\"Intercept\",\n                )\n            )\n            indexes[\"intercept\"] = len(weights) - 1\n        elif self.intercept == \"item-full\":\n            logging.info(\"Simple MNL intercept is not normalized to 0!\")\n            weights.append(\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items,)),\n                    name=\"Intercept\",\n                )\n            )\n            indexes[\"intercept\"] = len(weights) - 1\n        else:\n            weights.append(\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1,)),\n                    name=\"Intercept\",\n                )\n            )\n            indexes[\"intercept\"] = len(weights) - 1\n\n        self.instantiated = True\n        self.indexes = indexes\n        self._trainable_weights = weights\n        return indexes, weights\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Trainable weights of the model.\"\"\"\n        return self._trainable_weights\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Compute the utility of the model. Selects the right method to compute.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        tf.Tensor\n            Computed utilities of shape (n_choices, n_items).\n        \"\"\"\n        _ = choices\n\n        if \"shared_features\" in self.indexes.keys():\n            if isinstance(shared_features_by_choice, tuple):\n                shared_features_by_choice = tf.concat(*shared_features_by_choice, axis=1)\n            shared_features_by_choice = tf.cast(shared_features_by_choice, tf.float32)\n            shared_features_utilities = tf.tensordot(\n                shared_features_by_choice,\n                self.trainable_weights[self.indexes[\"shared_features\"]],\n                axes=1,\n            )\n            shared_features_utilities = tf.expand_dims(shared_features_utilities, axis=-1)\n        else:\n            shared_features_utilities = 0\n\n        if \"items_features\" in self.indexes.keys():\n            if isinstance(items_features_by_choice, tuple):\n                items_features_by_choice = tf.concat([*items_features_by_choice], axis=2)\n            items_features_by_choice = tf.cast(items_features_by_choice, tf.float32)\n            items_features_utilities = tf.tensordot(\n                items_features_by_choice,\n                self.trainable_weights[self.indexes[\"items_features\"]],\n                axes=1,\n            )\n        else:\n            items_features_utilities = tf.zeros(\n                (available_items_by_choice.shape[0], available_items_by_choice.shape[1])\n            )\n\n        if \"intercept\" in self.indexes.keys():\n            intercept = self.trainable_weights[self.indexes[\"intercept\"]]\n            if self.intercept == \"item\":\n                intercept = tf.concat([tf.constant([0.0]), intercept], axis=0)\n            if self.intercept in [\"item\", \"item-full\"]:\n                intercept = tf.expand_dims(intercept, axis=0)\n        else:\n            intercept = 0\n\n        return shared_features_utilities + items_features_utilities + intercept\n\n    def fit(self, choice_dataset, get_report=False, **kwargs):\n        \"\"\"Fit to estimate the parameters.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        if not self.instantiated:\n            # Lazy Instantiation\n            self.indexes, self._trainable_weights = self.instantiate(\n                n_items=choice_dataset.get_n_items(),\n                n_shared_features=choice_dataset.get_n_shared_features(),\n                n_items_features=choice_dataset.get_n_items_features(),\n            )\n            self.instantiated = True\n        fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n\n    def _fit_with_lbfgs(self, choice_dataset, sample_weight=None, get_report=False, **kwargs):\n        \"\"\"Specific fit function to estimate the parameters with LBFGS.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        n_epochs : int\n            Number of epochs to run.\n        sample_weight: Iterable, optional\n            list of each sample weight, by default None meaning that all samples have weight 1.\n        get_report: bool, optional\n            Whether or not to compute a report of the estimation, by default False.\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        if not self.instantiated:\n            # Lazy Instantiation\n            self.indexes, self._trainable_weights = self.instantiate(\n                n_items=choice_dataset.get_n_items(),\n                n_shared_features=choice_dataset.get_n_shared_features(),\n                n_items_features=choice_dataset.get_n_items_features(),\n            )\n            self.instantiated = True\n        fit = super()._fit_with_lbfgs(\n            choice_dataset=choice_dataset, sample_weight=sample_weight, **kwargs\n        )\n        if get_report:\n            self.report = self.compute_report(choice_dataset)\n        return fit\n\n    def compute_report(self, choice_dataset):\n        \"\"\"Compute a report of the estimated weights.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        pandas.DataFrame\n            A DF with estimation, Std Err, z_value and p_value for each coefficient.\n        \"\"\"\n\n        def phi(x):\n            \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n            return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n        weights_std = self.get_weights_std(choice_dataset)\n\n        names = []\n        z_values = []\n        estimations = []\n        p_z = []\n        i = 0\n        for weight in self.trainable_weights:\n            for j in range(weight.shape[0]):\n                if weight.shape[0] &gt; 1:\n                    names.append(f\"{weight.name[:-2]}_{j}\")\n                else:\n                    names.append(f\"{weight.name[:-2]}\")\n                estimations.append(weight.numpy()[j])\n                z_values.append(weight.numpy()[j] / weights_std[i].numpy())\n                p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n                i += 1\n\n        return pd.DataFrame(\n            {\n                \"Coefficient Name\": names,\n                \"Coefficient Estimation\": estimations,\n                \"Std. Err\": weights_std.numpy(),\n                \"z_value\": z_values,\n                \"P(.&gt;z)\": p_z,\n            },\n        )\n\n    def get_weights_std(self, choice_dataset):\n        \"\"\"Approximates Std Err with Hessian matrix.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            ChoiceDataset used for the estimation of the weights that will be\n            used to compute the Std Err of this estimation.\n\n        Returns\n        -------\n        tf.Tensor\n            Estimation of the Std Err for the weights.\n        \"\"\"\n        # Loops of differentiation\n        with tf.GradientTape() as tape_1:\n            with tf.GradientTape(persistent=True) as tape_2:\n                model = self.clone()\n                w = tf.concat(self.trainable_weights, axis=0)\n                tape_2.watch(w)\n                tape_1.watch(w)\n                mw = []\n                index = 0\n                for _w in self.trainable_weights:\n                    mw.append(w[index : index + _w.shape[0]])\n                    index += _w.shape[0]\n                model._trainable_weights = mw\n                for batch in choice_dataset.iter_batch(batch_size=-1):\n                    utilities = model.compute_batch_utility(*batch)\n                    probabilities = tf.nn.softmax(utilities, axis=-1)\n                    loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                        y_pred=probabilities,\n                        y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[-1]),\n                    )\n            # Compute the Jacobian\n            jacobian = tape_2.jacobian(loss, w)\n        # Compute the Hessian from the Jacobian\n        hessian = tape_1.jacobian(jacobian, w)\n        hessian = tf.linalg.inv(tf.squeeze(hessian))\n        return tf.sqrt([hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n\n    def clone(self):\n        \"\"\"Return a clone of the model.\"\"\"\n        clone = SimpleMNL(\n            add_exit_choice=self.add_exit_choice,\n            optimizer=self.optimizer_name,\n        )\n        if hasattr(self, \"history\"):\n            clone.history = self.history\n        if hasattr(self, \"is_fitted\"):\n            clone.is_fitted = self.is_fitted\n        if hasattr(self, \"instantiated\"):\n            clone.instantiated = self.instantiated\n        clone.loss = self.loss\n        clone.label_smoothing = self.label_smoothing\n        if hasattr(self, \"report\"):\n            clone.report = self.report\n        if hasattr(self, \"trainable_weights\"):\n            clone._trainable_weights = self.trainable_weights\n        if hasattr(self, \"indexes\"):\n            clone.indexes = self.indexes\n        if hasattr(self, \"intercept\"):\n            clone.intercept = self.intercept\n        if hasattr(self, \"lr\"):\n            clone.lr = self.lr\n        if hasattr(self, \"_items_features_names\"):\n            clone._items_features_names = self._items_features_names\n        if hasattr(self, \"_shared_features_names\"):\n            clone._shared_features_names = self._shared_features_names\n        return clone\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Trainable weights of the model.</p>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.__init__","title":"<code>__init__(add_exit_choice=False, intercept=None, optimizer='lbfgs', lr=0.001, **kwargs)</code>","text":"<p>Initialize of Simple-MNL.</p> <p>Parameters:</p> Name Type Description Default <code>add_exit_choice</code> <code>bool</code> <p>Whether or not to normalize the probabilities computation with an exit choice whose utility would be 1, by default True</p> <code>False</code> <code>intercept</code> <p>Type of intercept to use, by default None</p> <code>None</code> <code>optimizer</code> <p>TensorFlow optimizer to be used for estimation</p> <code>'lbfgs'</code> <code>lr</code> <p>Learning Rate to be used with optimizer.</p> <code>0.001</code> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def __init__(\n    self,\n    add_exit_choice=False,\n    intercept=None,\n    optimizer=\"lbfgs\",\n    lr=0.001,\n    **kwargs,\n):\n    \"\"\"Initialize of Simple-MNL.\n\n    Parameters\n    ----------\n    add_exit_choice : bool, optional\n        Whether or not to normalize the probabilities computation with an exit choice\n        whose utility would be 1, by default True\n    intercept: str, optional\n        Type of intercept to use, by default None\n    optimizer: str\n        TensorFlow optimizer to be used for estimation\n    lr: float\n        Learning Rate to be used with optimizer.\n    \"\"\"\n    super().__init__(add_exit_choice=add_exit_choice, optimizer=optimizer, lr=lr, **kwargs)\n    self.instantiated = False\n    self.intercept = intercept\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.clone","title":"<code>clone()</code>","text":"<p>Return a clone of the model.</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def clone(self):\n    \"\"\"Return a clone of the model.\"\"\"\n    clone = SimpleMNL(\n        add_exit_choice=self.add_exit_choice,\n        optimizer=self.optimizer_name,\n    )\n    if hasattr(self, \"history\"):\n        clone.history = self.history\n    if hasattr(self, \"is_fitted\"):\n        clone.is_fitted = self.is_fitted\n    if hasattr(self, \"instantiated\"):\n        clone.instantiated = self.instantiated\n    clone.loss = self.loss\n    clone.label_smoothing = self.label_smoothing\n    if hasattr(self, \"report\"):\n        clone.report = self.report\n    if hasattr(self, \"trainable_weights\"):\n        clone._trainable_weights = self.trainable_weights\n    if hasattr(self, \"indexes\"):\n        clone.indexes = self.indexes\n    if hasattr(self, \"intercept\"):\n        clone.intercept = self.intercept\n    if hasattr(self, \"lr\"):\n        clone.lr = self.lr\n    if hasattr(self, \"_items_features_names\"):\n        clone._items_features_names = self._items_features_names\n    if hasattr(self, \"_shared_features_names\"):\n        clone._shared_features_names = self._shared_features_names\n    return clone\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Compute the utility of the model. Selects the right method to compute.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed utilities of shape (n_choices, n_items).</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Compute the utility of the model. Selects the right method to compute.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    tf.Tensor\n        Computed utilities of shape (n_choices, n_items).\n    \"\"\"\n    _ = choices\n\n    if \"shared_features\" in self.indexes.keys():\n        if isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = tf.concat(*shared_features_by_choice, axis=1)\n        shared_features_by_choice = tf.cast(shared_features_by_choice, tf.float32)\n        shared_features_utilities = tf.tensordot(\n            shared_features_by_choice,\n            self.trainable_weights[self.indexes[\"shared_features\"]],\n            axes=1,\n        )\n        shared_features_utilities = tf.expand_dims(shared_features_utilities, axis=-1)\n    else:\n        shared_features_utilities = 0\n\n    if \"items_features\" in self.indexes.keys():\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tf.concat([*items_features_by_choice], axis=2)\n        items_features_by_choice = tf.cast(items_features_by_choice, tf.float32)\n        items_features_utilities = tf.tensordot(\n            items_features_by_choice,\n            self.trainable_weights[self.indexes[\"items_features\"]],\n            axes=1,\n        )\n    else:\n        items_features_utilities = tf.zeros(\n            (available_items_by_choice.shape[0], available_items_by_choice.shape[1])\n        )\n\n    if \"intercept\" in self.indexes.keys():\n        intercept = self.trainable_weights[self.indexes[\"intercept\"]]\n        if self.intercept == \"item\":\n            intercept = tf.concat([tf.constant([0.0]), intercept], axis=0)\n        if self.intercept in [\"item\", \"item-full\"]:\n            intercept = tf.expand_dims(intercept, axis=0)\n    else:\n        intercept = 0\n\n    return shared_features_utilities + items_features_utilities + intercept\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.compute_report","title":"<code>compute_report(choice_dataset)</code>","text":"<p>Compute a report of the estimated weights.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DF with estimation, Std Err, z_value and p_value for each coefficient.</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def compute_report(self, choice_dataset):\n    \"\"\"Compute a report of the estimated weights.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DF with estimation, Std Err, z_value and p_value for each coefficient.\n    \"\"\"\n\n    def phi(x):\n        \"\"\"Cumulative distribution function for the standard normal distribution.\"\"\"\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    weights_std = self.get_weights_std(choice_dataset)\n\n    names = []\n    z_values = []\n    estimations = []\n    p_z = []\n    i = 0\n    for weight in self.trainable_weights:\n        for j in range(weight.shape[0]):\n            if weight.shape[0] &gt; 1:\n                names.append(f\"{weight.name[:-2]}_{j}\")\n            else:\n                names.append(f\"{weight.name[:-2]}\")\n            estimations.append(weight.numpy()[j])\n            z_values.append(weight.numpy()[j] / weights_std[i].numpy())\n            p_z.append(2 * (1 - phi(tf.math.abs(z_values[-1]).numpy())))\n            i += 1\n\n    return pd.DataFrame(\n        {\n            \"Coefficient Name\": names,\n            \"Coefficient Estimation\": estimations,\n            \"Std. Err\": weights_std.numpy(),\n            \"z_value\": z_values,\n            \"P(.&gt;z)\": p_z,\n        },\n    )\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.fit","title":"<code>fit(choice_dataset, get_report=False, **kwargs)</code>","text":"<p>Fit to estimate the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Choice dataset to use for the estimation.</p> required <code>get_report</code> <p>Whether or not to compute a report of the estimation, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict with fit history.</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def fit(self, choice_dataset, get_report=False, **kwargs):\n    \"\"\"Fit to estimate the parameters.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Choice dataset to use for the estimation.\n    get_report: bool, optional\n        Whether or not to compute a report of the estimation, by default False\n\n    Returns\n    -------\n    dict\n        dict with fit history.\n    \"\"\"\n    if not self.instantiated:\n        # Lazy Instantiation\n        self.indexes, self._trainable_weights = self.instantiate(\n            n_items=choice_dataset.get_n_items(),\n            n_shared_features=choice_dataset.get_n_shared_features(),\n            n_items_features=choice_dataset.get_n_items_features(),\n        )\n        self.instantiated = True\n    fit = super().fit(choice_dataset=choice_dataset, **kwargs)\n    if get_report:\n        self.report = self.compute_report(choice_dataset)\n    return fit\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.get_weights_std","title":"<code>get_weights_std(choice_dataset)</code>","text":"<p>Approximates Std Err with Hessian matrix.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>ChoiceDataset used for the estimation of the weights that will be used to compute the Std Err of this estimation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Estimation of the Std Err for the weights.</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def get_weights_std(self, choice_dataset):\n    \"\"\"Approximates Std Err with Hessian matrix.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        ChoiceDataset used for the estimation of the weights that will be\n        used to compute the Std Err of this estimation.\n\n    Returns\n    -------\n    tf.Tensor\n        Estimation of the Std Err for the weights.\n    \"\"\"\n    # Loops of differentiation\n    with tf.GradientTape() as tape_1:\n        with tf.GradientTape(persistent=True) as tape_2:\n            model = self.clone()\n            w = tf.concat(self.trainable_weights, axis=0)\n            tape_2.watch(w)\n            tape_1.watch(w)\n            mw = []\n            index = 0\n            for _w in self.trainable_weights:\n                mw.append(w[index : index + _w.shape[0]])\n                index += _w.shape[0]\n            model._trainable_weights = mw\n            for batch in choice_dataset.iter_batch(batch_size=-1):\n                utilities = model.compute_batch_utility(*batch)\n                probabilities = tf.nn.softmax(utilities, axis=-1)\n                loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\")(\n                    y_pred=probabilities,\n                    y_true=tf.one_hot(choice_dataset.choices, depth=probabilities.shape[-1]),\n                )\n        # Compute the Jacobian\n        jacobian = tape_2.jacobian(loss, w)\n    # Compute the Hessian from the Jacobian\n    hessian = tape_1.jacobian(jacobian, w)\n    hessian = tf.linalg.inv(tf.squeeze(hessian))\n    return tf.sqrt([hessian[i][i] for i in range(len(tf.squeeze(hessian)))])\n</code></pre>"},{"location":"references/models/references_simple_mnl/#choice_learn.models.simple_mnl.SimpleMNL.instantiate","title":"<code>instantiate(n_items, n_shared_features, n_items_features)</code>","text":"<p>Instantiate the model from ModelSpecification object.</p> <p>Parameters:</p> Name Type Description Default <code>n_items</code> <code>int</code> <p>Number of items/aternatives to consider.</p> required <code>n_shared_features</code> <code>int</code> <p>Number of contexts features</p> required <code>n_items_features</code> <code>int</code> <p>Number of contexts items features</p> required <p>Returns:</p> Type Description <code>list of tf.Tensor</code> <p>List of the weights created coresponding to the specification.</p> Source code in <code>choice_learn/models/simple_mnl.py</code> <pre><code>def instantiate(self, n_items, n_shared_features, n_items_features):\n    \"\"\"Instantiate the model from ModelSpecification object.\n\n    Parameters\n    ----------\n    n_items : int\n        Number of items/aternatives to consider.\n    n_shared_features : int\n        Number of contexts features\n    n_items_features : int\n        Number of contexts items features\n\n    Returns\n    -------\n    list of tf.Tensor\n        List of the weights created coresponding to the specification.\n    \"\"\"\n    weights = []\n    indexes = {}\n    for n_feat, feat_name in zip(\n        [n_shared_features, n_items_features],\n        [\"shared_features\", \"items_features\"],\n    ):\n        if n_feat &gt; 0:\n            weights += [\n                tf.Variable(\n                    tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_feat,)),\n                    name=f\"Weights_{feat_name}\",\n                )\n            ]\n            indexes[feat_name] = len(weights) - 1\n    if self.intercept is None:\n        logging.info(\"No intercept in the model\")\n    elif self.intercept == \"item\":\n        weights.append(\n            tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items - 1,)),\n                name=\"Intercept\",\n            )\n        )\n        indexes[\"intercept\"] = len(weights) - 1\n    elif self.intercept == \"item-full\":\n        logging.info(\"Simple MNL intercept is not normalized to 0!\")\n        weights.append(\n            tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(n_items,)),\n                name=\"Intercept\",\n            )\n        )\n        indexes[\"intercept\"] = len(weights) - 1\n    else:\n        weights.append(\n            tf.Variable(\n                tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1,)),\n                name=\"Intercept\",\n            )\n        )\n        indexes[\"intercept\"] = len(weights) - 1\n\n    self.instantiated = True\n    self.indexes = indexes\n    self._trainable_weights = weights\n    return indexes, weights\n</code></pre>"},{"location":"references/models/references_tastenet/","title":"TasteNet Model","text":"<p>TasteNet model unofficial implementation.</p>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet","title":"<code>TasteNet</code>","text":"<p>             Bases: <code>ChoiceModel</code></p> <p>UnOfficial implementation of the TasteNet model.</p> <p>A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability, by Han, Y.; Calara Oereuran F.; Ben-Akiva, M.; Zegras, C. (2020).</p> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>class TasteNet(ChoiceModel):\n    \"\"\"UnOfficial implementation of the TasteNet model.\n\n    A neural-embedded discrete choice model: Learning taste representation with strengthened\n    interpretability, by Han, Y.; Calara Oereuran F.; Ben-Akiva, M.; Zegras, C. (2020).\n    \"\"\"\n\n    def __init__(\n        self,\n        taste_net_layers,\n        taste_net_activation,\n        items_features_by_choice_parametrization,\n        exp_paramater_mu=1.0,\n        **kwargs,\n    ):\n        \"\"\"Initialize of the model.\n\n        Parameters\n        ----------\n        taste_net_layers : list of ints\n            Width of the different layer to use in the taste network.\n        taste_net_activation : str\n            Activation function to use in the taste network.\n        items_features_by_choice_parametrization : list of lists\n            List of list of strings or floats. Each list corresponds to the features of an item.\n            Each string is the name of an activation function to apply to the feature.\n            Each float is a constant to multiply the feature by.\n            e.g. for the swissmetro that has 3 items with 4 features each:\n            [[-1., \"-exp\", \"-exp\", 0., \"linear\", 0., 0.],\n            [-1., \"-exp\", \"-exp\", \"linear\", 0., \"linear\", 0.],\n            [-1., \"-exp\", 0., 0., 0., 0., 0.]]\n        exp_paramater_mu : float\n            Parameter of the exponential function to use in the\n            items_features_by_choice_parametrization.\n            x = exp(x / exp_paramater_mu), default is 1.0.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.taste_net_layers = taste_net_layers\n        self.taste_net_activation = taste_net_activation\n        self.items_features_by_choice_parametrization = items_features_by_choice_parametrization\n        self.exp_paramater_mu = exp_paramater_mu\n\n        for item_params in items_features_by_choice_parametrization:\n            if len(item_params) != len(items_features_by_choice_parametrization[0]):\n                raise ValueError(\n                    f\"\"\"All items must have the same number of features parametrization.\n                                 Found {len(item_params)} and\n                                 {len(items_features_by_choice_parametrization[0])}\"\"\"\n                )\n        self.n_items = len(items_features_by_choice_parametrization)\n        self.n_items_features = len(items_features_by_choice_parametrization[0])\n        logging.info(\n            \"\"\"TasteNet model is instantiated for {self.n_items} items and\n                     {self.n_items_features} items_features.\"\"\"\n        )\n        self.instantiated = False\n\n    def get_activation_function(self, name):\n        \"\"\"Get a normalization function from its str name.\n\n        Parameters\n        ----------\n        name : str\n            Name of the function to apply.\n\n        Returns\n        -------\n        function\n            Tensorflow function to apply.\n        \"\"\"\n        if name == \"linear\":\n            return lambda x: x\n        if name == \"relu\":\n            return tf.nn.relu\n        if name == \"-relu\":\n            return lambda x: -tf.nn.relu(-x)\n        if name == \"exp\":\n            return lambda x: tf.exp(x / self.exp_paramater_mu)\n        if name == \"-exp\":\n            return lambda x: -tf.exp(-x / self.exp_paramater_mu)\n        if name == \"tanh\":\n            return tf.nn.tanh\n        if name == \"sigmoid\":\n            return tf.nn.sigmoid\n        raise ValueError(f\"Activation function {name} not supported.\")\n\n    def instantiate(self, n_shared_features):\n        \"\"\"Instantiate the model.\n\n        Parameters\n        ----------\n        n_shared_features : int\n            Number of shared_features or customer features.\n            It is needed to set-up the neural network input shape.\n        \"\"\"\n        # TODO: Add possibility for MNL-type weights\n        items_features_to_weight_index = {}\n        for i, item_param in enumerate(self.items_features_by_choice_parametrization):\n            for j, param in enumerate(item_param):\n                if isinstance(param, str):\n                    items_features_to_weight_index[(i, j)] = len(items_features_to_weight_index)\n        self.items_features_to_weight_index = items_features_to_weight_index\n\n        self.taste_params_module = get_feed_forward_net(\n            n_shared_features,\n            len(items_features_to_weight_index),\n            self.taste_net_layers,\n            self.taste_net_activation,\n        )\n        self.instantiated = True\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Argument to access the future trainable_weights throught the taste net.\n\n        Returns\n        -------\n        list\n            List of trainable weights.\n        \"\"\"\n        if self.instantiated:\n            return self.taste_params_module.trainable_variables\n        return []\n\n    def compute_batch_utility(\n        self,\n        shared_features_by_choice,\n        items_features_by_choice,\n        available_items_by_choice,\n        choices,\n    ):\n        \"\"\"Define how the model computes the utility of a product.\n\n        MUST be implemented in children classe !\n        For simpler use-cases this is the only method to be user-defined.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns\n        -------\n        np.ndarray\n            Utility of each product for each choice.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        _ = available_items_by_choice\n        # Restacking and dtyping of the item features\n        if isinstance(shared_features_by_choice, tuple):\n            shared_features_by_choice = tf.concat(\n                [\n                    tf.cast(shared_feature, tf.float32)\n                    for shared_feature in shared_features_by_choice\n                ],\n                axis=-1,\n            )\n        if isinstance(items_features_by_choice, tuple):\n            items_features_by_choice = tf.concat(\n                [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n                axis=-1,\n            )\n\n        taste_weights = self.taste_params_module(shared_features_by_choice)\n        item_utility_by_choice = []\n        for i, item_param in enumerate(self.items_features_by_choice_parametrization):\n            utility = tf.zeros_like(choices, dtype=tf.float32)\n            for j, param in enumerate(item_param):\n                if isinstance(param, str):\n                    weight = taste_weights[:, self.items_features_to_weight_index[(i, j)]]\n                    weight = self.get_activation_function(param)(weight)\n                    item_feature = items_features_by_choice[:, i, j] * weight\n\n                elif isinstance(param, float):\n                    item_feature = param * items_features_by_choice[:, i, j]\n                utility += item_feature\n            item_utility_by_choice.append(utility)\n        return tf.stack(item_utility_by_choice, axis=1)\n\n    def predict_tastes(self, shared_features_by_choice):\n        \"\"\"Predict the tastes of the model for a given dataset.\n\n        Parameters\n        ----------\n        shared_features_by_choice : np.ndarray\n            Shared Features by choice.\n\n        Returns\n        -------\n        np.ndarray\n            Taste of each product for each choice.\n            Shape is (n_choices, n_taste_parameters)\n        \"\"\"\n        return self.taste_params_module(shared_features_by_choice)\n\n    def fit(self, choice_dataset, **kwargs):\n        \"\"\"Fit to estimate the paramters.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        if not self.instantiated:\n            # Lazy Instantiation\n            if choice_dataset.get_n_items() != self.n_items:\n                raise ValueError(\n                    \"\"\"Number of items in the dataset does not match\n                    the number of items of the model.\"\"\"\n                )\n            if choice_dataset.get_n_items_features() != self.n_items_features:\n                raise ValueError(\n                    \"\"\"Number of items features in the dataset does not match the\n                    number of items features in the model.\"\"\"\n                )\n            self.instantiate(\n                n_shared_features=choice_dataset.get_n_shared_features(),\n            )\n            self.instantiated = True\n        return super().fit(choice_dataset=choice_dataset, **kwargs)\n\n    def _fit_with_lbfgs(self, choice_dataset, sample_weight=None, **kwargs):\n        \"\"\"Specific fit function to estimate the paramters with LBFGS.\n\n        Parameters\n        ----------\n        choice_dataset : ChoiceDataset\n            Choice dataset to use for the estimation.\n        n_epochs : int\n            Number of epochs to run.\n        sample_weight: Iterable, optional\n            list of each sample weight, by default None meaning that all samples have weight 1.\n\n        Returns\n        -------\n        dict\n            dict with fit history.\n        \"\"\"\n        if not self.instantiated:\n            if choice_dataset.get_n_items() != self.n_items:\n                raise ValueError(\n                    \"\"\"Number of items in the dataset does not match\n                    the number of items of the model.\"\"\"\n                )\n            if choice_dataset.get_n_items_features() != self.n_items_features:\n                raise ValueError(\n                    \"\"\"Number of items features in the dataset does not match\n                    the number of items features in the model.\"\"\"\n                )\n            # Lazy Instantiation\n            self.instantiate(\n                n_shared_features=choice_dataset.get_n_shared_features(),\n            )\n            self.instantiated = True\n        return super()._fit_with_lbfgs(\n            choice_dataset=choice_dataset, sample_weight=sample_weight, **kwargs\n        )\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.trainable_weights","title":"<code>trainable_weights</code>  <code>property</code>","text":"<p>Argument to access the future trainable_weights throught the taste net.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of trainable weights.</p>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.__init__","title":"<code>__init__(taste_net_layers, taste_net_activation, items_features_by_choice_parametrization, exp_paramater_mu=1.0, **kwargs)</code>","text":"<p>Initialize of the model.</p> <p>Parameters:</p> Name Type Description Default <code>taste_net_layers</code> <code>list of ints</code> <p>Width of the different layer to use in the taste network.</p> required <code>taste_net_activation</code> <code>str</code> <p>Activation function to use in the taste network.</p> required <code>items_features_by_choice_parametrization</code> <code>list of lists</code> <p>List of list of strings or floats. Each list corresponds to the features of an item. Each string is the name of an activation function to apply to the feature. Each float is a constant to multiply the feature by. e.g. for the swissmetro that has 3 items with 4 features each: [[-1., \"-exp\", \"-exp\", 0., \"linear\", 0., 0.], [-1., \"-exp\", \"-exp\", \"linear\", 0., \"linear\", 0.], [-1., \"-exp\", 0., 0., 0., 0., 0.]]</p> required <code>exp_paramater_mu</code> <code>float</code> <p>Parameter of the exponential function to use in the items_features_by_choice_parametrization. x = exp(x / exp_paramater_mu), default is 1.0.</p> <code>1.0</code> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def __init__(\n    self,\n    taste_net_layers,\n    taste_net_activation,\n    items_features_by_choice_parametrization,\n    exp_paramater_mu=1.0,\n    **kwargs,\n):\n    \"\"\"Initialize of the model.\n\n    Parameters\n    ----------\n    taste_net_layers : list of ints\n        Width of the different layer to use in the taste network.\n    taste_net_activation : str\n        Activation function to use in the taste network.\n    items_features_by_choice_parametrization : list of lists\n        List of list of strings or floats. Each list corresponds to the features of an item.\n        Each string is the name of an activation function to apply to the feature.\n        Each float is a constant to multiply the feature by.\n        e.g. for the swissmetro that has 3 items with 4 features each:\n        [[-1., \"-exp\", \"-exp\", 0., \"linear\", 0., 0.],\n        [-1., \"-exp\", \"-exp\", \"linear\", 0., \"linear\", 0.],\n        [-1., \"-exp\", 0., 0., 0., 0., 0.]]\n    exp_paramater_mu : float\n        Parameter of the exponential function to use in the\n        items_features_by_choice_parametrization.\n        x = exp(x / exp_paramater_mu), default is 1.0.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.taste_net_layers = taste_net_layers\n    self.taste_net_activation = taste_net_activation\n    self.items_features_by_choice_parametrization = items_features_by_choice_parametrization\n    self.exp_paramater_mu = exp_paramater_mu\n\n    for item_params in items_features_by_choice_parametrization:\n        if len(item_params) != len(items_features_by_choice_parametrization[0]):\n            raise ValueError(\n                f\"\"\"All items must have the same number of features parametrization.\n                             Found {len(item_params)} and\n                             {len(items_features_by_choice_parametrization[0])}\"\"\"\n            )\n    self.n_items = len(items_features_by_choice_parametrization)\n    self.n_items_features = len(items_features_by_choice_parametrization[0])\n    logging.info(\n        \"\"\"TasteNet model is instantiated for {self.n_items} items and\n                 {self.n_items_features} items_features.\"\"\"\n    )\n    self.instantiated = False\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.compute_batch_utility","title":"<code>compute_batch_utility(shared_features_by_choice, items_features_by_choice, available_items_by_choice, choices)</code>","text":"<p>Define how the model computes the utility of a product.</p> <p>MUST be implemented in children classe ! For simpler use-cases this is the only method to be user-defined.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>tuple of np.ndarray (choices_features)</code> <p>a batch of shared features Shape must be (n_choices, n_shared_features)</p> required <code>items_features_by_choice</code> <code>tuple of np.ndarray (choices_items_features)</code> <p>a batch of items features Shape must be (n_choices, n_items_features)</p> required <code>available_items_by_choice</code> <code>ndarray</code> <p>A batch of items availabilities Shape must be (n_choices, n_items)</p> required <code>choices_batch</code> <code>ndarray</code> <p>Choices Shape must be (n_choices, )</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Utility of each product for each choice. Shape must be (n_choices, n_items)</p> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def compute_batch_utility(\n    self,\n    shared_features_by_choice,\n    items_features_by_choice,\n    available_items_by_choice,\n    choices,\n):\n    \"\"\"Define how the model computes the utility of a product.\n\n    MUST be implemented in children classe !\n    For simpler use-cases this is the only method to be user-defined.\n\n    Parameters\n    ----------\n    shared_features_by_choice : tuple of np.ndarray (choices_features)\n        a batch of shared features\n        Shape must be (n_choices, n_shared_features)\n    items_features_by_choice : tuple of np.ndarray (choices_items_features)\n        a batch of items features\n        Shape must be (n_choices, n_items_features)\n    available_items_by_choice : np.ndarray\n        A batch of items availabilities\n        Shape must be (n_choices, n_items)\n    choices_batch : np.ndarray\n        Choices\n        Shape must be (n_choices, )\n\n    Returns\n    -------\n    np.ndarray\n        Utility of each product for each choice.\n        Shape must be (n_choices, n_items)\n    \"\"\"\n    _ = available_items_by_choice\n    # Restacking and dtyping of the item features\n    if isinstance(shared_features_by_choice, tuple):\n        shared_features_by_choice = tf.concat(\n            [\n                tf.cast(shared_feature, tf.float32)\n                for shared_feature in shared_features_by_choice\n            ],\n            axis=-1,\n        )\n    if isinstance(items_features_by_choice, tuple):\n        items_features_by_choice = tf.concat(\n            [tf.cast(items_feature, tf.float32) for items_feature in items_features_by_choice],\n            axis=-1,\n        )\n\n    taste_weights = self.taste_params_module(shared_features_by_choice)\n    item_utility_by_choice = []\n    for i, item_param in enumerate(self.items_features_by_choice_parametrization):\n        utility = tf.zeros_like(choices, dtype=tf.float32)\n        for j, param in enumerate(item_param):\n            if isinstance(param, str):\n                weight = taste_weights[:, self.items_features_to_weight_index[(i, j)]]\n                weight = self.get_activation_function(param)(weight)\n                item_feature = items_features_by_choice[:, i, j] * weight\n\n            elif isinstance(param, float):\n                item_feature = param * items_features_by_choice[:, i, j]\n            utility += item_feature\n        item_utility_by_choice.append(utility)\n    return tf.stack(item_utility_by_choice, axis=1)\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.fit","title":"<code>fit(choice_dataset, **kwargs)</code>","text":"<p>Fit to estimate the paramters.</p> <p>Parameters:</p> Name Type Description Default <code>choice_dataset</code> <code>ChoiceDataset</code> <p>Choice dataset to use for the estimation.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict with fit history.</p> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def fit(self, choice_dataset, **kwargs):\n    \"\"\"Fit to estimate the paramters.\n\n    Parameters\n    ----------\n    choice_dataset : ChoiceDataset\n        Choice dataset to use for the estimation.\n\n    Returns\n    -------\n    dict\n        dict with fit history.\n    \"\"\"\n    if not self.instantiated:\n        # Lazy Instantiation\n        if choice_dataset.get_n_items() != self.n_items:\n            raise ValueError(\n                \"\"\"Number of items in the dataset does not match\n                the number of items of the model.\"\"\"\n            )\n        if choice_dataset.get_n_items_features() != self.n_items_features:\n            raise ValueError(\n                \"\"\"Number of items features in the dataset does not match the\n                number of items features in the model.\"\"\"\n            )\n        self.instantiate(\n            n_shared_features=choice_dataset.get_n_shared_features(),\n        )\n        self.instantiated = True\n    return super().fit(choice_dataset=choice_dataset, **kwargs)\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.get_activation_function","title":"<code>get_activation_function(name)</code>","text":"<p>Get a normalization function from its str name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the function to apply.</p> required <p>Returns:</p> Type Description <code>function</code> <p>Tensorflow function to apply.</p> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def get_activation_function(self, name):\n    \"\"\"Get a normalization function from its str name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the function to apply.\n\n    Returns\n    -------\n    function\n        Tensorflow function to apply.\n    \"\"\"\n    if name == \"linear\":\n        return lambda x: x\n    if name == \"relu\":\n        return tf.nn.relu\n    if name == \"-relu\":\n        return lambda x: -tf.nn.relu(-x)\n    if name == \"exp\":\n        return lambda x: tf.exp(x / self.exp_paramater_mu)\n    if name == \"-exp\":\n        return lambda x: -tf.exp(-x / self.exp_paramater_mu)\n    if name == \"tanh\":\n        return tf.nn.tanh\n    if name == \"sigmoid\":\n        return tf.nn.sigmoid\n    raise ValueError(f\"Activation function {name} not supported.\")\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.instantiate","title":"<code>instantiate(n_shared_features)</code>","text":"<p>Instantiate the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_shared_features</code> <code>int</code> <p>Number of shared_features or customer features. It is needed to set-up the neural network input shape.</p> required Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def instantiate(self, n_shared_features):\n    \"\"\"Instantiate the model.\n\n    Parameters\n    ----------\n    n_shared_features : int\n        Number of shared_features or customer features.\n        It is needed to set-up the neural network input shape.\n    \"\"\"\n    # TODO: Add possibility for MNL-type weights\n    items_features_to_weight_index = {}\n    for i, item_param in enumerate(self.items_features_by_choice_parametrization):\n        for j, param in enumerate(item_param):\n            if isinstance(param, str):\n                items_features_to_weight_index[(i, j)] = len(items_features_to_weight_index)\n    self.items_features_to_weight_index = items_features_to_weight_index\n\n    self.taste_params_module = get_feed_forward_net(\n        n_shared_features,\n        len(items_features_to_weight_index),\n        self.taste_net_layers,\n        self.taste_net_activation,\n    )\n    self.instantiated = True\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.TasteNet.predict_tastes","title":"<code>predict_tastes(shared_features_by_choice)</code>","text":"<p>Predict the tastes of the model for a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>shared_features_by_choice</code> <code>ndarray</code> <p>Shared Features by choice.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Taste of each product for each choice. Shape is (n_choices, n_taste_parameters)</p> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def predict_tastes(self, shared_features_by_choice):\n    \"\"\"Predict the tastes of the model for a given dataset.\n\n    Parameters\n    ----------\n    shared_features_by_choice : np.ndarray\n        Shared Features by choice.\n\n    Returns\n    -------\n    np.ndarray\n        Taste of each product for each choice.\n        Shape is (n_choices, n_taste_parameters)\n    \"\"\"\n    return self.taste_params_module(shared_features_by_choice)\n</code></pre>"},{"location":"references/models/references_tastenet/#choice_learn.models.tastenet.get_feed_forward_net","title":"<code>get_feed_forward_net(input_width, output_width, layers_width, activation)</code>","text":"<p>Get a feed-forward neural network.</p> Source code in <code>choice_learn/models/tastenet.py</code> <pre><code>def get_feed_forward_net(input_width, output_width, layers_width, activation):\n    \"\"\"Get a feed-forward neural network.\"\"\"\n    net_input = tf.keras.layers.Input(shape=(input_width,))\n    net_output = net_input\n    for n_units in layers_width:\n        net_output = tf.keras.layers.Dense(n_units, activation=activation)(net_output)\n    net_output = tf.keras.layers.Dense(output_width, activation=None)(net_output)\n    return tf.keras.Model(inputs=net_input, outputs=net_output)\n</code></pre>"},{"location":"references/toolbox/references_assortment_optimizer/","title":"Tool: Assortment Optimizer","text":"<p>Tool function for assortment and pricing optimization.</p>"},{"location":"references/toolbox/references_assortment_optimizer/#choice_learn.toolbox.assortment_optimizer.LatentClassAssortmentOptimizer","title":"<code>LatentClassAssortmentOptimizer</code>","text":"<p>Assortment optimizer for latent class models.</p> <p>Implementation of the paper: Isabel M\u00e9ndez-D\u00edaz, Juan Jos\u00e9 Miranda-Bront, Gustavo Vulcano, Paula Zabala, A branch-and-cut algorithm for the latent-class logit assortment problem, Discrete Applied Mathematics, Volume 164, Part 1, 2014, Pages 246-263, ISSN 0166-218X, https://doi.org/10.1016/j.dam.2012.03.003.</p> Source code in <code>choice_learn/toolbox/assortment_optimizer.py</code> <pre><code>class LatentClassAssortmentOptimizer:\n    \"\"\"Assortment optimizer for latent class models.\n\n    Implementation of the paper:\n    Isabel M\u00e9ndez-D\u00edaz, Juan Jos\u00e9 Miranda-Bront, Gustavo Vulcano, Paula Zabala,\n    A branch-and-cut algorithm for the latent-class logit assortment problem,\n    Discrete Applied Mathematics,\n    Volume 164, Part 1,\n    2014,\n    Pages 246-263,\n    ISSN 0166-218X,\n    https://doi.org/10.1016/j.dam.2012.03.003.\n    \"\"\"\n\n    def __new__(\n        cls,\n        solver,\n        class_weights,\n        class_utilities,\n        itemwise_values,\n        assortment_size,\n        outside_option_given=False,\n    ):\n        \"\"\"Create the AssortmentOptimizer object.\n\n        Basically used to handle the choice of solver.\n\n        Parameters\n        ----------\n        solver: str\n            Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.\n        class_weights: Iterable\n            List of weights for each latent class.\n        class_utilities: Iterable\n            List of utilities for each item of each latent class.\n            Must have a shape of (n_classes, n_items)\n        itemwise_values: Iterable\n            List of to-be-optimized values for each item, e.g. prices.\n        assortment_size : int\n            maximum size of the requested assortment.\n        outside_option_given : bool\n            Whether the outside option is given or not (and thus is automatically added).\n        \"\"\"\n        if solver.lower() == \"gurobi\":\n            from .gurobi_opt import GurobiLatentClassAssortmentOptimizer\n\n            return GurobiLatentClassAssortmentOptimizer(\n                class_weights=class_weights,\n                class_utilities=class_utilities,\n                itemwise_values=itemwise_values,\n                assortment_size=assortment_size,\n                outside_option_given=outside_option_given,\n            )\n        if solver.lower() == \"or-tools\" or solver.lower() == \"ortools\":\n            from .or_tools_opt import ORToolsLatentClassAssortmentOptimizer\n\n            return ORToolsLatentClassAssortmentOptimizer(\n                class_weights=class_weights,\n                class_utilities=class_utilities,\n                itemwise_values=itemwise_values,\n                assortment_size=assortment_size,\n                outside_option_given=outside_option_given,\n            )\n\n        raise ValueError(\"Unknown solver. Please choose between 'gurobi' and 'or-tools'.\")\n</code></pre>"},{"location":"references/toolbox/references_assortment_optimizer/#choice_learn.toolbox.assortment_optimizer.LatentClassAssortmentOptimizer.__new__","title":"<code>__new__(solver, class_weights, class_utilities, itemwise_values, assortment_size, outside_option_given=False)</code>","text":"<p>Create the AssortmentOptimizer object.</p> <p>Basically used to handle the choice of solver.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <p>Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.</p> required <code>class_weights</code> <p>List of weights for each latent class.</p> required <code>class_utilities</code> <p>List of utilities for each item of each latent class. Must have a shape of (n_classes, n_items)</p> required <code>itemwise_values</code> <p>List of to-be-optimized values for each item, e.g. prices.</p> required <code>assortment_size</code> <code>int</code> <p>maximum size of the requested assortment.</p> required <code>outside_option_given</code> <code>bool</code> <p>Whether the outside option is given or not (and thus is automatically added).</p> <code>False</code> Source code in <code>choice_learn/toolbox/assortment_optimizer.py</code> <pre><code>def __new__(\n    cls,\n    solver,\n    class_weights,\n    class_utilities,\n    itemwise_values,\n    assortment_size,\n    outside_option_given=False,\n):\n    \"\"\"Create the AssortmentOptimizer object.\n\n    Basically used to handle the choice of solver.\n\n    Parameters\n    ----------\n    solver: str\n        Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.\n    class_weights: Iterable\n        List of weights for each latent class.\n    class_utilities: Iterable\n        List of utilities for each item of each latent class.\n        Must have a shape of (n_classes, n_items)\n    itemwise_values: Iterable\n        List of to-be-optimized values for each item, e.g. prices.\n    assortment_size : int\n        maximum size of the requested assortment.\n    outside_option_given : bool\n        Whether the outside option is given or not (and thus is automatically added).\n    \"\"\"\n    if solver.lower() == \"gurobi\":\n        from .gurobi_opt import GurobiLatentClassAssortmentOptimizer\n\n        return GurobiLatentClassAssortmentOptimizer(\n            class_weights=class_weights,\n            class_utilities=class_utilities,\n            itemwise_values=itemwise_values,\n            assortment_size=assortment_size,\n            outside_option_given=outside_option_given,\n        )\n    if solver.lower() == \"or-tools\" or solver.lower() == \"ortools\":\n        from .or_tools_opt import ORToolsLatentClassAssortmentOptimizer\n\n        return ORToolsLatentClassAssortmentOptimizer(\n            class_weights=class_weights,\n            class_utilities=class_utilities,\n            itemwise_values=itemwise_values,\n            assortment_size=assortment_size,\n            outside_option_given=outside_option_given,\n        )\n\n    raise ValueError(\"Unknown solver. Please choose between 'gurobi' and 'or-tools'.\")\n</code></pre>"},{"location":"references/toolbox/references_assortment_optimizer/#choice_learn.toolbox.assortment_optimizer.LatentClassPricingOptimizer","title":"<code>LatentClassPricingOptimizer</code>","text":"<p>Assortment optimizer for latent class models with additional pricing optimization.</p> <p>Implementation of the paper: Isabel M\u00e9ndez-D\u00edaz, Juan Jos\u00e9 Miranda-Bront, Gustavo Vulcano, Paula Zabala, A branch-and-cut algorithm for the latent-class logit assortment problem, Discrete Applied Mathematics, Volume 164, Part 1, 2014, Pages 246-263, ISSN 0166-218X, https://doi.org/10.1016/j.dam.2012.03.003.</p> Source code in <code>choice_learn/toolbox/assortment_optimizer.py</code> <pre><code>class LatentClassPricingOptimizer:\n    \"\"\"Assortment optimizer for latent class models with additional pricing optimization.\n\n    Implementation of the paper:\n    Isabel M\u00e9ndez-D\u00edaz, Juan Jos\u00e9 Miranda-Bront, Gustavo Vulcano, Paula Zabala,\n    A branch-and-cut algorithm for the latent-class logit assortment problem,\n    Discrete Applied Mathematics,\n    Volume 164, Part 1,\n    2014,\n    Pages 246-263,\n    ISSN 0166-218X,\n    https://doi.org/10.1016/j.dam.2012.03.003.\n    \"\"\"\n\n    def __new__(\n        cls,\n        solver,\n        class_weights,\n        class_utilities,\n        itemwise_values,\n        assortment_size,\n        outside_option_given=False,\n    ):\n        \"\"\"Create the AssortmentOptimizer object.\n\n        Basically used to handle the choice of solver.\n\n        Parameters\n        ----------\n        solver: str\n            Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.\n        class_weights: Iterable\n            List of weights for each latent class.\n        class_utilities: Iterable\n            List of utilities for each item of each latent class.\n            Must have a shape of (n_classes, n_items)\n        itemwise_values: Iterable\n            List of to-be-optimized values for each item, e.g. prices.\n        assortment_size : int\n            maximum size of the requested assortment.\n        outside_option_given : bool\n            Whether the outside option is given or not (and thus is automatically added).\n        \"\"\"\n        if solver.lower() == \"gurobi\":\n            from .gurobi_opt import GurobiLatentClassPricingOptimizer\n\n            return GurobiLatentClassPricingOptimizer(\n                class_weights=class_weights,\n                class_utilities=class_utilities,\n                itemwise_values=itemwise_values,\n                assortment_size=assortment_size,\n                outside_option_given=outside_option_given,\n            )\n        if solver.lower() == \"or-tools\" or solver.lower() == \"ortools\":\n            from .or_tools_opt import ORToolsLatentClassPricingOptimizer\n\n            return ORToolsLatentClassPricingOptimizer(\n                class_weights=class_weights,\n                class_utilities=class_utilities,\n                itemwise_values=itemwise_values,\n                assortment_size=assortment_size,\n                outside_option_given=outside_option_given,\n            )\n\n        raise ValueError(\"Unknown solver. Please choose between 'gurobi' and 'or-tools'.\")\n</code></pre>"},{"location":"references/toolbox/references_assortment_optimizer/#choice_learn.toolbox.assortment_optimizer.LatentClassPricingOptimizer.__new__","title":"<code>__new__(solver, class_weights, class_utilities, itemwise_values, assortment_size, outside_option_given=False)</code>","text":"<p>Create the AssortmentOptimizer object.</p> <p>Basically used to handle the choice of solver.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <p>Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.</p> required <code>class_weights</code> <p>List of weights for each latent class.</p> required <code>class_utilities</code> <p>List of utilities for each item of each latent class. Must have a shape of (n_classes, n_items)</p> required <code>itemwise_values</code> <p>List of to-be-optimized values for each item, e.g. prices.</p> required <code>assortment_size</code> <code>int</code> <p>maximum size of the requested assortment.</p> required <code>outside_option_given</code> <code>bool</code> <p>Whether the outside option is given or not (and thus is automatically added).</p> <code>False</code> Source code in <code>choice_learn/toolbox/assortment_optimizer.py</code> <pre><code>def __new__(\n    cls,\n    solver,\n    class_weights,\n    class_utilities,\n    itemwise_values,\n    assortment_size,\n    outside_option_given=False,\n):\n    \"\"\"Create the AssortmentOptimizer object.\n\n    Basically used to handle the choice of solver.\n\n    Parameters\n    ----------\n    solver: str\n        Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.\n    class_weights: Iterable\n        List of weights for each latent class.\n    class_utilities: Iterable\n        List of utilities for each item of each latent class.\n        Must have a shape of (n_classes, n_items)\n    itemwise_values: Iterable\n        List of to-be-optimized values for each item, e.g. prices.\n    assortment_size : int\n        maximum size of the requested assortment.\n    outside_option_given : bool\n        Whether the outside option is given or not (and thus is automatically added).\n    \"\"\"\n    if solver.lower() == \"gurobi\":\n        from .gurobi_opt import GurobiLatentClassPricingOptimizer\n\n        return GurobiLatentClassPricingOptimizer(\n            class_weights=class_weights,\n            class_utilities=class_utilities,\n            itemwise_values=itemwise_values,\n            assortment_size=assortment_size,\n            outside_option_given=outside_option_given,\n        )\n    if solver.lower() == \"or-tools\" or solver.lower() == \"ortools\":\n        from .or_tools_opt import ORToolsLatentClassPricingOptimizer\n\n        return ORToolsLatentClassPricingOptimizer(\n            class_weights=class_weights,\n            class_utilities=class_utilities,\n            itemwise_values=itemwise_values,\n            assortment_size=assortment_size,\n            outside_option_given=outside_option_given,\n        )\n\n    raise ValueError(\"Unknown solver. Please choose between 'gurobi' and 'or-tools'.\")\n</code></pre>"},{"location":"references/toolbox/references_assortment_optimizer/#choice_learn.toolbox.assortment_optimizer.MNLAssortmentOptimizer","title":"<code>MNLAssortmentOptimizer</code>","text":"<p>Base class for assortment optimization.</p> Source code in <code>choice_learn/toolbox/assortment_optimizer.py</code> <pre><code>class MNLAssortmentOptimizer:\n    \"\"\"Base class for assortment optimization.\"\"\"\n\n    def __new__(\n        cls, solver, utilities, itemwise_values, assortment_size, outside_option_given=False\n    ):\n        \"\"\"Create the AssortmentOptimizer object.\n\n        Basically used to handle the choice of solver.\n\n        Parameters\n        ----------\n        solver: str\n            Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.\n        utilities : Iterable\n            List of utilities for each item.\n        itemwise_values: Iterable\n            List of to-be-optimized values for each item, e.g. prices.\n        assortment_size : int\n            maximum size of the requested assortment.\n        outside_option_given : bool\n            Whether the outside option is given or not (and thus is automatically added).\n        \"\"\"\n        if solver.lower() == \"gurobi\":\n            from .gurobi_opt import GurobiMNLAssortmentOptimizer\n\n            return GurobiMNLAssortmentOptimizer(\n                utilities=utilities,\n                itemwise_values=itemwise_values,\n                assortment_size=assortment_size,\n                outside_option_given=outside_option_given,\n            )\n        if solver.lower() == \"or-tools\" or solver.lower() == \"ortools\":\n            from .or_tools_opt import ORToolsMNLAssortmentOptimizer\n\n            return ORToolsMNLAssortmentOptimizer(\n                utilities=utilities,\n                itemwise_values=itemwise_values,\n                assortment_size=assortment_size,\n                outside_option_given=outside_option_given,\n            )\n\n        raise ValueError(\"Unknown solver. Please choose between 'gurobi' and 'or-tools'.\")\n</code></pre>"},{"location":"references/toolbox/references_assortment_optimizer/#choice_learn.toolbox.assortment_optimizer.MNLAssortmentOptimizer.__new__","title":"<code>__new__(solver, utilities, itemwise_values, assortment_size, outside_option_given=False)</code>","text":"<p>Create the AssortmentOptimizer object.</p> <p>Basically used to handle the choice of solver.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <p>Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.</p> required <code>utilities</code> <code>Iterable</code> <p>List of utilities for each item.</p> required <code>itemwise_values</code> <p>List of to-be-optimized values for each item, e.g. prices.</p> required <code>assortment_size</code> <code>int</code> <p>maximum size of the requested assortment.</p> required <code>outside_option_given</code> <code>bool</code> <p>Whether the outside option is given or not (and thus is automatically added).</p> <code>False</code> Source code in <code>choice_learn/toolbox/assortment_optimizer.py</code> <pre><code>def __new__(\n    cls, solver, utilities, itemwise_values, assortment_size, outside_option_given=False\n):\n    \"\"\"Create the AssortmentOptimizer object.\n\n    Basically used to handle the choice of solver.\n\n    Parameters\n    ----------\n    solver: str\n        Name of the solver to be used. Currently only \"gurobi\" and \"or-tools\" is supported.\n    utilities : Iterable\n        List of utilities for each item.\n    itemwise_values: Iterable\n        List of to-be-optimized values for each item, e.g. prices.\n    assortment_size : int\n        maximum size of the requested assortment.\n    outside_option_given : bool\n        Whether the outside option is given or not (and thus is automatically added).\n    \"\"\"\n    if solver.lower() == \"gurobi\":\n        from .gurobi_opt import GurobiMNLAssortmentOptimizer\n\n        return GurobiMNLAssortmentOptimizer(\n            utilities=utilities,\n            itemwise_values=itemwise_values,\n            assortment_size=assortment_size,\n            outside_option_given=outside_option_given,\n        )\n    if solver.lower() == \"or-tools\" or solver.lower() == \"ortools\":\n        from .or_tools_opt import ORToolsMNLAssortmentOptimizer\n\n        return ORToolsMNLAssortmentOptimizer(\n            utilities=utilities,\n            itemwise_values=itemwise_values,\n            assortment_size=assortment_size,\n            outside_option_given=outside_option_given,\n        )\n\n    raise ValueError(\"Unknown solver. Please choose between 'gurobi' and 'or-tools'.\")\n</code></pre>"},{"location":"notebooks/models/simple_mnl/","title":"Simple MultiNomial Model","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\n\nfrom choice_learn.models.simple_mnl import SimpleMNL\nfrom choice_learn.data import ChoiceDataset\nfrom choice_learn.datasets.base import load_heating\n</code></pre> <p>Let's recreate this tutorial by Yves Croissant for the mlogit R package.</p> <p>It uses the Heating dataset, where we try to predict which heating hardware a houseold will chose available in choice_learn.datasets !</p> <pre><code>heating_df = load_heating(as_frame=True)\n\nshared_features_by_choice = [\"income\", \"agehed\", \"rooms\"]\nchoice = [\"depvar\"]\nitems_features_by_choice = [\"ic.\", \"oc.\"]\nitems = [\"hp\", \"gc\", \"gr\", \"ec\", \"er\"]\n\nchoices = np.array([items.index(val) for val in heating_df[choice].to_numpy().ravel()])\nshared_features_by_choice = heating_df[shared_features_by_choice].to_numpy().astype(\"float32\")\nitems_features_by_choice = np.stack([heating_df[[feat + item for feat in items_features_by_choice]].to_numpy() for item in items], axis=1)\n</code></pre> <p>First part estimates a simple MNL without intercept from the 'ic' and 'oc' features. By default, SimpleMNL does not integrate any intercept, but you can precise 'None'.</p> <pre><code>dataset = ChoiceDataset(items_features_by_choice=items_features_by_choice,\n                        choices=choices)\nmodel = SimpleMNL(intercept=None)\nhistory = model.fit(dataset, get_report=True, verbose=2)\n</code></pre> <pre><code>print(\"Estimation Negative LogLikelihood:\",\n      model.evaluate(dataset) * len(dataset))\n</code></pre> <pre><code>Estimation Negative LogLikelihood: tf.Tensor(1095.2418, shape=(), dtype=float32)\n</code></pre> <p>Model analysis and Comparison with R's mlogit package</p> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 Weights_items_features_0 -0.006232 0.000353 -17.665276 0.0 1 Weights_items_features_1 -0.004580 0.000322 -14.216597 0.0 <p>We reach very similar results. The second part is about modelling useing the ic + oc/0.12 ratio. Here is how it can be done:</p> <pre><code>ratio_items_features = []\nfor case in range(items_features_by_choice.shape[0]):\n    feat = []\n    for item in range(items_features_by_choice.shape[1]):\n        feat.append([items_features_by_choice[case, item, 0] + items_features_by_choice[case, item, 1] / 0.12])\n    ratio_items_features.append(feat)\nratio_contexts_items = np.array(ratio_items_features)\nratio_contexts_items.shape\n</code></pre> <pre><code>ratio_dataset = ChoiceDataset(items_features_by_choice=ratio_items_features, choices=choices)\nmodel = SimpleMNL()\nhistory = model.fit(ratio_dataset, get_report=False)\n</code></pre> <pre><code>print(\"Weights:\", model.trainable_weights)\nprint(\"Estimation Negative LogLikelihood:\", model.evaluate(ratio_dataset) * len(ratio_dataset))\n</code></pre> <pre><code>Weights: [&lt;tf.Variable 'Weights_items_features:0' shape=(1,) dtype=float32, numpy=array([-0.00071585], dtype=float32)&gt;]\nEstimation Negative LogLikelihood: tf.Tensor(1248.7051, shape=(), dtype=float32)\n</code></pre> <p>Finally, to add itemwise intercept for the last part, here is how it can be done:</p> <pre><code>model = SimpleMNL(intercept=\"item\")\nhistory = model.fit(dataset, get_report=True)\n</code></pre> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 Weights_items_features_0 -0.001533 0.000621 -2.469422 1.353315e-02 1 Weights_items_features_1 -0.006996 0.001554 -4.501969 6.732662e-06 2 Intercept_0 1.710969 0.226742 7.545903 4.485301e-14 3 Intercept_1 0.308263 0.206591 1.492140 1.356624e-01 4 Intercept_2 1.658846 0.448416 3.699345 2.161564e-04 5 Intercept_3 1.853437 0.361952 5.120667 3.044562e-07 <pre><code>\n</code></pre>"},{"location":"notebooks/models/learning_mnl/","title":"The Learning-MNL model","text":"<p>In this notebook we use choice-learn implementation of the L-MNL model (from the paper Enhance Discrete Choice Models with Representation Learning) to obtain the same results as presented by the authors.</p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>from choice_learn.datasets import load_swissmetro\nfrom choice_learn.data import ChoiceDataset\n</code></pre> <pre><code>test_df = pd.read_csv(\"https://raw.githubusercontent.com/BSifringer/EnhancedDCM/refs/heads/master/ready_example/swissmetro_paper/swissmetro_test.dat\", sep=\"\\t\")\ntrain_df = pd.read_csv(\"https://raw.githubusercontent.com/BSifringer/EnhancedDCM/refs/heads/master/ready_example/swissmetro_paper/swissmetro_train.dat\", sep=\"\\t\")\n</code></pre> <pre><code>test_df.head()\n</code></pre> <pre><code>train_df.head()\n</code></pre> <pre><code># Preprocessing the dataset\n\ntest_df = test_df.loc[test_df.CAR_AV == 1]\ntest_df = test_df.loc[test_df.SM_AV == 1]\ntest_df = test_df.loc[test_df.TRAIN_AV == 1]\n\ntrain_df = train_df.loc[train_df.CAR_AV == 1]\ntrain_df = train_df.loc[train_df.SM_AV == 1]\ntrain_df = train_df.loc[train_df.TRAIN_AV == 1]\n</code></pre> <pre><code># Normalizing values by 100\ntrain_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] = (\n    train_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] / 100.0\n)\n\ntrain_df[[\"TRAIN_HE\", \"SM_HE\"]] = (\n    train_df[[\"TRAIN_HE\", \"SM_HE\"]] / 100.0\n)\n\ntrain_df[\"train_free_ticket\"] = train_df.apply(\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\n)\ntrain_df[\"sm_free_ticket\"] = train_df.apply(\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\n)\n\ntrain_df[\"TRAIN_travel_cost\"] = train_df.apply(\n    lambda row: (row[\"TRAIN_CO\"] * (1 - row[\"train_free_ticket\"])) / 100, axis=1\n)\ntrain_df[\"SM_travel_cost\"] = train_df.apply(\n    lambda row: (row[\"SM_CO\"] * (1 - row[\"sm_free_ticket\"])) / 100, axis=1\n)\ntrain_df[\"CAR_travel_cost\"] = train_df.apply(lambda row: row[\"CAR_CO\"] / 100, axis=1)\n\n\n# Normalizing values by 100\ntest_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] = (\n    test_df[[\"TRAIN_TT\", \"SM_TT\", \"CAR_TT\"]] / 100.0\n)\n\ntest_df[[\"TRAIN_HE\", \"SM_HE\"]] = (\n    test_df[[\"TRAIN_HE\", \"SM_HE\"]] / 100.0\n)\n\ntest_df[\"train_free_ticket\"] = test_df.apply(\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\n)\ntest_df[\"sm_free_ticket\"] = test_df.apply(\n    lambda row: (row[\"GA\"] == 1).astype(int), axis=1\n)\n\ntest_df[\"TRAIN_travel_cost\"] = test_df.apply(\n    lambda row: (row[\"TRAIN_CO\"] * (1 - row[\"train_free_ticket\"])) / 100, axis=1\n)\ntest_df[\"SM_travel_cost\"] = test_df.apply(\n    lambda row: (row[\"SM_CO\"] * (1 - row[\"sm_free_ticket\"])) / 100, axis=1\n)\ntest_df[\"CAR_travel_cost\"] = test_df.apply(lambda row: row[\"CAR_CO\"] / 100, axis=1)\n</code></pre> <pre><code>swiss_df.SM_SEATS = swiss_df.SM_SEATS.astype(\"float32\")\ntrain_df.SM_SEATS = train_df.SM_SEATS.astype(\"float32\")\ntest_df.SM_SEATS = test_df.SM_SEATS.astype(\"float32\")\n\ntrain_df.CHOICE = train_df.CHOICE - 1\ntest_df.CHOICE = test_df.CHOICE - 1\n</code></pre> <pre><code>dataset = ChoiceDataset.from_single_wide_df(df=swiss_df, choices_column=\"CHOICE\", items_id=[\"TRAIN\", \"SM\", \"CAR\"],\nshared_features_columns=[\"GA\", \"AGE\", \"SM_SEATS\", \"LUGGAGE\", \"SM_SEATS\", 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'],\nitems_features_suffixes=[\"TT\", \"travel_cost\", \"HE\"], choice_format=\"items_index\")\n\ntrain_dataset = ChoiceDataset.from_single_wide_df(df=train_df, choices_column=\"CHOICE\", items_id=[\"TRAIN\", \"SM\", \"CAR\"],\nshared_features_columns=[\"GA\", \"AGE\", \"SM_SEATS\", \"LUGGAGE\", \"SM_SEATS\", 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'],\nitems_features_suffixes=[\"TT\", \"travel_cost\", \"HE\"], choice_format=\"items_index\")\n\ntest_dataset = ChoiceDataset.from_single_wide_df(df=test_df, choices_column=\"CHOICE\", items_id=[\"TRAIN\", \"SM\", \"CAR\"],\nshared_features_columns=[\"GA\", \"AGE\", \"SM_SEATS\", \"LUGGAGE\", \"SM_SEATS\", 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'],\nitems_features_suffixes=[\"TT\", \"travel_cost\", \"HE\"], choice_format=\"items_index\")\n</code></pre> <pre><code>print(len(train_dataset), len(test_dataset))\n</code></pre> <pre><code>from choice_learn.models import ConditionalLogit\n\nclogit = ConditionalLogit()\nclogit.add_shared_coefficient(feature_name=\"TT\", coefficient_name=\"beta_time\",  items_indexes=[0, 1, 2])\nclogit.add_shared_coefficient(feature_name=\"travel_cost\", coefficient_name=\"beta_cost\", items_indexes=[0, 1, 2])\nclogit.add_shared_coefficient(feature_name=\"HE\",  coefficient_name=\"beta_freq\",items_indexes=[0, 1])\nclogit.add_shared_coefficient(feature_name=\"GA\",  coefficient_name=\"beta_GA\",items_indexes=[0, 1])\nclogit.add_shared_coefficient(feature_name=\"AGE\", coefficient_name=\"beta_age\", items_indexes=[0])\nclogit.add_shared_coefficient(feature_name=\"LUGGAGE\", coefficient_name=\"beta_luggage\", items_indexes=[2])\nclogit.add_shared_coefficient(feature_name=\"SM_SEATS\", coefficient_name=\"beta_seats\", items_indexes=[1])\nclogit.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2])\n\nclogit.fit(train_dataset)\n</code></pre> <pre><code>clogit.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_time:0' shape=(1, 1) dtype=float32, numpy=array([[-1.2917504]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_cost:0' shape=(1, 1) dtype=float32, numpy=array([[-0.69039553]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_freq:0' shape=(1, 1) dtype=float32, numpy=array([[-0.7038978]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_GA:0' shape=(1, 1) dtype=float32, numpy=array([[1.5400317]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_age:0' shape=(1, 1) dtype=float32, numpy=array([[0.17458557]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_luggage:0' shape=(1, 1) dtype=float32, numpy=array([[-0.11316068]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_seats:0' shape=(1, 1) dtype=float32, numpy=array([[0.21595138]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[1.1861055, 1.203332 ]], dtype=float32)&gt;]\n</code></pre> <pre><code>clogit.evaluate(train_dataset) * len(train_dataset), clogit.evaluate(test_dataset) * len(test_dataset)\n</code></pre> <pre><code>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=5766.751&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=1433.0833&gt;)\n</code></pre> <pre><code>from choice_learn.models.learning_mnl import LearningMNL\n\nswiss_model = LearningMNL(optimizer=\"Adam\", lr=0.005,\nnn_features=['PURPOSE', 'FIRST', 'TICKET', 'WHO', 'MALE', 'INCOME', 'ORIGIN', 'DEST'], nn_layers_widths=[200], epochs=200, batch_size=32)\nswiss_model.add_shared_coefficient(feature_name=\"TT\", items_indexes=[0, 1, 2])\nswiss_model.add_shared_coefficient(feature_name=\"travel_cost\", items_indexes=[0, 1, 2])\nswiss_model.add_shared_coefficient(feature_name=\"HE\", items_indexes=[0, 1])\nswiss_model.add_shared_coefficient(feature_name=\"GA\", items_indexes=[0, 1])\nswiss_model.add_shared_coefficient(feature_name=\"AGE\", items_indexes=[0])\nswiss_model.add_shared_coefficient(feature_name=\"LUGGAGE\", items_indexes=[2])\nswiss_model.add_shared_coefficient(feature_name=\"SM_SEATS\", items_indexes=[1])\nswiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2])\n\nhist = swiss_model.fit(train_dataset, val_dataset=test_dataset, verbose=1)\n</code></pre> <pre><code>swiss_model.assign_lr(0.001)\nhist2 = swiss_model.fit(train_dataset, val_dataset=test_dataset, verbose=1)\n</code></pre> <pre><code> swiss_model.evaluate(train_dataset, batch_size=32) * 7234\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=4485.126&gt;\n</code></pre> <pre><code>swiss_model.evaluate(test_dataset, batch_size=32) * 1802\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=1165.2987&gt;\n</code></pre> <pre><code># Estimated parameters:\nswiss_model.trainable_weights[:8]\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_TT:0' shape=(1, 1) dtype=float32, numpy=array([[-1.6142633]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_travel_cost:0' shape=(1, 1) dtype=float32, numpy=array([[-1.536603]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_HE:0' shape=(1, 1) dtype=float32, numpy=array([[-0.9000557]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_GA:0' shape=(1, 1) dtype=float32, numpy=array([[0.7521398]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_AGE:0' shape=(1, 1) dtype=float32, numpy=array([[0.37502143]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_LUGGAGE:0' shape=(1, 1) dtype=float32, numpy=array([[0.17362104]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_SM_SEATS:0' shape=(1, 1) dtype=float32, numpy=array([[0.01463094]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[0.38485298, 0.13572218]], dtype=float32)&gt;]\n</code></pre> <p>Results are very similar to the one presented in the paper (see Table 7).</p> <pre><code>\n</code></pre>"},{"location":"notebooks/models/halo_mnl/","title":"Simple Halo-MNL Model","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\n\nfrom choice_learn.models.halo_mnl import LowRankHaloMNL, HaloMNL\nfrom choice_learn.data import ChoiceDataset\nfrom choice_learn.datasets.base import load_heating\n</code></pre> <pre><code>heating_df = load_heating(as_frame=True)\n\nshared_features_by_choice = [\"income\", \"agehed\", \"rooms\"]\nchoice = [\"depvar\"]\nitems_features_by_choice = [\"ic.\", \"oc.\"]\nitems = [\"hp\", \"gc\", \"gr\", \"ec\", \"er\"]\n\nchoices = np.array([items.index(val) for val in heating_df[choice].to_numpy().ravel()])\nshared_features_by_choice = heating_df[shared_features_by_choice].to_numpy().astype(\"float32\")\nitems_features_by_choice = np.stack([heating_df[[feat + item for feat in items_features_by_choice]].to_numpy() for item in items], axis=1)\n</code></pre> <pre><code>dataset = ChoiceDataset(items_features_by_choice=items_features_by_choice,\n                        choices=choices)\nmodel = LowRankHaloMNL(halo_latent_dim=2, intercept=None)\nhistory = model.fit(dataset, verbose=2, get_report=True)\n</code></pre> <pre><code>model = HaloMNL(intercept=\"item\", optimizer=\"lbfgs\")\nhistory = model.fit(dataset, verbose=0, get_report=True)\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport tensorflow as tf\n\nplt.imshow(tf.linalg.set_diag(model.trainable_weights[-1], model.zero_diag))\nplt.title(\"Estimated Halo Matrix\")\nplt.xticks([0., 1., 2., 3., 4.], [\"hp\", \"gc\", \"gr\", \"ec\", \"er\"])\nplt.yticks([0., 1., 2., 3., 4.], [\"hp\", \"gc\", \"gr\", \"ec\", \"er\"])\n\nplt.show()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/models/nested_logit/","title":"The Nested Logit Model","text":"<p>The Nested Logit model considers sub-groups of alternatives totally substitutables, called 'nests'. The general idea is that a customer might choose its transportation mode between publics transport and its private car. And then, if he decides to use public transportations the customer chooses between taking the train or the bus.\\ The classical Conditional Logit does not account for such decision process. Hence the introduction of the Nested Logit. More detailed information are available here.</p> <p>In this notebook we reproduce results from other packages showing how to speficy a Nested Logit model with Choice-Learn and that we reach the right results.</p>"},{"location":"notebooks/models/nested_logit/#summary","title":"Summary","text":"<ul> <li>[1][Nested Logit on the SwissMetro dataset](#1--nested-logit-on-the-swissmetro-dataset)<ul> <li>Short Introduction to the Nested Logit model</li> <li>Specification and estimation with Choice-Learn</li> </ul> </li> <li>[2][Nested Logit on the HC dataset](#2--nested-logit-with-the-hc-dataset)<ul> <li>First formulation</li> <li>Second formulation</li> <li>Third formulation</li> <li>Fourth formulation</li> </ul> </li> </ul> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n\nimport numpy as np\nimport pandas as pd\n</code></pre>"},{"location":"notebooks/models/nested_logit/#import-the-nested-logit-from-choice-learn","title":"Import the Nested Logit from Choice-Learn !","text":"<pre><code>from choice_learn.models import NestedLogit\n</code></pre>"},{"location":"notebooks/models/nested_logit/#1-nested-logit-on-the-swissmetro-dataset","title":"1- Nested Logit on the SwissMetro dataset","text":"<p>We reproduce the results from Biogeme that is also reproduced in PyLogit.\\ This example uses the SwissMetro dataset further described in the data introduction.</p> <pre><code>from choice_learn.datasets import load_swissmetro\nswiss_dataset = load_swissmetro(preprocessing=\"biogeme_nested\")\nprint(swiss_dataset.summary())\n</code></pre> <pre><code>%=====================================================================%\n%%% Summary of the dataset:\n%=====================================================================%\nNumber of items: 3\nNumber of choices: 6768\n%=====================================================================%\n No Shared Features by Choice registered\n\n\n Items Features by Choice:\n 2 items features \n with names: (['cost', 'travel_time'],)\n%=====================================================================%\n</code></pre>"},{"location":"notebooks/models/nested_logit/#short-introduction-to-nested-logit","title":"Short Introduction to Nested Logit","text":"<p>The model specified in Biogeme defines two nests: - The existing modes nest with the train and car (items indexes of 0 and 2) - The future modes nest with the swissmetro (item index of 1)</p> <p>And the utility form is the following:\\</p> <p>\u00a0 \u00a0 \u00a0 $U(i) = \\beta^{inter}_i + \\beta^{tt} \\cdot TT(i) + \\beta^{co} \\cdot CO(i)$\\ with:</p> <ul> <li>$TT(i)$ the travel time of alternative $i$</li> <li>$CO(i)$ the cost of alternative $i$</li> <li>$\\beta^{inter}_{sm} = 0$</li> </ul> <p>The Nested Logit formulates the following probabilities from such utility:</p> <p> </p> <p>To better understand this expression, one way is to split it into two parts:</p> <p> </p> <p>with:\\ $ \\mathbb{P}(i | nest(i)) = \\frac{e^{\\frac{U(i)}{\\gamma_{nest(i)}}} }{\\sum_{k \\in nest(i)} e^{\\frac{U(k)}{\\gamma_{nest(i)}}}} $ \u00a0 and \u00a0</p> <p>$ \\mathbb{P}(nest(i))= \\frac{\\left( \\sum_{k \\in nest(i)} e^{\\frac{U(k)}{\\gamma_{nest(i)}}} \\right)^{\\gamma_{nest(i)}}}{\\sum_{m \\in \\mathcal{Nests}} \\left( \\sum_{k \\in m} e^{\\frac{U(k)}{\\gamma_{m}}} \\right)^{\\gamma_m}} $</p> <p>Therefore we have 4 weights in the utility function and the $\\gamma_{nest}$ values to estimate. The 'new' nest containing only one alternative, its correlation value $\\gamma^{new}$ has no impact, we only need to estimate $\\gamma^{old}$.</p>"},{"location":"notebooks/models/nested_logit/#specification-and-estimation-with-choice-learn","title":"Specification and estimation with Choice-Learn","text":"<p>With Choice-Learn, the Nested Logit model specification is similar to the Conditional Logit specification. The few differences are: - When the model is instantiated, the nested need to be specified as a list of nests with the concerned items indexes. In the example, we specify <code>items_nests=[[0, 2], [1]]</code> saying that first nest contains the items of indexes 0 (train) and 2 (car) and the second nest the item of index 1 (swiss metro). - The \"fast\" dict-base specifications has another alternative with <code>coefficients={feature_name: \"nest\"}</code> creating for the feature feature_name one coefficient to estimate by nest, this coefficient being shared by all alternatives of the nest.</p> <pre><code># Initialization of the model\nswiss_model = NestedLogit(optimizer=\"lbfgs\", items_nests=[[0, 2], [1]])\n\n# Intercept for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 2])\n\n# betas TT and CO shared by train and sm\nswiss_model.add_shared_coefficient(feature_name=\"travel_time\",\n                                   items_indexes=[0, 1, 2])\nswiss_model.add_shared_coefficient(feature_name=\"cost\",\n                                   items_indexes=[0, 1, 2])\n</code></pre> <pre><code># Estimation of the model\nhistory = swiss_model.fit(swiss_dataset, get_report=True, verbose=2)\n</code></pre> <pre><code>WARNING:tensorflow:AutoGraph could not transform &lt;bound method Socket.send of &lt;zmq.Socket(zmq.PUSH) at 0x7f46435341a0&gt;&gt; and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n\n\nWARNING:root:At least one gamma value for nests is below 0.05 and is\n        clipped to 0.05 for numeric optimization purposes.\nWARNING:tensorflow:AutoGraph could not transform &lt;bound method Socket.send of &lt;zmq.Socket(zmq.PUSH) at 0x7f46435341a0&gt;&gt; and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n\n\nWARNING: AutoGraph could not transform &lt;bound method Socket.send of &lt;zmq.Socket(zmq.PUSH) at 0x7f46435341a0&gt;&gt; and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n\n\nWARNING:root:L-BFGS Opimization finished:\nWARNING:root:---------------------------------------------------------------\nWARNING:root:Number of iterations: 21\nWARNING:root:Algorithm converged before reaching max iterations: True\n\n\nUsing L-BFGS optimizer, setting up .fit() function\n</code></pre> <pre><code>swiss_model.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[-0.51194817, -0.1671558 ]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_travel_time:0' shape=(1, 1) dtype=float32, numpy=array([[-0.89866394]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_cost:0' shape=(1, 1) dtype=float32, numpy=array([[-0.85666543]], dtype=float32)&gt;,\n &lt;tf.Variable 'gammas_nests:0' shape=(1, 1) dtype=float32, numpy=array([[0.4868395]], dtype=float32)&gt;]\n</code></pre> <pre><code>swiss_model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 beta_intercept_0 -0.511948 0.046159 -11.090872 0.000000 1 beta_intercept_1 -0.167156 0.036682 -4.556830 0.000005 2 beta_travel_time -0.898664 0.054548 -16.474657 0.000000 3 beta_cost -0.856665 0.046482 -18.430079 0.000000 4 gammas_nests 0.486840 0.029635 16.427719 0.000000 <pre><code># Looking at the weights\nswiss_model.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[-0.51194817, -0.1671558 ]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_travel_time:0' shape=(1, 1) dtype=float32, numpy=array([[-0.89866394]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_cost:0' shape=(1, 1) dtype=float32, numpy=array([[-0.85666543]], dtype=float32)&gt;,\n &lt;tf.Variable 'gammas_nests:0' shape=(1, 1) dtype=float32, numpy=array([[0.4868395]], dtype=float32)&gt;]\n</code></pre> <pre><code># Estimating the total summed Negative Log-Likelihood\nswiss_model.evaluate(swiss_dataset) * len(swiss_dataset)\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=5236.8994&gt;\n</code></pre> <pre><code># Probabilities can be easily computed:\nprobas = swiss_model.predict_probas(swiss_dataset)\nprint(probas[:4])\n</code></pre> <pre><code>tf.Tensor(\n[[0.15937711 0.6218435  0.21877931]\n [0.1940201  0.6445151  0.16146483]\n [0.11813082 0.597691   0.28417817]\n [0.12110618 0.52606976 0.3528241 ]], shape=(4, 3), dtype=float32)\n</code></pre>"},{"location":"notebooks/models/nested_logit/#interpretation-and-comparison-with-biogeme-results","title":"Interpretation and comparison with Biogeme results","text":""},{"location":"notebooks/models/nested_logit/#2-nested-logit-with-the-hc-dataset","title":"2- Nested Logit with the HC Dataset","text":"<p>We reproduce results from mlogit that are also presented in Torch-Choice.</p>"},{"location":"notebooks/models/nested_logit/#first-formulation","title":"First Formulation","text":"<pre><code>from choice_learn.datasets import load_hc\nfrom choice_learn.data import ChoiceDataset\n\n# Loading\nhc_df = load_hc(as_frame=True)\n\n# HC dataset is loaded as a pandas.DataFrame for the example.\n# It can be downloaded as a ChoiceDataset with the argument `as_frame=False`\n</code></pre> <pre><code>hc_df.head()\n</code></pre> rownames depvar ich.gcc ich.ecc ich.erc ich.hpc ich.gc ich.ec ich.er icca och.gcc och.ecc och.erc och.hpc och.gc och.ec och.er occa income 0 1 erc 9.70 7.86 8.79 11.36 24.08 24.50 7.37 27.28 2.26 4.09 3.85 1.73 2.26 4.09 3.85 2.95 20.0 1 2 hpc 8.77 8.69 7.09 9.37 28.00 32.71 9.33 26.49 2.30 2.69 3.45 1.65 2.30 2.69 3.45 1.63 50.0 2 3 gcc 7.43 8.86 6.94 11.70 25.71 31.68 8.14 22.63 2.28 5.25 4.35 1.44 2.28 5.25 4.35 2.18 50.0 3 4 gcc 9.18 8.93 7.22 12.13 29.72 26.73 8.04 25.33 2.62 4.89 4.85 1.93 2.62 4.89 4.85 2.70 50.0 4 5 gcc 8.05 7.02 8.44 10.51 23.90 28.35 7.15 25.45 2.52 3.71 3.64 1.63 2.52 3.71 3.64 2.77 60.0 <p>It is possible to pre-process the dataset like in the examples to 'easily' specify the Nested Logit model:</p> <pre><code>items_id = [\"gcc\", \"ecc\", \"erc\", \"hpc\", \"gc\", \"ec\", \"er\"]\ncooling_modes = [\"gcc\", \"ecc\", \"erc\", \"hpc\"]\nroom_modes = [\"erc\", \"er\"]\nnon_cooling_modes = [\"gc\", \"ec\", \"er\"]\n\nfor mode in items_id:\n    if mode in cooling_modes:\n        hc_df[f\"icca.{mode}\"] = hc_df[\"icca\"]\n        hc_df[f\"occa.{mode}\"] = hc_df[\"occa\"]\n    else:\n        hc_df[f\"icca.{mode}\"] = 0.\n        hc_df[f\"occa.{mode}\"] = 0.\n\nfor item in items_id:\n    if item in cooling_modes:\n        hc_df[f\"int_cooling.{item}\"] = 1.\n        hc_df[f\"inc_cooling.{item}\"] = hc_df.income\n    else:\n        hc_df[f\"int_cooling.{item}\"] = 0.\n        hc_df[f\"inc_cooling.{item}\"] = 0.\n    if item in room_modes:\n        hc_df[f\"inc_room.{item}\"] = hc_df.income\n    else:\n        hc_df[f\"inc_room.{item}\"] = 0\n</code></pre> <pre><code># Creating the dataset from this preprocessed dataframe\ndataset = ChoiceDataset.from_single_wide_df(df=hc_df,\n                                            items_features_prefixes=[\"ich\", \"och\", \"occa\", \"icca\",\n                                                                     \"int_cooling\", \"inc_cooling\",\n                                                                     \"inc_room\"],\n                                            delimiter=\".\",\n                                            items_id=items_id,\n                                            choices_column=\"depvar\",\n                                            choice_format=\"items_id\")\n</code></pre> <p>We can use the fast specification using a dictionnary with the 'constant' keyword.</p> <pre><code>spec = {\n    \"ich\": \"constant\",\n    \"och\": \"constant\",\n    \"occa\": \"constant\",\n    \"icca\": \"constant\",\n    \"int_cooling\":\"constant\",\n    \"inc_cooling\": \"constant\",\n    \"inc_room\": \"constant\"\n}\nmodel = NestedLogit(\n    coefficients=spec,\n    items_nests=[[0, 1, 2, 3], [4, 5, 6]],\n    optimizer=\"lbfgs\",\n    shared_gammas_over_nests=True # Note the argument specifying that all nests have the same gamma value\n)\n</code></pre> <pre><code>Using L-BFGS optimizer, setting up .fit() function\n</code></pre> <pre><code>hist = model.fit(dataset, get_report=True, verbose=1)\n</code></pre> <pre><code>WARNING:root:At least one gamma value for nests is below 0.05 and is\n        clipped to 0.05 for numeric optimization purposes.\nWARNING:root:L-BFGS Opimization finished:\nWARNING:root:---------------------------------------------------------------\nWARNING:root:Number of iterations: 55\nWARNING:root:Algorithm converged before reaching max iterations: True\n\n\nUsing L-BFGS optimizer, setting up .fit() function\n</code></pre> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 ich_w_0 -0.554876 0.174471 -3.180328 0.001471 1 och_w_1 -0.857881 0.300332 -2.856437 0.004284 2 occa_w_2 -1.089362 1.056226 -1.031371 0.302367 3 icca_w_3 -0.225069 0.112268 -2.004749 0.044990 4 int_cooling_w_4 -6.000777 4.986898 -1.203308 0.228857 5 inc_cooling_w_5 0.249571 0.053589 4.657146 0.000003 6 inc_room_w_6 -0.378969 0.116035 -3.265980 0.001091 7 gamma_nests 0.585920 0.242312 2.418043 0.015604 <p>Another possibility is to keep the dataset as is and specify manually the model:</p> <pre><code># Creating the dataset\ndataset = ChoiceDataset.from_single_wide_df(df=hc_df,\n                                            shared_features_columns=[\"income\"],\n                                            items_features_prefixes=[\"ich\", \"och\", \"occa\", \"icca\"],\n                                            delimiter=\".\",\n                                            items_id=items_id,\n                                            choices_column=\"depvar\",\n                                            choice_format=\"items_id\")\n</code></pre> <p>Using the manual specification we define each weight and the indexes of the concerned items.</p> <pre><code>model = NestedLogit(items_nests=[[0, 1, 2, 3], [4, 5, 6]],\n                    optimizer=\"lbfgs\",\n                    shared_gammas_over_nests=True)\n# Coefficients that are for all the alternatives\nmodel.add_shared_coefficient(feature_name=\"ich\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"och\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"icca\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"occa\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\n\n# The coefficients concerning the income are split into two groups of alternatives:\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[0, 1, 2, 3], coefficient_name=\"income_cooling\")\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[2, 6], coefficient_name=\"income_room\")\n\n# Finally only one nest has an intercept\nmodel.add_shared_coefficient(feature_name=\"intercept\", items_indexes=[0, 1, 2, 3])\n</code></pre> <pre><code>Using L-BFGS optimizer, setting up .fit() function\n</code></pre> <pre><code>hist = model.fit(dataset, get_report=True, verbose=1)\n</code></pre> <pre><code>WARNING:root:At least one gamma value for nests is below 0.05 and is\n        clipped to 0.05 for numeric optimization purposes.\nWARNING:root:L-BFGS Opimization finished:\nWARNING:root:---------------------------------------------------------------\nWARNING:root:Number of iterations: 39\nWARNING:root:Algorithm converged before reaching max iterations: True\n\n\nUsing L-BFGS optimizer, setting up .fit() function\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7f459f242b60&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7f459f242b60&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7f459f16e520&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7f459f16e520&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</code></pre> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 beta_ich -0.554878 0.174468 -3.180405 0.001471 1 beta_och -0.857885 0.300328 -2.856494 0.004283 2 beta_icca -0.225067 0.112267 -2.004751 0.044990 3 beta_occa -1.089337 1.056223 -1.031352 0.302376 4 income_cooling 0.249571 0.053588 4.657206 0.000003 5 income_room -0.378970 0.116032 -3.266064 0.001091 6 beta_intercept -6.000929 4.986704 -1.203386 0.228827 7 gamma_nests 0.585921 0.242307 2.418097 0.015602 <pre><code>model.trainable_weights\n</code></pre> <pre><code># The gamma value can be retrieved to compute the correlation as follow:\ncorrelation = 1 -model.trainable_weights[-1][0][0].numpy()\nprint(\"Correlation over alternatives within each nest:\", correlation)\n</code></pre>"},{"location":"notebooks/models/nested_logit/#second-formulation","title":"Second Formulation","text":"<pre><code>model = NestedLogit(items_nests=[[0, 1, 3, 4, 5], [2, 6]],\n                    optimizer=\"lbfgs\",\n                    shared_gammas_over_nests=True)\n# Coefficients that are for all the alternatives\nmodel.add_shared_coefficient(feature_name=\"ich\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"och\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"icca\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"occa\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\n\n# The coefficients concerning the income are split into two groups of alternatives:\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[0, 1, 2, 3],\n                             coefficient_name=\"income_cooling\")\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[2, 6],\n                             coefficient_name=\"income_room\")\n\n# Finally only one nest has an intercept\nmodel.add_shared_coefficient(feature_name=\"intercept\", items_indexes=[0, 1, 2, 3],\ncoefficient_name=\"int.cooling\")\n</code></pre> <pre><code>hist = model.fit(dataset, get_report=True, verbose=2)\n</code></pre> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 beta_ich -1.106775 1.271377 -0.870533 0.384009 1 beta_och -1.774187 2.188208 -0.810794 0.417484 2 beta_icca -0.329113 0.308635 -1.066349 0.286266 3 beta_occa -1.990666 2.507875 -0.793766 0.427332 4 income_cooling 0.405710 0.425043 0.954515 0.339823 5 income_room -0.737662 0.742359 -0.993673 0.320382 6 int.cooling -13.468433 18.365850 -0.733341 0.463350 7 gamma_nests 1.323886 1.889999 0.700469 0.483634 <pre><code># NLL:\nmodel.evaluate(dataset) * len(dataset)\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=180.03148&gt;\n</code></pre>"},{"location":"notebooks/models/nested_logit/#third-formulation","title":"Third formulation","text":"<pre><code>model = NestedLogit(items_nests=[[0, 1, 2, 3], [4, 5, 6]],\n                    optimizer=\"lbfgs\",\n                    shared_gammas_over_nests=False)\n# Coefficients that are for all the alternatives\nmodel.add_shared_coefficient(feature_name=\"ich\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"och\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"icca\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"occa\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\n\n# The coefficients concerning the income are split into two groups of alternatives:\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[0, 1, 2, 3], coefficient_name=\"income_cooling\")\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[2, 6], coefficient_name=\"income_room\")\n\n# Finally only one nest has an intercept\nmodel.add_shared_coefficient(feature_name=\"intercept\", items_indexes=[0, 1, 2, 3])\n</code></pre> <pre><code>hist = model.fit(dataset, get_report=False, verbose=2)\n</code></pre> <pre><code>model.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_ich:0' shape=(1, 1) dtype=float32, numpy=array([[-0.5336982]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_och:0' shape=(1, 1) dtype=float32, numpy=array([[-0.83580786]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_icca:0' shape=(1, 1) dtype=float32, numpy=array([[-0.22605541]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_occa:0' shape=(1, 1) dtype=float32, numpy=array([[-1.1159254]], dtype=float32)&gt;,\n &lt;tf.Variable 'income_cooling:0' shape=(1, 1) dtype=float32, numpy=array([[0.24864283]], dtype=float32)&gt;,\n &lt;tf.Variable 'income_room:0' shape=(1, 1) dtype=float32, numpy=array([[-0.36347896]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_intercept:0' shape=(1, 1) dtype=float32, numpy=array([[-5.644782]], dtype=float32)&gt;,\n &lt;tf.Variable 'gammas_nests:0' shape=(1, 2) dtype=float32, numpy=array([[0.57864326, 0.42460972]], dtype=float32)&gt;]\n</code></pre>"},{"location":"notebooks/models/nested_logit/#fourth-formulation","title":"Fourth Formulation","text":"<pre><code>model = NestedLogit(items_nests=[[0, 1, 2], [3], [4, 5, 6]],\n                    optimizer=\"lbfgs\",\n                    shared_gammas_over_nests=True)\n# Coefficients that are for all the alternatives\nmodel.add_shared_coefficient(feature_name=\"ich\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"och\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"icca\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\nmodel.add_shared_coefficient(feature_name=\"occa\", items_indexes=[0, 1, 2, 3, 4, 5, 6])\n\n# The coefficients concerning the income are split into two groups of alternatives:\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[0, 1, 2, 3], coefficient_name=\"income_cooling\")\nmodel.add_shared_coefficient(feature_name=\"income\", items_indexes=[2, 6], coefficient_name=\"income_room\")\n\n# Finally only one nest has an intercept\nmodel.add_shared_coefficient(feature_name=\"intercept\", items_indexes=[0, 1, 2, 3])\n</code></pre> <pre><code>hist = model.fit(dataset, get_report=True, verbose=2)\n</code></pre> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 beta_ich -0.838395 0.119850 -6.995352 0.000000e+00 1 beta_och -1.331599 0.251385 -5.297053 1.192093e-07 2 beta_icca -0.256129 0.127123 -2.014806 4.392505e-02 3 beta_occa -1.405654 1.152076 -1.220105 2.224251e-01 4 income_cooling 0.311357 0.055357 5.624510 0.000000e+00 5 income_room -0.571352 0.082983 -6.885149 0.000000e+00 6 beta_intercept -10.413467 5.268852 -1.976421 4.810715e-02 7 gamma_nests 0.956544 0.430562 2.221618 2.630913e-02 <pre><code>\n</code></pre>"},{"location":"notebooks/models/reslogit/","title":"ResLogit model Usage","text":""},{"location":"notebooks/models/reslogit/#introduction-to-modelling-with-reslogit","title":"Introduction to modelling with ResLogit","text":"<p>We use the Swissmetro dataset to demonstrate how to use the ResLogit model [1]. </p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove/Add GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nimport timeit\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom choice_learn.data import ChoiceDataset\nfrom choice_learn.models import ResLogit\nfrom choice_learn.datasets import load_swissmetro\n</code></pre> <p>First, we create a ChoiceDataset from the dataframe.</p> <pre><code>dataset = load_swissmetro(as_frame=False)\n</code></pre> <pre><code>dataset.summary()\nprint(f\"\\n\\n{type(dataset)=}\")\nprint(f\"\\n{np.shape(dataset.items_features_by_choice)=}\")\nprint(f\"{np.shape(dataset.shared_features_by_choice)=}\")\n</code></pre> <pre><code>n_items = np.shape(dataset.items_features_by_choice)[2]\nn_items_features = np.shape(dataset.items_features_by_choice)[3]\nn_shared_features = np.shape(dataset.shared_features_by_choice)[2]\nn_vars = n_items_features + n_shared_features\nn_choices = len(np.unique(dataset.choices))\n\nprint(f\"{n_items=}\\n{n_items_features=}\\n{n_shared_features=}\\n{n_vars, n_choices=}\\n\\n\")\n</code></pre> <p>We split the dataset into train and test subsets.</p> <pre><code>n_samples = len(dataset.choices)\n# Slicing index for train and valid split\nslice = np.floor(0.7 * n_samples).astype(int)\ntrain_indexes = np.arange(0, slice)\ntest_indexes = np.arange(slice, n_samples)\n\ntrain_dataset = dataset[train_indexes]\ntest_dataset = dataset[test_indexes]\n</code></pre> <p>Now we can fit several ResLogit models with different numbers of residual layers. We will use the same learning rate and number of epochs for all models. We add itemwise intercept to all the models.</p> <pre><code>model_args = {\n    \"intercept\": \"item\",\n    \"optimizer\": \"SGD\",\n    \"lr\": 1e-6,\n    \"epochs\": 100,\n}\nprint(f\"{model_args=}\")\n</code></pre> <pre><code>list_n_layers = [k for k in range(1, 17)]\nmetrics = pd.DataFrame(columns=[\"n_layers\", \"fit_losses\", \"train_loss\", \"test_loss\", \"initial_trainable_weights\", \"final_trainable_weights\", \"execution_time\"])\n\nfor n_layers in list_n_layers:\n    print(\"\\n------------------------------------\"\n          \"------------------------------------\"\n          f\"\\n{n_layers=}\")\n\n    start_time = timeit.default_timer()\n    model = ResLogit(n_layers=n_layers, **model_args)\n    model.instantiate(n_items=n_items, n_shared_features=n_shared_features, n_items_features=n_items_features)\n\n    initial_trainable_weights = [model.trainable_weights[i].numpy() for i in range(len(model.trainable_weights))]\n\n    fit_losses = model.fit(choice_dataset=train_dataset, val_dataset=test_dataset)\n\n    end_time = timeit.default_timer()\n    execution_time = end_time - start_time\n    print(f\"Execution time with {n_layers} residual layers: {execution_time} seconds\")\n\n    final_trainable_weights = [model.trainable_weights[i].numpy() for i in range(len(model.trainable_weights))]\n\n    new_metric_row = pd.DataFrame({\n        \"n_layers\": [n_layers],\n        \"fit_losses\": [fit_losses],\n        \"train_loss\": [model.evaluate(train_dataset)],\n        \"test_loss\": [model.evaluate(test_dataset)],\n        \"initial_trainable_weights\": [initial_trainable_weights],\n        \"final_trainable_weights\": [final_trainable_weights],\n        \"execution_time\": [execution_time]\n    })\n    metrics = pd.concat([metrics, new_metric_row], ignore_index=True)\n</code></pre> <pre><code>metrics.head()\n</code></pre> <pre><code>for index, row in metrics.iterrows():\n    plt.plot(row[\"fit_losses\"][\"train_loss\"], label=f\"n_layers={row['n_layers']}\")\n\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Training loss through the epochs\")\nplt.title(\"ResLogit model with different number of residual layers\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>train_losses = [row[\"train_loss\"] for index, row in metrics.iterrows()]\ntest_losses = [row[\"test_loss\"] for index, row in metrics.iterrows()]\n\nplt.plot(list_n_layers, train_losses, label=\"Train loss after the last epoch\")\nplt.plot(list_n_layers, test_losses, label=\"Test loss\")\n\nplt.xlabel(\"Number of residual layers\")\nplt.ylabel(\"Loss\")\nplt.title(\"ResLogit model with different number of residual layers\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>execution_times = [row[\"execution_time\"] for index, row in metrics.iterrows()]\n\nplt.plot(list_n_layers, execution_times)\n\nplt.xlabel(\"Number of residual layers\")\nplt.ylabel(\"Execution time (s)\")\nplt.title(\"ResLogit model with different number of residual layers\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"notebooks/models/reslogit/#references","title":"References","text":"<p>[1] ResLogit: A residual neural network logit model for data-driven choice modelling, Wong, M.; Farooq, B. (2021), Transportation Research Part C: Emerging Technologies 126\\ (URL: https://doi.org/10.1016/j.trc.2021.103050)</p>"},{"location":"notebooks/models/logistic_regression/","title":"Logistic Regression: 3-class Classifier","text":"<p>The Conditional MNL is a generalization of the multi-class Logistic Regression. Here, we recreate the scikit-learn tutorial that can be found here.</p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nfrom choice_learn.models import ConditionalLogit\nfrom choice_learn.data import ChoiceDataset\n</code></pre> <pre><code># import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n</code></pre> <p>We need to create a ChoiceDataset object. Features are contexts_features as they are shared by the three outcomes. The class labels are ''choices''.</p> <pre><code>X.shape\n</code></pre> <pre><code>dataset = ChoiceDataset(shared_features_by_choice=X,\n                        shared_features_by_choice_names=[\"sepal_length\", \"sepal_width\"],\n                        choices=Y)\n</code></pre> <p>The logitistic regression is equivalent to Multinomial Logit model. The classes are the choice of our alternatives, and we estimate one set of parameters by class.\\  Meaning that for parametrization, we specify that we want to learn one weight by outcome for each feature: 'sepal_length', 'sepal_width' and the intercept.\\ This is done with the keyword \"item-full\" in Choice-Learn specification.</p> <pre><code>parametrization = {\n    \"intercept\": \"item-full\",\n    \"sepal_length\": \"item-full\",\n    \"sepal_width\": \"item-full\"\n}\n\n# Let's estimate the weights\nmodel = ConditionalLogit(coefficients=parametrization, optimizer=\"lbfgs\", epochs=100)\nhist = model.fit(dataset)\n</code></pre> <p>Let's display the resulting model, just as in the sk-learn tutorial.</p> <pre><code>feature_1, feature_2 = np.meshgrid(\n    np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5),\n    np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n)\ngrid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n\n# choices are the indicator that there 3 choices possible thus 3 different items\nchoices=  np.ones(len(grid), )\nchoices[0] = 0\nchoices[1] = 2\ngrid_dataset = ChoiceDataset(shared_features_by_choice=grid,\nshared_features_by_choice_names=[\"sepal_length\", \"sepal_width\"],\n                        choices=choices)\n</code></pre> <pre><code>y_pred = np.reshape(np.argmax(model.predict_probas(grid_dataset), axis=1), feature_1.shape)\ndisplay = DecisionBoundaryDisplay(\n    xx0=feature_1, xx1=feature_2, response=y_pred\n)\ndisplay.plot(plot_method=\"pcolormesh\",\n    cmap=plt.cm.Paired,\n    shading=\"auto\",\n    xlabel=\"Sepal length\",\n    ylabel=\"Sepal width\")\ndisplay.ax_.scatter(\n    X[:, 0], X[:, 1], c=Y, edgecolor=\"black\", \n    cmap=plt.cm.Paired,\n)\nplt.show()\n</code></pre> <p></p> <p>It sure looks alike !</p>"},{"location":"notebooks/models/latent_class_model/","title":"Example of use of Latent Class MNL","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport matplotlib as mpl\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\n</code></pre> <p>Let's use the Electricity Dataset used in this tutorial.</p> <pre><code>from choice_learn.datasets import load_electricity\n\nelec_dataset = load_electricity(as_frame=False)\n</code></pre> <pre><code>from choice_learn.models.simple_mnl import SimpleMNL\nfrom choice_learn.models.latent_class_mnl import LatentClassSimpleMNL\n</code></pre> <pre><code>lc_model = LatentClassSimpleMNL(n_latent_classes=3, fit_method=\"mle\", optimizer=\"lbfgs\", epochs=1000, lbfgs_tolerance=1e-20)\nhist, results = lc_model.fit(elec_dataset, verbose=1)\n</code></pre> <pre><code>print(\"Latent Class Model weights:\")\nprint(\"Classes Logits:\", lc_model.latent_logits)\nfor i in range(3):\n    print(\"\\n\")\n    print(f\"Model Nb {i}, weights:\", lc_model.models[i].weights)\n</code></pre> <pre><code>nll = (lc_model.evaluate(elec_dataset) * len(elec_dataset)).numpy()\nprint(f\"Negative Log-Likelihood: {nll}\")\n</code></pre> <pre><code>report = lc_model.compute_report(elec_dataset)\n\ndef format_color_groups(df):\n    cmap = mpl.cm.get_cmap(\"Set1\")\n    colors = [mpl.colors.rgb2hex(cmap(i)) for i in range(cmap.N)]\n    x = df.copy()\n    factors = list(x['Latent Class'].unique())\n    i = 0\n    for factor in factors:\n        style = f'background-color: {colors[i]}'\n        x.loc[x['Latent Class'] == factor, :] = style\n        i += 1\n    return x\n\nreport.style.apply(format_color_groups, axis=None)\n</code></pre> <pre><code>Using L-BFGS optimizer, setting up .fit() function\nUsing L-BFGS optimizer, setting up .fit() function\nUsing L-BFGS optimizer, setting up .fit() function\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7fa18fb41af0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7fa18fb41af0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7fa18fb92280&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nWARNING:tensorflow:6 out of the last 6 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7fa18fb92280&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n/var/folders/zz/r1py7zhj35q75v09h8_42nzh0000gp/T/ipykernel_27459/1263996749.py:4: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  cmap = mpl.cm.get_cmap(\"Set1\")\n</code></pre> Latent Class Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 0 Weights_items_features_0 -0.675645 0.023987 -28.167109 0.000000 1 0 Weights_items_features_1 -0.060604 0.008162 -7.424849 0.000000 2 0 Weights_items_features_2 1.851951 0.054914 33.724579 0.000000 3 0 Weights_items_features_3 1.322549 0.048159 27.462420 0.000000 4 0 Weights_items_features_4 -5.857089 0.191162 -30.639460 0.000000 5 0 Weights_items_features_5 -6.513206 0.195680 -33.285046 0.000000 6 1 Weights_items_features_0 -1.817566 0.077771 -23.370796 0.000000 7 1 Weights_items_features_1 -1.726365 0.058838 -29.340986 0.000000 8 1 Weights_items_features_2 3.696567 0.160258 23.066404 0.000000 9 1 Weights_items_features_3 4.111840 0.157179 26.160225 0.000000 10 1 Weights_items_features_4 -26.693516 3.274723 -8.151381 0.000000 11 1 Weights_items_features_5 -14.925840 0.634699 -23.516403 0.000000 12 2 Weights_items_features_0 -2.104791 0.104296 -20.181009 0.000000 13 2 Weights_items_features_1 -1.652622 0.073820 -22.387188 0.000000 14 2 Weights_items_features_2 -5.554287 0.245318 -22.641151 0.000000 15 2 Weights_items_features_3 -13.565555 0.544168 -24.928965 0.000000 16 2 Weights_items_features_4 -9.794930 0.631004 -15.522781 0.000000 17 2 Weights_items_features_5 -12.126673 0.681118 -17.804060 0.000000 <pre><code>lc_model_2 = LatentClassSimpleMNL(n_latent_classes=3, fit_method=\"EM\", optimizer=\"lbfgs\", epochs=2000, lbfgs_tolerance=1e-6)\nhist, results = lc_model_2.fit(elec_dataset, verbose=1)\n</code></pre> <pre><code>lc_model_2.latent_logits\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nplt.plot(results)\n</code></pre> <pre><code>break\n</code></pre> <pre><code>lc_model_2.instantiate(\n                n_items=elec_dataset.get_n_items(),\n                n_shared_features=elec_dataset.get_n_shared_features(),\n                n_items_features=elec_dataset.get_n_items_features(),\n            )\n</code></pre> <pre><code>hist_logits = []\nhist_loss = []\n\n# Initialization\ninit_sample_weight = np.random.rand(3, len(elec_dataset))\ninit_sample_weight = init_sample_weight / np.sum(init_sample_weight, axis=0, keepdims=True)\nfor i, model in enumerate(lc_model_2.models):\n    # model.instantiate()\n    model.fit(elec_dataset, sample_weight=np.clip(init_sample_weight[i], 1e-4, 1), verbose=2)\n</code></pre> <pre><code>lc_model_2.models[2].exact_nll.epsilon\n</code></pre> <pre><code>init_sample_weight[2].min()\n</code></pre> <pre><code>lc_model_2.models[2].predict_probas(elec_dataset)\n</code></pre> <pre><code>tf.math.log(1.0 * 1e-40)\n</code></pre> <pre><code>lc_model_2.models[2]._trainable_weights\n</code></pre> <pre><code>myw, myl, myloss = [], [], []\n\nfor i in range(10):\n    lc_model_2.weights, loss = lc_model_2._expectation(elec_dataset)\n    lc_model_2.latent_logits = lc_model_2._maximization(elec_dataset, verbose=2)\n\n    myw.append(lc_model_2.weights)\n    myl.append(lc_model_2.latent_logits)\n    myloss.append(loss)\n    if np.sum(np.isnan(self.latent_logits)) &gt; 0:\n            print(\"Nan in logits\")\n            break\n</code></pre> <pre><code>for i in tqdm.trange(self.epochs):\n    self.weights, loss = self._expectation(choice_dataset)\n    self.latent_logits = self._maximization(choice_dataset, verbose=verbose)\n    hist_logits.append(self.latent_logits)\n    hist_loss.append(loss)\n    if np.sum(np.isnan(self.latent_logits)) &gt; 0:\n        print(\"Nan in logits\")\n        break\nreturn hist_logits, hist_loss\n</code></pre>"},{"location":"notebooks/models/latent_class_model/#latent-conditional-logit","title":"Latent Conditional Logit","text":"<p>We used a very simple MNL. Here we simulate the same MNL, by using the Conditional-Logit formulation.\\ Don't hesitate to read the conditional-MNL tutorial to better understand how to use this formulation.</p> <pre><code>from choice_learn.models.latent_class_mnl import LatentClassConditionalLogit\n</code></pre> <pre><code>lc_model_2 = LatentClassConditionalLogit(n_latent_classes=3,\n                                       fit_method=\"mle\",\n                                       optimizer=\"lbfgs\",\n                                       epochs=1000,\n                                       lbfgs_tolerance=1e-12)\n</code></pre> <p>For each feature, let's add a coefficient that is shared by all items:</p> <pre><code>lc_model_2.add_shared_coefficient(coefficient_name=\"pf\",\n                                  feature_name=\"pf\",\n                                  items_indexes=[0, 1, 2, 3])\nlc_model_2.add_shared_coefficient(coefficient_name=\"cl\",\n                                  feature_name=\"cl\",\n                                    items_indexes=[0, 1, 2, 3])\nlc_model_2.add_shared_coefficient(coefficient_name=\"loc\",\n                                  feature_name=\"loc\",\n                                  items_indexes=[0, 1, 2, 3])\nlc_model_2.add_shared_coefficient(coefficient_name=\"wk\",\n                                feature_name=\"wk\",\n                                items_indexes=[0, 1, 2, 3])\nlc_model_2.add_shared_coefficient(coefficient_name=\"tod\",\n                                  feature_name=\"tod\",\n                                  items_indexes=[0, 1, 2, 3])\nlc_model_2.add_shared_coefficient(coefficient_name=\"seas\",\n                                  feature_name=\"seas\",\n                                  items_indexes=[0, 1, 2, 3])\n</code></pre> <pre><code># Fit\nhist2 = lc_model_2.fit(elec_dataset, verbose=1)\n</code></pre> <pre><code>print(\"Negative Log-Likelihood:\", lc_model_2.evaluate(elec_dataset)*len(elec_dataset))\n</code></pre> <pre><code>print(\"Latent Class Model weights:\")\nprint(\"Classes Logits:\", lc_model_2.latent_logits)\nfor i in range(3):\n    print(\"\\n\")\n    print(f\"Model Nb {i}, weights:\", lc_model_2.models[i].trainable_weights)\n</code></pre> <p>Just like any ChoiceModel you can get the probabilities:</p> <pre><code>lc_model.predict_probas(elec_dataset[:4])\n</code></pre> <p>If you want to use more complex formulations of Latent Class models, you can directly use the BaseLatentClassModel from choice_learn.models.base_model:</p> <pre><code>from choice_learn.models.latent_class_base_model import BaseLatentClassModel\n</code></pre> <pre><code>manual_lc = BaseLatentClassModel(\n                                 model_class=SimpleMNL,\n                                 model_parameters={\"add_exit_choice\": False},\n                                 n_latent_classes=3,\n                                 fit_method=\"mle\",\n                                 epochs=1000,\n                                 optimizer=\"lbfgs\",\n                                 lbfgs_tolerance=1e-12\n                                 )\nmanual_lc.instantiate(n_items=4,\n                      n_shared_features=0,\n                      n_items_features=6)\nmanual_hist = manual_lc.fit(elec_dataset, verbose=1)\n</code></pre> <pre><code>print(manual_lc.evaluate(elec_dataset) * len(elec_dataset))\n</code></pre> <p>If you need to go deeper, you can look here to see different implementations that could help you.</p> <pre><code>\n</code></pre>"},{"location":"notebooks/models/rumnet/","title":"RUMnet model Usage","text":""},{"location":"notebooks/models/rumnet/#introduction-to-modelling-with-rumnet","title":"Introduction to modelling with RUMnet","text":"<p>We reproduce in this notebook the results of the paper Representing Random Utility Choice Models with Neural Networks on the SwissMetro dataset.</p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove/Add GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom choice_learn.data import ChoiceDataset\nfrom choice_learn.models import RUMnet\nfrom choice_learn.datasets import load_swissmetro\n</code></pre> <p>Note that there are two implementations of RUMnet: one more CPU-oriented and one more GPU-oriented. The import of the right model is automatically done. You can also import the model directly with:</p> <pre><code>from choice_learn.models import CPURUMnet, GPURUMnet\n</code></pre> <p>First, we download the SwissMetro dataset:</p> <p>We follow the same data preparation as in the original paper in order to get the exact same results.</p> <p>Now, we can create our ChoiceDataset from the dataframe.</p> <pre><code>dataset = load_swissmetro(as_frame=False, preprocessing=\"rumnet\")\n</code></pre> <p>Let's Cross-Validate ! We keep a scikit-learn-like structure. To avoid creating dependancies, we use a different train/test split code, but the following would totally work:</p> <pre><code>from sklearn.model_selection import ShuffleSplit\n\nrs = ShuffleSplit(n_splits=5, test_size=.2, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(rs.split(dataset.choices)):\n    train_dataset = dataset[train_index]\n    test_dataset = dataset[test_index]\n\n    model = RUMnet(**args)\n    model.instantiate()\n    model.fit(train_dataset)\n    model.evaluate(test_dataset)\n</code></pre> <p>We just use a numpy based split, but the core code is the same!</p> <pre><code>model_args = {\n    \"num_products_features\": 6,\n    \"num_customer_features\": 83,\n    \"width_eps_x\": 20,\n    \"depth_eps_x\": 5,\n    \"heterogeneity_x\": 10,\n    \"width_eps_z\": 20,\n    \"depth_eps_z\": 5,\n    \"heterogeneity_z\": 10,\n    \"width_u\": 20,\n    \"depth_u\": 5,\n    \"optimizer\": \"Adam\",\n    \"lr\": 0.0002,\n    \"logmin\": 1e-10,\n    \"label_smoothing\": 0.02,\n    \"callbacks\": [],\n    \"epochs\": 140,\n    \"batch_size\": 32,\n    \"tol\": 0,\n}\n</code></pre> <pre><code>indexes = np.random.permutation(list(range(len(dataset))))\n\nfit_losses = []\ntest_eval = []\nfor i in range(5):\n    test_indexes = indexes[int(len(indexes) * 0.2 * i):int(len(indexes) * 0.2 * (i + 1))]\n    train_indexes = np.concatenate([indexes[:int(len(indexes) * 0.2 * i)],\n                                    indexes[int(len(indexes) * 0.2 * (i + 1)):]],\n                                   axis=0)\n\n    train_dataset = dataset[train_indexes]\n    test_dataset = dataset[test_indexes]\n\n    model = RUMnet(**model_args)\n    model.instantiate()\n\n    losses = model.fit(train_dataset, val_dataset=test_dataset)\n    probas = model.predict_probas(test_dataset)\n    eval = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_pred=model.predict_probas(test_dataset), y_true=tf.one_hot(test_dataset.choices, 3))\n    test_eval.append(eval)\n    print(test_eval)\n\n    fit_losses.append(losses)\n</code></pre> <pre><code>cmap = plt.cm.coolwarm\ncolors = [cmap(j / 4) for j in range(5)]\nfor i in range(len(fit_losses)):\n    plt.plot(fit_losses[i][\"train_loss\"], c=colors[i], linestyle=\"--\")\n    plt.plot(fit_losses[i][\"val_loss\"], label=f\"fold {i}\", c=colors[i])\nplt.legend()\n</code></pre> <pre><code>model.evaluate(test_dataset)\n</code></pre> <pre><code>print(\"Average LogLikeliHood on test:\", np.mean(test_eval))\n</code></pre>"},{"location":"notebooks/models/rumnet/#using-early-stopping","title":"Using Early-Stopping","text":"<pre><code>callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                              patience=3, mode=\"min\",\n                                              restore_best_weights=True,)\nmodel_args = {\n    \"num_products_features\": 6,\n    \"num_customer_features\": 83,\n    \"width_eps_x\": 20,\n    \"depth_eps_x\": 5,\n    \"heterogeneity_x\": 10,\n    \"width_eps_z\": 20,\n    \"depth_eps_z\": 5,\n    \"heterogeneity_z\": 10,\n    \"width_u\": 20,\n    \"depth_u\": 5,\n    \"optimizer\": \"Adam\",\n    \"lr\": 0.0002,\n    \"logmin\": 1e-10,\n    \"label_smoothing\": 0.02,\n    \"callbacks\": [callback],\n    \"epochs\": 140,\n    \"batch_size\": 32,\n    \"tol\": 0,\n}\n\nmodel = RUMnet(**model_args)\nmodel.instantiate()\n\nindexes = np.random.permutation(list(range(len(dataset))))\n\ntest_indexes = indexes[:int(len(indexes) * 0.2)]\ntrain_indexes = indexes[int(len(indexes) * 0.2):]\n\ntrain_dataset = dataset[train_indexes]\ntest_dataset = dataset[test_indexes]\n\nlosses = model.fit(train_dataset, val_dataset=test_dataset)\n</code></pre> <pre><code>plt.plot(losses[\"train_loss\"], c=\"b\", linestyle=\"--\", label=\"train loss\")\nplt.plot(losses[\"val_loss\"], label=\"validation loss\", c=\"orange\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"notebooks/models/rumnet/#a-larger-and-more-complex-dataset-expedia-icdm-2013","title":"A larger and more complex dataset: Expedia ICDM 2013","text":"<p>The RUMnet paper benchmarks the model on a second dataset. If you want to use it you need to download the file from Kaggle and place the train.csv file in the folder choice_learn/datasets/data with the name expedia.csv.</p> <pre><code>from choice_learn.datasets import load_expedia\n\n# It takes some time...\nexpedia_dataset = load_expedia(preprocessing=\"rumnet\")\n</code></pre> <pre><code>test_dataset = expedia_dataset[int(len(expedia_dataset)*0.8):]\ntrain_dataset = expedia_dataset[:int(len(expedia_dataset)*0.8)]\n\nmodel_args = {\n    \"num_products_features\": 46,\n    \"num_customer_features\": 84,\n    \"width_eps_x\": 10,\n    \"depth_eps_x\": 3,\n    \"heterogeneity_x\": 5,\n    \"width_eps_z\": 10,\n    \"depth_eps_z\": 3,\n    \"heterogeneity_z\": 5,\n    \"width_u\": 10,\n    \"depth_u\": 3,\n    \"tol\": 0,\n    \"optimizer\": \"Adam\",\n    \"lr\": 0.001,\n    \"logmin\": 1e-10,\n    \"label_smoothing\": 0.02,\n    \"callbacks\": [],\n    \"epochs\": 15,\n    \"batch_size\": 128,\n    \"tol\": 1e-5,\n}\nmodel = RUMnet(**model_args)\nmodel.instantiate()\n\nlosses = model.fit(train_dataset, val_dataset=test_dataset)\nprobas = model.predict_probas(test_dataset)\ntest_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_pred=model.predict_probas(test_dataset), y_true=tf.one_hot(test_dataset.choices, 39))\n\nprint(test_loss)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/models/tastenet/","title":"TasteNet","text":"<p>The TasteNet model, developped in [1] is available in Choice-Learn. Here is a small example on how it can be used.\\ Following the paper, we will use it on the SwissMetro [2] dataset.</p>"},{"location":"notebooks/models/tastenet/#summary","title":"Summary","text":"<ul> <li>Data Loading</li> <li>Model Parametrization</li> <li>Model Estimation</li> <li>Estimated Tastes Analysis</li> <li>References</li> </ul> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom choice_learn.datasets import load_swissmetro\nfrom choice_learn.models.tastenet import TasteNet\n</code></pre>"},{"location":"notebooks/models/tastenet/#data-loading","title":"Data Loading","text":"<pre><code># The preprocessing=\"tastenet\" let us format the data just like in the paper\ncustomers_id, dataset = load_swissmetro(preprocessing=\"tastenet\", as_frame=False)\n</code></pre> <p>We retrieved the SwissMetro dataset in the right format, let' look at it:</p> <pre><code>print(\"Items Features:\", dataset.items_features_by_choice_names)\nprint(\"Shared Features:\", dataset.shared_features_by_choice_names)\n</code></pre>"},{"location":"notebooks/models/tastenet/#model-parametrization","title":"Model Parametrization","text":"<p>The dataset items are order: \"TRAIN\", \"SM\" and \"CAR\". We can now set up TasteNet model' hyperparameters. - taste_net_layers: list of neurons number for each layer in the taste neural network - taste_net_activation: activation function to be used within the taste neural network - items_features_by_choice_parametrization: parametrization of the estimated coefficients for the Items Features.</p> <p>TasteNet uses the customer features (shared_features_by_choice) to estimate different coefficient to be mutliplied with alternative features (items_features_by_choice) to estimate the utility:  </p> <p>With $f$ a normalizing function that can be used to set up some constraints such as positivity.</p> <p>items_features_by_choice_parametrization describes the paramtrization of each alternative features and thus needs to have the same shape, (3, 7) in our case. The indexes also need to match. - if the parameter is a float the value is directly used to multiply the corresponding feature. - if the parameter is a string it indicates that which function $f$ to use meaning that we will use the taste neural network to estimate a parameter before using $f$.</p> <pre><code>taste_net_layers = []\ntaste_net_activation = \"relu\"\nitems_features_by_choice_parametrization = [[-1., \"-exp\", \"-exp\", 0., \"linear\", 0., 0.],\n                            [-1., \"-exp\", \"-exp\", \"linear\", 0., \"linear\", 0.],\n                            [-1., \"-exp\", 0., 0., 0., 0., 0.]]\n</code></pre> <p>In this example from the paper, the utilities defined by items_features_by_choice_parametrization are the following:</p> <p>With $\\mathcal{C}$ the customer features and $NN_k$ the output of the taste embedding neural network:  </p> <p>In order to evaluate the model we work with a Cross-Validation scheme. We need to pay attention that the split take into account the fact that the same person has answered several times and appears several time in the dataset. We work with a GroupOut strategy meaning that one person has all his answers in the same testing fold.</p>"},{"location":"notebooks/models/tastenet/#model-estimation","title":"Model Estimation","text":"<pre><code>from sklearn.model_selection import GroupKFold\n\nfolds_history = []\nfolds_test_nll = []\ngkf = GroupKFold(n_splits=5)\n# specift customer_id to regroup each customer answer\nfor train, test in gkf.split(list(range(len(dataset))), list(range(len(dataset))), customers_id): \n    tastenet = TasteNet(taste_net_layers=taste_net_layers,\n                    taste_net_activation=taste_net_activation,\n                    items_features_by_choice_parametrization=items_features_by_choice_parametrization,\n                    optimizer=\"Adam\",\n                    epochs=40,\n                    lr=0.001,\n                    batch_size=32)\n    train_dataset, test_dataset = dataset[train], dataset[test]\n    hist = tastenet.fit(train_dataset, val_dataset=test_dataset)\n    folds_history.append(hist)\n    folds_test_nll.append(tastenet.evaluate(test_dataset))\n</code></pre> <p>We need to pay attention to overfitting, here is a plot to understand each fold train/test over the fitting epochs:</p> <pre><code>import matplotlib.pyplot as plt\nfor hist, color in zip(folds_history,\n                       [\"darkblue\", \"slateblue\", \"mediumpurple\", \"violet\", \"hotpink\"]):\n    plt.plot(hist[\"train_loss\"], c=color)\n    plt.plot(hist[\"test_loss\"], c=color, linestyle=\"dotted\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>print(\"Average NegativeLogLikelihood on testing set:\", np.mean(folds_test_nll))\n</code></pre>"},{"location":"notebooks/models/tastenet/#estimated-tastes-analysis","title":"Estimated Tastes Analysis","text":"<p>In order to analyze the model, one can look at the average output of the taste network. It is possible to reach the taste network with tastenet.taste_params_module or to call tastenet.predict_tastes.</p> <pre><code>for (item_index, feature_index), nn_output_index in tastenet.items_features_to_weight_index.items():\n    print(\"Alternative:\", [\"train\", \"sm\", \"car\"][item_index])\n    print(\"Feature:\", dataset.items_features_by_choice_names[0][feature_index])\n    print(\"Average value over dataset:\")\n    act = tastenet.get_activation_function(items_features_by_choice_parametrization[item_index][feature_index])\n    print(np.mean(act(tastenet.predict_tastes(dataset.shared_features_by_choice[0])[:, nn_output_index])))\n    print(\"----------------------------\\n\")\n</code></pre>"},{"location":"notebooks/models/tastenet/#references","title":"References","text":"<p>[1][A Neural-embedded Discrete Choice Model: Learning Taste Representation with Strengthened Interpretability](https://arxiv.org/abs/2002.00922), Han, Y.; Calara Oereuran F.; Ben-Akiva, M.; Zegras, C. (2020)\\ [2][The Acceptance of Model Innovation: The Case of Swissmetro](https://www.researchgate.net/publication/37456549_The_acceptance_of_modal_innovation_The_case_of_Swissmetro), Bierlaire, M.; Axhausen, K., W.; Abay, G. (2001)\\</p>"},{"location":"notebooks/models/generic_and_useful_tutorials/saving_loading_models/","title":"Saving loading models","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import numpy as np\nimport tensorflow as tf\n\n# Enabling eager execution sometimes decreases fitting time\ntf.compat.v1.enable_eager_execution()\n</code></pre> <pre><code>from choice_learn.models import ConditionalLogit\n</code></pre> <pre><code>from choice_learn.datasets import load_swissmetro\n\nswiss_dataset = load_swissmetro(preprocessing=\"tutorial\")\nprint(swiss_dataset.summary())\n</code></pre> <pre><code># Initialization of the model\nswiss_model = ConditionalLogit(optimizer=\"Adam\", epochs=25, lr=0.01)\n\n# Intercept for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 1])\n# beta_he for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"headway\",\n                             items_indexes=[0, 1],\n                             coefficient_name=\"beta_he\")\n# beta_co for all items\nswiss_model.add_coefficients(feature_name=\"cost\",\n                             items_indexes=[0, 1, 2])\n# beta first_class for train\nswiss_model.add_coefficients(feature_name=\"regular_class\",\n                             items_indexes=[0])\n# beta seats for train\nswiss_model.add_coefficients(feature_name=\"seats\", items_indexes=[1])\n# betas luggage for car\nswiss_model.add_coefficients(feature_name=\"single_luggage_piece\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_luggage=1\")\nswiss_model.add_coefficients(feature_name=\"multiple_luggage_piece\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_luggage&gt;1\")\n# beta TT only for car\nswiss_model.add_coefficients(feature_name=\"travel_time\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_tt_car\")\n\n# betas TT and HE shared by train and sm\nswiss_model.add_shared_coefficient(feature_name=\"travel_time\",\n                                   items_indexes=[0, 1])\nswiss_model.add_shared_coefficient(feature_name=\"train_survey\",\n                                   items_indexes=[0, 1],\n                                   coefficient_name=\"beta_survey\")\n</code></pre> <pre><code># Estimation of the model\nhistory = swiss_model.fit(swiss_dataset, get_report=False)\n</code></pre> <pre><code>isinstance(swiss_model.optimizer.get_config()[\"learning_rate\"], np.float32), isinstance(swiss_model.optimizer.get_config()[\"learning_rate\"], np.ndarray)\n</code></pre> <pre><code>swiss_model.save_model(\"test_save\")\n</code></pre> <pre><code>swiss_model2 = ConditionalLogit.load_model(\"test_save\")\n</code></pre> <pre><code>hist = swiss_model2.fit(swiss_dataset)\n</code></pre> <pre><code>import shutil\n\nshutil.rmtree(\"test_save\")\n</code></pre>"},{"location":"notebooks/models/generic_and_useful_tutorials/saving_loading_models/#save-every-n-epochs-with-a-custom-tfcallback","title":"Save every n epochs with a custom tf.Callback","text":"<pre><code>class SaveCallback(tf.keras.callbacks.Callback):\n    \"\"\"Callback to save regularly the model during training.\"\"\"\n\n    def __init__(self, base_dir, save_every_n, *args, **kwargs):\n        \"\"\"Instantiate callback.\"\"\"\n        self.base_dir = base_dir\n        self.save_every_n = save_every_n\n        super().__init__(*args, **kwargs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Define saving at the end of each epoch.\"\"\"\n        _ = logs\n        if (epoch + 1) % self.save_every_n == 0:\n            self._save_model(epoch=epoch)\n\n    def _save_model(self, epoch):\n        \"\"\"Handle model saving internally.\"\"\"\n        dirname = os.path.join(self.base_dir, f\"epoch_{epoch}\")\n        self.model.save_model(dirname)\n</code></pre> <pre><code># Initialization of the model\nswiss_model = ConditionalLogit(optimizer=\"Adam\", epochs=25, lr=0.01, callbacks=[SaveCallback(base_dir=\"test_save_cb\", save_every_n=2)])\n\n# Intercept for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 1])\n# beta_he for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"headway\",\n                             items_indexes=[0, 1],\n                             coefficient_name=\"beta_he\")\n# beta_co for all items\nswiss_model.add_coefficients(feature_name=\"cost\",\n                             items_indexes=[0, 1, 2])\n# beta first_class for train\nswiss_model.add_coefficients(feature_name=\"regular_class\",\n                             items_indexes=[0])\n# beta seats for train\nswiss_model.add_coefficients(feature_name=\"seats\", items_indexes=[1])\n# betas luggage for car\nswiss_model.add_coefficients(feature_name=\"single_luggage_piece\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_luggage=1\")\nswiss_model.add_coefficients(feature_name=\"multiple_luggage_piece\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_luggage&gt;1\")\n# beta TT only for car\nswiss_model.add_coefficients(feature_name=\"travel_time\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_tt_car\")\n\n# betas TT and HE shared by train and sm\nswiss_model.add_shared_coefficient(feature_name=\"travel_time\",\n                                   items_indexes=[0, 1])\nswiss_model.add_shared_coefficient(feature_name=\"train_survey\",\n                                   items_indexes=[0, 1],\n                                   coefficient_name=\"beta_survey\")\n</code></pre> <pre><code># Estimation of the model\nhistory = swiss_model.fit(swiss_dataset, get_report=True)\n</code></pre> <pre><code># remove\nshutil.rmtree(\"test_save_cb\")\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/basket_models/shopper/","title":"Shopper model Usage","text":""},{"location":"notebooks/basket_models/shopper/#introduction-to-basket-modelling-with-shopper","title":"Introduction to basket modelling with SHOPPER","text":"<p>We use a synthetic dataset to demonstrate how to use the SHOPPER model.</p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove/Add GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom choice_learn.basket_models import Shopper, Trip\n</code></pre>"},{"location":"notebooks/basket_models/shopper/#dataset","title":"Dataset","text":"<p>For simplicity, we load a trip dataset whose creation is detailed in the notebook data.ipynb (link).</p> <p>Please run the notebook data.ipynb to know more about this dataset.</p> <pre><code>from synthetic_dataset import get_dataset\n\ndata = get_dataset()\n</code></pre> <pre><code>print(data)\nprint(f\"\\nThe TripDataset 'data' contains {data.n_items} distinct items that appear in {data.n_samples} transactions carried out at {data.n_stores} point(s) of sale with {data.n_assortments} different assortments.\")\n</code></pre>"},{"location":"notebooks/basket_models/shopper/#training-shopper-model","title":"Training SHOPPER model","text":"<p>Now we can fit a SHOPPER model.</p> <pre><code># Hyperparameters\n\n# Preferences and price effects are represented by latent variables of size 4 and 3, respectively.\nlatent_sizes = {\"preferences\": 4, \"price\": 3}\n# We use 1 negative sample for each positive sample during the training phase.\nn_negative_samples = 1\noptimizer = \"adam\"\nlr = 1e-2\nepochs = 500\nbatch_size = 128\n</code></pre> <pre><code># Model: items fixed effect + items interactions + price effects\nshopper = Shopper(\n    item_intercept=False,\n    price_effects=True,\n    seasonal_effects=False,\n    latent_sizes=latent_sizes,\n    n_negative_samples=n_negative_samples,\n    optimizer=optimizer,\n    lr=lr,\n    epochs=epochs,\n    batch_size=batch_size,\n)\n# Feel free to explore other models by changing the hyperparameters!\n</code></pre> <p>The SHOPPER model can integrate store effects as well as seasonality. Check the documentation if you want to know more about it.</p> <pre><code># Instantiate the model\nshopper.instantiate(n_items=data.n_items)\n\n# Train the model\nhistory = shopper.fit(trip_dataset=data)\n</code></pre> <pre><code>plt.plot(history[\"train_loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Loss\")\nplt.legend()\nplt.title(\"Training of SHOPPER model\")\nplt.show()\n</code></pre>"},{"location":"notebooks/basket_models/shopper/#inference-with-shopper-model","title":"Inference with SHOPPER model","text":"<p>We evaluate the model on the validation dataset.</p> <pre><code>n_permutations = 2\n\n# You can choose how many basket permutations are used to evaluate the model\nnll = shopper.evaluate(data, n_permutations)\n</code></pre> <pre><code>print(f\"Mean negative log-likelihood on the dataset: {nll:.4f}.\")\n\nprint(\"\\nWe can see that the more complex the model, the lower the negative log-likelihood.\")\n</code></pre> <p>We can also compute various utilities and probabilities.</p> <pre><code>item_batch_inference=np.array([2, 0, 4])\nbasket_inference = np.array([1, 3])\nfull_basket_inference = np.array([1, 3, 0])\nprice_inference = 5.\navailable_items_inference = np.ones(data.n_items)\navailable_items_inference[4] = 0  # Consider that item 4 is not available during inference\nassortment_inference = np.array(\n    [\n        item_id for item_id in data.get_all_items() if available_items_inference[item_id] == 1\n    ]\n)\n\ntrip_inference = Trip(\n    purchases=full_basket_inference,\n    prices=np.random.uniform(1, 10, data.n_items),\n    assortment=available_items_inference,\n    store=0,  # 0 if not defined\n    week=0,  # 0 if not defined\n)\n</code></pre> <pre><code># Item utilities\nitem_utilities = shopper.compute_batch_utility(\n    item_batch=item_batch_inference,\n    basket_batch=np.tile(basket_inference, (3, 1)),\n    store_batch=np.array([0]*3), # 0 if not defined\n    week_batch=np.array([0]*3), # 0 if not defined\n    price_batch=np.array([price_inference]*3),\n    available_item_batch=np.tile(available_items_inference, (3, 1)),\n)\n\nprint(\n    f\"Considering the assortment (ie the set of available items) {assortment_inference} with prices {price_inference},\",\n    f\"and a basket with the items {basket_inference}.\\n\",\n    f\"Under these circumstances, the utility of the selected items are:\"\n)\nfor i, item_id in enumerate(item_batch_inference):\n    if item_id == 0:\n        print(f\"    - Item {item_id} (checkout item): {item_utilities[i]:.4f}\")\n    else:\n        print(f\"    - Item {item_id}: {item_utilities[i]:.4f}\")\n</code></pre> <pre><code>item_likelihoods = shopper.compute_item_likelihood(trip=trip_inference)\n\nprint(\n    f\"Considering the assortment (ie the set of available items) {assortment_inference} with prices {price_inference},\",\n    f\"and a basket with the items {basket_inference}.\\n\",\n    f\"Under these circumstances, the likelihoods that each item will be the next item added to the basket are:\"\n)\nfor i, item_id in enumerate(data.get_all_items()):\n    if item_id == 0:\n        print(f\"    - Item {item_id} (checkout item, the customer decides to end his shopping trip): {item_likelihoods[i]:.4f}\")\n    else:\n        print(f\"    - Item {item_id}: {item_likelihoods[i]:.4f}\")\nprint(f\"\\nN.B.: The item likelihoods sum to {np.sum(item_likelihoods):.4f}.\")\n</code></pre> <pre><code># (Unordered) basket likelihood\nn_permutations = 2\nbasket_likelihood = shopper.compute_basket_likelihood(trip=trip_inference, n_permutations=n_permutations)\n\nprint(f\"Likelihood for (unordered) basket {full_basket_inference}: {basket_likelihood:.4f} (with {n_permutations} permutations to approximate all possible orders).\")\n</code></pre> <pre><code># Ordered basket likelihood\nbasket_ordered_likelihood = shopper.compute_ordered_basket_likelihood(trip=trip_inference)\n\nprint(f\"Likelihood for ordered basket {full_basket_inference}: {basket_ordered_likelihood:.4f}.\")\n</code></pre> <pre><code># Ordered basket likelihood of the other permutation of the basket\nbasket_ordered_likelihood = shopper.compute_ordered_basket_likelihood(\n    basket=np.array([3, 1, 0]),\n    available_items=trip_inference.assortment,\n    store=trip_inference.store,\n    week=trip_inference.store,\n    prices=trip_inference.prices,\n)\n\nprint(f\"Likelihood for ordered basket {[3, 1, 0]}: {basket_ordered_likelihood:.4f}.\")\n</code></pre>"},{"location":"notebooks/basket_models/self_attention/","title":"Self attention","text":""},{"location":"notebooks/basket_models/self_attention/#0-init","title":"0. Init","text":"<pre><code>import sys\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom choice_learn.basket_models.self_attention_model import SelfAttentionModel\n</code></pre> <pre><code>physical_devices = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs:\", len(physical_devices))\n</code></pre>"},{"location":"notebooks/basket_models/self_attention/#i-synthetic-dataset","title":"I.  Synthetic Dataset","text":"<p>Dataset We build a synthetic dataset for which the catalog has 8 items, I= {1,...,8}, with the following interactions: - Cannibalization: {0,1, 2} on the one hand and {3,4,5} on the other hand form groups of items cannibalizing each other. - Complementarity: each of the items in {0,1,2} are complementary to each of the items in {3,4,5}; - Neutral: 6, 7 and 8 are neutral in the sense that they don\u2019t have specific interaction with other items. - When choosing among the first nest, user 0 prefers item 0, user 1 the item 1 and user 2 the item 2.</p> <pre><code>from choice_learn.basket_models.datasets.synthetic_dataset import SyntheticDataGenerator\n\n\"\"\"items_nest : dict\n                Dictionary defining item sets and their relations.\n                Key should be next index and values list of items indexes, e.g.\"\"\"\n\nitems_nest = { 0:[0, 1,2],\n                1: [3,4,5],\n                2: [6,7,8]}\n\n\"\"\"nests_interactions: list\n                List of interactions between nests for each nest. Symmetry should\n                be ensure by users, e.g.\"\"\"\n\nnests_interactions = [[\"\", \"compl\", \"neutral\", \"neutral\"],\n                    [\"compl\", \"\", \"neutral\", \"neutral\"],\n                    [\"neutral\", \"neutral\", \"\", \"neutral\"]]\n\n\"\"\" proba_complementary_items : float\n        Probability of adding complementary items to the basket.\n    proba_neutral_items : float\n        Probability of adding neutral items to the basket.\n    noise_proba : float\n        Probability of adding noise items to the basket.\"\"\"\n\nuser_profile = {0:{ \"nest\" : 0, \"item\" : 0}, 1: {\"nest\" : 0, \"item\" : 1}, 2: {\"nest\" : 0, \"item\" : 2}}\n\n\"\"\"user_profile : dict\n                Dictionary defining user profiles.\n                Key should be user index and values a dict with 'nest' and 'item' keys\"\"\"\n\ndata = SyntheticDataGenerator(items_nest=items_nest,\n                       nests_interactions=nests_interactions,\n                       proba_complementary_items=1,\n                       proba_neutral_items=0.0,\n                       noise_proba=0.0,\n                       user_profile=user_profile\n                        )\n\ndata = data.generate_trip_dataset(n_baskets=1000, assortments_matrix=np.ones((1, 9)))\n\ndata.available_items\n</code></pre>"},{"location":"notebooks/basket_models/self_attention/#ii-self-attention-model","title":"II. Self Attention Model","text":"<pre><code>from tensorflow.keras.callbacks import EarlyStopping\n\nlr = 0.005\nn_epochs = 20\nbatch_size = 32\nlatent_sizes = {\"short_term\": 2, \"long_term\": 2}\nL = 7\nhinge_margin = 0.7\nshort_term_ratio = 0.3\nn_negative_samples = 1\noptimizer = \"adam\"\n\u03bb = 0.0\ndropout_rate = 0.0\n</code></pre> <pre><code>model = SelfAttentionModel(\n    optimizer=optimizer,\n    n_negative_samples=n_negative_samples,\n    lr=lr,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    latent_sizes=latent_sizes,\n    hinge_margin=hinge_margin,\n    short_term_ratio=short_term_ratio,\n    l2_regularization=\u03bb,\n    dropout_rate=dropout_rate,\n\n)\n\nmodel.instantiate(n_items=data.n_items, n_users=data.n_users)\n</code></pre> <pre><code>history = model.fit(trip_dataset=data, verbose=2)\n</code></pre> <pre><code>plt.plot(history[\"train_loss\"])\nplt.plot(history[\"val_loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Training Loss\")\n\n\n\nplt.show()\n</code></pre>"},{"location":"notebooks/basket_models/self_attention/#iii-embedding-visualisation","title":"III. Embedding Visualisation","text":"<pre><code>X = model.X\nU = model.U\nV = model.V\nd = model.d_long\n</code></pre> <pre><code>from sklearn.decomposition import PCA\nimport seaborn as sns\n\nembedding = model.V\nbasket_batch = [[0,1,3,7]]\nm_batch, affinity_matrix = model.embed_context(basket_batch, is_training=False)\n\n\nif d ==1:\n    embedding = np.hstack([embedding, np.zeros((embedding.shape[0],1))])\n    U = tf.concat([U, tf.zeros((U.shape[0],1))], axis=1)\n\nif d &gt; 2:\n    pca = PCA(n_components=2)\n    V_pca = pca.fit_transform(embedding)\n    U_pca = pca.transform(U.numpy())\nelse:\n    V_pca = embedding\n    U_pca = U.numpy()\nplt.figure(figsize=(10, 8))\nplt.scatter(V_pca[:, 0], V_pca[:, 1])#, c=color_group)\n\nplt.scatter(U_pca[:,0], U_pca[:,1], color='red', marker='x', s=100, label='User 0')\n\nfor i in range(V_pca.shape[0]):\n    plt.annotate(str(i), (V_pca[i, 0], V_pca[i, 1]+0.05), \n                     fontsize=8, ha='center', va='center')\nfor i in range(U_pca.shape[0]):\n    plt.annotate(f'U{i}', (U_pca[i, 0], U_pca[i, 1]+0.05), \n                     fontsize=8, ha='center', va='center')\n\nplt.title(\"PCA visualization of item embeddings\")\nplt.axis()\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.show()\n\n\nsns.heatmap(affinity_matrix[0],\n            annot=True,     \n            fmt='.2f',\n            ) \n</code></pre>"},{"location":"notebooks/basket_models/basket_data/","title":"Introduction data handling, the TripDataset","text":""},{"location":"notebooks/basket_models/basket_data/#how-to-create-a-tripdataset","title":"How to create a TripDataset","text":"<p>We create here a synthetic dataset to demonstrate how to use the Trip and TripDataset classes.</p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove/Add GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n</code></pre> <pre><code>import numpy as np\n\nfrom choice_learn.basket_models.data import Trip, TripDataset\n</code></pre>"},{"location":"notebooks/basket_models/basket_data/#dataset","title":"Dataset","text":"<p>Let's consider a simple dataset where we have only six items sold in two different stores: - The first store sells items [0, 1, 2, 3, 4] and has observed baskets [1, 0], [2, 0], [1, 3, 4, 0]; - The second store sells items [0, 1, 5, 6] and has observed baskets [1, 0], [6, 5, 0];</p> <p>with 0 the checkout item.</p> <pre><code>n_items = 7\n\npurchases_stores_1 =[[1, 0], [2, 0], [1, 3, 4, 0]]\npurchases_stores_2 = [[1, 0], [6, 5, 0]]\n\nassortment_store_1 = np.array([1, 1, 1, 1, 1, 0, 0])\nassortment_store_2 = np.array([1, 1, 0, 0, 0, 1, 1])\navailable_items = np.array([assortment_store_1, assortment_store_2])\n</code></pre> <pre><code>print(f\"The list of available items are encoded as availability matrices indicating the availability (1) or not (0) of the products:\\n{available_items=}\\n\")\nprint(\n    \"Here, the variable 'available_items' can be read as:\\n\",\n    f\"- Assortment 1 = {[i for i in range(n_items) if assortment_store_1[i]==1]}\\n\",\n    f\"- Assortment 2 = {[i for i in range(n_items) if assortment_store_2[i]==1]}\"\n)\n</code></pre> <p>Let's say that each basket has been seen 100 times. We can create Trip objects based on these shopping baskets and assortments with fixed prices.</p> <pre><code># Create a list of Trip objects:\nnum_baskets = 100\ntrips_list = []\n\nfor _ in range(num_baskets):\n    trips_list += [\n        Trip(\n            purchases=purchases_stores_1[0],\n            # Let's consider here totally random prices for the products\n            prices=np.random.uniform(1, 10, n_items),\n            assortment=0\n        ),\n        Trip(\n            purchases=purchases_stores_1[1],\n            prices=np.random.uniform(1, 10, n_items),\n            assortment=0\n        ),\n        Trip(\n            purchases=purchases_stores_1[2],\n            prices=np.random.uniform(1, 10, n_items),\n            assortment=0\n        ),\n        Trip(\n            purchases=purchases_stores_2[0],\n            prices=np.random.uniform(1, 10, n_items),\n            assortment=1\n        ),\n        Trip(\n            purchases=purchases_stores_2[1],\n            prices=np.random.uniform(1, 10, n_items),\n            assortment=1\n        )\n    ]\n</code></pre> <p>Now that we have our Trip objects, we can instantiate a TripDataset that can be fed to a basket model.</p> <pre><code>data = TripDataset(trips=trips_list, available_items=available_items)\n</code></pre> <pre><code>print(data)\nprint(f\"\\nThe TripDataset 'data' contains {data.n_items} distinct items that appear in {data.n_samples} transactions carried out at {data.n_stores} point(s) of sale with {data.n_assortments} different assortments.\")\nprint(f\"\\nDescription of the first trip of the dataset:\\n{data.get_trip(0)}\")\n</code></pre> <p>This code is reused in the synthetic_dataset.py file to be called in other notebooks.</p> <pre><code>\n</code></pre>"},{"location":"notebooks/basket_models/alea_carta/","title":"AleaCarta model Usage","text":""},{"location":"notebooks/basket_models/alea_carta/#introduction-to-modelling-with-aleacarta","title":"Introduction to modelling with AleaCarta","text":"<p>We use a synthetic dataset to demonstrate how to use the AleaCarta model [1]</p> <pre><code># Install necessary packages that are not already installed in the environment\n# !pip install matplotlib\n</code></pre> <pre><code>import os\nimport sys\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nsys.path.append(\"./../../\")\nprint(os.getcwd())\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom synthetic_dataset import get_dataset_ace\nfrom choice_learn.basket_models import AleaCarta\n</code></pre> <pre><code>data = get_dataset_ace()\n\nprint(data)\nprint(f\"\\nThe TripDataset 'data' contains {data.n_items} distinct items that appear in {data.n_samples} transactions carried out at {data.n_stores} point(s) of sale with {data.n_assortments} different assortments.\")\n</code></pre> <pre><code>latent_sizes = {\"preferences\": 6, \"price\": 3, \"season\": 3}\nn_negative_samples = 2\noptimizer = \"adam\"\nlr = 1e-2\nepochs = 200\n# epochs = 1000\nbatch_size = 32\n</code></pre> <pre><code>model = AleaCarta(\n    # item_intercept=True,\n    item_intercept=False,\n    price_effects=False,\n    seasonal_effects=False,\n    latent_sizes=latent_sizes,\n    n_negative_samples=n_negative_samples,\n    optimizer=optimizer,\n    lr=lr,\n    epochs=epochs,\n    batch_size=batch_size,\n)\n\nmodel.instantiate(n_items=7, n_stores=2)\n</code></pre> <pre><code>history = model.fit(trip_dataset=data)\n</code></pre> <pre><code>plt.plot(history[\"train_loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Loss\")\nplt.legend()\nplt.title(\"Training\")\nplt.show()\n</code></pre> <pre><code>model.compute_batch_utility(item_batch=np.array(list(range(2))),\n        basket_batch=np.array([[1, 2], [3, 4]]),\n        store_batch=np.array([0, 0]),\n        week_batch=np.array([0, 0]),\n        price_batch=np.array([0, 0]),\n        available_item_batch=np.ones((2, data.n_items))\n        )\n</code></pre>"},{"location":"notebooks/basket_models/alea_carta/#a-more-complex-dataset","title":"A more complex dataset","text":"<p>We will use the Badminton Dataset[1] to showcase how the AleaCarta model can capture complementarity &amp; cannibalization effects:</p> <pre><code>from choice_learn.basket_models.datasets.synthetic_dataset import SyntheticDataGenerator\n</code></pre> <p>We load the model and define 4 nests of items with interaction effects that are either complementarity or neutral. We consider that the different items within a nest cannibalize each others sales.</p> <pre><code>data_gen = SyntheticDataGenerator(\n    proba_complementary_items=0.7,\n    proba_neutral_items=0.3,\n    noise_proba=0.15,\n    items_nest = {0:[0, 1, 2],\n                   1: [3, 4, 5],\n                   2: [6],\n                   3: [7]},\n    nests_interactions = [[\"\", \"compl\", \"neutral\", \"neutral\"],\n                          [\"compl\", \"\", \"neutral\", \"neutral\"],\n                          [\"neutral\", \"neutral\", \"\", \"neutral\"],\n                          [\"neutral\", \"neutral\", \"neutral\", \"\"]])\n\n# Load the dataset\ntrip_dataset = data_gen.generate_trip_dataset(n_baskets=1000, assortments_matrix=np.ones((1, 8)))\n</code></pre> <p>It is possible to instantiate and train the model as follows:</p> <pre><code>model = AleaCarta(\n    item_intercept=True,\n    price_effects=False,\n    seasonal_effects=False,\n    latent_sizes=latent_sizes,\n    n_negative_samples=n_negative_samples,\n    optimizer=optimizer,\n    lr=lr,\n    epochs=epochs,\n    batch_size=batch_size,\n)\n\nmodel.instantiate(n_items=8, n_stores=1)\nhist = model.fit(trip_dataset)\n</code></pre> <p>Here is a way to observe the marginal probabilities $\\mathbb{P}(i | \\mathcal{B} = {j})$:</p> <pre><code>proba_matrix = []\nfor item in range(8):\n    probabilities = model.compute_item_likelihood(\n        basket=np.array([item]),\n        available_items=np.ones((8, )),\n        store=np.array(0),\n        week=np.array(0),\n        prices=np.zeros((8, )),\n    )\n    proba_matrix.append(probabilities)\nproba_matrix = np.vstack(proba_matrix)\n</code></pre> <pre><code>plt.imshow(proba_matrix, vmin=0., vmax=1., cmap=\"coolwarm\")\nplt.colorbar()\nplt.xlabel(\"Item $j$\")\nplt.ylabel(\"Item $i$\")\n</code></pre> <p>For model evaluation with a TripDataset, one can simply do:</p> <pre><code>model.evaluate(trip_dataset=trip_dataset)\n</code></pre>"},{"location":"notebooks/basket_models/alea_carta/#references","title":"References","text":"<p>[1] Better Capturing Interactions between Products in Retail: Revisited Negative Sampling for Basket Choice Modeling, D\u00e9sir, J.; Auriau, V.; Mo\u017eina, M.; Malherbe, E. (2025), ECML-PKDD</p>"},{"location":"notebooks/basket_models/basic_attention/","title":"Implementation of an attention-based model for item recommendation.","text":"<p>Wang, Shoujin, Liang Hu, Longbing Cao, Xiaoshui Huang, Defu Lian, and Wei Liu. \"Attention-based transactional context embedding for next-item recommendation.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018.</p> <pre><code>import os\nimport sys\nimport json\nimport time\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nsys.path.append(\"./../../\")\nfrom choice_learn.basket_models.data import TripDataset\nfrom choice_learn.basket_models.basic_attention_model import AttentionBasedContextEmbedding\nfrom choice_learn.basket_models.datasets.synthetic_dataset import SyntheticDataGenerator\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#parameters","title":"Parameters","text":"<pre><code>lr = 0.05\nepochs = 30\nn_baskets = 1000\nembedding_dim = 4\nn_negative_samples = 3\nfull_assortment_matrix = np.array([[1,1,1,1,1,1,1,1]])\nn_items = full_assortment_matrix.shape[1]\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#synthetic-data-generator","title":"Synthetic Data Generator","text":"<pre><code>data_gen = SyntheticDataGenerator(\n    proba_complementary_items=0.7,\n    proba_neutral_items=0.3,\n    noise_proba=0.15,\n    items_nest = {0:[0, 1, 2],\n                   1: [3, 4, 5],\n                   2: [6],\n                   3: [7]},\n    nests_interactions = [[\"\", \"compl\", \"neutral\", \"neutral\"],\n                          [\"compl\", \"\", \"neutral\", \"neutral\"],\n                          [\"neutral\", \"neutral\", \"\", \"neutral\"],\n                          [\"neutral\", \"neutral\", \"neutral\", \"\"]])\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#two-functions-to-visualise-distributions","title":"Two functions to visualise distributions","text":"<ul> <li><code>visualise_tripdataset_trips</code> : to show on a heatmap the conditional items distribution P(i|j) in a tripdataset</li> <li><code>get_model_representation</code> : to show on a heatmap the conditional items distribution P(i|j) when calling a model's predict()</li> </ul> <pre><code>def visualise_tripdataset_trips(dataset, n_items):\n    \"\"\"\n    Visualize the conditional probability P(i|j) of items co-occurring in baskets.\n\n    Parameters\n    ----------\n    dataset : TripDataset\n        The dataset containing trips.\n    n_items : int\n        Number of unique items.\n    \"\"\"\n    distribution_matrix = np.zeros((n_items, n_items))\n    for trip in dataset.trips:\n        basket = trip.purchases\n        for i in basket:\n            for j in basket:\n                if i != j:\n                    distribution_matrix[i, j] += 1\n    row_sums = distribution_matrix.sum(axis=1, keepdims=True)\n    for i in range(len(row_sums)):\n        if row_sums[i] != 0:\n            distribution_matrix[i] = distribution_matrix[i]/row_sums[i]\n\n    return distribution_matrix\n\n\n\ndef get_model_representation(model, n_items, test_dataset=None, assortment_matrix=None):\n    \"\"\"\n    Visualize the model's conditional probability matrix and training loss history.\n\n    Parameters\n    ----------\n    model : AttentionBasedContextEmbedding\n        The trained model.\n    hist : dict\n        Training history with \"train_loss\" key.\n    n_items : int\n        Number of unique items.\n    test_dataset : TripDataset, optional\n        Dataset for evaluation. If None, uses single-item contexts.\n    assortment_matrix : np.ndarray, optional\n        Binary matrix indicating available items.\n    \"\"\"\n    if assortment_matrix is None:\n        assortment_matrix = np.ones((1, n_items), dtype=int)\n\n    if test_dataset is None:\n        available_items = assortment_matrix[0]\n        contexts = tf.constant([[i] for i in range(n_items)], dtype=tf.int32)\n\n    else:\n        contexts = []\n        for batch in test_dataset.iter_batch(1, data_method=\"aleacarta\"):\n            contexts.append(batch[1][0])\n        contexts = tf.ragged.constant(\n            [row[row != -1] for row in contexts], dtype=tf.int32\n        )\n        available_items = batch[-1][0]\n\n    context_prediction = model.predict(contexts, available_items=available_items)\n    predicted_items = [np.argmax(context_prediction[i]) for i in range(context_prediction.shape[0])]\n\n    if test_dataset is None:\n        distribution_matrix = np.stack(context_prediction)\n        for i in range(len(available_items)):\n            if available_items[i] == 0:\n                distribution_matrix[i] *= 0\n    else:\n        distribution_matrix = np.zeros((n_items, n_items))\n        for i in range(contexts.shape[0]):\n            for j in contexts[i]:\n                distribution_matrix[predicted_items[i], j] += 1\n\n    row_sums = distribution_matrix.sum(axis=1, keepdims=True)\n    for i in range(len(row_sums)):\n        if row_sums[i] != 0:\n            distribution_matrix[i] = distribution_matrix[i]/row_sums[i]\n\n    return distribution_matrix\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#generate-full-assortments-synthetic-dataset-train-test","title":"Generate full assortments synthetic dataset : train &amp; test","text":"<pre><code>trip_dataset_train = data_gen.generate_trip_dataset(n_baskets,full_assortment_matrix)\ntrip_dataset_test = data_gen.generate_trip_dataset(n_baskets,full_assortment_matrix)\n\n\ndistribution_matrix = visualise_tripdataset_trips(trip_dataset_train,n_items)\n\nplt.figure(figsize=(4, 3))\nplt.imshow(distribution_matrix, vmin=0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\nplt.colorbar(label='P(i|j)')\nplt.title('Items distribution in the train dataset (A_full)')\nplt.xlabel('j')\nplt.ylabel('i')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#instantiate-and-train-the-model-1-on-a_full","title":"Instantiate and train the model 1 on A_full","text":"<p>-&gt; model 1 uses the true NCE sampling distribution; items frequencies aware</p> <pre><code>model1 = AttentionBasedContextEmbedding(\n    epochs=epochs,\n    lr=lr,\n    latent_size=embedding_dim,\n    n_negative_samples=n_negative_samples)\n\nmodel1.instantiate(n_items=len(full_assortment_matrix[0]))\nhistory1 = model1.fit(trip_dataset_train)\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#instantiate-and-train-the-model-2-on-a_full","title":"Instantiate and train the model 2 on A_full","text":"<p>-&gt; model 2 uses a uniform sampling distribution for NCE (1/(n_items-1))</p> <pre><code>model2 = AttentionBasedContextEmbedding(\n    epochs=epochs,\n    lr=lr,\n    latent_size=embedding_dim,\n    n_negative_samples=n_negative_samples,\n    nce_distribution=\"uniform\")\n\nmodel2.instantiate(n_items=len(full_assortment_matrix[0]))\nhistory2 = model2.fit(trip_dataset_train)\n</code></pre> <p>Model 1 is using the true NCE distribution</p> <p>The following plot of the evaluation on the test dataset shows P(i|j) in the predictions for the test dataset</p> <pre><code>M1 = get_model_representation(model1, n_items)\nM2 = get_model_representation(model1, n_items, trip_dataset_test)\nM3 = visualise_tripdataset_trips(trip_dataset_train, n_items)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout = True)\n\nim1 = axes[0].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[0].set_title(\"Model1 evaluated on [[0], [1], ...]\")\n\nim2 = axes[1].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[1].set_title(\"Training distribution\")\n\ncbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\ncbar.set_label(\"Probability\")\n\nplt.show()\n\nloss_train_dataset = model1.evaluate(trip_dataset_train)\nloss_test_dataset = model1.evaluate(trip_dataset_test)\nprint(f\"Loss of model1 on the train dataset {loss_train_dataset}\")\nprint(f\"Loss of model1 on the test dataset {loss_test_dataset}\")\nprint(\"Used loss for evaluation: NLL\")\n</code></pre> <p>Model 2 is using a uniform sampling distribution for NCE</p> <p>The following plot of the evaluation on the test dataset shows P(i|j) in the predictions for the test dataset</p> <pre><code>M1 = get_model_representation(model2, n_items)\nM2 = get_model_representation(model2, n_items, trip_dataset_test)\nM3 = visualise_tripdataset_trips(trip_dataset_train, n_items)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout = True)\n\nim1 = axes[0].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[0].set_title(\"Model2 evaluated on [[0], [1], ...]\")\n\nim2 = axes[1].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[1].set_title(\"Training distribution\")\n\ncbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\ncbar.set_label(\"Probability\")\n\nplt.show()\n\nloss_train_dataset = model2.evaluate(trip_dataset_train)\nloss_test_dataset = model2.evaluate(trip_dataset_test)\nprint(f\"Loss of model2 on the train dataset {loss_train_dataset}\")\nprint(f\"Loss of model2 on the test dataset {loss_test_dataset}\")\nprint(\"Used loss for evaluation: NLL\")\n</code></pre> <pre><code>import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\ndef plot_Pi_given_not_i(model, n_items, available_items=None, max_context_size=None):\n    \"\"\"\n    Plot the average probability P(i | context not containing i) for each item i,\n    averaged over all possible contexts that do not include i.\n\n    Parameters\n    ----------\n    model : object\n        The trained model with a .predict() method.\n    n_items : int\n        Number of unique items.\n    available_items : array-like or None, optional\n        Binary array indicating available items. If None, all items are available.\n    max_context_size : int or None, optional\n        Maximum context size to consider for tractability. If None, uses all sizes.\n    \"\"\"\n    Pi_given_not_i = np.zeros(n_items)\n    counts = np.zeros(n_items)\n\n    for i in range(n_items):\n        context_candidates = [j for j in range(n_items) if j != i]\n        if max_context_size is not None:\n            context_sizes = range(1, max_context_size + 1)\n        else:\n            context_sizes = range(1, n_items)\n        for k in context_sizes:\n            for context in itertools.combinations(context_candidates, k):\n                context_tensor = tf.ragged.constant([list(context)], dtype=tf.int32)\n                avail = np.ones(n_items, dtype=np.float32) if available_items is None else available_items\n                probas = model.predict(context_tensor, available_items=avail)\n                Pi_given_not_i[i] += probas[0, i]\n                counts[i] += 1\n\n    return Pi_given_not_i / np.maximum(counts, 1)\n\ndef empirical_Pi_given_not_i(tripdataset, n_items, max_context_size=None):\n    \"\"\"\n    Compute empirical P(i | context not containing i) from a TripDataset.\n\n    Parameters\n    ----------\n    tripdataset : TripDataset\n        The dataset containing trips.\n    n_items : int\n        Number of unique items.\n    max_context_size : int or None\n        If set, only consider contexts up to this size.\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (n_items,) with empirical P(i | context not containing i).\n    \"\"\"\n    numerators = np.zeros(n_items)\n    denominators = np.zeros(n_items)\n\n    for trip in tripdataset.trips:\n        basket = list(trip.purchases)\n        for idx, target in enumerate(basket):\n            context = basket[:idx] + basket[idx+1:]\n            if target &gt;= n_items:\n                continue\n            if max_context_size is not None and len(context) &gt; max_context_size:\n                continue\n            # For every item, if it is NOT in the context, increment denominator\n            for i in range(n_items):\n                if i not in context:\n                    denominators[i] += 1\n                    if target == i:\n                        numerators[i] += 1\n\n    return numerators / np.maximum(denominators, 1)\n\npi_not_i_train_dataset = empirical_Pi_given_not_i(trip_dataset_train, n_items)\npi_not_i_model1 = plot_Pi_given_not_i(model1, n_items)\npi_not_i_model2 = plot_Pi_given_not_i(model2, n_items)\n\nall_values = np.concatenate([pi_not_i_model1, pi_not_i_model2, pi_not_i_train_dataset])\nymin = 0\nymax = np.max(all_values) * 1.05\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\naxes[0].bar(np.arange(n_items), pi_not_i_model1)\naxes[0].set_ylabel(r\"Average $P(i\\,|\\,\\mathrm{context\\ not\\ containing}\\ i)$\")\naxes[0].set_title(\"Model 1\")\naxes[0].set_ylim(ymin, ymax)\n\naxes[1].bar(np.arange(n_items), pi_not_i_model2)\naxes[1].set_title(\"Model 2\")\naxes[1].set_ylim(ymin, ymax)\n\naxes[2].bar(np.arange(n_items), pi_not_i_train_dataset)\naxes[2].set_title(\"Train dataset\")\naxes[2].set_ylim(ymin, ymax)\n\nfig.supxlabel(\"Item index\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#train-on-a1-a2-a3-and-test-on-a4-cf-jdesir-vauriau-e-malherbes-paper","title":"Train on A1, A2, A3 and test on A4 cf J.D\u00e9sir, V.Auriau, E. Malherbes paper","text":"<pre><code># Assortments definition\nassortment1 = np.array([[1,1,0,1,1,1,1,1]])\nassortment2 = np.array([[1,0,1,0,1,1,1,1]])\nassortment3 = np.array([[0,1,1,1,0,1,1,1]])\nassortment4 = np.array([[1,1,0,0,1,1,1,1]])\nassortment_full = np.array([[1,1,1,1,1,1,1,1]])\n\nn_baskets = 500\n\ntrip_dataset_1 = data_gen.generate_trip_dataset(n_baskets,assortment1)\ntrip_dataset_2 = data_gen.generate_trip_dataset(n_baskets,assortment2)\ntrip_dataset_3 = data_gen.generate_trip_dataset(n_baskets,assortment3)\n\n\npaper_trip_dataset_train = trip_dataset_1.concatenate(trip_dataset_2).concatenate(trip_dataset_3)\npaper_trip_dataset_test_a4 = data_gen.generate_trip_dataset(10*n_baskets,assortment4)\npaper_trip_dataset_test_full = data_gen.generate_trip_dataset(10*n_baskets,assortment_full)\n\n\nM1 = visualise_tripdataset_trips(paper_trip_dataset_train,n_items)\nM2 = visualise_tripdataset_trips(paper_trip_dataset_test_a4,n_items)\nM3 = visualise_tripdataset_trips(paper_trip_dataset_test_full,n_items)\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 4), constrained_layout = True)\n\nim1 = axes[0].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[0].set_title(\"Train dataset (A1,A2,A3)\")\n\nim2 = axes[1].imshow(M2, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[1].set_title(\"Test dataset (A4)\")\n\nim3 = axes[2].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[2].set_title(\"Test dataset (A_full)\")\n\ncbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\ncbar.set_label(\"Probability\")\n\nplt.show()\n</code></pre>"},{"location":"notebooks/basket_models/basic_attention/#instantiate-and-train-model-3-on-a1a2a3-with-uniform-sampling-for-nce","title":"Instantiate and train model 3 on A1,A2,A3 with uniform sampling for NCE","text":"<pre><code>model3 = AttentionBasedContextEmbedding(\n    epochs=50,\n    lr=0.05,\n    embedding_dim=3,\n    n_negative_samples=3)\n\nmodel3.instantiate(n_items=n_items,use_true_nce_distribution = False)\nhistory3 = model3.fit(paper_trip_dataset_train)\n</code></pre> <pre><code>M1 = get_model_representation(model3, n_items, assortment_matrix = [[1,1,0,0,1,1,1,1]])\nM2 = get_model_representation(model3, n_items)\nM3 = visualise_tripdataset_trips(paper_trip_dataset_train,n_items)\n\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 4), constrained_layout = True)\n\nim1 = axes[0].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[0].set_title(\"Train dataset (A1,A2,A3)\")\n\nim2 = axes[1].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[1].set_title(\"Model3 evaluated on A4\")\n\nim3 = axes[2].imshow(M2, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\naxes[2].set_title(\"Model3 evaluated on Afull\")\n\n\ncbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\ncbar.set_label(\"Probability\")\n\nplt.show()\n\nloss_paper_train_dataset = model3.evaluate(paper_trip_dataset_train)\nloss_paper_test_dataset_a4 = model3.evaluate(paper_trip_dataset_test_a4)\nloss_paper_test_dataset_afull = model3.evaluate(paper_trip_dataset_test_full)\nprint(f\"Loss of model3 on the train dataset {loss_paper_train_dataset}\")\nprint(f\"Loss of model3 on the test dataset A4 {loss_paper_test_dataset_a4}\")\nprint(f\"Loss of model2 on the test dataset Afull {loss_paper_test_dataset_afull}\")\nprint(\"Used loss for evaluation : NLL\")\n</code></pre> <pre><code>pi_not_i_train_dataset = empirical_Pi_given_not_i(trip_dataset_train, n_items)\npi_not_i_model3_a4 = plot_Pi_given_not_i(model3, n_items, available_items = [1,1,0,0,1,1,1,1])\npi_not_i_model3_full = plot_Pi_given_not_i(model3, n_items)\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\naxes[0].bar(np.arange(n_items), pi_not_i_model3_full)\naxes[0].set_ylabel(r\"Average $P(i\\,|\\,\\mathrm{context\\ not\\ containing}\\ i)$\")\naxes[0].set_title(\"Model 3 on Afull\")\n\naxes[1].bar(np.arange(n_items), pi_not_i_model3_a4)\naxes[1].set_title(\"Model 3 on A4\")\n\naxes[2].bar(np.arange(n_items), pi_not_i_train_dataset)\naxes[2].set_title(\"Train dataset\")\n\nfig.supxlabel(\"Item index\")\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/data/dataset_creation/","title":"Exhaustive example of ChoiceDataset creation","text":""},{"location":"notebooks/data/dataset_creation/#the-different-possible-ways-to-create-a-choicedataset","title":"The different possible ways to create a ChoiceDataset","text":"<p>Listed below:</p> <ul> <li>From a single long format DataFrame</li> <li>From a single wide format DataFrame</li> <li>From several DataFrames</li> <li>From several np.ndarrays</li> </ul> <p></p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\n\nfrom choice_learn.data import ChoiceDataset\nfrom choice_learn.data.storage import FeaturesStorage\n</code></pre> <p>We will use the CanadaMode dataset for this example. We can download it directly:</p> <pre><code>from choice_learn.datasets import load_modecanada\n\ncanada_df = load_modecanada(as_frame=True)\ncanada_df.head()\n</code></pre> case alt choice dist cost ivt ovt freq income urban noalt 0 1 train 0 83 28.25 50 66 4 45.0 0 2 1 1 car 1 83 15.77 61 0 0 45.0 0 2 2 2 train 0 83 28.25 50 66 4 25.0 0 2 3 2 car 1 83 15.77 61 0 0 25.0 0 2 4 3 train 0 83 28.25 50 66 4 70.0 0 2 <p>Let's create a column indicating whether the considered transport alternative is individual or not transport.</p>"},{"location":"notebooks/data/dataset_creation/#from-a-single-long-format-dataframe","title":"From a single long format dataframe","text":"<pre><code>dataset = ChoiceDataset.from_single_long_df(df=canada_df,\n                                       shared_features_columns=[\"dist\", \"income\", \"urban\"],\n                                       items_features_columns=[\"freq\", \"cost\", \"ivt\", \"ovt\"],\n                                       items_id_column=\"alt\",\n                                       choices_id_column=\"case\",\n                                       choices_column=\"choice\",\n                                       # the choice columns indicates if the item is chosen (1) or not (0)\n                                       choice_format=\"one_zero\",\n                                       )\nprint(dataset.summary())\n</code></pre> <pre><code>%=====================================================================%\n%%% Summary of the dataset:\n%=====================================================================%\nNumber of items: 4\nNumber of choices: 4324\n%=====================================================================%\n Shared Features by Choice:\n 3 shared features\n with names: (['dist', 'income', 'urban'],)\n\n\n Items Features by Choice:\n4 items features \n with names: (['freq', 'cost', 'ivt', 'ovt'],)\n%=====================================================================%\n</code></pre> <p>Another mode is possible, if the dataframe indicates the name of the chosen item instead of ones and zeros:</p> <pre><code>canada_df = load_modecanada(as_frame=True, choice_format=\"items_id\")\ncanada_df.head()\n</code></pre> case alt choice dist cost ivt ovt freq income urban noalt 0 1 train car 83 28.25 50 66 4 45.0 0 2 1 1 car car 83 15.77 61 0 0 45.0 0 2 2 2 train car 83 28.25 50 66 4 25.0 0 2 3 2 car car 83 15.77 61 0 0 25.0 0 2 4 3 train car 83 28.25 50 66 4 70.0 0 2 <p>This time, the choice is not given by ones and zeros but actually names for each context which alternative (item) has been chosen. The ChoiceDataset handles this case easily, by specifying 'choice_format=\"items_id\"'.</p> <pre><code>dataset = ChoiceDataset.from_single_long_df(df=canada_df,\n                                       shared_features_columns=[\"dist\", \"income\", \"urban\"],\n                                       items_features_columns=[\"freq\", \"cost\", \"ivt\", \"ovt\"],\n                                       items_id_column=\"alt\",\n                                       choices_id_column=\"case\",\n                                       choices_column=\"choice\",\n                                       # the choice columns indicates the id of the chosen item\n                                       choice_format=\"items_id\",\n                                       )\nprint(dataset.summary())\n</code></pre> <pre><code>%=====================================================================%\n%%% Summary of the dataset:\n%=====================================================================%\nNumber of items: 4\nNumber of choices: 4324\n%=====================================================================%\n Shared Features by Choice:\n 3 shared features\n with names: (['dist', 'income', 'urban'],)\n\n\n Items Features by Choice:\n4 items features \n with names: (['freq', 'cost', 'ivt', 'ovt'],)\n%=====================================================================%\n</code></pre>"},{"location":"notebooks/data/dataset_creation/#from-a-single-wide-format-dataframe","title":"From a single wide format DataFrame","text":"<p>If your DataFrame is in the wide format you can use the 'from_single_wide_df' method. Here is an example with the SwissMetro dataset.</p> <pre><code>from choice_learn.datasets import load_swissmetro\n\nswiss_df = load_swissmetro(as_frame=True)\nswiss_df.head()\n</code></pre> GROUP SURVEY SP ID PURPOSE FIRST TICKET WHO LUGGAGE AGE ... TRAIN_CO TRAIN_HE SM_TT SM_CO SM_HE SM_SEATS CAR_TT CAR_CO CHOICE CAR_HE 0 2 0 1 1 1 0 1 1 0 3 ... 48 120 63 52 20 0 117 65 1 0.0 1 2 0 1 1 1 0 1 1 0 3 ... 48 30 60 49 10 0 117 84 1 0.0 2 2 0 1 1 1 0 1 1 0 3 ... 48 60 67 58 30 0 117 52 1 0.0 3 2 0 1 1 1 0 1 1 0 3 ... 40 30 63 52 20 0 72 52 1 0.0 4 2 0 1 1 1 0 1 1 0 3 ... 36 60 63 42 20 0 90 84 1 0.0 <p>5 rows \u00d7 29 columns</p> <pre><code>dataset = ChoiceDataset.from_single_wide_df(\n    df=swiss_df,\n    items_id=[\"TRAIN\", \"SM\", \"CAR\"],\n    shared_features_columns=[\"GROUP\", \"SURVEY\", \"SP\", \"PURPOSE\", \"FIRST\", \"TICKET\", \"WHO\", \"LUGGAGE\", \"AGE\",\n                               \"MALE\", \"INCOME\", \"GA\", \"ORIGIN\", \"DEST\"],\n    items_features_suffixes=[\"CO\", \"TT\", \"HE\", \"SEATS\"],\n    available_items_suffix=\"AV\", # [\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"] also works\n    choices_column=\"CHOICE\",\n    choice_format=\"item_index\",\n)\n</code></pre>"},{"location":"notebooks/data/dataset_creation/#from-several-dataframes","title":"From several DataFrames","text":"<p>Now, let's say that you have your data split into several files. It can happen if you store the different type of features in different SQL Tables for example. You will only need to follow some restrictions:</p> <pre><code>shared_features, items_features, choices =\\\nload_modecanada(as_frame=True, split_features=True, add_is_public=True)\n</code></pre> <p>fixed_items_features need to have a column named \"item_id\" referencing the item. Others columns are free to be any feature.</p> <p>contexts_features need to have a \"context_id\" column (otherwise index is used). Other columns are free to be any feature.</p> <pre><code>shared_features.head()\n</code></pre> choice_id income dist urban 0 1 45.0 83 0 2 2 25.0 83 0 4 3 70.0 83 0 6 4 70.0 83 0 8 5 55.0 83 0 <p>contexts_items_features need to have the column \"item_id\" and is recommended to have the column \"context_id\" (otherwise index is used).\\ Of course \"item_id\" and \"context_id\" should match fixed_items_features and contexts_features.</p> <pre><code>items_features.head()\n</code></pre> choice_id item_id cost freq ovt ivt is_public 0 1 train 28.25 4 66 50 1.0 1 1 car 15.77 0 0 61 0.0 2 2 train 28.25 4 66 50 1.0 3 2 car 15.77 0 0 61 0.0 4 3 train 28.25 4 66 50 1.0 <p>choices should have a column \"context_id\" and a column \"choice\". The value in \"choice\" should match the values in the column \"item_id\" in items_features and contexts_items_features.</p> <pre><code>choices.head()\n</code></pre> choice_id choice 1 1 car 3 2 car 5 3 car 7 4 car 9 5 car <pre><code># And now you can create the dataset with:\ndataset = ChoiceDataset(shared_features_by_choice=shared_features,\n                        items_features_by_choice=items_features,\n                        choices=choices)\nprint(dataset.summary())\n</code></pre> <pre><code>WARNING:root:Shared Features Names were not provided, will not be able to\n                                    fit models needing them such as Conditional Logit.\nWARNING:root:Items Features Names were not provided, will not be able to\n                                fit models needing them such as Conditional Logit.\n\n\n%=====================================================================%\n%%% Summary of the dataset:\n%=====================================================================%\nNumber of items: 4\nNumber of choices: 4324\n%=====================================================================%\n Shared Features by Choice:\n 3 shared features\n with names: (Index(['income', 'dist', 'urban'], dtype='object'),)\n\n\n Items Features by Choice:\n5 items features \n with names: (Index(['cost', 'freq', 'is_public', 'ivt', 'ovt'], dtype='object'),)\n%=====================================================================%\n</code></pre>"},{"location":"notebooks/data/dataset_creation/#from-several-npndarrays","title":"From several np.ndarrays","text":"<p>Finally, another alternative is to specify each type of feature as np.ndarrays. You can or not also give features names. It is not necessary unless you plan to use a model with specification w.r.t. to those features names.</p> <pre><code>shared_features, items_features, available_items_by_choice, choices =\\\nload_modecanada(as_frame=False, split_features=True)\n</code></pre> <p>If you are using this method, it is your job to make sure that the arrays are well organized.\\ First, shared_features_by_choice, items_features_by_choice, available_items_by_choice and choices must be in the right order and their dimension (first one) must match.\\ Second, available_items_by_choice and items_features must also have the same number of items and ordered the sames, in their second dimension. Third, choices must indicate the index of the chosen item as ordered items_features_by_choice and available_items_by_choice. Finally you have to precise the available_items_by_choice, or which items were available (1) or not (0) for each context/choice.</p> <p>To summarize the shape of the arrays must be: - (n_choices, n_shared_features) for shared_features_by_choice - (n_choices, n_items, n_items_features) for items_features_by_choice - (n_choices, n_items) for available_items_by_choice - (n_choices, ) for choices</p> <pre><code>print(\"For our example here are the arrays shapes:\")\nprint(f\"Contexts Features shape: {shared_features.shape}, 4324 choices, 3 features (income, dist, urban)\")\nprint(f\"Contexts Items Features shape: {items_features.shape}, 4324 choices, 4 items, 4 features (freq, cost, ivt, ovt)\")\nprint(f\"Contexts Items Availabilities shape: {available_items_by_choice.shape}, 4324 choices, 4 items\")\nprint(f\"Choices shape: {choices.shape}, 4324 choices\")\n</code></pre> <pre><code>For our example here are the arrays shapes:\nContexts Features shape: (4324, 3), 4324 choices, 3 features (income, dist, urban)\nContexts Items Features shape: (4324, 4, 4), 4324 choices, 4 items, 4 features (freq, cost, ivt, ovt)\nContexts Items Availabilities shape: (4324, 4), 4324 choices, 4 items\nChoices shape: (4324,), 4324 choices\n</code></pre> <pre><code>dataset = ChoiceDataset(shared_features_by_choice=shared_features,\n                        items_features_by_choice=items_features,\n                        choices=choices,\n                        available_items_by_choice=available_items_by_choice,\n                        # We can give the name of the features as follows, with the right order:\n                        shared_features_by_choice_names=[\"income\", \"dist\", \"urban\"],\n                        items_features_by_choice_names=[\"freq\", \"cost\", \"ivt\", \"ovt\"],\n                        )\nprint(dataset.summary())\n</code></pre> <pre><code>%=====================================================================%\n%%% Summary of the dataset:\n%=====================================================================%\nNumber of items: 4\nNumber of choices: 4324\n%=====================================================================%\n Shared Features by Choice:\n 3 shared features\n with names: (['income', 'dist', 'urban'],)\n\n\n Items Features by Choice:\n4 items features \n with names: (['freq', 'cost', 'ivt', 'ovt'],)\n%=====================================================================%\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/","title":"Deep dive on FeaturesStorage","text":"<p>Here is are detailed explanations of what's possible with FeaturesStorage and its use as features_by_ids in ChoiceDataset.</p>"},{"location":"notebooks/data/features_byID_examples/#summary","title":"Summary","text":"<ul> <li> <p>Different instantiations of FeaturesStorage</p> <ul> <li>from a dict</li> <li>from a list</li> <li>from a list, without ids</li> <li>from a pandas.DataFrame</li> </ul> </li> <li> <p>Different instatiations of OneHotStorage</p> <ul> <li>from several lists</li> <li>from a single list</li> <li>from a dict</li> </ul> </li> <li> <p>Using FeaturesStorage or OneHotStorage in a ChoiceDataset</p> </li> <li> <p>Example with the SwissMetro</p> </li> <li> <p>Link to another example: Expedia Dataset</p> </li> </ul> <p></p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\nimport pandas as pd\n</code></pre> <pre><code>from choice_learn.data.storage import FeaturesStorage\nfrom choice_learn.data import ChoiceDataset\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#different-instantiation-possibilities-for-storage","title":"Different Instantiation Possibilities for Storage:","text":""},{"location":"notebooks/data/features_byID_examples/#1-from-dict","title":"1 - from dict","text":"<pre><code>features = {\"customerA\": [1, 2, 3], \"customerB\": [4, 5, 6], \"customerC\": [7, 8, 9]}\n# dict must be {id: features}\nstorage = FeaturesStorage(values=features,\n                          values_names=[\"age\", \"income\", \"children_nb\"],\n                          name=\"customers_features\")\n</code></pre> <pre><code>DictStorage\n</code></pre> <pre><code># Subset in order to only keep som ids\nsub_storage = storage[[\"customerA\", \"customerC\"]]\n</code></pre> <pre><code>DictStorage\n</code></pre> <pre><code># Batch to access the features values\nstorage.batch[[\"customerA\", \"customerC\", \"customerA\", \"customerC\"]]\n</code></pre> <pre><code>array([[1, 2, 3],\n       [7, 8, 9],\n       [1, 2, 3],\n       [7, 8, 9]])\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#2-from-list","title":"2 - from list","text":"<pre><code>features = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nids = [\"customerA\", \"customerB\", \"customerC\"]\n\nstorage = FeaturesStorage(ids=ids,\n                          values=features,\n                          values_names=[\"age\", \"income\", \"children_nb\"],\n                          name=\"customers\")\n# We get the same result as before\nstorage.batch[[\"customerA\", \"customerC\", \"customerA\", \"customerC\"]]\n</code></pre> <pre><code>DictStorage\n\n\n\n\n\narray([[1, 2, 3],\n       [7, 8, 9],\n       [1, 2, 3],\n       [7, 8, 9]])\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#3-from-list-without-ids","title":"3 - from list, without ids","text":"<p>The ids are generated automatically as increasing integers:</p> <pre><code>features = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nstorage = FeaturesStorage(values=features, values_names=[\"age\", \"income\", \"children_nb\"], name=\"customers\")\nstorage.batch[[0, 2, 0, 2]]\n</code></pre> <pre><code>array([[1, 2, 3],\n       [7, 8, 9],\n       [1, 2, 3],\n       [7, 8, 9]])\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#4-from-pandasdataframe","title":"4 - from pandas.DataFrame","text":"<pre><code># Here the DataFrame has a column \"id\" that identifies the keys from the features values\nfeatures = {\"age\": [1, 4, 7], \"income\": [2, 5, 8], \"children_nb\": [3, 6, 9], \"id\": [\"customerA\", \"customerB\", \"customerC\"]}\nfeatures = pd.DataFrame(features)\nstorage = FeaturesStorage(values=features, name=\"customers\")\nstorage.batch[[\"customerA\", \"customerC\", \"customerA\", \"customerC\"]]\n</code></pre> <pre><code>DictStorage\n\n\n\n\n\narray([[1, 2, 3],\n       [7, 8, 9],\n       [1, 2, 3],\n       [7, 8, 9]])\n</code></pre> <pre><code># Here the DataFrame does not have a column \"id\" that identifies the keys from the features values\n# We thus specify the 'index'\nfeatures = {\"age\": [1, 4, 7], \"income\": [2, 5, 8], \"children_nb\": [3, 6, 9]}\nfeatures = pd.DataFrame(features, index=[\"customerA\", \"customerB\", \"customerC\"])\nstorage = FeaturesStorage(values=features, name=\"customers\")\nstorage.batch[[\"customerA\", \"customerC\", \"customerA\", \"customerC\"]]\n</code></pre> <pre><code>DictStorage\n\n\n\n\n\narray([[1, 2, 3],\n       [7, 8, 9],\n       [1, 2, 3],\n       [7, 8, 9]])\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#different-instantiations-of-onehotstorage","title":"Different instantiations of OneHotStorage","text":""},{"location":"notebooks/data/features_byID_examples/#5-onehotstorage-from-lists","title":"5 - OneHotStorage from lists","text":"<pre><code>ids = [0, 1, 2, 3, 4]\nvalues = [4, 3, 2, 1, 0]\n\n# Here the Storage will map the ids to the values\n# value = 4 means that the fifth value is a one, the rest are zeros\noh_storage = FeaturesStorage(ids=ids, values=values, as_one_hot=True, name=\"OneHotTest\")\n</code></pre> <pre><code># Get OneHot vectors:\noh_storage.batch[[0, 2, 4]]\n</code></pre> <pre><code>array([[0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0]], dtype=uint8)\n</code></pre> <pre><code># Get the Storage value\noh_storage.get_element_from_index(0), oh_storage.storage\n</code></pre> <pre><code>(4, {0: 4, 1: 3, 2: 2, 3: 1, 4: 0})\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#6-onehotstorage-from-single-list","title":"6 - OneHotStorage from single list","text":"<p>If only the values are given, the ids are created as increasing integers.</p> <pre><code>oh_storage = FeaturesStorage(values=values, as_one_hot=True, name=\"OneHotTest\")\noh_storage.batch[[0, 2, 4]]\n</code></pre> <pre><code>array([[0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0]], dtype=uint8)\n</code></pre> <p>If the values are not given, they are also created from the ids as increasing integers.</p> <pre><code>oh_storage = FeaturesStorage(ids=ids, as_one_hot=True, name=\"OneHotTest\")\noh_storage.batch[[0, 2, 4]]\n# Note that here it changes the order !\n</code></pre> <pre><code>array([[1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1]], dtype=uint8)\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#7-onehotstorage-from-dict","title":"7 - OneHotStorage from dict","text":"<pre><code>values_dict = {k:v for k, v in zip(ids, values)}\noh_storage = FeaturesStorage(values=values_dict, as_one_hot=True, name=\"OneHotTest\")\noh_storage.batch[[0, 2, 4]]\n</code></pre> <pre><code>array([[0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0]], dtype=uint8)\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#use-of-featuresbyid-and-storage-in-the-choicedataset","title":"Use of FeaturesByID and Storage in the ChoiceDataset","text":"<p>Here is a small example on how a ChoiceDataset is instantiated with a FeatureStorage. For it to fully work you need to: - Give the different FeatureStorage in a list in the features_by_ids argument - The name of the FeaturesStorage needs to be the same as the name of the columns containing the ids in shared_features_by_choice or items_features_by_choice - Make sure that all ids in shared_features_by_choice or items_features_by_choice have a corresponding id in the FeaturesStorage</p> <pre><code>features = {\"customerA\": [1, 2, 3], \"customerB\": [4, 5, 6], \"customerC\": [7, 8, 9]}\ncustomer_storage = FeaturesStorage(values=features,\n                          values_names=[\"age\", \"income\", \"children_nb\"],\n                          name=\"customers_features\")\nshared_features_by_choice = pd.DataFrame({\"is_weekend\": [0, 1, 1, 0],\n                                          # This column is the one matching with the FeaturesStorage customer_storage\n                                          # It follows the conditions 2/ and 3/ about naming and ids\n                                          \"customers_features\": [\"customerA\", \"customerB\", \"customerA\", \"customerC\"]})\n</code></pre> <pre><code>DictStorage\n</code></pre> <pre><code>features = {\"item1\": [1, 2, 3], \"item2\": [4, 5, 6], \"item3\": [7, 8, 9], \"item4\": [10, 11, 12]}\nstorage = FeaturesStorage(values=features, values_names=[\"f1\", \"f2\", \"f3\"], name=\"items_features\")\n\nprice_storage = {\"price1\": [1], \"price2\": [2], \"price3\": [3], \"price4\": [4]}\nprice_storage = FeaturesStorage(values=price_storage, values_names=[\"price\"], name=\"items_prices\")\n\nprices = [[[4, 1], [4, 1], [5, 1]], [[5, 2], [4, 2], [6, 2]],\n          [[6, 3], [7, 3], [8, 3]], [[4, 4], [5, 4], [4, 4]]]\nitems_features_by_choice = [[[\"item1\", \"price1\"], [\"item2\", \"price2\"], [\"item3\", \"price3\"]],\n                           [[\"item1\", \"price1\"], [\"item4\", \"price2\"], [\"item3\", \"price4\"]],\n                           [[\"item1\", \"price1\"], [\"item2\", \"price3\"], [\"item3\", \"price4\"]],\n                           [[\"item1\", \"price1\"], [\"item2\", \"price3\"], [\"item3\", \"price4\"]]]\nchoices = [0, 1, 2, 2]\n\ndataset = ChoiceDataset(\n    choices=choices,\n    shared_features_by_choice=shared_features_by_choice,\n    items_features_by_choice=items_features_by_choice,\n    features_by_ids=[storage, price_storage, customer_storage],\n    items_features_by_choice_names=[\"items_features\", \"items_prices\"],\n    )\n</code></pre> <p>Now we can use the ChoiceDataset as any other one to estimate a choice model. In particular the .batch argument will make reconstruct all features:</p> <pre><code>batch = dataset.batch[[0, 2]]\nprint(\"Shared features by choice:\", batch[0])\nprint(\"Items features by choice:\", batch[1])\nprint(\"Available items by choice:\", batch[2])\nprint(\"Choices:\", batch[3])\n</code></pre> <pre><code>Shared features by choice: [[0 1 2 3]\n [1 1 2 3]]\nItems features by choice: [[[1 2 3 1]\n  [4 5 6 2]\n  [7 8 9 3]]\n\n [[1 2 3 1]\n  [4 5 6 3]\n  [7 8 9 4]]]\nAvailable items by choice: [[1. 1. 1.]\n [1. 1. 1.]]\nChoices: [0 2]\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#example-with-the-swissmetro-dataset","title":"Example with the SwissMetro dataset","text":"<pre><code>from choice_learn.datasets import load_swissmetro\n\nswiss_df = load_swissmetro(as_frame=True)\nswiss_df.head()\n</code></pre> GROUP SURVEY SP ID PURPOSE FIRST TICKET WHO LUGGAGE AGE ... TRAIN_CO TRAIN_HE SM_TT SM_CO SM_HE SM_SEATS CAR_TT CAR_CO CHOICE CAR_HE 0 2 0 1 1 1 0 1 1 0 3 ... 48 120 63 52 20 0 117 65 1 0.0 1 2 0 1 1 1 0 1 1 0 3 ... 48 30 60 49 10 0 117 84 1 0.0 2 2 0 1 1 1 0 1 1 0 3 ... 48 60 67 58 30 0 117 52 1 0.0 3 2 0 1 1 1 0 1 1 0 3 ... 40 30 63 52 20 0 72 52 1 0.0 4 2 0 1 1 1 0 1 1 0 3 ... 36 60 63 42 20 0 90 84 1 0.0 <p>5 rows \u00d7 29 columns</p> <p>The ID column refers to a unique participant to the survey. Each participant answered several cases. We therefore have several times the features concerning this participant. A perfect example for FeaturesStorage.</p> <pre><code>customer_columns = ['ID', 'GROUP', 'SURVEY', 'SP', 'PURPOSE', 'FIRST', 'TICKET', 'WHO',\n                    'LUGGAGE', 'AGE', 'MALE', 'INCOME', 'GA', 'ORIGIN', 'DEST']\ncustomer_features = swiss_df[customer_columns].drop_duplicates()\ncustomer_features = customer_features.rename(columns={\"ID\": \"id\"})\ncustomer_storage = FeaturesStorage(values=customer_features, name=\"customer_features\")\n\nshared_features_by_choice = swiss_df[[\"ID\"]]\nshared_features_by_choice = shared_features_by_choice.rename(columns={\"ID\": \"customer_features\"})\n</code></pre> <pre><code>available_items_by_choice = swiss_df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]].to_numpy()\nitems_features_by_choice = np.stack([swiss_df[[\"TRAIN_TT\", \"TRAIN_CO\", \"TRAIN_HE\"]].to_numpy(),\n                                    swiss_df[[\"SM_TT\", \"SM_CO\", \"SM_HE\"]].to_numpy(),\n                                    swiss_df[[\"CAR_TT\", \"CAR_CO\", \"CAR_HE\"]].to_numpy()], axis=1)\nchoices = swiss_df.CHOICE.to_numpy()\n</code></pre> <pre><code>choice_dataset = ChoiceDataset(shared_features_by_choice=shared_features_by_choice,\n                               items_features_by_choice=items_features_by_choice,\n                               available_items_by_choice=available_items_by_choice,\n                               choices=choices,\n                               features_by_ids=[customer_storage],)\n</code></pre> <p>Et voil\u00e0 !</p> <pre><code>batch = choice_dataset.batch[[0, 10, 200]]\nprint(\"Shared features by choice:\", batch[0])\nprint(\"Items features by choice:\", batch[1])\nprint(\"Available items by choice:\", batch[2])\nprint(\"Choices:\", batch[3])\n</code></pre> <pre><code>Shared features by choice: [[ 2  0  1  1  0  1  1  0  3  0  2  0  2  1]\n [ 2  0  1  1  0  1  1  1  2  0  1  0 22  1]\n [ 2  0  1  1  0  3  2  1  2  1  2  0 15  1]]\nItems features by choice: [[[112.  48. 120.]\n  [ 63.  52.  20.]\n  [117.  65.   0.]]\n\n [[170.  62.  30.]\n  [ 70.  66.  10.]\n  [  0.   0.   0.]]\n\n [[116.  54.  60.]\n  [ 53.  83.  30.]\n  [ 78.  40.   0.]]]\nAvailable items by choice: [[1. 1. 1.]\n [1. 1. 0.]\n [1. 1. 1.]]\nChoices: [1 1 0]\n</code></pre>"},{"location":"notebooks/data/features_byID_examples/#link-to-another-example","title":"Link to another example","text":"<p>Finally you can find here a good examples of how memory efficient FeaturesStorage can be. The Expedia datasets incorporates several OneHot features that are encoded as OneHotStorage saving up a lot of memory.</p>"},{"location":"notebooks/introduction/4_model_customization/","title":"Building a custom choice model and handling hyper-parameters","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\n# Remove GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n</code></pre>"},{"location":"notebooks/introduction/4_model_customization/#introduction-to-customization","title":"Introduction to customization","text":"<p>The Choice-Learn package aims at providing structure and helpful functions in order to design any choice model. The main idea is to write the utility function and let the package work its magic. It is recommended to read the data tutorial before to understand the ChoiceDataset class.</p>"},{"location":"notebooks/introduction/4_model_customization/#summary","title":"Summary","text":"<ul> <li>BaseClass: ChoiceModel <ul> <li>EndPoints</li> <li>Parameters</li> <li>SubClassing</li> </ul> </li> <li>Example 1: Rewriting Conditional Logit as custom model</li> <li>Example 2: Defining a non-linear utility function with TensorFlow</li> </ul>"},{"location":"notebooks/introduction/4_model_customization/#baseclass-choicemodel","title":"BaseClass: ChoiceModel","text":"<p>Choice-Learn models are built on the ChoiceModel base class and most of them follow the same structure.\\ In this tutorial, we will delve into the details of modelling and the possibilities of the package. In particular we will see how Choice-Learn helps for manual formulation of a choice model.</p> <pre><code># Let's import ChoiceModel\n\nfrom choice_learn.models.base_model import ChoiceModel\n</code></pre>"},{"location":"notebooks/introduction/4_model_customization/#the-different-endpoints","title":"The different EndPoints","text":"<p>The ChoiceModel class revolves around several methods that are shared by most models: - Model Specification\\     .__init__() and/or .instantiate() are used to specify the form of the model</p> <ul> <li> <p>Model Estimation\\     .fit() uses a ChoiceDataset to find the best values for the different trainable weights</p> </li> <li> <p>Use of the model\\     .evaluate() can be used to estimate the negative log likelihood of the model's choice probabilities compared to the ground truth from a ChoiceDataset\\     .predict_probas() can be used to predict the model's choice probabilities related to a ChoiceDataset\\     .compute_batch_utility() can be used to predict a batch items utilities</p> </li> </ul>"},{"location":"notebooks/introduction/4_model_customization/#parameters","title":"Parameters","text":"<p>A few parameters are shared through the ChoiceModel class and can be changed. A full list is available, here are the most useful:</p> <ul> <li>optimizer: Name of the optimizer to use. Default is lbfgs<ul> <li>Non-stochastic: It is recommended to use them - and in particular lbfgs - for smaller datasets and models. It is faster but needs all data in memory, therefore the batch_size argument is not used. More info on the TensorFlow documentation.</li> <li>Stochastic Gradient Descent optimizers - such as Adam. They will lead to slower convergence but work well with batching. List is here.</li> </ul> </li> <li>batch_size: Data batch size to use when stochastic gradient descent optimizer is used. Default is 32.</li> <li>lr: Learning Rate of the optimizer to use when stochastic gradient descent optimizer is used. Default is 0.001.</li> <li>epochs: Max number of iterations before stopping optimization. Default is 1000.</li> </ul>"},{"location":"notebooks/introduction/4_model_customization/#subclassing","title":"Subclassing","text":"<p>Inheritance is used for better code formatting in Choice-Learn. It is also optimized to let anyone easily define its own utility model. The idea is that by subclassing ChoiceModel one only needs to define the utility function with TensorFlow for it to work.\\ The advantages are twofold: - It needs little time. An example will follow to show you how it can be done in a few minutes. - It is possible to use non-linear formulations of the utility. As long as it is written with TensorFlow operations, Choice-Learn and TensorFlow handle the optimization. For the more adventurers, you can even define your own operations as long as you provide the gradients.</p>"},{"location":"notebooks/introduction/4_model_customization/#example-1-rewriting-the-conditional-mnl-on-modecanada","title":"Example 1: Rewriting the conditional MNL on ModeCanada","text":"<p>We download the ModeCanada dataset as a ChoiceDataset, see here for more details.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom choice_learn.datasets import load_modecanada\n\ndataset = load_modecanada(as_frame=False, preprocessing=\"tutorial\", add_items_one_hot=False)\n</code></pre> <p>We will subclass the parent class ChoiceModel that we need to import. It mainly works with TensorFlow as a backend, it is thus recommended to use  their operation as much as possible. Most NumPy operations have a TensorFlow equivalent. You can look at the documentation here.</p> <p>For our custom model to work, we need to specify: - Weights initialization in init() - the utility function in compute_batch_utility()</p> <pre><code>import tensorflow as tf\nfrom choice_learn.models.base_model import ChoiceModel\n</code></pre>"},{"location":"notebooks/introduction/4_model_customization/#utility-formulation","title":"Utility formulation","text":"<p>Following the Conditional Logit tutorial we want to estimate the following utility function:  You can check the cLogit example for more details</p>"},{"location":"notebooks/introduction/4_model_customization/#coefficients-initialization","title":"Coefficients Initialization","text":"<p>Following our utility formula we need four coefficients vectors: - $\\beta^{inter}$ has 3 values - $\\beta^{price}$, $\\beta^{freq}$, $\\beta^{ovt}$ are regrouped and each has one value, shared by all items - $\\beta^{income}$ has 3 values - $\\beta^{ivt}$ has 4 values</p>"},{"location":"notebooks/introduction/4_model_customization/#utility-computation","title":"Utility Computation","text":"<p>In the method compute_utility, we need to define how to estimate each item utility for each choice using  the features and initialized weights. The arguments of the function are a batch of each features type of the ChoiceDataset class:</p> Order Argument shape Features for ModeCanada 2 shared_features_by_choice (batch_size, n_shared_features) Customer Income 3 items_features_by_choice (batch_size, n_items, n_items_features) Cost, Freq, Ivt, Ovt values of each mode 4 available_items_by_choice (batch_size, n_items) Not Used 5 choices (batch_size, ) Not Used <p>batch_size represents the number of choices given in the batch. The method needs to return the utilities, in the form of a matrix of shape (n_choices, n_items), representing the utility of each item for each choice.</p> <pre><code># You can verify the names and order of the features:\nprint(dataset.shared_features_by_choice_names, dataset.items_features_by_choice_names)\n</code></pre> <pre><code>class CustomCanadaConditionalLogit(ChoiceModel):\n    \"\"\"Conditional Logit following for ModeCanada.\n\n    Arguments:\n    ----------\n    optimizer : str\n        tf.keras.optimizer to use for training, default is Adam\n    lr: float\n        learning rate for optimizer, default is 1e-3\n    \"\"\"\n\n    def __init__(\n        self,\n        add_exit_choice=False, # Whether to add exit choice with utility=1\n        optimizer=\"lbfgs\", # Optimizer to use\n        lbfgs_tolerance=1e-8, # Absolute function tolerance for optimality if lbfgs is used\n        lr=0.001, # learning rate if stochastic gradient descent optimizer\n        epochs=1000, # maximum number of epochs\n        batch_size=32, # batch size if stochastic gradient descent optimizer\n    ):\n        \"\"\"Model coefficients instantiation.\"\"\"\n        super().__init__(add_exit_choice=add_exit_choice,\n                         optimizer=optimizer,\n                         lbfgs_tolerance=lbfgs_tolerance,\n                         lr=lr,\n                         epochs=epochs,\n                         batch_size=batch_size)\n\n        # Create model weights. Basically is one weight by feature + one for intercept\n        self.beta_inter = tf.Variable(tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, 3)),\n                                 name=\"beta_inter\")\n        self.beta_freq_cost_ovt = tf.Variable(\n            tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, 3)),\n            name=\"beta_freq_cost_ovt\"\n            )\n        self.beta_income = tf.Variable(tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, 3)),\n                             name=\"beta_income\")\n        self.beta_ivt = tf.Variable(tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, 4)),\n                               name=\"beta_ivt\")\n\n    # Do not forget to add them to the list of trainable_weights, it is mandatory !\n    @property\n    def trainable_weights(self):\n        \"\"\"Do not forget to add the weights to the list of trainable_weights.\n\n        It is needed to use the @property definition as here.\n\n        Return:\n        -------\n        list:\n            list of tf.Variable to be optimized\n        \"\"\"\n        return [self.beta_inter, self.beta_freq_cost_ovt, self.beta_income, self.beta_ivt]\n\n\n    def compute_batch_utility(self,\n                              shared_features_by_choice,\n                              items_features_by_choice,\n                              available_items_by_choice,\n                              choices):\n        \"\"\"Method that defines how the model computes the utility of a product.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Returns:\n        --------\n        np.ndarray\n            Utility of each product for each choice.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        _ = (available_items_by_choice, choices)  # Avoid unused variable warning\n\n        # Adding the 0 value intercept of first item to get the right shape\n        full_beta_inter = tf.concat([tf.constant([[.0]]), self.beta_inter], axis=-1)\n        # Concatenation to reach right shape for dot product\n        full_beta_income = tf.concat([tf.constant([[.0]]), self.beta_income], axis=-1)  # shape = (1, n_items)\n\n        items_ivt_by_choice = items_features_by_choice[:, :, 3] # shape = (n_choices, n_items, )\n        items_cost_freq_ovt_by_choice = items_features_by_choice[:, :, :3 ]# shape = (n_choices, n_items, 3)\n        u_cost_freq_ovt = tf.squeeze(tf.tensordot(items_cost_freq_ovt_by_choice,\n                                                  tf.transpose(self.beta_freq_cost_ovt), axes=1)) # shape = (n_choices, n_items)\n        u_ivt = tf.multiply(items_ivt_by_choice, self.beta_ivt) # shape = (n_choices, n_items)\n\n        u_income = tf.tensordot(shared_features_by_choice, full_beta_income, axes=1)  # shape = (n_choices, n_items)\n\n        # Reshaping the intercept that is constant over all choices (n_items, ) -&gt; (n_choices, n_items)\n        u_intercept = tf.concat([full_beta_inter] * (u_income.shape[0]), axis=0)\n        return u_intercept + u_cost_freq_ovt + u_income + u_ivt\n</code></pre> <pre><code>dataset.items_features_by_choice[0].shape\n</code></pre> <pre><code>model = CustomCanadaConditionalLogit()\nhistory = model.fit(dataset)\n</code></pre>"},{"location":"notebooks/introduction/4_model_customization/#decomposition-of-the-utility-operations","title":"Decomposition of the utility operations","text":""},{"location":"notebooks/introduction/4_model_customization/#intercept","title":"Intercept","text":"<ul> <li>$U_{inter}[air, s] = \\beta^{inter}_{air} = 0$</li> <li>$U_{inter}[bus, s] = \\beta^{inter}_{bus}$</li> <li>$U_{inter}[car, s] = \\beta^{inter}_{car}$</li> <li>$U_{inter}[train, s] = \\beta^{inter}_{train}$</li> </ul> <p>$\\beta^{inter} = \\left(\\right)$</p> <p>$U_{inter} = \\beta^{inter.T}$</p>"},{"location":"notebooks/introduction/4_model_customization/#price-freq-ovt","title":"Price, Freq, OVT","text":"<ul> <li>$U_{price, freq, ovt}[air, s] = \\beta^{price} \\cdot price[air, s] + \\beta^{freq} \\cdot freq[air, s] + \\beta^{ovt} \\cdot ovt[air, s]$</li> <li>$U_{price, freq, ovt}[bus, s] = \\beta^{price} \\cdot price[bus, s] + \\beta^{freq} \\cdot freq[bus, s] + \\beta^{ovt} \\cdot ovt[bus, s]$</li> <li>$U_{price, freq, ovt}[car, s] = \\beta^{price} \\cdot price[car, s) + \\beta^{freq} \\cdot freq[car, s] + \\beta^{ovt} \\cdot ovt(car, s]$</li> <li>$U_{price, freq, ovt}[train, s] = \\beta^{price} \\cdot price[train, s] + \\beta^{freq} \\cdot freq[train, s] + \\beta^{ovt} \\cdot ovt[train, s]$</li> </ul> <p>$\\beta^{price, freq, ovt} = \\left(\\right)$ and $items_feature_by_choice[0, :3] = \\left(\\right)$</p> <p>$U_{price, freq, ovt} = \\beta^{price, freq, ovt .T} \\cdot items_feature_by_choice[:, :3]$</p> <p>Note that in the matrix we didn't illustrate the choices dimension, explaining the [0, :3] -&gt; [:, :3]. items_features_by_choice[:, :3] has a shape of (batch_size, 4, 3) and $ \\beta^{price, freq, ovt}$ a shape of (1, 3). Resulting $U_{price, freq, ovt} $ has therefore a shape of (batch_size, 4)</p>"},{"location":"notebooks/introduction/4_model_customization/#ivt","title":"IVT","text":"<ul> <li>$U_{ivt}[air, s] = \\beta^{ivt}_{air} \\cdot ivt[air, s]$</li> <li>$U_{ivt}[bus, s] = \\beta^{ivt}_{bus} \\cdot ivt[bus, s]$</li> <li>$U_{ivt}[car, s] = \\beta^{ivt}_{car} \\cdot ivt[car, s]$</li> <li>$U_{ivt}[train, s] = \\beta^{ivt}_{train} \\cdot ivt[train, s]$</li> </ul> <p>$\\beta^{ivt} = \\left(\\right)$\\ and\\ $items_features_by_choice[:, 3] = \\left(\\right)$</p> <p>$U_{ivt} = \\beta^{ivt} * items_features_by_choice[:, 3]$ of shape (batch_size, 4)</p>"},{"location":"notebooks/introduction/4_model_customization/#income","title":"Income","text":"<ul> <li>$U_{income}[air, s] = \\beta^{income}_{air} \\cdot income[s]$</li> <li>$U_{income}[bus, s] = \\beta^{income}_{bus} \\cdot income[s]$</li> <li>$U_{income}[car, s] = \\beta^{income}_{car} \\cdot income[s]$</li> <li>$U_{income}[train, s] = \\beta^{income}_{train} \\cdot income[s]$</li> </ul> <p>$\\beta^{income} = \\left(\\right)$ and $shared_features = \\left(\\right)$</p> <p>$U_{income} = \\beta^{income .T} \\cdot shared_features$</p> <p>By concatenating batch_size times $U_{inter}$ over the choices we obtain 4 matrixes of shape (batch_size, 4).</p> <p>The final utility is then: $U = U_{inter} + U_{price, freq, ovt} + U_{ivt} + U_{income}$</p>"},{"location":"notebooks/introduction/4_model_customization/#results","title":"Results","text":"<p>We can now test that we obtain the same results:</p> <pre><code>print(model.trainable_weights[0])\nprint(model.trainable_weights[1])\nprint(model.trainable_weights[2])\nprint(model.trainable_weights[3])\n</code></pre> <pre><code>&lt;tf.Variable 'beta_inter:0' shape=(1, 3) dtype=float32, numpy=array([[0.6983521, 1.8441089, 3.2741907]], dtype=float32)&gt;\n&lt;tf.Variable 'beta_freq_cost_ovt:0' shape=(1, 3) dtype=float32, numpy=array([[-0.03333881,  0.09252932, -0.0430035 ]], dtype=float32)&gt;\n&lt;tf.Variable 'beta_income:0' shape=(1, 3) dtype=float32, numpy=array([[-0.08908677, -0.02799308, -0.03814653]], dtype=float32)&gt;\n&lt;tf.Variable 'beta_ivt:0' shape=(1, 4) dtype=float32, numpy=\narray([[ 0.05950957, -0.0067836 , -0.00646028, -0.00145035]],\n      dtype=float32)&gt;\n</code></pre> <p>The coefficients are organized differently but reach the same values. It is also the case for negative log-lilkelihood:</p> <pre><code>print(\"Total Neg LikeliHood;\", model.evaluate(dataset) * len(dataset))\n</code></pre> <pre><code>Total Neg LikeliHood; tf.Tensor(1874.363, shape=(), dtype=float32)\n</code></pre>"},{"location":"notebooks/introduction/4_model_customization/#example-2-defining-a-non-linear-utility-function-with-tensorflow","title":"Example 2: Defining a non-linear utility function with TensorFlow","text":"<p>In this example we have used a simple linear function for utility computation. We could use any function we would like. Particularly we can use neural networks and activation functions to add non-linearities.</p> <p>A simple example would be:</p> <pre><code>from tensorflow.keras.layers import Dense\n\nclass NeuralNetUtility(ChoiceModel):\n    def __init__(self, n_neurons, **kwargs):\n        super().__init__(**kwargs)\n        self.n_neurons = n_neurons\n\n        # Items Features Layer\n        self.dense_items_features = Dense(units=n_neurons, activation=\"elu\")\n\n        # Shared Features Layer\n        self.dense_shared_features = Dense(units=n_neurons, activation=\"elu\")\n\n        # Third layer: embeddings to utility (dense representation of features &gt; U)\n        self.final_layer = Dense(units=1, activation=\"linear\")\n\n    # We do not forget to specify self.trainable_weights with all coefficients that need to be estimated.\n    # Small trick using @property to acces future weights of layers\n    # that have not been instantiated yet !\n    @property\n    def trainable_weights(self):\n        \"\"\"Endpoint to acces model's trainable_weights.\n\n        Returns:\n        --------\n        list\n            list of trainable_weights\n        \"\"\"\n        return self.dense_items_features.trainable_variables\\\n              + self.dense_shared_features.trainable_variables\\\n                  + self.final_layer.trainable_variables\n\n    def compute_batch_utility(self,\n                              shared_features_by_choice,\n                              items_features_by_choice,\n                              available_items_by_choice,\n                              choices):\n        \"\"\"Computes batch utility from features.\"\"\"\n        _, _ = available_items_by_choice, choices\n        # We apply the neural network to all items_features_by_choice for all the items\n        # We then concatenate the utilities of each item of shape (n_choices, 1) into a single one of shape (n_choices, n_items)\n        shared_features_embeddings = self.dense_shared_features(shared_features_by_choice)\n\n        items_features_embeddings = []\n        for i in range(items_features_by_choice[0].shape[1]):\n            # Utility is Dense(embeddings sum)\n            item_embedding = shared_features_embeddings + self.dense_items_features(items_features_by_choice[:, i])\n            items_features_embeddings.append(self.final_layer(item_embedding))\n\n        # Concatenation to get right shape (n_choices, n_items, )\n        item_utility_by_choice = tf.concat(items_features_embeddings, axis=1)\n\n        return item_utility_by_choice\n</code></pre> <pre><code>model = NeuralNetUtility(n_neurons=10, optimizer=\"Adam\", epochs=200)\nhistory = model.fit(dataset)\n</code></pre> <pre><code>model.evaluate(dataset) * len(dataset)\n</code></pre> <p>If you want more complex examples, you can look at the following implementations: - RUMnet</p>"},{"location":"notebooks/introduction/1_introductive_example/","title":"Introductive Example","text":"<sup><sub> Introduction Notebook </sub></sup> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <p>Choice-Learn is a Python package designed to help building discrete choice models. In particular you will find:</p> <ul> <li>Optimized Data handling with the ChoiceDataset object and ready-to-use datasets</li> <li>Modelling tools with:<ul> <li>Efficient well-known choice models</li> <li>Customizable class ChoiceModel to build your own model</li> <li>Estimation options such as choosing the method (LBFGS, Gradient Descent, etc...)</li> </ul> </li> <li>Divers Tools revolving around choice models such as an Assortment Optimizer</li> </ul>"},{"location":"notebooks/introduction/1_introductive_example/#discrete-choice-modelling","title":"Discrete Choice Modelling","text":"<p>Discrete choice models aim at explaining or predicting a choice from a set of alternatives. Well known use-cases include analyzing people choice of mean of transport or products purchases in stores.</p> <p>If you are new to choice modelling, you can check this resource. </p>"},{"location":"notebooks/introduction/1_introductive_example/#tutorial","title":"Tutorial","text":"<p>In this notebook we will describe step-by-step the estimation of a choice model.</p> <ul> <li>Data Handling</li> <li>Modelling</li> </ul>"},{"location":"notebooks/introduction/1_introductive_example/#data","title":"Data","text":""},{"location":"notebooks/introduction/1_introductive_example/#items-features-and-choices","title":"Items, features and choices","text":"<p>The data structure for choice modelling is somehow different than usual prediction use-cases. We consider a set of variable size of different alternatives. Each alternative is described by features and one is chosen among the set. Some contexts features (describing a customer, or time) can also affect the choice. Let's take an example where we want to predict a customer's next purchase.</p> <p>Three different items, i<sub>1</sub>, i<sub>2</sub> and i<sub>3</sub> are sold and we have gathered a small dataset:</p> 1st Purchase: 2nd Purchase:3rd Purchase:   **Shelf**:  | Item           | Price   | Promotion | | -------------- | ------- | --------- | | i<sub>1</sub>  | $100    | no        | | i<sub>2</sub>  | $140    | no        | | i<sub>3</sub>  | $200    | no        |  **Customer Purchase:** i<sub>1</sub>   **Shelf**:  | Item           | Price   | Promotion | | -------------- | ------- | --------- | | i<sub>1</sub>  | $100    | no        | | i<sub>2</sub>  | $120    | yes       | | i<sub>3</sub>  | $200    | no        |  **Customer Purchase:** i<sub>2</sub>   **Shelf**:  | Item           | Price        | Promotion    | | -------------- | ------------ | ------------ | | i<sub>1</sub>  | $100         | no           | | i<sub>2</sub>  | Out-Of-Stock | Out-Of-Stock | | i<sub>3</sub>  | $180         | yes          |  **Customer Purchase:** i<sub>3</sub> <p>Indexing the items in the same order, we create the ChoiceDataset as follows:</p> <pre><code>choices = [0, 1, 2] # Indexes of the items chosen\n\nitems_features_by_choice =  [\n    [\n        [100., 0.], # choice 1, Item 1 [price, promotion]\n        [140., 0.], # choice 1, Item 2 [price, promotion]\n        [200., 0.], # choice 1, Item 3 [price, promotion]\n    ],\n    [\n        [100., 0.], # choice 2, Item 1 [price, promotion]\n        [120., 1.], # choice 2, Item 2 [price, promotion]\n        [200., 0.], # choice 2, Item 3 [price, promotion]\n    ],\n    [\n        [100., 0.], # choice 3, Item 1 [price, promotion]\n        [120., 1.], # choice 3, Item 2 [price, promotion]\n        [180., 1.], # choice 3, Item 3 [price, promotion]\n    ],\n]\n</code></pre> <p>Item i<sub>2</sub> was out of stock during the last choice. Thus it could not have been chosen. In order to keep this information we create a matric indicating which items were available during each of the choices:</p> <pre><code>available_items_by_choice = [\n    [1, 1, 1], # All items available for choice 1\n    [1, 1, 1], # All items available for choice 2\n    [1, 0, 1], # Item 2 not available for choice 3\n]\n</code></pre> <p>And now let's create the ChoiceDataset! We can also specify the features names if we want to.</p> <pre><code>from choice_learn.data import ChoiceDataset\n\ndataset = ChoiceDataset(\n    choices=choices,\n    items_features_by_choice=items_features_by_choice,\n    items_features_by_choice_names=[\"price\", \"promotion\"],\n    available_items_by_choice=available_items_by_choice,\n)\n</code></pre>"},{"location":"notebooks/introduction/1_introductive_example/#modelling","title":"Modelling","text":""},{"location":"notebooks/introduction/1_introductive_example/#estimation-and-choice-probabilities","title":"Estimation and choice probabilities","text":"<p>A first and simple model to predict a customer choice is the Multinomial Logit.</p> <p>We consider that customers attribute a utility to each product and that he chooses the product with hightest utility.</p> <p>We formulate the utility as a linear function of our features:</p> <p> </p> <p>Considering that this estimation is noisy, we use the softmax function over the available products to get the purchase probability. For example using our first data sample we obtain:</p> <p> </p> <p>For the third sample only two items are still available, making the probability:  </p> <p>The parameters $\\alpha_i$, $\\beta$ and $\\gamma$ are estimated by maximizing the Negative Log-Likelihood. Here is how it goes with Choice-Learn:</p> <pre><code>from choice_learn.models import SimpleMNL\n\nmodel = SimpleMNL(intercept=\"item\")\nhistory = model.fit(dataset)\n</code></pre> <p>To access the weights estimation:</p> <pre><code>print(\"Features coefficients are:\")\nprint(model.trainable_weights[0])\nprint(\"Items intercepts:\")\nprint([0], \"and\", model.trainable_weights[1])\n</code></pre> <pre><code>Features coefficients are:\n&lt;tf.Variable 'Weights_items_features:0' shape=(2,) dtype=float32, numpy=array([-0.37710273, 40.983475  ], dtype=float32)&gt;\nItems intercepts:\n[0] and &lt;tf.Variable 'Intercept:0' shape=(2,) dtype=float32, numpy=array([-11.027451,  12.578588], dtype=float32)&gt;\n</code></pre> <p>In order to compute the average Negative Log-Likelihood of the model, we can use the following code:</p> <pre><code>model.evaluate(dataset)\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.001363e-05&gt;\n</code></pre> <p>You can now acces estimated choice probabilities using a ChoiceDataset:</p> <pre><code>probabilities = model.predict_probas(dataset)\nprint(\"Probabilities are:\")\nprint(probabilities)\n</code></pre> <pre><code>Probabilities are:\ntf.Tensor(\n[[9.9998999e-01 4.5697122e-12 1.2174261e-11]\n [1.8438762e-10 9.9998999e-01 2.2448054e-21]\n [6.9211727e-11 0.0000000e+00 9.9998999e-01]], shape=(3, 3), dtype=float32)\n</code></pre>"},{"location":"notebooks/introduction/1_introductive_example/#useful-jupyter-notebooks","title":"Useful Jupyter Notebooks","text":"<p>If you want to go further, here are a few useful Jupyter Notebooks:</p> <p>Data: - A more complete example here - A detailed use of FeaturesByIDs here if you want to minimize your RAM footprint</p> <p>Modelling: - A more complete example using the Conditional-MNL here - An example to easily build custom models here</p> <p>Tools: - An example of assortment optimization using a choice model and Gurobi here</p> <p>Here are complementary Notebooks that might interest you: - A comparison with the R package mlogit here - A reconstruction of the experiments of the RUMnet paper here - An example of estimation of a Latent Class MNL here - An example of estimation of the Nested Logit model here - A reconstruction using Choice-Learn of scikit-learn's Logistic Regression tutorial here</p>"},{"location":"notebooks/introduction/1_introductive_example/#documentation","title":"Documentation","text":"<p>The full documentation also hosts a lot of useful details and information.</p>"},{"location":"notebooks/introduction/1_introductive_example/#additional-questions-requests-etc","title":"Additional Questions, Requests, etc...","text":"<p>If you have ideas, questions, features request or any other input, do not hesitate to reach out by opening an issue on GitHub.</p>"},{"location":"notebooks/introduction/2_data_handling/","title":"Introduction to choice-learn's data management","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#an-introduction-to-choicedataset","title":"An introduction to ChoiceDataset","text":"<p>Choice-Learn's ChoiceDataset aims at being able to handle large datasets, typically by limiting the usage of memory to store several times the same feature. Its structure is made to fit a choice modelling setup it is needed to estimate choice models weights.  This notebook introduces how the package handles data. Here is a summary of the different points that will be adressed:</p>"},{"location":"notebooks/introduction/2_data_handling/#summary","title":"Summary","text":"<ul> <li>Introduction<ul> <li>Example dataset: SwissMetro</li> <li>The different types of data</li> </ul> </li> <li>ChoiceDataset's Instantiation from a single DataFrame<ul> <li>Wide format</li> <li>Long format</li> </ul> </li> <li>ChoiceDataset's Instantiation from separate objects<ul> <li>Specifying each data type</li> <li>Stacking features</li> </ul> </li> <li>How to use the ChoiceDataset?<ul> <li>Estimating choice models</li> <li>Slicing into batches</li> </ul> </li> <li>Further optimizing RAM usage<ul> <li>The FeaturesStorage object</li> <li>Complete example of FeaturesStorage use</li> <li>Specific example of the OneHotStorage</li> </ul> </li> <li>Additional Examples<ul> <li>The ModeCanada Dataset</li> <li>A step-by-step FeaturesStorage use</li> </ul> </li> <li>List of Ready-To-Use datasets</li> <li>References</li> </ul> <pre><code>from choice_learn.data import ChoiceDataset\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#our-example-dataset-swissmetro","title":"Our example dataset: SwissMetro","text":"<p>The SwissMetro[2] is a well-known dataset used to illustrate choice modelling. The dataset is provided with the Choice-Learn package and can be downloaded as follows:</p> <pre><code>from choice_learn.datasets import load_swissmetro\n\nswissmetro_df = load_swissmetro(as_frame=True)\n</code></pre> <p>The SwissMetro is a collection of answers to a survey about mode transportation choice in Switzerland. Before building a costly new public transport line the government decided to better understand the needs of the future customers. A complete description of the dataset and the columns can be found here. We will use a subset of the information during the tutorial.</p> <p>Available Modes: - The current, existing train, 'TRAIN' - The potentially future SwissMetro, 'SM' - The customer car, 'CAR'</p> <p>Columns: - PURPOSE: What is the customer's travel purpose: - AGE: The customer's age category - mode_AV: Whether the mode is available (1) or not (0) - mode_TT: The mode travel time - mode_CO: The mode cost - CHOICE: the transport mode chosen by the customer</p> <pre><code>kept_columns = [\"PURPOSE\", \"AGE\", \"ORIGIN\", \"CAR_AV\", \"TRAIN_AV\", \"SM_AV\", \"CAR_TT\",\n                \"TRAIN_TT\", \"SM_TT\", \"CAR_CO\", \"TRAIN_CO\", \"SM_CO\", \"CHOICE\"]\nswissmetro_df = swissmetro_df[kept_columns]\nswissmetro_df.head()\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#the-different-type-of-data","title":"The different type of data","text":"<p>We can split the columns into three distincts categories that are common to most choice modelling use-cases:</p> <ul> <li>Choices - or outputs of our model: it's what we want to predict</li> <li>Features - or inputs of our model</li> <li>Availabilities - or the description of the set among which the customer chooses</li> </ul> <p>Going further, we have two types of features: the features describing the customer and the features describing the mean of transportation. Those are the four types of data that can be specified in a ChoiceDataset.</p> <p>Vocabulary:</p> <p>Items represent a product, an alternative that can be chosen by the customer at some point.</p> <p>Throughout Choice-Learn examples and code here is the naming of our four types of data:</p> <ul> <li> <p>choices: which item has been chosen among all availables</p> </li> <li> <p>shared_features_by_choice: It represents all the features that might change from one choice to another and that are common to all items (e.g. day of week, customer features, etc...).</p> </li> <li> <p>items_features_by_choice: The features each of the available item for a choice (e.g. prices might change from one choice to another and are specific to each sold item).</p> </li> <li> <p>available_items_by_choice: For each choice it represents whether each item is proposed to the customer (1.) or not (0.).</p> </li> </ul> <p>Summary:</p> index feature typical shape Example Taken Values 1 shared_features_by_choice (n_choices, n_features) customer age, day of week float, int 2 items_features_by_choice (n_choices, n_items, n_items_features) price float, int 3 available_items_by_choice (n_choices, n_items) 1.(av) or 0. (not av.) 4 choices (n_choices,) int: index of chosen item <p></p>"},{"location":"notebooks/introduction/2_data_handling/#hands-on-example-from-a-pandas-dataframe","title":"Hands-on: example from a pandas' DataFrame","text":"<p>The easiest way create a ChoiceDataset is to use a pandas DataFrame.</p> <p>First, here is a small explanation about wide vs long format, in case you have never heard about it, from Wikipedia.</p> <p>Long (or narrow) Format:  One column containing all the values and another column listing the context of the value\\ Wide Format: Each different data variable in a separate column.</p> Example Long Format: Example Wide Format:   | choice id | item | price | availability | choice | |---|---|---|---|---| | 1 | A | 2.0 | 1 | 1 | | 1 | B | 6.0 | 1 | 0 | | 2 | A | 1.5 | 1 | 0 | | 2 | B | 5.5 | 1 | 1 |    | choice id | price_A | price_B | availability_A | availability_B | choice | |---|---|---|---|---|---| | 1 | 2.0 | 6.0 | 1 | 1 | A | | 2 | 1.5 | 5.5 | 1 | 1 | B |   <p>Choice-Learn handles both formats, but slightly differently: - example for wide format - example for long format</p>"},{"location":"notebooks/introduction/2_data_handling/#creating-a-choicedataset-from-a-wide-dataframe","title":"Creating a ChoiceDataset from a wide DataFrame","text":"<p>Our example dataframe on SwissMetro is on the wide format. Each row indicates a choice and each item has its specific features columns.</p> <pre><code>dataset = ChoiceDataset.from_single_wide_df(\n    # The main DataFrame\n    df=swissmetro_df,\n    # The names of the items, will be used to find columns and organize them\n    items_id=[\"TRAIN\", \"SM\", \"CAR\"],\n\n    # The column containing the choices\n    choices_column=\"CHOICE\",\n    # How the choices are encoded: item_index means that the choice is the index of the item in the items_id list\n    choice_format=\"items_index\",\n\n    # Columns for shared_features_by_choice\n    shared_features_columns=[\"PURPOSE\", \"AGE\"],\n\n    # Columns for items_features_by_choice\n    # They will be reconstructed as item_id + delimiter + feature_suffix\n    items_features_suffixes=[\"CO\", \"TT\"],\n    # Same with availabilities\n    available_items_suffix=\"AV\",\n    delimiter=\"_\",\n)\n</code></pre> <p>Options</p> <p>choice_format: \"items_index\" or \"items_id\"</p> \"items_index\" \"items_id\"   | choice_column| |---| | 0 | | 1 | | 0 | | 2 |    | choice_column| |---| | \"TRAIN\" | | \"SM\" | | \"TRAIN\" | | \"CAR\" |   <p>items_features_by_choice and available_items_by_choice:</p> <p>It is possible to precise: - Suffixes: in this case the column used will be \"item_id\" + \"delimiter\" + \"suffix\" - Prefixes: in this case the column used will be \"prefix\" + \"delimiter\" + \"item_id\" - Columns: each item's features in list. In this case it is you duty to ensure coherence in terms of items and features orders. For our example it would be:</p> <pre><code>```python\nitems_features_by_choice_columns=[[\"TRAIN_CO\", \"TRAIN_TT\"], [\"SM_CO\", \"SM_TT\"], [\"CAR_CO\", \"CAR_TT\"]],\navailable_items_by_choice_columns=[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"],\n```\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#creating-a-choicedataset-from-a-long-dataframe","title":"Creating a ChoiceDataset from a long DataFrame","text":"<p>The long format is also commonly used in which each row represents an alternative. One of its benefits is represent unavailability through missing rows - taking litteraly zero memory space. On the contrary the 'shared_features' such as customer features must be duplicated on each row.\\ The ChoiceDataset object can be instantiated from a long DF. It will infer the availabilities from existing/missing rows, if it is not specified.\\ It is needed to precise: - columns representing the features ('shared_features_columns' and 'items_features_columns') - the column in which the choice is given and how it is formatted ('choices_columns' and 'choice_format') - which column can identify the items ('items_id_column') - which column can identify all the rows corresponding to the same choice ('choices_id_column')</p> <pre><code># Transformation of our dataset on the long format\nlong_df = load_swissmetro(preprocessing=\"long_format\")\nlong_df.head()\n</code></pre> <pre><code># Example of the long format instantiation\ndataset = ChoiceDataset.from_single_long_df(\n    df=long_df,\n    items_id_column=\"item_id\",\n    choices_id_column=\"choice_id\",\n\n    shared_features_columns=[\"PURPOSE\", \"AGE\"],\n    items_features_columns=[\"TT\", \"CO\"],\n\n    choices_column=\"CHOICE\",\n    choice_format=\"one_zero\")\n</code></pre> <p>Options</p> <p>choice_format: \"one_zero\" or \"items_id\"</p> \"one_zero\" \"items_id\"   | choice_id_column | item_id_column | choice_column| |---|---|---| | 1 | \"CAR\" | 0 | | 1 | \"SM\" | 1 | | 2 | \"CAR\" | 1 | | 2 | \"SM\" | 0 |    | choice_id_column | item_id_column | choice_column| |---|---|---| | 1 | \"CAR\" | \"SM\" | | 1 | \"SM\" | \"SM\" | | 2 | \"CAR\" | \"CAR\" | | 2 | \"SM\" | \"CAR\" |"},{"location":"notebooks/introduction/2_data_handling/#instantiation-from-different-objects","title":"Instantiation from different objects","text":"<p>For RAM optimization purposes or just because of the format of the data source, it might happen that a dataset is split into separate files. You can instantiate a ChoiceDataset keeping this structure, saving time to concatenate everything.\\ You can work either with pandas.DataFrames or numpy.ndarrays.</p>"},{"location":"notebooks/introduction/2_data_handling/#separating-data-types","title":"Separating data types","text":"<p>The four distinct data types: choices, shared_features_by_choice, items_features_by_choice, available_items_by_choice can be manually given to the ChoiceDataset:</p> <pre><code># Using pandas.DataFrames\ndataset = ChoiceDataset(\n    choices=swissmetro_df[\"CHOICE\"],\n    shared_features_by_choice=swissmetro_df[[\"PURPOSE\", \"AGE\"]],\n    items_features_by_choice=long_df[[\"choice_id\", \"item_id\", \"CO\", \"TT\"]]\n)\n</code></pre> <p>Note that if you pass items_features_by_choice as a pandas.DataFrame, it needs to be in the long format and with the columns 'choice_id' and 'item_id'. They will be used to get the features in the right order.</p> <pre><code># Using numpy.ndarrays\n# Be aware of items_features_by_choices shape that is (n_choices, n_items, n_features)\n\nitems_features_by_choice = np.stack([swissmetro_df[[\"TRAIN_CO\", \"TRAIN_TT\"]].to_numpy(),\n                                     swissmetro_df[[\"SM_CO\", \"SM_TT\"]].to_numpy(),\n                                     swissmetro_df[[\"CAR_CO\", \"CAR_TT\"]].to_numpy()],\n                                     axis=1)\nshared_features_by_choice = swissmetro_df[[\"PURPOSE\", \"AGE\"]].to_numpy()\navailable_items_by_choice = swissmetro_df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]].to_numpy()\n\nprint(\"The data shapes are:\")\nprint(f\"choices: {swissmetro_df['CHOICE'].shape}\")\nprint(f\"shared_features_by_choice: {shared_features_by_choice.shape}\")\nprint(f\"items_features_by_choice: {items_features_by_choice.shape}\")\nprint(f\"available_items_by_choice: {available_items_by_choice.shape}\")\n\ndataset = ChoiceDataset(\n    choices=swissmetro_df[\"CHOICE\"].to_numpy(),\n    shared_features_by_choice=shared_features_by_choice,\n    items_features_by_choice=items_features_by_choice,\n    available_items_by_choice=available_items_by_choice,\n\n    # Features names can optionally be provided\n    # the structure of data and names must match\n    shared_features_by_choice_names=[\"PURPOSE\", \"AGE\"],\n    items_features_by_choice_names=[\"CO\", \"TT\"],\n)\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#stacking-features","title":"Stacking Features","text":"<p>It is allowed to specify more than one features object by wrapping them in a tuple. This structure is kept inside the ChoiceDataset object as well as with the slicing into batches.</p> <pre><code># Using pandas.DataFrames - Similar with np.ndarrays\ndataset = ChoiceDataset(\n    choices=swissmetro_df[\"CHOICE\"],\n    shared_features_by_choice=(swissmetro_df[[\"PURPOSE\"]], swissmetro_df[[\"AGE\"]]),\n    items_features_by_choice=(long_df[[\"choice_id\", \"item_id\", \"CO\"]],\n                              long_df[[\"choice_id\", \"item_id\", \"TT\"]]),\n    available_items_by_choice=swissmetro_df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]],\n)\n</code></pre> <p>Other examples are provided here.</p>"},{"location":"notebooks/introduction/2_data_handling/#using-the-choicedataset-object","title":"Using the ChoiceDataset object","text":""},{"location":"notebooks/introduction/2_data_handling/#estimating-choice-models","title":"Estimating choice models","text":"<p>With your ChoiceDataset instantiated, it can be used as is to fit choice models. An illustration can be found in the conditional MNL introduction notebook.</p>"},{"location":"notebooks/introduction/2_data_handling/#slicing-and-batching","title":"Slicing and batching","text":"<p>ChoiceDatasets are indexed by choice, meaning that accessing the i-th index corresponds to the i-th choice. Differently said it is the i-th value of the object given as 'choices' in the ChoiceDataset instantiation.</p> <p>A ChoiceDataset can be sliced commonly using the [.] Python method:</p> <pre><code>sub_dataset = dataset[[0, 2, 4]]\n</code></pre> <p>sub_dataset will be a ChoiceDataset containing only the 0th, 2nd and 4th choice of dataset. The other data (shared_featurs_by_choice, items_features_by_choice and available_items_by_choice) are also kept and sliced accordingly.</p> <p>In order to only get a chunk of data, it is possible to use .batch[.]. It will return the different data types sliced along choices in a raw np.ndarray format. Use .iter_batch() to iterate over all data in the ChoiceDataset by setting the batch_size argument to control the length of each chunk.</p> <p>Also note that <code>batch_size=-1</code>returns the whole dataset</p> <pre><code>dataset.choices\n</code></pre> <pre><code>batch = dataset.batch[[0, 2, 4]]\nprint(\"\")\nfor batch in dataset.iter_batch(batch_size=1024):\n    print(\"Num choices:\", len(batch[-1]))\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#more-advanced-use-the-featurestorage-ram-optimization","title":"More Advanced use: the FeatureStorage &amp; RAM optimization","text":"<p>In many use-cases we will see features or group of features values being repeated over the dataset. For example if one customer comes several times, its features will be repeated. With One-Hot representations, it can create memory-heavy repetitions.\\ Choice-Learn introduces FeaturesStorage and FeaturesByIds in order to limit the memory usage before accessing a batch of data.</p>"},{"location":"notebooks/introduction/2_data_handling/#featuresstorage-why-should-i-use-it","title":"FeaturesStorage, why should I use it ?","text":"<p>If you are not using a large dataset with many features you can pass this part. FeaturesStorage are here if you want to further optimize your memory consumption and if you take some time to understand it.\\ It is mainly built to work well with ChoiceDataset, but here is a small introduction on how it works.</p> <p></p>"},{"location":"notebooks/introduction/2_data_handling/#example-on-the-swissmetro-dataset","title":"Example on the SwissMetro dataset","text":"<p>/!\\ Disclaimer For the sake of the example, some features will be introduced and created. They are totally made up and do not exist in the original - and true - version of the SwissMetro Dataset.</p> <p>Let's consider the survey that happened in the three cantons: Geneva, Berne and Z\u00fcrich. Now we want to integrate localization features.</p> Canton Inhabitants (M) Surface (km^2) Origin Code Geneva 0.5 282 25 Z\u00fcrich 1.5 1729 1 Berne 1.0 5959 2 <p>A naive way to integreate those features is to add them as 'shared_features_by_choice'.</p> <pre><code># Filtering cantons\nswiss_df = swissmetro_df.loc[swissmetro_df.ORIGIN.isin([1, 2, 25])]\n\n# Adding features\nswiss_df.loc[:, \"CANTON_SURFACE\"] = swiss_df.apply(lambda row: {1: 1729, 2: 5959, 25: 282}[row.ORIGIN], axis=1)\nswiss_df[\"CANTON_INHAB\"] = swiss_df.apply(lambda row: {1: 1.5, 2: 1.0, 25: 0.5}[row.ORIGIN], axis=1)\n\ndataset = ChoiceDataset.from_single_wide_df(\n    df=swiss_df,\n    items_id=[\"TRAIN\", \"SM\", \"CAR\"],\n\n    choices_column=\"CHOICE\",\n    choice_format=\"items_index\",\n\n    # The new features are added here compared to example above\n    shared_features_columns=[\"PURPOSE\", \"AGE\", \"CANTON_SURFACE\", \"CANTON_INHAB\"],\n    items_features_suffixes=[\"CO\", \"TT\"],\n    available_items_suffix=\"AV\",\n    delimiter=\"_\",\n)\n</code></pre> <p>The main caveat is that the same features are repeated over the rows of the dataset. If we consider hundreds of stores on several millions - or billions - of choices, it would become... unreasonable!\\ One idea is to regroup the features behind an ID (the canton id for example) and to reconstruct the features only in batches.</p> <pre><code>from choice_learn.data import FeaturesStorage\n\norigin_canton_features = {1: [1.5, 1729], 2: [1.0, 5959], 25: [0.5, 282]}\ncanton_storage = FeaturesStorage(values=origin_canton_features, name=\"ORIGIN\") # Remark that the name matches the ID column name in the DF\n</code></pre> <p>The FeaturesStorage is basically a Python dictionnary with a wrap-up to easily get batches of data.\\ You can ask for a sequence of features with .batch. It works with the keys of our dictionnary that can be int, float, str, etc...</p> <pre><code>print(\"Retrieving features of canton id 1:\")\nprint(canton_storage.batch[1])\nprint(\"Retrieving a batch of features:\")\nprint(canton_storage.batch[[1, 25, 1]])\n</code></pre> <p>The FeaturesStorage is handy for its transparent use with ChoiceDataset. For it to work well it is needed to: - specify a FeaturesStorage name that matches the feature names given to the ChoiceDataset - match FeaturesStorage ids with the sequence (types and values) - specify the FeaturesStorage objects listed with the features_by_ids argument</p> <p>In our case we call our FeaturesStorage \"canton_storage\", the ids are now strings, let's make the sequence match:</p> <pre><code>storage_dataset = ChoiceDataset(choices=swiss_df[\"CHOICE\"],\n                                items_features_by_choice=np.stack([swiss_df[[\"TRAIN_CO\", \"TRAIN_TT\"]].to_numpy(),\n                                                                   swiss_df[[\"SM_CO\", \"SM_TT\"]].to_numpy(),\n                                                                   swiss_df[[\"CAR_CO\", \"CAR_TT\"]].to_numpy()],\n                                                                   axis=1),\n                                shared_features_by_choice=swiss_df[[\"AGE\", \"PURPOSE\", \"ORIGIN\"]].to_numpy(),\n                                features_by_ids=[canton_storage],\n                                items_features_by_choice_names=[\"CO\", \"TT\"],\n                                shared_features_by_choice_names=[\"AGE\", \"PURPOSE\", \"ORIGIN\"],\n)\n</code></pre> <p>Looking at a batch of data, here is how it looks like:</p> <pre><code># batching the dataset as before\nbatch = storage_dataset.batch[[1, 2, 3]]\nprint(\"Batch of shared_features_by_choice:\", batch[0])\nprint(\"Batch of choices:\", batch[3])\n</code></pre> <p>The features stored in the FeaturesStorage have been stacked with the usual 'shared_features_by_choice' !</p>"},{"location":"notebooks/introduction/2_data_handling/#specific-case-of-the-onehot-storage","title":"Specific case of the OneHot Storage","text":"<p>Manually looking for canton features is quiet time consuming. Another idea is to represent each canton by a unique one-hot vector. A recurring usecase is the use of OneHot representation of features. The OneHotStorage is built specifically for one-hot encoded features and further improves memory consumption. The storage is to be used the same way as FeaturesStorage, but behind will only keep the index of the one of each element and will consitute the one-hot vector only when needed.\\ In order terms it stores a sparse version of the vectors and returns a dense representation when batched.</p> <pre><code>from choice_learn.data import OneHotStorage\n</code></pre> <pre><code>storage = OneHotStorage(ids=swissmetro_df.ORIGIN.unique())\n\nprint(\"RAM storage of the OneHotStore:\", storage.storage)\n# When indexing with .batch, we can access the one-hot encoding of the element using its id\nprint(\"One-hot vector batch: storage.batch[2]\", storage.batch[2])\nprint(\"One-hot vector batch: storage.batch[[5, 20, 18, 23, 25, 15,  5, 20]]\")\nprint(storage.batch[[5, 20, 18, 23, 25, 15,  5, 20]])\n</code></pre> <p>Other examples of features_by_ids usage can be found here.</p>"},{"location":"notebooks/introduction/2_data_handling/#additional-examples","title":"Additional Examples","text":""},{"location":"notebooks/introduction/2_data_handling/#the-modecanada-dataset","title":"The ModeCanada dataset","text":"<p>We will use the ModeCanada [1] dataset for this example. The dataset is originally in the long format. It is provided with the choice-learn package and can loaded as follows:</p> <pre><code>from choice_learn.datasets import load_modecanada\n\ncanada_transport_df = load_modecanada(as_frame=True)\ncanada_transport_df.head()\n</code></pre> <p>An extensive description of the dataset can be found here. An extract indicates:</p> <p>\"The dataset was assembled in 1989 by VIA Rail (the Canadian national rail carrier) to estimate the demand for high-speed rail in the Toronto-Montreal corridor. The main information source was a Passenger Review administered to business travelers augmented by information about each trip. The observations consist of a choice between four modes of transportation (train, air, bus, car) with information about the travel mode and about the passenger. The posted dataset has been balanced to only include cases where all four travel modes are recorded. The file contains 11,116 observations on 2779 individuals. \"</p> <p>Alright ! If we go back to our dataframe, we can see the following columns:</p> <p>case: an ID of the traveler alt: the alternative concerned by the row choice: 1 if the alternative was chosen, 0 otherwise dist: trip distance cost: trip cost ivt: travel time in-vehicule (minutes) ovt: travel time out-vehicule (minutes) income: housold income of traveler ($) urban: 1 if origin or destination is a large city noalt: the number of alternative among which the traveler had to chose freq: the frequence of the alternative (0 for car) (e.g. how many train by hour) Following our specification, we can see that one case corresponds to one customer thus one choice. In our choice-learn language it corresponds to \"one context\": a set of available alternatives and their features/specificites resulting in one choice. Let's regroup our features:</p> <p>choices: Easy ! It is the alternative whenever the value is one.</p> <p>shared_features_by_choice: The income, urban and distance (also noalt which is not really a feature) features are the same for all the alternatives within a single choice. They are all constant with respect to (case=traveler_ID).</p> <p>items_features_by_choice: Ivt, Ovt, cost and freq depends on and describe each of the alternative.</p> <p>available_items_by_choice: It in not directly indicated, however it can be easily deduced. Whenever an alternative is not available, it is not precised for its case. For example for the case=1, our first choice, only train and car are given as alternatives, meaning that air and bus could not be chosen/were not available.</p> <pre><code>dataset = ChoiceDataset.from_single_long_df(\n    df=canada_transport_df,\n    choices_column=\"choice\",\n    items_id_column=\"alt\",\n    choices_id_column=\"case\",\n    shared_features_columns=[\"income\", \"urban\", \"dist\"],\n    items_features_columns=[\"cost\", \"freq\", \"ovt\", \"ivt\"],\n    choice_format=\"one_zero\")\n</code></pre> <p>In this example the 'choice_format' is \"one_zero\" while it was \"item_id\" in our previous SwissMetro example. As a short memento it specifies how the chosen alternative is precised: with ones (chosen) and zeros (not chosen) or directlu with the item_id of the chosen item.</p> \"one_zero\" \"item_id\"    | | case | alt | choice | dist | cost | ivt | ovt | freq |  income | |---|---|---|---|---|---|---|---|---|---| | 1 | 1 | train | 0 | 83 | 28.25 | 50 | 66 | 4 | 45 | | 2 | 1 | car | 1 | 83 | 15.77 | 61 | 0 | 0 | 45 | | 3 | 2 | train | 0 | 83 | 28.25 | 50 | 66 | 4 | 25 | | 4 | 2 | car | 1 | 83 | 15.77 | 61 | 0 | 0 | 25 | | 5 | 3 | train | 0 | 83 | 28.25 | 50 | 66 | 4 | 70 |     | | case | alt | choice | dist | cost | ivt | ovt | freq |  income | |---|---|---|---|---|---|---|---|---|---| | 1 | 1 | train | car | 83 | 28.25 | 50 | 66 | 4 | 45 | | 2 | 1 | car | car | 83 | 15.77 | 61 | 0 | 0 | 45 | | 3 | 2 | train | car | 83 | 28.25 | 50 | 66 | 4 | 25 | | 4 | 2 | car | car | 83 | 15.77 | 61 | 0 | 0 | 25 | | 5 | 3 | train | car | 83 | 28.25 | 50 | 66 | 4 | 70 |   <p>In the first 5 examples, the chosen transportation is always the car.</p> <p>That's it !</p>"},{"location":"notebooks/introduction/2_data_handling/#a-manual-example-of-featuresstorage","title":"A manual example of FeaturesStorage","text":"<p>Let's consider a case where we consider three supermarkets:  - supermarket_1 with surface of 100 and 250 average nb of customers - supermarket_2 with surface of 150 and 500 average nb of customers - supermarket_3 with surface of 80 and 100 average nb of customers </p> <p>In each store, we have 4 available products for which we have little information. For the example'sake, let's consider the following utility:  With $S_s$ the surface of the store and $C_s$ its average number of customers.</p> <p>We want to estimate the base utilities $u_i$ and the two coefficients: $\\beta_1$ and $\\beta_2$.</p> <p>Let's start with creating a ChoiceDataset without the FeaturesStorage:</p> <p>Let's consider a case where we consider three supermarkets:  - supermarket_1 with surface of 100 and 250 average nb of customers - supermarket_2 with surface of 150 and 500 average nb of customers - supermarket_3 with surface of 80 and 100 average nb of customers </p> <p>In each store, we have 4 available products for which we have little information. For the example'sake, let's consider the following utility:  With $S_s$ the surface of the store and $C_s$ its average number of customers.</p> <p>We want to estimate the base utilities $u_i$ and the two coefficients: $\\beta_1$ and $\\beta_2$.</p> <p>Let's start with creating a ChoiceDataset without the FeaturesStorage:</p> <pre><code># Here are our choices:\nchoices = [0, 1, 2, 0, 2, 1, 1, 0, 2, 1, 2, 0, 2, 0, 1, 2, 1, 0]\nsupermarket_features = [[100, 250], [150, 500], [80, 100]]\n# Now our store sequence of supermarkets is:\nsupermarkets_sequence = [1, 1, 2, 3, 2, 1, 2, 1, 1, 2, 3, 2, 1, 2, 2, 3, 1, 2]\n\n# The usual way to store the features would be to create the contexts_features array that contains\n# the right features:\nusual_supermarket_features = np.array([supermarket_features[supermarket_id - 1] for supermarket_id in supermarkets_sequence])\nprint(\"Usual Supermakerket Features Shape:\", usual_supermarket_features.shape)\n</code></pre> <pre><code>Usual Supermakerket Features Shape: (18, 2)\n</code></pre> <p>Supermarket features being repeated several times, it's a great opportunity to use a FeaturesStorage !\\ Let's see how to use strings as IDs.</p> <pre><code>features_dict = {f\"supermarket_{i+1}\": supermarket_features[i] for i in range(3)}\nstorage = FeaturesStorage(values=features_dict, name=\"supermarket_features\")\n</code></pre> <pre><code>print(\"Retrieving features of first supermarket:\")\nprint(storage.batch[\"supermarket_1\"])\nprint(\"Retrieving a batch of features:\")\nprint(storage.batch[[\"supermarket_1\", \"supermarket_2\", \"supermarket_1\"]])\n</code></pre> <pre><code>Retrieving features of first supermarket:\n[100 250]\nRetrieving a batch of features:\n[[100 250]\n [150 500]\n [100 250]]\n</code></pre> <p>Reminder:</p> <p>It is needed to: - specify a FeaturesStorage name - match FeaturesStorage ids with the sequence</p> <p>In our case we call our FeaturesStorage \"supermarket_features\", the ids are now strings, let's make the sequence match:</p> <pre><code>str_supermarkets_sequence = [[f\"supermarket_{i}\"] for i in supermarkets_sequence]\n</code></pre> <p>And now we can create our ChoiceDataset:</p> <pre><code>storage_dataset = ChoiceDataset(choices=choices,\n                                shared_features_by_choice=str_supermarkets_sequence,\n                                shared_features_by_choice_names=[\"supermarket_features\"],\n                                available_items_by_choice=np.ones((len(choices), 3)),\n                                features_by_ids=[storage],\n)\n</code></pre> <p>And now let's see how batches work:</p> <pre><code>batch = storage_dataset.batch[0]\nprint(\"Batch Shared Items Features:\", batch[0])\nprint(\"Batch Items Features:\", batch[1])\nprint(\"Batch Choice:\", batch[3])\nprint(\"%-------------------------%\")\nbatch = storage_dataset.batch[[1, 2, 3]]\nprint(\"Batch Shared Items Features:\", batch[0])\nprint(\"Batch Items Features:\", batch[1])\nprint(\"Batch Choice:\", batch[3])\nprint(\"%-------------------------%\")\nbatch = storage_dataset.batch[[0, 1, 5]]\nprint(\"Batch Shared Items Features:\", batch[0])\nprint(\"Batch Items Features:\", batch[1])\nprint(\"Batch Choice:\", batch[3])\n</code></pre> <pre><code>Batch Shared Items Features: [100 250]\nBatch Items Features: None\nBatch Choice: 0\n%-------------------------%\nBatch Shared Items Features: [[100 250]\n [150 500]\n [ 80 100]]\nBatch Items Features: None\nBatch Choice: [1 2 0]\n%-------------------------%\nBatch Shared Items Features: [[100 250]\n [100 250]\n [100 250]]\nBatch Items Features: None\nBatch Choice: [0 1 1]\n</code></pre> <p>Everything is mapped as needed. And the great thing is that you can easily mix ''classical'' features with FeaturesStorages.\\ Let's add a 'is_week_end' feature to our problem that will also be stored as a contexts_features.</p> <pre><code>shared_features = pd.DataFrame({\"supermarket_features\": np.array(str_supermarkets_sequence).squeeze(),\n\"is_week_end\": [0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0]})\nshared_features.head()\n</code></pre> supermarket_features is_week_end 0 supermarket_1 0 1 supermarket_1 0 2 supermarket_2 0 3 supermarket_3 1 4 supermarket_2 1 <pre><code># Creation of the ChoiceDataset\nstorage_dataset = ChoiceDataset(choices=choices,\n                                shared_features_by_choice=shared_features,\n                                available_items_by_choice=np.ones((len(choices), 3)),\n                                features_by_ids=[storage],\n)\n</code></pre> <pre><code># And now it's ready\nbatch = storage_dataset.batch[[1, 2, 3]]\nprint(\"Batch Shared Items Features:\", batch[0])\nprint(\"Batch Items Features:\", batch[1])\nprint(\"Batch Choice:\", batch[3])\n</code></pre> <p>Note that: - We use strings as ids for the example, however we recommend to use integers. - FeaturesStorage can be instantiated from dict, np.ndarray, list, pandas.DataFrame, etc... - More in-depth examples and explanations can be found here</p>"},{"location":"notebooks/introduction/2_data_handling/#ready-to-use-datasets","title":"Ready-to-use datasets","text":"<p>A few well-known open source datasets are directly integrated and the package and can be downloaded in one line: - SwissMetro from Bierlaire et al (2001) [2] - ModeCanada from Koppleman et al. (1993) [1] - The Train dataset from Ben Akiva et al. (1993) [4] - The Heating &amp; Electricity datasets from Kenneth Train [3] - The TaFeng dataset from Kaggle [5]</p> <p>If you feel like another open-source dataset could be included, reach out !</p> <pre><code>from choice_learn.datasets import (load_swissmetro,\n                                   load_modecanada,\n                                   load_train,\n                                   load_heating,\n                                   load_electricity,\n                                   load_tafeng\n                                   )\n\ncanada_choice_dataset = load_modecanada()\nswissmetro_choice_dataset = load_swissmetro()\n</code></pre> <p>The datasets can also be downloaded as dataframes:</p> <pre><code>swissmetro_df = load_swissmetro(as_frame=True)\nswissmetro_df.head()\n</code></pre>"},{"location":"notebooks/introduction/2_data_handling/#references","title":"References","text":"<p>[1] Koppelman et al. (1993), Application and Interpretation of Nested Logit Models of Intercity Mode Choice\\ [2] Bierlaire, M., Axhausen, K. and Abay, G. (2001), The Acceptance of Modal Innovation: The Case of SwissMetro\\ [3] Train, K.E. (2003) Discrete Choice Methods with Simulation. Cambridge University Press.\\ [4] Ben-Akiva M.; Bolduc D.; Bradley M. (1993) Estimation of Travel Choice Models with Randomly Distributed Values of Time\\ [5] The Ta Feng Grocery dataset on Kaggle</p>"},{"location":"notebooks/introduction/3_model_clogit/","title":"Introduction to choice-learn's modelling","text":"<pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n# Uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n</code></pre> <pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\n\n# Enabling eager execution sometimes decreases fitting time\ntf.compat.v1.enable_eager_execution()\n</code></pre>"},{"location":"notebooks/introduction/3_model_clogit/#summary","title":"Summary","text":"<ul> <li>Example 1: ConditionalLogit with Swissmetro<ul> <li>A few words on c-MNL formulation</li> <li>Instantiation and estimation with Choice-Learn</li> </ul> </li> <li>Example 2: ConditionalLogit with ModeCanda<ul> <li>Utility Formulation</li> <li>Model Specification</li> <li>Lighter method for specification</li> <li>Comparison with other implementations</li> <li>Get utility estimation and probabilities</li> <li>Using Gradient Descent Optimizers</li> </ul> </li> </ul> <p>For model customization and more explanation on ChoiceModel and the endpoints, you can go here</p>"},{"location":"notebooks/introduction/3_model_clogit/#example-1-swissmetro","title":"Example 1: SwissMetro","text":"<p>The choice-learn package offers a high level API to conceive and estimate discrete choice models. Several models are ready to be used, you can check the list here. If you want to create your own model or another one that is not in the list, the lower level API can help you. Check the notebook here.</p> <p>Let's begin this tutorial with the estimation of a Conditional Logit Model on the SwissMetro dataset[3]. It follows the specifications described in PyLogit and Biogeme.</p> <p>First, we download our data as a ChoiceDataset. See the data management tutorial first if needed.</p> <pre><code>from choice_learn.datasets import load_swissmetro\nswiss_dataset = load_swissmetro(preprocessing=\"tutorial\")\nprint(swiss_dataset.summary())\n</code></pre>"},{"location":"notebooks/introduction/3_model_clogit/#the-conditional-logit-model","title":"The Conditional Logit model","text":"<p>The conditional Logit [2] specifies a linear utility for each features of item $i$:  The probability to choose $i$ among the set of available alternatives $\\mathcal{A}$ is then:</p> <p> </p> <p>With the SwissMetro dataset we are trying to predict a customer mean of transport among train, swissmetro and car from the features: - TT (transit time) - CO (cost) - HE (headway) - Survey (where the survey took place) - Luggage number - seats configuration in the swissmetro - first class or not</p> <p>An important step is to define the right utility function for the model to fit well the dataset. Let's take the following formulation defined in PyLogit:</p> <ul> <li> <p>$ U(train) = \\beta_{train}^{inter} + \\beta^{tt}{train/sm} \\cdot TT(train) + \\beta^{co} \\cdot SV(train)$} \\cdot CO(train) + \\beta^{he}_{train} \\cdot HE(train) + \\beta^{survey</p> </li> <li> <p>$ U(sm) = \\beta_{sm}^{inter} + \\beta^{tt}{train/sm} \\cdot TT(sm) + \\beta^{co} \\cdot FC(sm)$} \\cdot CO(sm) + \\beta^{he}_{sm} \\cdot HE(sm) + \\beta^{survey} \\cdot SV(sm) + \\beta^{seat} \\cdot SEAT(sm) + \\beta^{first_class</p> </li> <li> <p>$ U(car) = \\beta^{tt}{car} \\cdot TT(car) + \\beta^{co}} \\cdot CO(car) + \\beta^{luggage==1} \\cdot \\mathbb{1{Luggage==1} + \\beta^{luggage&gt;1} \\cdot \\mathbb{1}$</p> </li> </ul> <p>Note that we want to estimate:</p> <ul> <li>one $\\beta^{tt}{train/sm}$ shared for train and sm items and one $\\beta^{tt}$ for the car item. Indeed, one can argue that customers have the same sensibility toward travel time for all public transport and a different one for private ones.</li> <li>one $\\beta^{co}$ coefficient for each item.</li> <li>one $\\beta^{inter}$ and $\\beta^{he}$ for train and sm, and zeroed for the car alternative</li> <li>one $\\beta^{survey}$, $\\beta^{seat}$, $\\beta^{first_class}$, $\\beta^{luggage==1}$ and $\\beta^{luggage&gt;1}$ shared or not by different items</li> </ul> <p>To build a model, we need to specify for each weight $\\beta$: - the name of the feature it goes with:     - it must match the feature name in the ChoiceDataset     - \"intercept\" is the standardized name used for intercept, pay attention not to override it - items_indexes: the items concerned, as indexed in the ChoiceDataset - (optionally) a unique weight name</p> <p>Attention</p> <ul> <li>add_coefficients is to be used to get one coefficient by given items_indexes\\</li> <li>add_shared_coefficients is to be used to get on coefficient that is used for utility of all given items_indexes</li> </ul> <p>Here is how to create a model following our defined utility functions:</p>"},{"location":"notebooks/introduction/3_model_clogit/#conditional-logit-estimation-with-choice-learn","title":"Conditional Logit Estimation with Choice-Learn","text":"<pre><code>from choice_learn.models import ConditionalLogit\n</code></pre> <pre><code># Initialization of the model\nswiss_model = ConditionalLogit(optimizer=\"lbfgs\")\n\n# Intercept for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 1])\n# beta_he for train &amp; sm\nswiss_model.add_coefficients(feature_name=\"headway\",\n                             items_indexes=[0, 1],\n                             coefficient_name=\"beta_he\")\n# beta_co for all items\nswiss_model.add_coefficients(feature_name=\"cost\",\n                             items_indexes=[0, 1, 2])\n# beta first_class for train\nswiss_model.add_coefficients(feature_name=\"regular_class\",\n                             items_indexes=[0])\n# beta seats for train\nswiss_model.add_coefficients(feature_name=\"seats\", items_indexes=[1])\n# betas luggage for car\nswiss_model.add_coefficients(feature_name=\"single_luggage_piece\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_luggage=1\")\nswiss_model.add_coefficients(feature_name=\"multiple_luggage_piece\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_luggage&gt;1\")\n# beta TT only for car\nswiss_model.add_coefficients(feature_name=\"travel_time\",\n                             items_indexes=[2],\n                             coefficient_name=\"beta_tt_car\")\n\n# betas TT and HE shared by train and sm\nswiss_model.add_shared_coefficient(feature_name=\"travel_time\",\n                                   items_indexes=[0, 1])\nswiss_model.add_shared_coefficient(feature_name=\"train_survey\",\n                                   items_indexes=[0, 1],\n                                   coefficient_name=\"beta_survey\")\n</code></pre> <pre><code># Estimation of the model\nhistory = swiss_model.fit(swiss_dataset, get_report=True)\n</code></pre> <p>Once the model is estimated, we can look at the weights with the .trainable_weights argument:</p> <pre><code>swiss_model.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[-1.2929296, -0.5025745]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_he:0' shape=(1, 2) dtype=float32, numpy=array([[-0.31433573, -0.37731653]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_cost:0' shape=(1, 3) dtype=float32, numpy=array([[-0.5617626 , -0.2816758 , -0.51384646]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_regular_class:0' shape=(1, 1) dtype=float32, numpy=array([[0.5650171]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_seats:0' shape=(1, 1) dtype=float32, numpy=array([[-0.78244746]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_luggage=1:0' shape=(1, 1) dtype=float32, numpy=array([[0.42275968]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_luggage&gt;1:0' shape=(1, 1) dtype=float32, numpy=array([[1.4139789]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_tt_car:0' shape=(1, 1) dtype=float32, numpy=array([[-0.7229831]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_travel_time:0' shape=(1, 1) dtype=float32, numpy=array([[-0.69901365]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_survey:0' shape=(1, 1) dtype=float32, numpy=array([[2.542475]], dtype=float32)&gt;]\n</code></pre> <p>We can easily acces the negative log likelihood value for the training dataset or another one using the .evaluate() method:</p> <pre><code>len(swiss_dataset) * swiss_model.evaluate(swiss_dataset)\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=5159.3047&gt;\n</code></pre> <p>If you set get_report to True in .fit, the model automatically creates a report for each of the coefficient, with its estimation, its standard deviation and more:</p> <pre><code># Let's add items for better readability\nitems = [\"train\", \"sm\", \"train\", \"sm\", \"train\", \"sm\", \"car\", \"train\", \"sm\", \"car\", \"car\", \"car\", \"train &amp; sm\", \"train &amp; sm\"]\nswiss_model.report = pd.concat([swiss_model.report, pd.Series(items, name=\"item\")], axis=1)\n</code></pre> <pre><code>swiss_model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) item 0 beta_intercept_0 -1.292930 0.145000 -8.916755 0.000000e+00 train 1 beta_intercept_1 -0.502575 0.109712 -4.580845 4.649162e-06 sm 2 beta_he_0 -0.314336 0.064734 -4.855812 1.192093e-06 train 3 beta_he_1 -0.377317 0.191370 -1.971662 4.864824e-02 sm 4 beta_cost_0 -0.561763 0.094500 -5.944569 0.000000e+00 train 5 beta_cost_1 -0.281676 0.042252 -6.666523 0.000000e+00 sm 6 beta_cost_2 -0.513846 0.101592 -5.057931 4.768372e-07 car 7 beta_regular_class 0.565017 0.079958 7.066461 0.000000e+00 train 8 beta_seats -0.782447 0.085864 -9.112632 0.000000e+00 sm 9 beta_luggage=1 0.422760 0.063327 6.675845 0.000000e+00 car 10 beta_luggage&gt;1 1.413979 0.190378 7.427222 0.000000e+00 car 11 beta_tt_car -0.722983 0.044959 -16.080963 0.000000e+00 car 12 beta_travel_time -0.699014 0.038635 -18.092785 0.000000e+00 train &amp; sm 13 beta_survey 2.542475 0.098162 25.900934 0.000000e+00 train &amp; sm <p>We find the same results (estimation of parameters and negative log-likelihood) as the PyLogit package. One can easily interpret the coefficient. For example $\\beta_{cost}$ represent the average price elasticity of the customers. First, it's negative meaning that the most expensive it is the less likely the alternative will be chose, second we can observe that the elasticity is smaller for the SwissMetro. It is not suprising for a \"premium\" product, meaning that people choosing this alternative agreed to pay more for comfort for example and are less regarding on the price.</p> <p>We can also observe that $\\beta_{luggage&gt;1} &gt; \\beta_{luggage=1} &gt;  \\beta_{luggage=0} = 0$ meaning that customers with luggae are more likely to use their car for transport, and it is event further the case if they have more than one piece of luggage.</p>"},{"location":"notebooks/introduction/3_model_clogit/#example-2-the-modecanada-dataset","title":"Example #2: the ModeCanada Dataset","text":""},{"location":"notebooks/introduction/3_model_clogit/#utility-formulation","title":"Utility formulation","text":"<p>Let's reproduce a common example from Torch-Choice on the ModeCanada [1] dataset:  </p> <p>A description of the dataset and the features can be found in [1]. We want to predict the future mean of transport from [train, air, bus, car] using frequence, price, in-vehicule transport time (ivt) and out-of-vehicule transport time (ovt).</p>"},{"location":"notebooks/introduction/3_model_clogit/#model-formulation","title":"Model Formulation","text":"<pre><code># If you want to check what's in the dataset:\nfrom choice_learn.datasets import load_modecanada\n\ntransport_df = load_modecanada(as_frame=True)\ntransport_df.head()\n</code></pre> Unnamed: 0 case alt choice dist cost ivt ovt freq income urban noalt 0 1 1 train 0 83 28.25 50 66 4 45.0 0 2 1 2 1 car 1 83 15.77 61 0 0 45.0 0 2 2 3 2 train 0 83 28.25 50 66 4 25.0 0 2 3 4 2 car 1 83 15.77 61 0 0 25.0 0 2 4 5 3 train 0 83 28.25 50 66 4 70.0 0 2 <p>We want to estimate:</p> <ul> <li>one $\\beta^{price}$, $\\beta^{freq}$ and $\\beta^{ovt}$ coefficient. They are shared by all items.</li> <li>one $\\beta^{ivt}$ coefficient for each item.</li> <li>one $\\beta^{inter}$ and $\\beta^{income}$ coefficient for each item, with additional constraint to be 0 for the first item (air).</li> </ul> <p>One notes that it makes sense to include an intercept $\\beta^{inter}$ for each item since $ivt(i, c)$ and $income(c)$ depends on each choice $c$.</p> <p>Additionally to previous example we manually specify the weights names:</p> <pre><code># Loading the ChoiceDataset\ncanada_dataset = load_modecanada(as_frame=False, preprocessing=\"tutorial\")\n\nprint(canada_dataset.summary())\n</code></pre> <pre><code>%=====================================================================%\n%%% Summary of the dataset:\n%=====================================================================%\nNumber of items: 4\nNumber of choices: 2779\n%=====================================================================%\n Shared Features by Choice:\n 1 shared features\n with names: (['income'],)\n\n\n Items Features by Choice:\n4 items features \n with names: (['cost', 'freq', 'ovt', 'ivt'],)\n%=====================================================================%\n</code></pre> <pre><code>from choice_learn.models import ConditionalLogit\n\n# Initialization of the model\nmodel = ConditionalLogit()\n\n# Creation of the different weights:\n\n# shared_coefficient add one coefficient that is used for all items specified in the items_indexes:\n# Here, cost, freq and ovt coefficients are shared between all items\nmodel.add_shared_coefficient(feature_name=\"cost\", items_indexes=[0, 1, 2, 3])\n# You can specify you own coefficient name\nmodel.add_shared_coefficient(feature_name=\"freq\",\n                             coefficient_name=\"beta_frequence\",\n                             items_indexes=[0, 1, 2, 3])\nmodel.add_shared_coefficient(feature_name=\"ovt\", items_indexes=[0, 1, 2, 3])\n\n# ivt is added for each item:\nmodel.add_coefficients(feature_name=\"ivt\", items_indexes=[0, 1, 2, 3])\n\n# add_coefficients adds one coefficient for each specified item_index\n# intercept, and income are added for each item except the first one that needs to be zeroed\nmodel.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2, 3])\nmodel.add_coefficients(feature_name=\"income\", items_indexes=[1, 2, 3])\n</code></pre> <pre><code>history = model.fit(canada_dataset, get_report=True, verbose=2)\n</code></pre> <pre><code>model.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'beta_cost:0' shape=(1, 1) dtype=float32, numpy=array([[-0.03333882]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_frequence:0' shape=(1, 1) dtype=float32, numpy=array([[0.09252929]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_ovt:0' shape=(1, 1) dtype=float32, numpy=array([[-0.04300351]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_ivt:0' shape=(1, 4) dtype=float32, numpy=\n array([[ 0.05950943, -0.00678369, -0.00646029, -0.00145037]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'beta_intercept:0' shape=(1, 3) dtype=float32, numpy=array([[0.6983663, 1.8440988, 3.2741835]], dtype=float32)&gt;,\n &lt;tf.Variable 'beta_income:0' shape=(1, 3) dtype=float32, numpy=array([[-0.08908683, -0.02799302, -0.03814651]], dtype=float32)&gt;]\n</code></pre> <pre><code>print(\"The average neg-loglikelihood is:\", model.evaluate(canada_dataset).numpy())\nprint(\"The total neg-loglikelihood is:\", model.evaluate(canada_dataset).numpy()*len(canada_dataset))\n</code></pre> <pre><code>The average neg-loglikelihood is: 0.67447394\nThe total neg-loglikelihood is: 1874.3630829453468\n</code></pre> <pre><code>model.report\n</code></pre> Coefficient Name Coefficient Estimation Std. Err z_value P(.&gt;z) 0 beta_cost -0.033339 0.007095 -4.699046 2.622604e-06 1 beta_frequence 0.092529 0.005097 18.151926 0.000000e+00 2 beta_ovt -0.043004 0.003225 -13.335685 0.000000e+00 3 beta_ivt_0 0.059509 0.010073 5.908032 0.000000e+00 4 beta_ivt_1 -0.006784 0.004433 -1.530138 1.259825e-01 5 beta_ivt_2 -0.006460 0.001898 -3.403067 6.663799e-04 6 beta_ivt_3 -0.001450 0.001187 -1.221395 2.219366e-01 7 beta_intercept_0 0.698366 1.280191 0.545517 5.853977e-01 8 beta_intercept_1 1.844099 0.708427 2.603088 9.238839e-03 9 beta_intercept_2 3.274184 0.624342 5.244213 1.192093e-07 10 beta_income_0 -0.089087 0.018347 -4.855648 1.192093e-06 11 beta_income_1 -0.027993 0.003872 -7.228693 0.000000e+00 12 beta_income_2 -0.038147 0.004083 -9.342735 0.000000e+00"},{"location":"notebooks/introduction/3_model_clogit/#faster-specification","title":"Faster Specification","text":"<p>A faster specification can be done using a dictionnary. It follows torch-choice method to create conditional logit models. The parameters dict needs to be as follows: - The key is the feature name - The value is the mode. Currently three modes are available:     - constant: the learned coefficient is shared by all items     - item: one coefficient by item is estimated, the value for the item at index 0 is set to 0     - item-full: one coefficient by item is estimated</p> <p>In order to create the same model for the ModeCanada dataset, it looks as follows:</p> <pre><code># Instantiation with the coefficients dictionnary\ncoefficients = {\"income\": \"item\",\n \"cost\": \"constant\",\n \"freq\": \"constant\",\n \"ovt\": \"constant\",\n \"ivt\": \"item-full\",\n \"intercept\": \"item\"}\n\n# Instantiation of the model\ncmnl = ConditionalLogit(coefficients=coefficients, epochs=1000, optimizer=\"lbfgs\")\n</code></pre> <pre><code>history = cmnl.fit(canada_dataset)\nprint(cmnl.trainable_weights)\nprint(cmnl.evaluate(canada_dataset).numpy())\n</code></pre> <pre><code>[&lt;tf.Variable 'income_w_0:0' shape=(1, 3) dtype=float32, numpy=array([[-0.08908679, -0.02799301, -0.0381465 ]], dtype=float32)&gt;, &lt;tf.Variable 'cost_w_1:0' shape=(1, 1) dtype=float32, numpy=array([[-0.03333884]], dtype=float32)&gt;, &lt;tf.Variable 'freq_w_2:0' shape=(1, 1) dtype=float32, numpy=array([[0.09252931]], dtype=float32)&gt;, &lt;tf.Variable 'ovt_w_3:0' shape=(1, 1) dtype=float32, numpy=array([[-0.04300352]], dtype=float32)&gt;, &lt;tf.Variable 'ivt_w_4:0' shape=(1, 4) dtype=float32, numpy=\narray([[ 0.05950936, -0.00678372, -0.0064603 , -0.00145037]],\n      dtype=float32)&gt;, &lt;tf.Variable 'intercept_w_5:0' shape=(1, 3) dtype=float32, numpy=array([[0.69836473, 1.8440932 , 3.2741785 ]], dtype=float32)&gt;]\n0.67447394\n</code></pre>"},{"location":"notebooks/introduction/3_model_clogit/#comparison-with-other-implementations-results","title":"Comparison with other implementations results","text":"<pre><code>import tensorflow as tf\n\n# Here are the values obtained in the references:\ngt_weights = [\n    tf.constant([[-0.0890796, -0.0279925, -0.038146]]),\n    tf.constant([[-0.0333421]]),\n    tf.constant([[0.0925304]]),\n    tf.constant([[-0.0430032]]),\n    tf.constant([[0.0595089, -0.00678188, -0.00645982, -0.00145029]]),\n    tf.constant([[0.697311, 1.8437, 3.27381]]),\n]\ngt_model = ConditionalLogit(coefficients=coefficients)\ngt_model.instantiate(canada_dataset)\ncanada_dataset\n# Here we estimate the negative log-likelihood with these coefficients (also, we obtain same value as in those papers):\ngt_model._trainable_weights = gt_weights\nprint(\"'Ground Truth' Negative LogLikelihood:\", gt_model.evaluate(canada_dataset) * len(canada_dataset))\n</code></pre> <pre><code>Using L-BFGS optimizer, setting up .fit() function\n'Ground Truth' Negative LogLikelihood: tf.Tensor(1874.3633, shape=(), dtype=float32)\n</code></pre>"},{"location":"notebooks/introduction/3_model_clogit/#estimate-utility-probabilities","title":"Estimate Utility &amp; probabilities","text":"<p>In order to estimate the utilities, use the .predict_utility() method. In order to estimate the probabilities, use the .predict_probas() method.</p> <pre><code>print(\"Utilities of each item for the first 5 sessions:\", cmnl.compute_batch_utility(*canada_dataset.batch[:5]))\n</code></pre> <pre><code>Utilities of each item for the first 5 sessions: tf.Tensor(\n[[ -4.250798   -8.238913   -3.4962516  -3.5083752]\n [ -4.250798  -10.466084   -4.196077   -4.4620376]\n [ -4.250798   -7.348045   -3.2163215  -3.1269102]\n [ -4.250798  -10.466084   -4.196077   -4.4620376]\n [ -4.250798  -10.466084   -4.196077   -4.4620376]], shape=(5, 4), dtype=float32)\n</code></pre> <pre><code>print(\"Purchase probability of each item for the first 5 sessions:\", cmnl.predict_probas(canada_dataset)[:5])\n</code></pre> <pre><code>Purchase probability of each item for the first 5 sessions: tf.Tensor(\n[[0.1906133  0.00353295 0.40536723 0.40048242]\n [0.348695   0.00069692 0.3683078  0.2822966 ]\n [0.14418297 0.00651324 0.40567806 0.44362125]\n [0.348695   0.00069692 0.3683078  0.2822966 ]\n [0.348695   0.00069692 0.3683078  0.2822966 ]], shape=(5, 4), dtype=float32)\n</code></pre>"},{"location":"notebooks/introduction/3_model_clogit/#using-gradient-descent-optimizers","title":"Using Gradient Descent Optimizers","text":"<p>For very large datasets that do not fit entirely in the memory, we have to work with data batches. In this case, the LBFGS method cannot be used, because of the large memory usage of the algorithm. In those cases, we will prefer stochastic gradient descent optimizers.</p> <p>In this case, it is possible to obtain the same coefficients estimation, also it is a little tricky to get it quickly. We need to adjust the learning rate over time for the optimization not to be too slow. L-BFGS is more efficient for small dataset - Gradient Descent for large ones !</p> <pre><code>cmnl = ConditionalLogit(coefficients=coefficients, optimizer=\"Adam\", epochs=2000, batch_size=-1)\nhistory = cmnl.fit(canada_dataset)\ncmnl.optimizer.lr = cmnl.optimizer.lr / 5\ncmnl.epochs = 4000\nhistory2 = cmnl.fit(canada_dataset)\ncmnl.optimizer.lr = cmnl.optimizer.lr  / 10\ncmnl.epochs = 20000\nhistory3 = cmnl.fit(canada_dataset)\n</code></pre> <pre><code>Epoch 1999 Train Loss 0.6801: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:11&lt;00:00, 176.83it/s]\nEpoch 3999 Train Loss 0.6776: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000/4000 [00:20&lt;00:00, 198.77it/s]\nEpoch 19999 Train Loss 0.6767: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [01:38&lt;00:00, 202.55it/s]\n</code></pre> <p>It can be useful to look at the loss (negative loglikelihood) over time to see how the estimation goes:</p> <pre><code>import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history[\"train_loss\"])\nplt.title(\"First part of the gradient descent.\")\nplt.subplot(1, 2, 2)\nplt.plot(history2[\"train_loss\"] + history3[\"train_loss\"])\nplt.title(\"Second and third part of the gradient descent.\")\n</code></pre> <pre><code>Text(0.5, 1.0, 'Second and third part of the gradient descent.')\n</code></pre> <p></p> <pre><code>cmnl.trainable_weights\n</code></pre> <pre><code>[&lt;tf.Variable 'income_w_0:0' shape=(1, 3) dtype=float32, numpy=array([[-0.08402919, -0.02359896, -0.03233599]], dtype=float32)&gt;,\n &lt;tf.Variable 'cost_w_1:0' shape=(1, 1) dtype=float32, numpy=array([[-0.05140902]], dtype=float32)&gt;,\n &lt;tf.Variable 'freq_w_2:0' shape=(1, 1) dtype=float32, numpy=array([[0.09645309]], dtype=float32)&gt;,\n &lt;tf.Variable 'ovt_w_3:0' shape=(1, 1) dtype=float32, numpy=array([[-0.04099115]], dtype=float32)&gt;,\n &lt;tf.Variable 'ivt_w_4:0' shape=(1, 4) dtype=float32, numpy=\n array([[ 0.05871323, -0.00726116, -0.00368665, -0.00105638]],\n       dtype=float32)&gt;,\n &lt;tf.Variable 'intercept_w_5:0' shape=(1, 3) dtype=float32, numpy=array([[-1.687437  , -0.39638832,  1.1344599 ]], dtype=float32)&gt;]\n</code></pre> <pre><code>cmnl.evaluate(canada_dataset)\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.67665595&gt;\n</code></pre>"},{"location":"notebooks/introduction/3_model_clogit/#references","title":"References","text":"<p>[1] ModeCanada dataset in Application and interpretation of nested logit models of intercity mode choice, Christophier, V. F.; Koppelman, S. (1993)\\ [2] Conditional MultinomialLogit, Train, K.; McFadden, D.; Ben-Akiva, M. (1987)\\ [3] Siwssmetro dataset in The acceptance of modal innovation: The case of Swissmetro, Bierlaire, M.; Axhausen, K.; Abay, G (2001)\\</p>"},{"location":"notebooks/auxiliary_tools/assortment_example/","title":"Assortment Example","text":"<p>A short example for assortment optimization under the conditional Logit and more.</p> <ul> <li>Intoduction<ul> <li>Dataset</li> <li>Choice model specification</li> <li>Choice model estimation</li> </ul> </li> <li>Assortment Optimization<ul> <li>Preparing the data</li> <li>Choice-Learn's AssortmentOptimizer</li> <li>Assortment with Latent Class models</li> <li>Adding Capacity Constraints</li> </ul> </li> <li>Pricing and assortment</li> </ul> <p></p> <pre><code># Install necessary requirements\n\n# If you run this notebook on Google Colab, or in standalone mode, you need to install\n# the required packages. Just uncomment the following lines:\n\n# !pip install choice-learn\n\n# If you run the notebook within the GitHub repository, you need to run the following lines,\n# that can skipped otherwise:\nimport os\nimport sys\n\nsys.path.append(\"../../\")\n\n# Remove GPU use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport numpy as np\n</code></pre>"},{"location":"notebooks/auxiliary_tools/assortment_example/#introduction","title":"Introduction","text":""},{"location":"notebooks/auxiliary_tools/assortment_example/#dataset","title":"Dataset","text":"<p>We will use the TaFeng Dataset that is available on Kaggle. You can load it automatically with Choice-Learn !</p> <pre><code>from choice_learn.datasets import load_tafeng\n</code></pre> <pre><code># Short illustration of the dataset\ntafeng_df = load_tafeng(as_frame=True)\ntafeng_df.head()\n</code></pre> TRANSACTION_DT CUSTOMER_ID AGE_GROUP PIN_CODE PRODUCT_SUBCLASS PRODUCT_ID AMOUNT ASSET SALES_PRICE 0 11/1/2000 1104905 45-49 115 110411 4710199010372 2 24 30 1 11/1/2000 418683 45-49 115 120107 4710857472535 1 48 46 2 11/1/2000 1057331 35-39 115 100407 4710043654103 2 142 166 3 11/1/2000 1849332 45-49 Others 120108 4710126092129 1 32 38 4 11/1/2000 1981995 50-54 115 100205 4710176021445 1 14 18"},{"location":"notebooks/auxiliary_tools/assortment_example/#choice-model-specification","title":"Choice Model Specification","text":"<p>In this example we will use the sales_price and age_group features to estimate a discrete choice model in the form of a conditional MNL:</p> <p>for a customer $z$ and a product $i$, we define the utility function:</p> <p> </p> <p>with: - $u_i$ the base utility of product $i$ - $p_i$ the price of product $i$ - $e_{dem(z)}$ the price elasticity of customer $z$ depending of its age</p> <p>We decide to estimate three coefficients of price elasticity for customers &lt;=25 y.o, 26&lt;=.&lt;=55 y.o. and =&gt;56 y.o.</p> <pre><code># Let's reload the TaFeng dataset as a Choice Dataset\ndataset = load_tafeng(as_frame=False, preprocessing=\"assort_example\")\n\n# The age categories are encoded as OneHot features:\nprint(\"Age Categories Encoding for choices 0, 4 and 16:\")\nprint(dataset.shared_features_by_choice[0][[0, 4, 16]])\n</code></pre> <pre><code>WARNING:root:Shared Features Names were not provided, will not be able to\n                                    fit models needing them such as Conditional Logit.\nWARNING:root:Items Features Names were not provided, will not be able to\n                                fit models needing them such as Conditional Logit.\n\n\nAge Categories Encoding for choices 0, 4 and 16:\n[[0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n</code></pre> <p>Let's define a custom model that would fit our formulation using Choice-Learn's ChoiceModel inheritance:</p> <pre><code>import tensorflow as tf\n\nfrom choice_learn.models.base_model import ChoiceModel\n\n\nclass TaFengMNL(ChoiceModel):\n    \"\"\"Custom model for the TaFeng dataset.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Instantiate of our custom model.\"\"\"\n        # Standard inheritance stuff\n        super().__init__(**kwargs)\n        # Directly initialize weights in _trainable_weights\n        self._trainable_weights = [\n            tf.Variable(tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, 25))),\n            tf.Variable(tf.random_normal_initializer(0.0, 0.02, seed=42)(shape=(1, 3)))\n        ]\n\n    @property\n    def trainable_weights(self):\n        \"\"\"Return all the models trainable weights.\"\"\"\n        return self._trainable_weights\n\n    @property\n    def base_utilities(self):\n        \"\"\"Return itemwise utilities.\"\"\"\n        return self._trainable_weights[0]\n\n    @property\n    def price_elasticities(self):\n        \"\"\"Return the price elasticities.\"\"\"\n        return self._trainable_weights[1]\n\n    def compute_batch_utility(self,\n                              shared_features_by_choice,\n                              items_features_by_choice,\n                              available_items_by_choice,\n                              choices):\n        \"\"\"Define how the model computes the utility of a product.\n\n        Parameters\n        ----------\n        shared_features_by_choice : tuple of np.ndarray (choices_features)\n            a batch of shared features\n            Shape must be (n_choices, n_shared_features)\n        items_features_by_choice : tuple of np.ndarray (choices_items_features)\n            a batch of items features\n            Shape must be (n_choices, n_items_features)\n        available_items_by_choice : np.ndarray\n            A batch of items availabilities\n            Shape must be (n_choices, n_items)\n        choices_batch : np.ndarray\n            Choices\n            Shape must be (n_choices, )\n\n        Return:\n        --------\n        np.ndarray\n            Utility of each product for each choice.\n            Shape must be (n_choices, n_items)\n        \"\"\"\n        # Unused arguments\n        _ = (available_items_by_choice, choices)\n\n        # Get the right price elasticity coefficient according to the age cateogry\n        price_coeffs = tf.tensordot(shared_features_by_choice,\n                                    tf.transpose(self.price_elasticities),\n                                    axes=1)\n        # Compute the utility: u_i + p_i * c\n        return tf.multiply(items_features_by_choice[:, :, 0], price_coeffs) + self.base_utilities\n</code></pre>"},{"location":"notebooks/auxiliary_tools/assortment_example/#choice-model-estimation","title":"Choice Model Estimation","text":"<p>We estimate the coefficients values using .fit:</p> <pre><code>model = TaFengMNL(optimizer=\"lbfgs\", epochs=1000, lbfgs_tolerance=1e-4)\nhistory = model.fit(dataset, verbose=1)\n</code></pre> <pre><code>Using L-BFGS optimizer, setting up .fit() function\n\n\nWARNING:root:L-BFGS Opimization finished:\nWARNING:root:---------------------------------------------------------------\nWARNING:root:Number of iterations: 225\nWARNING:root:Algorithm converged before reaching max iterations: True\n</code></pre> <p>We can observe estimated coefficients with the .weights argument:</p> <pre><code>print(\"Model Negative Log-Likelihood: \", model.evaluate(dataset))\nprint(\"Model Weights:\")\nprint(\"Base Utilities u_i:\", model.trainable_weights[0].numpy())\nprint(\"Price Elasticities:\", model.trainable_weights[1].numpy())\n</code></pre> <pre><code>Model Negative Log-Likelihood:  tf.Tensor(2.7657256, shape=(), dtype=float32)\nModel Weights:\nBase Utilities u_i: [[ 0.5068263   2.935736    1.998015    0.5470789   0.72602475  1.0055478\n  -0.7196758  -0.970541   -0.00946927 -3.042058    1.0770373   1.6368566\n  -3.6405432  -1.2479168   3.0117846   1.6831478   1.8547137  -1.2627332\n  -1.1671457  -0.08575154 -1.773998   -1.9642268  -1.7941352   1.5037025\n  -0.7460297 ]]\nPrice Elasticities: [[-0.06286521 -0.05761966 -0.05427208]]\n</code></pre> <p>As a short analysis we can observe that the price elasticiy in negative as expected and the younger the population the more impacted by the price.\\ Our models looks good enough for a first and fast modelization. Now let's see how to compute an optimal assortment using our model.</p>"},{"location":"notebooks/auxiliary_tools/assortment_example/#assortment-optimization","title":"Assortment Optimization","text":""},{"location":"notebooks/auxiliary_tools/assortment_example/#preparing-the-data","title":"Preparing the data","text":"<p>The first step is to compute the utility of each product. Here, let's consider that the last prices will also be the future prices of our products in our future assortment.\\ It can be easily adapted if theses prices were to be changed.\\ We can compute each age category utility using the compute_batch_utility method of our ChoiceModel:</p> <pre><code>future_prices = np.stack([dataset.items_features_by_choice[0][-1]]*3, axis=0)\nage_category = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]).astype(\"float32\")\npredicted_utilities = model.compute_batch_utility(shared_features_by_choice=age_category,\n                                                  items_features_by_choice=future_prices,\n                                                  available_items_by_choice=None,\n                                                  choices=None\n                                                  )\n</code></pre> <p>We compute the ratio of each age category appearance in our dataset to obtain an average utility for each product.</p> <pre><code>age_frequencies = np.mean(dataset.shared_features_by_choice[0], axis=0)\n\nfinal_utilities = []\nfor freq, ut in zip(age_frequencies, predicted_utilities):\n    final_utilities.append(freq*ut)\nfinal_utilities = np.mean(final_utilities, axis=0)\nprint(\"Estimated final utilities for each product:\", final_utilities)\n</code></pre> <pre><code>Estimated final utilities for each product: [-0.24978125 -0.3917887  -0.7043624  -0.5408898  -0.4812412  -0.38806686\n -0.6586153  -0.93256587 -0.72640586 -1.5850058  -1.3158809  -0.17763059\n -1.6322378  -0.83469564 -0.49966928 -0.80931807 -1.0566555  -0.8396344\n -0.8077719  -0.69473463 -0.99102306 -1.0163671  -1.0167683  -1.3830209\n -0.4294889 ]\n</code></pre> <p>We need to define what quantity needs to be optimized by our assortment. A usual answer is to optimize the revenue or margin. In our case we do not have these values, so let's say that we want to obtain the assortment with 12 products that will generate the highest turnover.</p>"},{"location":"notebooks/auxiliary_tools/assortment_example/#choice-learns-assortmentoptimizer","title":"Choice-Learn's AssortmentOptimizer","text":"<p>Choice-Learn integrates algorithms for assortment planning based on Gurobi or OR-Tools. You can choose which solver you want by specifying <code>solver=\"gurobi\"</code> or <code>solver=\"or-tools\"</code>.\\ Gurobi needs a license (free for Academics), however, it is usually faster than the open-source OR-Tools.\\ Let's see an example.</p> <pre><code>solver = \"gurobi\"\n# solver = \"or-tools\"\n</code></pre> <pre><code>from choice_learn.toolbox.assortment_optimizer import MNLAssortmentOptimizer\n\nopt = MNLAssortmentOptimizer(\n    solver=solver,\n    utilities=np.exp(final_utilities), # Utilities need to be transformed with exponential function\n    itemwise_values=future_prices[0][:, 0], # Values to optimize for each item, here price that is used to compute turnover\n    assortment_size=12) # Size of the assortment we want\n</code></pre> <pre><code>assortment, opt_obj = opt.solve()\nprint(\"Our Optimal Assortment is:\")\nprint(assortment)\nprint(\"With an estimated average revenue of:\", opt_obj)\n</code></pre> <pre><code>Our Optimal Assortment is:\n[0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n 0.]\nWith an estimated average revenue of: 51.57688285623652\n</code></pre>"},{"location":"notebooks/auxiliary_tools/assortment_example/#latent-class-assortment-optimizer","title":"Latent Class Assortment Optimizer","text":"<p>This simplistic version is not optimal since it uses an averaged utility of each product over the population resulting in an approximative probability.\\ Choice-Learn also proposes an implementation of the Mixed-Integer Programming approach described in [1]. This version works for latent class models and fits well in our case with different populations.\\ The usage is similar with the object LatentClassAssortmentOptimizer.</p> <pre><code>from choice_learn.toolbox.assortment_optimizer import LatentClassAssortmentOptimizer\n\nopt = LatentClassAssortmentOptimizer(\n    solver=solver,\n    class_weights=age_frequencies, # Weights of each class\n    class_utilities=np.exp(predicted_utilities), # utilities in the shape (n_classes, n_items)\n    itemwise_values=future_prices[0][:, 0], # Values to optimize for each item, here price that is used to compute turnover\n    assortment_size=12) # Size of the assortment we want\n</code></pre> <pre><code>assortment, opt_obj = opt.solve()\nprint(\"Our Optimal Assortment is:\")\nprint(assortment)\nprint(\"With an estimated average revenue of:\", opt_obj)\nprint(\"Totalling\", np.sum(assortment), \"items in the assortment, which is fine with our limit of 12.\")\n</code></pre> <pre><code>Our Optimal Assortment is:\n[0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n 0.]\nWith an estimated average revenue of: 35.79552031037327\nTotalling 12.0 items in the assortment, which is fine with our limit of 12.\n</code></pre> <p>With this version, our results are slightly more precise - however we used integer in the LP formulation that can lead to slower results with large number of items.</p>"},{"location":"notebooks/auxiliary_tools/assortment_example/#adding-capacity-constraints","title":"Adding capacity constraints","text":"<p>It is possible to add some constraints. A recurrent case is that the assortment of product will be placed in store and we need to take into account the available space.</p> <p>For the example we will imaginary values for each item size and a maximum total size of the assortment of 35.</p> <pre><code>np.random.seed(123)\nsizes = np.random.randint(1, 10, size=len(assortment))\nprint(\"The random items sizes are:\", sizes)\nprint(\"Capacity of previous optimal assortment:\", np.sum(sizes * assortment))\nprint(\"Higher than our limit of 35!\")\n</code></pre> <pre><code>The random items sizes are: [3 3 7 2 4 7 2 1 2 1 1 4 5 1 1 5 2 8 4 3 5 8 3 5 9]\nCapacity of previous optimal assortment: 43.0\nHigher than our limit of 35!\n</code></pre> <pre><code>opt = LatentClassAssortmentOptimizer(\n    solver=solver,\n    class_weights=age_frequencies, # Weights of each class\n    class_utilities=np.exp(predicted_utilities), # utilities in the shape (n_classes, n_items)\n    itemwise_values=future_prices[0][:, 0], # Values to optimize for each item, here price that is used to compute turnover\n    assortment_size=12) # Size of the assortment we want\n\nopt.add_maximal_capacity_constraint(itemwise_capacities=sizes, maximum_capacity=35)\n\nassortment, opt_obj = opt.solve()\nprint(\"Our Optimal Assortment is:\")\nprint(assortment)\nprint(\"With an estimated average revenue of:\", opt_obj)\nprint(\"Size of our assortment:\", np.sum((assortment &gt; 0)), \"which is fine with our limit of 12!\")\nprint(\"Capacity of our new assortment:\", np.sum(sizes * assortment), \"which is below our limit of 35!\")\n</code></pre> <pre><code>Our Optimal Assortment is:\n[0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n 0.]\nWith an estimated average revenue of: 35.46667393198682\nSize of our assortment: 10 which is fine with our limit of 12!\nCapacity of our new assortment: 34.0 which is below our limit of 35!\n</code></pre> <p>The opposite constraint exists with .add_minimal_capacity_constraint() that adds a minimal value to be exceeded.</p>"},{"location":"notebooks/auxiliary_tools/assortment_example/#pricing-and-assortment-optimization","title":"Pricing and Assortment Optimization","text":"<p>Since our model takes the price into consideration, it is possible to optimize both the assortment and the price of the products of the assortment at the same time !</p> <p>The optimization is slightly more complex. The idea is to define a discretization of the prices with the correspondign utilities and itemwise values.</p> <p>Let's take as an example a product $i$ whose utility function is $U(i) = u_i - p_i$ with $u_i$=1.5 and $p_i$ its price.\\ We decide that the price range we accept to sell $i$ at is [2.5, 3.5] and to discretize into 6 values.\\ If we have the cost $c_i=2.0$ we can use the margin $m_i = p_i -c_i$ as itemwise value otherwise we can take the revenue, $r_i=p_i$.</p>   | Price | Utility | Margin | Revenue | | --- | --- | --- | --- | | 2.5 | -1.0 | 0.5 | 2.5\u00a0| | 2.7 | -1.2 | 0.7 | 2.7\u00a0| | 2.9 | -1.4 | 0.9 | 2.9\u00a0| | 3.1 | -1.6 | 1.1 | 3.1\u00a0| | 3.3 | -1.8 | 1.3 | 3.3\u00a0| | 3.5 | -2.0 | 1.5 | 3.5\u00a0|   <p>The idea in the optimization is either not to choose the item because another item is more valuable or to choose at most one price that is optimal toward our objective.</p> <p>Coming back to our example:</p> <pre><code># Lets create a discretized grid of prices for each item\nprices_grid = []\nfor item_index in range(25):\n    min_price = 0.9 * np.min(dataset.items_features_by_choice[0][:, item_index])\n    max_price = 1.1 * np.max(dataset.items_features_by_choice[0][:, item_index])\n    prices_grid.append(np.linspace(min_price, max_price, 10))\nprices_grid = np.stack(prices_grid, axis=0)\n\n# Computing the corresponding utilities\nitems_utilities = []\nfor age_index in [0, 1, 2]:\n    age_category = np.zeros((len(prices_grid[0]), 3)).astype(\"float32\")\n    age_category[:, age_index] = 1.\n    predicted_utilities = model.compute_batch_utility(shared_features_by_choice=age_category,\n                                                  items_features_by_choice=np.expand_dims(np.transpose(prices_grid), axis=-1),\n                                                  available_items_by_choice=None,\n                                                  choices=None\n                                                  )\n    items_utilities.append(np.exp(predicted_utilities).T)\nitem_utilities = np.stack(items_utilities, axis=0)\n\nprint(prices_grid.shape, item_utilities.shape)\n</code></pre> <pre><code>(25, 10) (3, 25, 10)\n</code></pre> <p>We use another AssortmentOptimizer class:</p> <pre><code>from choice_learn.toolbox.assortment_optimizer import LatentClassPricingOptimizer\n</code></pre> <pre><code>opt = LatentClassPricingOptimizer(\n    solver=solver,\n    class_weights=age_frequencies, # Weights of each class\n    class_utilities=item_utilities, # utilities in the shape (n_classes, n_items)\n    itemwise_values=prices_grid, # Values to optimize for each item, here price that is used to compute turnover\n    assortment_size=12) # Size of the assortment we want\n\n# opt.add_maximal_capacity_constraint(itemwise_capacities=sizes, maximum_capacity=35)\n\nassortment, opt_obj = opt.solve()\nprint(\"Our Optimal Assortment is:\")\nprint(assortment)\nprint(\"With an estimated average revenue of:\", opt_obj)\nprint(\"Size of our assortment:\", np.sum((assortment &gt; 0)), \"which is fine with our limit of 12!\")\n</code></pre> <pre><code>Our Optimal Assortment is:\n[ 0.         59.7        59.7        41.8        41.8        41.8\n  0.          0.          0.          0.         60.01111111 41.8\n  0.          0.         59.62222222 56.06666667 57.6         0.\n  0.         53.12222222  0.          0.          0.         56.7\n  0.        ]\nWith an estimated average revenue of: 41.21468607531388\nSize of our assortment: 12 which is fine with our limit of 12!\n</code></pre> <p>We can first observe that the estimated average revenue is higher than the previous one with the chosen prices.\\ Let's look at the difference:</p> <pre><code>print(\"| Previous price\",\"|\", \"Optimized price |\")\nprint(\"------------------------------------\")\nfor i in range(len(assortment)):\n    if assortment[i] &gt; 0:\n        print(\"|     \", future_prices[0][i, 0], \"     |     \", np.round(assortment[i], 1), \"      |\")\n</code></pre> <pre><code>| Previous price | Optimized price |\n------------------------------------\n|      72.0      |      59.7       |\n|      72.0      |      59.7       |\n|      38.0      |      41.8       |\n|      38.0      |      41.8       |\n|      38.0      |      41.8       |\n|      88.0      |      60.0       |\n|      38.0      |      41.8       |\n|      79.0      |      59.6       |\n|      72.0      |      56.1       |\n|      88.0      |      57.6       |\n|      35.0      |      53.1       |\n|      99.0      |      56.7       |\n</code></pre> <p>As previously, we can add capacity constraints:</p> <pre><code>opt = LatentClassPricingOptimizer(\n    solver=solver,\n    class_weights=age_frequencies, # Weights of each class\n    class_utilities=item_utilities, # utilities in the shape (n_classes, n_items)\n    itemwise_values=prices_grid, # Values to optimize for each item, here price that is used to compute turnover\n    assortment_size=12) # Size of the assortment we want\n\nopt.add_maximal_capacity_constraint(itemwise_capacities=sizes, maximum_capacity=35)\n\nassortment, opt_obj = opt.solve()\nprint(\"Our Optimal Assortment is:\")\nprint(assortment)\nprint(\"With an estimated average revenue of:\", opt_obj)\nprint(\"Size of our assortment:\", np.sum((assortment &gt; 0)), \"which is fine with our limit of 12!\")\nprint(\"Capacity of our new assortment:\", np.sum(sizes * (assortment &gt; 0)), \"which is below our limit of 35!\")\n</code></pre> <pre><code>Our Optimal Assortment is:\n[ 0.         59.7        59.7        41.8         0.          0.\n  0.          0.         41.8         0.         60.01111111 41.8\n  0.          0.         59.62222222 56.06666667 57.6         0.\n  0.         53.12222222  0.          0.          0.         56.7\n  0.        ]\nWith an estimated average revenue of: 41.164100003155916\nSize of our assortment: 11 which is fine with our limit of 12!\nCapacity of our new assortment: 35 which is below our limit of 35!\n</code></pre>"},{"location":"notebooks/auxiliary_tools/assortment_example/#ending-notes","title":"Ending Notes","text":"<ul> <li>In this example, the outside option is automatically integrated in the AssortmentOptimizer and not computed through the model. If you compute the outside option utility and give it to AssortmentOptimizer you can set its attribute outside_option_given to True.</li> <li>The current AssortmentOptimzer uses Gurobi for which you need a license (free for Academics) or OR-Tools that is OpenSource.</li> <li>If you want to add custom constraints you can use the base code of the AssortmentOptimizer and manually add your constraints. Future developments will add an easy interface to integrate such needs.</li> </ul>"},{"location":"notebooks/auxiliary_tools/assortment_example/#references","title":"References","text":"<p>[1] Isabel M\u00e9ndez-D\u00edaz, Juan Jos\u00e9 Miranda-Bront, Gustavo Vulcano, Paula Zabala, A branch-and-cut algorithm for the latent-class logit assortment problem, Discrete Applied Mathematics, Volume 164, Part 1, 2014, Pages 246-263, ISSN 0166-218X, https://doi.org/10.1016/j.dam.2012.03.003.</p>"}]}