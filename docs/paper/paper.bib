@TechReport{Bierlaire:2023,
  author = {Michel Bierlaire},
  title = {A short introduction to {Biogeme}},
  institution = {Transport and Mobility Laboratory, Ecole Polytechnique F\'ed\'erale de Lausanne},
  year = {2023},
  type = {Technical Report},
  number = {TRANSP-OR 230620},
  address = {Lausanne, Switzerland}
}

@article{Brathwaite:2018,
   title={Asymmetric, closed-form, finite-parameter models of multinomial choice},
   volume={29},
   ISSN={1755-5345},
   journal={Journal of Choice Modelling},
   publisher={Elsevier BV},
   author={Brathwaite, Timothy and Walker, Joan L.},
   year={2018},
   month=dec,
   pages={78–112}
}

@article{Du:2023,
      title={Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python},
      author={Tianyu Du and Ayush Kanodia and Susan Athey},
      year={2023},
  journal={arXiv preprint arXiv:{2304.01906}},
}

@article{Aouad:2023,
  title={Representing random utility choice models with neural networks},
  author={Aouad, Ali and D{\'e}sir, Antoine},
  journal={arXiv preprint arXiv:2207.12877},
  year={2022}
}

@article{Han:2022,
title = {A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability},
journal = {Transportation Research Part B: Methodological},
volume = {163},
pages = {166-186},
year = {2022},
issn = {0191-2615},
author = {Yafei Han and Francisco Camara Pereira and Moshe Ben-Akiva and Christopher Zegras},
keywords = {Discrete choice models, Neural networks, Taste heterogeneity, Interpretability, Utility specification, Machine learning, Deep learning},
abstract = {Discrete choice models (DCMs) require a priori knowledge of the utility functions, especially how tastes vary across individuals. Utility misspecification may lead to biased estimates, inaccurate interpretations and limited predictability. In this paper, we utilize a neural network to learn taste representation. Our formulation consists of two modules: a neural network (TasteNet) that learns taste parameters (e.g., time coefficient) as flexible functions of individual characteristics; and a multinomial logit (MNL) model with utility functions defined with expert knowledge. Taste parameters learned by the neural network are fed into the choice model and link the two modules. Our approach extends the L-MNL model (Sifringer et al., 2020) by allowing the neural network to learn the interactions between individual characteristics and alternative attributes. Moreover, we formalize and strengthen the interpretability condition — requiring realistic estimates of behavior indicators (e.g., value-of-time, elasticity) at the disaggregated level, which is crucial for a model to be suitable for scenario analysis and policy decisions. Through a unique network architecture and parameter transformation, we incorporate prior knowledge and guide the neural network to output realistic behavior indicators at the disaggregated level. We show that TasteNet-MNL reaches the ground-truth model’s predictability and recovers the nonlinear taste functions on synthetic data. Its estimated value-of-time and choice elasticities at the individual level are close to the ground truth. In contrast, exemplary logit models with misspecified systematic utility lead to biased parameter estimates and lower prediction accuracy. On a publicly available Swissmetro dataset, TasteNet-MNL outperforms benchmarking MNLs and Mixed Logit model’s predictability. It learns a broader spectrum of taste variations within the population and suggests a higher average value-of-time. Our source code is available for research and application.}
}

@article{Salvadé:2024,
  title={RUMBoost: Gradient Boosted Random Utility Models},
  author={Salvad{\'e}, Nicolas and Hillel, Tim},
  journal={arXiv preprint arXiv:2401.11954},
  year={2024}
}

@article{Train:1987,
 ISSN = {07416261},
 abstract = {We present an empirical model of households' choices among local telephone service options (for example, between flat-rate and measured service) and the interrelation of these choices with the number and average duration of local calls households make at each time of day to each geographical zone. Using a nested logit model with estimation performed on a randomly selected subset of the households' calling patterns, we calculate elasticities of demand for each local service option, number of calls, average duration, and revenues with respect to the fixed monthly charges and the usage charges for calling under each option. We find moderate price elasticities of number of calls with respect to usage charges for households subscribing to measured service. Nevertheless, raising usage charges has a negligible effect on revenues, since a sufficient number of households either originally subscribe to flat-rate service or convert to flat-rate service in response to higher usage charges. We find a high elasticity of demand for each service option with respect to its fixed monthly fee. This indicates high substitutability among service options. The shift among service options induces new calling patterns, which we find to be a small but not negligible indirect effect.},
 author = {Kenneth E. Train and Daniel L. McFadden and Moshe Ben-Akiva},
 journal = {The RAND Journal of Economics},
 number = {1},
 pages = {109--123},
 publisher = {[RAND Corporation, Wiley]},
 title = {The Demand for Local Telephone Service: A Fully Discrete Model of Residential Calling Patterns and Service Choices},
 volume = {18},
 year = {1987}
}

@Article{Harris:2020,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 publisher     = {Springer Science and Business Media {LLC}},
}

@software{Abadi:2015,
author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
license = {Apache-2.0},
month = nov,
title = {{TensorFlow, Large-scale machine learning on heterogeneous systems}},
year = {2015}
}

@Inbook{Nocedal:2006,
title="Large-Scale Unconstrained Optimization",
author ={Nocedal, Jorge and Wright, Stephen J.},
bookTitle="Numerical Optimization",
year="2006",
publisher="Springer New York",
address="New York, NY",
pages="164--192",
isbn="978-0-387-40065-5",
}

@article{Kingma:2017,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
  journal={arXiv preprint arXiv:{1412.6980}},
}

@article{Tieleman:2012,
  title={Lecture 6.5 {RMSProp}, coursera: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={University of Toronto, Technical Report},
  volume={6},
  year={2012}
}

@article{Expedia:2013,
      title={Personalize Expedia Hotel Searches},
      author={Ben Hamner, Adam and Friedman, Dan},
  journal={ICDM},
      year={2013},
      howpublished={\url{https://www.kaggle.com/c/expedia-personalized-sort} (last checked on 2024-06-07)},
}

@article{AouadMarket:2023,
  title={Market segmentation trees},
  author={Aouad, Ali and Elmachtoub, Adam N and Ferreira, Kris J and McNellis, Ryan},
  journal={Manufacturing \& Service Operations Management},
  volume={25},
  number={2},
  pages={648--667},
  year={2023},
  publisher={INFORMS}
}

@article{MendezDiaz:2014,
title = {A branch-and-cut algorithm for the latent-class logit assortment problem},
journal = {Discrete Applied Mathematics},
volume = {164},
pages = {246-263},
year = {2014},
note = {Combinatorial Optimization},
issn = {0166-218X},
author = {Isabel Méndez-Díaz and Juan José Miranda-Bront and Gustavo Vulcano and Paula Zabala},
keywords = {Retail operations, Revenue management, Choice behavior, Multinomial logit, Integer programming, Fractional programming},
abstract = {We study the product assortment problem of a retail operation that faces a stream of customers who are heterogeneous with respect to preferences. Each customer belongs to a market segment characterized by a consideration set that includes the alternatives viewed as options, and by the preference weights that the segment assigns to each of those alternatives. Upon arrival, he checks the offer set displayed by the firm, and either chooses one of those products or quits without purchasing according to a multinomial-logit (MNL) criterion. The firm’s goal is to maximize the expected revenue extracted during a fixed time horizon. This problem also arises in the growing area of choice-based, network revenue management, where computational speed is a critical factor for the practical viability of a solution approach. This so-called latent-class, logit assortment problem is known to be NP-Hard. In this paper, we analyze unconstrained and constrained (i.e., with a limited number of products to display) versions of it, and propose a branch-and-cut algorithm that is computationally fast and leads to (nearly) optimal solutions.}
}

@software{pandas:2020,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: {P}andas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
}

@inproceedings{Bierlaire:2001,
  title={The acceptance of modal innovation: The case of Swissmetro},
  author={Bierlaire, Michel and Axhausen, Kay and Abay, Georg},
  booktitle={Swiss transport research conference},
  year={2001}
}

@article{Pedregosa:2011,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@misc{Helveston:2023,
title={Convert data from wide to long format},
howpublished={\url{https://xlogit.readthedocs.io/en/latest/notebooks/convert_data_wide_to_long.html} (last checked on 2024-06-07)},
author={Forsythe, C. and Helveston, J},
  year={2023},
  urldate     = {2024-06-07},
}
@misc{Chollet:2015,
  title={Keras},
  author={Chollet, F. and others},
  year={2015},
  howpublished={\url{https://keras.io} (last checked on 2024-06-07)},
}

@article{McFadden:2000,
  title={Mixed MNL models for discrete response},
  author={McFadden, Daniel and Train, Kenneth},
  journal={Journal of applied Econometrics},
  volume={15},
  number={5},
  pages={447--470},
  year={2000},
  publisher={Wiley Online Library}
}

@misc{Gurobi:2023,
  author = {{Gurobi Optimization, LLC}},
  title = {{Gurobi Optimizer Reference Manual}},
  year = 2023,
  howpublished = {\url{https://www.gurobi.com} (last checked on 2024-06-07)}
}

@software{ORTools:2024,
  title = {OR-Tools},
  version = { v9.9 },
  author = {Laurent Perron and Vincent Furnon},
  organization = {Google},
  date = { 2024-03-07 },
  howpublished = {\url{https://developers.google.com/optimization/} (last checked on 2024-06-07)}
}
