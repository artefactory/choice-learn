@TechReport{Bierlaire:2023,
  author = {Michel Bierlaire},
  title = {A short introduction to {Biogeme}},
  institution = {Transport and Mobility Laboratory, Ecole Polytechnique F\'ed\'erale de Lausanne},
  year = {2023},
  type = {Technical Report},
  number = {TRANSP-OR 230620},
  address = {Lausanne, Switzerland}
}

@article{Brathwaite:2018,
   title={Asymmetric, closed-form, finite-parameter models of multinomial choice},
   volume={29},
   ISSN={1755-5345},
   url={http://dx.doi.org/10.1016/j.jocm.2018.01.002},
   DOI={10.1016/j.jocm.2018.01.002},
   journal={Journal of Choice Modelling},
   publisher={Elsevier BV},
   author={Brathwaite, Timothy and Walker, Joan L.},
   year={2018},
   month=dec,
   pages={78–112}
}

@misc{Du:2023,
      title={Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python},
      author={Tianyu Du and Ayush Kanodia and Susan Athey},
      year={2023},
      eprint={2304.01906},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Aouad:2023,
      title={Representing Random Utility Choice Models with Neural Networks},
      author={Ali Aouad and Antoine Désir},
      year={2023},
      eprint={2207.12877},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Han:2022,
title = {A neural-embedded discrete choice model: Learning taste representation with strengthened interpretability},
journal = {Transportation Research Part B: Methodological},
volume = {163},
pages = {166-186},
year = {2022},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191261522001138},
author = {Yafei Han and Francisco Camara Pereira and Moshe Ben-Akiva and Christopher Zegras},
keywords = {Discrete choice models, Neural networks, Taste heterogeneity, Interpretability, Utility specification, Machine learning, Deep learning},
abstract = {Discrete choice models (DCMs) require a priori knowledge of the utility functions, especially how tastes vary across individuals. Utility misspecification may lead to biased estimates, inaccurate interpretations and limited predictability. In this paper, we utilize a neural network to learn taste representation. Our formulation consists of two modules: a neural network (TasteNet) that learns taste parameters (e.g., time coefficient) as flexible functions of individual characteristics; and a multinomial logit (MNL) model with utility functions defined with expert knowledge. Taste parameters learned by the neural network are fed into the choice model and link the two modules. Our approach extends the L-MNL model (Sifringer et al., 2020) by allowing the neural network to learn the interactions between individual characteristics and alternative attributes. Moreover, we formalize and strengthen the interpretability condition — requiring realistic estimates of behavior indicators (e.g., value-of-time, elasticity) at the disaggregated level, which is crucial for a model to be suitable for scenario analysis and policy decisions. Through a unique network architecture and parameter transformation, we incorporate prior knowledge and guide the neural network to output realistic behavior indicators at the disaggregated level. We show that TasteNet-MNL reaches the ground-truth model’s predictability and recovers the nonlinear taste functions on synthetic data. Its estimated value-of-time and choice elasticities at the individual level are close to the ground truth. In contrast, exemplary logit models with misspecified systematic utility lead to biased parameter estimates and lower prediction accuracy. On a publicly available Swissmetro dataset, TasteNet-MNL outperforms benchmarking MNLs and Mixed Logit model’s predictability. It learns a broader spectrum of taste variations within the population and suggests a higher average value-of-time. Our source code is available for research and application.}
}
@misc{Salvadé:2024,
      title={RUMBoost: Gradient Boosted Random Utility Models},
      author={Nicolas Salvadé and Tim Hillel},
      year={2024},
      eprint={2401.11954},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Train:1987,
 ISSN = {07416261},
 URL = {http://www.jstor.org/stable/2555538},
 abstract = {We present an empirical model of households' choices among local telephone service options (for example, between flat-rate and measured service) and the interrelation of these choices with the number and average duration of local calls households make at each time of day to each geographical zone. Using a nested logit model with estimation performed on a randomly selected subset of the households' calling patterns, we calculate elasticities of demand for each local service option, number of calls, average duration, and revenues with respect to the fixed monthly charges and the usage charges for calling under each option. We find moderate price elasticities of number of calls with respect to usage charges for households subscribing to measured service. Nevertheless, raising usage charges has a negligible effect on revenues, since a sufficient number of households either originally subscribe to flat-rate service or convert to flat-rate service in response to higher usage charges. We find a high elasticity of demand for each service option with respect to its fixed monthly fee. This indicates high substitutability among service options. The shift among service options induces new calling patterns, which we find to be a small but not negligible indirect effect.},
 author = {Kenneth E. Train and Daniel L. McFadden and Moshe Ben-Akiva},
 journal = {The RAND Journal of Economics},
 number = {1},
 pages = {109--123},
 publisher = {[RAND Corporation, Wiley]},
 title = {The Demand for Local Telephone Service: A Fully Discrete Model of Residential Calling Patterns and Service Choices},
 urldate = {2024-03-19},
 volume = {18},
 year = {1987}
}

@Article{Harris:2020,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@software{Abadi:2015,
author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
doi = {10.5281/zenodo.4724125},
license = {Apache-2.0},
month = nov,
title = {{TensorFlow, Large-scale machine learning on heterogeneous systems}},
year = {2015}
}

@Inbook{Nocedal:2006,
title="Large-Scale Unconstrained Optimization",
author ={Nocedal, Jorge and Wright, Stephen J.},
bookTitle="Numerical Optimization",
year="2006",
publisher="Springer New York",
address="New York, NY",
pages="164--192",
isbn="978-0-387-40065-5",
doi="10.1007/978-0-387-40065-5_7",
url="https://doi.org/10.1007/978-0-387-40065-5_7"
}

@misc{Kingma:2017,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Tieleman:2012,
  title={Lecture 6.5 {RMSProp}, coursera: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={University of Toronto, Technical Report},
  volume={6},
  year={2012}
}

@misc{Expedia:2013,
      title={Personalize Expedia Hotel Searches - ICDM 2013},
      author={Ben Hamner, Adam and Friedman, Dan},
      year={2013},
      eprint={https://www.kaggle.com/c/expedia-personalized-sort},
      URL={https://www.kaggle.com/c/expedia-personalized-sort},
}
@misc{AouadMarket:2023,
      title={Market Segmentation Trees},
      author={Ali Aouad and Adam N. Elmachtoub and Kris J. Ferreira and Ryan McNellis},
      year={2023},
      eprint={1906.01174},
      archivePrefix={arXiv},
      primaryClass={stat.AP}
}
