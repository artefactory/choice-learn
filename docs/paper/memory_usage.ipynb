{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAM usage with Choice-Learn\n",
    "\n",
    "## On the ICDM 2013 Expedia Dataset\n",
    "\n",
    "- [Choice-Learn's FeaturesStorage](#features-storage-efficiency-with-dummy-data)\n",
    "- [pandas.DataFrame on Long format](#pandasdataframe-long-format-(pylogit))\n",
    "- [pandas.DataFrame on Wide format](#pandasdataframe-wide-format-(biogeme))\n",
    "- [Torch-Choice](#torch-choice)\n",
    "- [Plots and Illustrations](#plots-and-illustrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from choice_learn.datasets import load_expedia\n",
    "from choice_learn.data import ChoiceDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the RAM usag is actually not that obvious. Here is a code snippet finding all references link to an object in order to addition their memory consumption.\n",
    "This is explained [here](https://stackoverflow.com/questions/13530762/how-to-know-bytes-size-of-python-object-like-arrays-and-dictionaries-the-simp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "\n",
    "def get_obj_size(obj):\n",
    "    marked = {id(obj)}\n",
    "    obj_q = [obj]\n",
    "    sz = 0\n",
    "\n",
    "    while obj_q:\n",
    "        sz += sum(map(sys.getsizeof, obj_q))\n",
    "\n",
    "        # Lookup all the object referred to by the object in obj_q.\n",
    "        all_refr = ((id(o), o) for o in gc.get_referents(*obj_q))\n",
    "\n",
    "        # Filter object that are already marked.\n",
    "        new_refr = {o_id: o for o_id, o in all_refr if o_id not in marked and not isinstance(o, type)}\n",
    "\n",
    "        obj_q = new_refr.values()\n",
    "        marked.update(new_refr.keys())\n",
    "\n",
    "    return sz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Storage efficiency with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.data import OneHotStorage\n",
    "\n",
    "### Small Example\n",
    "\n",
    "n_locations = 10\n",
    "n_data = 100\n",
    "\n",
    "indexes = np.random.randint(n_locations, size=(n_data, ))\n",
    "\n",
    "dense_features = np.zeros((n_data, n_locations))\n",
    "dense_features[np.arange(n_data), indexes] = 1\n",
    "\n",
    "storage = OneHotStorage(ids=list(range(n_locations)))\n",
    "\n",
    "assert (storage.batch[indexes] == dense_features).all()\n",
    "\n",
    "### Dense features memory usage:\n",
    "print(\"Dense memory usage:\", get_obj_size(dense_features))\n",
    "\n",
    "### FeaturesByIDs memory usage:\n",
    "# Storage memory usage + ids memory stirage\n",
    "print(\"FeaturesByIDs memory usage:\", get_obj_size(storage)+get_obj_size(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.data import OneHotStorage\n",
    "\n",
    "\n",
    "dense_sizes = []\n",
    "fbid_sizes = []\n",
    "ds_lengths = [10, 100, 1000, 10000, 100000, 10000000]\n",
    "\n",
    "\n",
    "n_locations = 10\n",
    "for n_data in ds_lengths:\n",
    "\n",
    "    # random draw of store apparition\n",
    "    indexes = np.random.randint(n_locations, size=(n_data, ))\n",
    "\n",
    "    # Creation of the \"dense\" dataset\n",
    "    dense_dataset = np.zeros((n_data, n_locations))\n",
    "    dense_dataset[np.arange(n_data), indexes] = 1\n",
    "\n",
    "    # Creation of the Features storage\n",
    "    storage = OneHotStorage(ids=list(range(n_locations)))\n",
    "    \n",
    "    # Memory print of the features storage and the indexes\n",
    "    fbid_sizes.append(get_obj_size(indexes) + get_obj_size(storage))\n",
    "    # Memory print of the dense dataset\n",
    "    dense_sizes.append(get_obj_size(dense_dataset))\n",
    "\n",
    "plt.plot(ds_lengths, dense_sizes, label='w/o FeaturesById - n_locations=10', c=\"darkblue\")\n",
    "plt.plot(ds_lengths, fbid_sizes, label='w/ FeaturesById - n_locations=10', c=\"turquoise\")\n",
    "plt.scatter(ds_lengths, dense_sizes, c=\"darkblue\")\n",
    "plt.scatter(ds_lengths, fbid_sizes, c=\"turquoise\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "\n",
    "n_locations = 100\n",
    "\n",
    "\n",
    "dense_sizes = []\n",
    "fbid_sizes = []\n",
    "for n_data in ds_lengths:\n",
    "\n",
    "\n",
    "    # random draw of store apparition\n",
    "    indexes = np.random.randint(n_locations, size=(n_data, ))\n",
    "\n",
    "    # Creation of the \"dense\" dataset\n",
    "    dense_dataset = np.zeros((n_data, n_locations))\n",
    "    dense_dataset[np.arange(n_data), indexes] = 1\n",
    "\n",
    "    # Creation of the Features storage\n",
    "    storage = OneHotStorage(ids=list(range(n_locations)))\n",
    "\n",
    "    # Memory print of the features storage and the indexes\n",
    "    fbid_sizes.append(get_obj_size(indexes) + get_obj_size(storage))\n",
    "\n",
    "    # Memory print of the dense dataset\n",
    "    dense_sizes.append(get_obj_size(dense_dataset))\n",
    "    \n",
    "plt.plot(ds_lengths, dense_sizes, label='w/o FeaturesById - n_locations=100', c=\"cornflowerblue\")\n",
    "plt.plot(ds_lengths, fbid_sizes, label='w/ FeaturesById - n_locations=100', c=\"teal\")\n",
    "plt.scatter(ds_lengths, dense_sizes, c=\"cornflowerblue\")\n",
    "plt.scatter(ds_lengths, fbid_sizes, c=\"teal\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expedia Dataset\n",
    "\n",
    "You might want to run the different methods individually.\\\n",
    "The results are stored later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Choice-Learn and FeaturesByIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes some time\n",
    "dataset = load_expedia(as_frame=False, preprocessing=\"rumnet\")\n",
    "\n",
    "# Defining tested data lengths\n",
    "data_lengths = [100, 1000, 10000, 100000, 397618]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearn_memory_size = []\n",
    "for length in data_lengths:\n",
    "    sub_dataset = dataset[:length]\n",
    "    clearn_memory_size.append(get_obj_size(sub_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas.DataFrame Long format (PyLogit)\n",
    "\n",
    "The raw dataframe needs formatting to have the right format. It can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load DF\n",
    "expedia_df = load_expedia(as_frame=True)\n",
    "\n",
    "# Format dates & time features\n",
    "expedia_df.date_time = pd.to_datetime(expedia_df.date_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "expedia_df.loc[:, \"day_of_week\"] = expedia_df.loc[:, \"date_time\"].dt.dayofweek\n",
    "expedia_df.loc[:, \"month\"] = expedia_df.loc[:, \"date_time\"].dt.month\n",
    "expedia_df.loc[:, \"hour\"] = expedia_df.loc[:, \"date_time\"].dt.hour\n",
    "\n",
    "# Filtering ids with less than 1000 occurrences\n",
    "for id_col in [\n",
    "    \"site_id\",\n",
    "    \"visitor_location_country_id\",\n",
    "    \"prop_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "]:\n",
    "    value_counts = expedia_df[[\"srch_id\", id_col]].drop_duplicates()[id_col].value_counts()\n",
    "    kept_ids = value_counts.index[value_counts.gt(1000)]\n",
    "    for id_ in expedia_df[id_col].unique():\n",
    "        if id_ not in kept_ids:\n",
    "            expedia_df.loc[expedia_df[id_col] == id_, id_col] = -1\n",
    "\n",
    "# \"Filtering DF for price, stay length, booking window, etc.\n",
    "expedia_df = expedia_df[expedia_df.price_usd <= 1000]\n",
    "expedia_df = expedia_df[expedia_df.price_usd >= 10]\n",
    "expedia_df[\"log_price\"] = expedia_df.price_usd.apply(np.log)\n",
    "expedia_df = expedia_df[expedia_df.srch_length_of_stay <= 14]\n",
    "expedia_df = expedia_df[expedia_df.srch_booking_window <= 365]\n",
    "expedia_df[\"booking_window\"] = np.log(expedia_df[\"srch_booking_window\"] + 1)\n",
    "expedia_df = expedia_df.fillna(-1)\n",
    "\n",
    "# Sorting DF columns\n",
    "order_cols = [\n",
    "    \"srch_id\",\n",
    "    \"prop_id\",\n",
    "    \"prop_starrating\",\n",
    "    \"prop_review_score\",\n",
    "    \"prop_brand_bool\",\n",
    "    \"prop_location_score1\",\n",
    "    \"prop_location_score2\",\n",
    "    \"prop_log_historical_price\",\n",
    "    \"position\",\n",
    "    \"promotion_flag\",\n",
    "    \"srch_length_of_stay\",\n",
    "    \"srch_adults_count\",\n",
    "    \"srch_children_count\",\n",
    "    \"srch_room_count\",\n",
    "    \"srch_saturday_night_bool\",\n",
    "    \"orig_destination_distance\",\n",
    "    \"random_bool\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"log_price\",\n",
    "    \"booking_window\",\n",
    "    \"site_id\",\n",
    "    \"visitor_location_country_id\",\n",
    "    \"prop_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "    \"click_bool\",\n",
    "    \"booking_bool\",\n",
    "]\n",
    "expedia_df = expedia_df[order_cols]\n",
    "\n",
    "# Creating dummy availabilities\n",
    "# getting rid of search & prop_id and the clickbool and bookingbool\n",
    "# adding no_purchase fixed effect\n",
    "expedia_df[\"is_no_purchase\"] = 0\n",
    "\n",
    "# adding the no_purchase option to the data\n",
    "df1 = (\n",
    "    expedia_df.groupby(\"srch_id\")\n",
    "    .filter(lambda x: x.booking_bool.sum() == 1)\n",
    "    .groupby(\"srch_id\")\n",
    "    .max()\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "df1.loc[:, \"is_no_purchase\"] = 1\n",
    "df1.loc[:, \"log_price\"] = 0\n",
    "df1.loc[:, \"booking_bool\"] = 0\n",
    "\n",
    "df2 = (\n",
    "    expedia_df.groupby(\"srch_id\")\n",
    "    .filter(lambda x: x.booking_bool.sum() == 0)\n",
    "    .groupby(\"srch_id\")\n",
    "    .max()\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "df2.loc[:, \"is_no_purchase\"] = 1\n",
    "df2.loc[:, \"log_price\"] = 0\n",
    "df2.loc[:, \"booking_bool\"] = 1\n",
    "\n",
    "# Concatenating the created DFs\n",
    "expedia_df = pd.concat([expedia_df, df1, df2])\n",
    "\n",
    "# One Hot encoding \n",
    "site_id_one_hot = pd.get_dummies(expedia_df.site_id, prefix=\"site_id\")\n",
    "visitor_location_country_id_one_hot = pd.get_dummies(expedia_df.visitor_location_country_id, prefix=\"visitor_location_country_id\")\n",
    "srch_destination_id_one_hot =pd.get_dummies(expedia_df.srch_destination_id, prefix=\"srch_destination_id\")\n",
    "prop_country_id_one_hpt = pd.get_dummies(expedia_df.prop_country_id, prefix=\"prop_country_id\")\n",
    "expedia_df = pd.concat([expedia_df, site_id_one_hot, visitor_location_country_id_one_hot, srch_destination_id_one_hot, prop_country_id_one_hpt], axis=1)\n",
    "\n",
    "# Sorting\n",
    "expedia_df = expedia_df.sort_values(\"srch_id\")\n",
    "choices = [\"booking_bool\"]\n",
    "\n",
    "# Final selection of the columns to match the ChoiceDataset\n",
    "contexts_features_names = [\n",
    "    \"srch_id\",\n",
    "    \"srch_length_of_stay\",\n",
    "    \"srch_adults_count\",\n",
    "    \"srch_children_count\",\n",
    "    \"srch_room_count\",\n",
    "    \"srch_saturday_night_bool\",\n",
    "    \"booking_window\",\n",
    "    \"random_bool\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"site_id\",\n",
    "    \"visitor_location_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "]\n",
    "contexts_features_names += site_id_one_hot.columns.tolist()\n",
    "contexts_features_names += visitor_location_country_id_one_hot.columns.tolist()\n",
    "contexts_features_names += srch_destination_id_one_hot.columns.tolist()\n",
    "contexts_features_names += prop_country_id_one_hpt.columns.tolist()\n",
    "\n",
    "contexts_items_features_names = [\n",
    "    \"prop_starrating\",\n",
    "    \"prop_review_score\",\n",
    "    \"prop_brand_bool\",\n",
    "    \"prop_location_score1\",\n",
    "    \"prop_location_score2\",\n",
    "    \"prop_log_historical_price\",\n",
    "    \"position\",\n",
    "    \"promotion_flag\",\n",
    "    \"orig_destination_distance\",\n",
    "    \"log_price\",\n",
    "    \"prop_country_id\",\n",
    "]\n",
    "\n",
    "long_df = expedia_df[contexts_features_names + contexts_items_features_names + choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessinng RAM usage\n",
    "long_df_memory_size = []\n",
    "for length in data_lengths:\n",
    "    srch_ids = long_df.srch_id.unique()[:length]\n",
    "    sub_long_df = long_df[long_df.srch_id.isin(srch_ids)]\n",
    "    long_df_memory_size.append(get_obj_size(sub_long_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas.DataFrame wide format (Biogeme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = load_expedia(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation of the pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expedia_df = load_expedia(as_frame=True)\n",
    "\n",
    "# Format dates & time features\n",
    "expedia_df.date_time = pd.to_datetime(expedia_df.date_time, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "expedia_df.loc[:, \"day_of_week\"] = expedia_df.loc[:, \"date_time\"].dt.dayofweek\n",
    "expedia_df.loc[:, \"month\"] = expedia_df.loc[:, \"date_time\"].dt.month\n",
    "expedia_df.loc[:, \"hour\"] = expedia_df.loc[:, \"date_time\"].dt.hour\n",
    "\n",
    "# Filtering ids with less than 1000 occurrences\n",
    "for id_col in [\n",
    "    \"site_id\",\n",
    "    \"visitor_location_country_id\",\n",
    "    \"prop_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "]:\n",
    "    value_counts = expedia_df[[\"srch_id\", id_col]].drop_duplicates()[id_col].value_counts()\n",
    "    kept_ids = value_counts.index[value_counts.gt(1000)]\n",
    "    for id_ in expedia_df[id_col].unique():\n",
    "        if id_ not in kept_ids:\n",
    "            expedia_df.loc[expedia_df[id_col] == id_, id_col] = -1\n",
    "\n",
    "# \"Filtering DF for price, stay length, booking window, etc.\n",
    "expedia_df = expedia_df[expedia_df.price_usd <= 1000]\n",
    "expedia_df = expedia_df[expedia_df.price_usd >= 10]\n",
    "expedia_df[\"log_price\"] = expedia_df.price_usd.apply(np.log)\n",
    "expedia_df = expedia_df[expedia_df.srch_length_of_stay <= 14]\n",
    "expedia_df = expedia_df[expedia_df.srch_booking_window <= 365]\n",
    "expedia_df[\"booking_window\"] = np.log(expedia_df[\"srch_booking_window\"] + 1)\n",
    "expedia_df = expedia_df.fillna(-1)\n",
    "\n",
    "# Sorting DF columns\n",
    "order_cols = [\n",
    "    \"srch_id\",\n",
    "    \"prop_id\",\n",
    "    \"prop_starrating\",\n",
    "    \"prop_review_score\",\n",
    "    \"prop_brand_bool\",\n",
    "    \"prop_location_score1\",\n",
    "    \"prop_location_score2\",\n",
    "    \"prop_log_historical_price\",\n",
    "    \"position\",\n",
    "    \"promotion_flag\",\n",
    "    \"srch_length_of_stay\",\n",
    "    \"srch_adults_count\",\n",
    "    \"srch_children_count\",\n",
    "    \"srch_room_count\",\n",
    "    \"srch_saturday_night_bool\",\n",
    "    \"orig_destination_distance\",\n",
    "    \"random_bool\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"log_price\",\n",
    "    \"booking_window\",\n",
    "    \"site_id\",\n",
    "    \"visitor_location_country_id\",\n",
    "    \"prop_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "    \"click_bool\",\n",
    "    \"booking_bool\",\n",
    "]\n",
    "expedia_df = expedia_df[order_cols]\n",
    "\n",
    "# Creating dummy availabilities\n",
    "# getting rid of search & prop_id and the clickbool and bookingbool\n",
    "# adding no_purchase fixed effect\n",
    "expedia_df[\"is_no_purchase\"] = 0\n",
    "\n",
    "# adding the no_purchase option to the data\n",
    "df1 = (\n",
    "    expedia_df.groupby(\"srch_id\")\n",
    "    .filter(lambda x: x.booking_bool.sum() == 1)\n",
    "    .groupby(\"srch_id\")\n",
    "    .max()\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "df1.loc[:, \"is_no_purchase\"] = 1\n",
    "df1.loc[:, \"log_price\"] = 0\n",
    "df1.loc[:, \"booking_bool\"] = 0\n",
    "\n",
    "df2 = (\n",
    "    expedia_df.groupby(\"srch_id\")\n",
    "    .filter(lambda x: x.booking_bool.sum() == 0)\n",
    "    .groupby(\"srch_id\")\n",
    "    .max()\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "df2.loc[:, \"is_no_purchase\"] = 1\n",
    "df2.loc[:, \"log_price\"] = 0\n",
    "df2.loc[:, \"booking_bool\"] = 1\n",
    "\n",
    "# Concatenating the created DFs\n",
    "expedia_df = pd.concat([expedia_df, df1, df2])\n",
    "\n",
    "site_id_one_hot = pd.get_dummies(expedia_df.site_id, prefix=\"site_id\")\n",
    "visitor_location_country_id_one_hot = pd.get_dummies(expedia_df.visitor_location_country_id, prefix=\"visitor_location_country_id\")\n",
    "srch_destination_id_one_hot = pd.get_dummies(expedia_df.srch_destination_id, prefix=\"srch_destination_id\")\n",
    "prop_country_id_one_hpt = pd.get_dummies(expedia_df.prop_country_id, prefix=\"prop_country_id\")\n",
    "expedia_df = pd.concat([expedia_df, site_id_one_hot, visitor_location_country_id_one_hot, srch_destination_id_one_hot, prop_country_id_one_hpt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_items_features_names = [\n",
    "    \"prop_starrating\",\n",
    "    \"prop_review_score\",\n",
    "    \"prop_brand_bool\",\n",
    "    \"prop_location_score1\",\n",
    "    \"prop_location_score2\",\n",
    "    \"prop_log_historical_price\",\n",
    "    \"position\",\n",
    "    \"promotion_flag\",\n",
    "    \"orig_destination_distance\",\n",
    "    \"log_price\",\n",
    "    \"prop_country_id\",\n",
    "]\n",
    "contexts_features_names = [\n",
    "    \"srch_id\",\n",
    "    \"srch_length_of_stay\",\n",
    "    \"srch_adults_count\",\n",
    "    \"srch_children_count\",\n",
    "    \"srch_room_count\",\n",
    "    \"srch_saturday_night_bool\",\n",
    "    \"booking_window\",\n",
    "    \"random_bool\",\n",
    "    \"day_of_week\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"site_id\",\n",
    "    \"visitor_location_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "]\n",
    "for col in expedia_df.columns:\n",
    "    if col.startswith(\"prop_country_id\"):\n",
    "        contexts_items_features_names += [col]\n",
    "    if col.startswith(\"site_id\"):\n",
    "        contexts_features_names += [col]\n",
    "    if col.startswith(\"visitor_location_country_id\"):\n",
    "        contexts_features_names += [col]\n",
    "    if col.startswith(\"srch_destination_id\"):\n",
    "        contexts_features_names += [col]\n",
    "\n",
    "wide_items = []\n",
    "for i in range(39):\n",
    "    try:\n",
    "        sub_df = expedia_df.groupby(\"srch_id\").apply(lambda x: x[contexts_items_features_names].iloc[i])\n",
    "    except IndexError:\n",
    "        # Add dummy row\n",
    "        sub_df = pd.DataFrame({col: -1 for col in contexts_items_features_names}, index=[0])\n",
    "    wide_items.append(sub_df)\n",
    "contexts_features = expedia_df.groupby(\"srch_id\").apply(lambda x: x[contexts_features_names].iloc[0])\n",
    "wide_df = pd.concat(wide_items+[contexts_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_memory_size = []\n",
    "for length in data_lengths:\n",
    "    sub_wide_df = wide_df.iloc[:length].copy()\n",
    "    wide_df_memory_size.append(get_obj_size(sub_wide_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch-Choice\n",
    "\n",
    "For this part you will need the torch-choice package: ```pip install torch-choice```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_choice.utils.easy_data_wrapper import EasyDatasetWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create long_df with previous section\n",
    "\n",
    "long_df = long_df.reset_index(drop=True)\n",
    "long_df.sort_values(\"srch_id\", inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ids = []\n",
    "for nit in long_df.srch_id.value_counts().sort_index():\n",
    "    items_ids.append(np.arange(nit))\n",
    "long_df[\"items_id\"] = np.concatenate(items_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_sizes = []\n",
    "data_lengths = [100, 1000, 10000, 100000, 397618]\n",
    "for length in data_lengths:\n",
    "    ids = long_df.srch_id.unique()[:length]\n",
    "    sub_long_df = long_df[long_df.srch_id.isin(ids)].copy(deep=True)\n",
    "\n",
    "    data_1 = EasyDatasetWrapper(main_data=sub_long_df,\n",
    "                            purchase_record_column='srch_id',\n",
    "                            choice_column='booking_bool',\n",
    "                            item_name_column='items_id',\n",
    "                            session_index_column='srch_id',\n",
    "                            user_index_column='srch_id',\n",
    "                            # it can be derived from columns of the dataframe or supplied as\n",
    "                            user_observable_columns=['srch_length_of_stay',\n",
    "                                                    'srch_adults_count',\n",
    "                                                    'srch_children_count',\n",
    "                                                    'srch_room_count',\n",
    "                                                    'srch_saturday_night_bool'],\n",
    "                            price_observable_columns=['log_price'],\n",
    "                            device=\"cpu\")\n",
    "    mem_sizes.append(get_obj_size(data_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "| Dataset size | 100 | 1.000 | 10.000 | 100.000 | 397.618 |\n",
    "|---|---|---|---|---|---|\n",
    "| C-L w. FeaturesByIDs | 85.640 | 771.440 | 7.629.440 | 76.209.440 | 302.994.356 |\n",
    "| C-L wo FeaturesByIDs | 220.400 | 2.198.600 | 21.980.600 | 219.800.600 | 873.964.964 |\n",
    "| Torch-Choice | 5.825.168 | 5.5093.676 | 550.480.667 | 5.515.236.600 | 10.448.857.759 |\n",
    "| Long format DF | 5.521.360 | 52.463.080 | 524.667.470 | 5.234.198.450 | 20.815.361.140 |\n",
    "| Wide format DF | 3.503.109 | 31.784.709 | 314.600.709 | 3.142.760.709 | 12.495.108.741 |\n",
    "\n",
    "\n",
    "data_lengths: [100, 1000, 10000, 100000, 397618]\\\n",
    "Choice-Learn with FeaturesByIDs: [85640, 771440, 7629440, 76209440, 302994356]\\\n",
    "Choice-Learn without FeaturesByIDs: [220400, 2198600, 21980600, 219800600, 873964964]\\\n",
    "Torch-Choice: [5.825.168, 55.093.676, 550.480.667, 5.515.236.600, 10.448.857.871]\\\n",
    "DF Long Format: [5521360, 52463080, 524667470, 5234198450, 20815361140]\\\n",
    "DF Wide Format: [3503109, 31784709, 314600709, 3142760709, 12495108741]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Illustrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\"#e69f00\", \"#56b4e9\", \"#009e73\", \"#0072b2\", \"#d55e00\", \"#cc79a7\", \"#f0e442\"]\n",
    "linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ds_lengths = [10, 100, 1000, 10000, 100000, 10000000]\n",
    "dense_1 = [928, 8128, 80128, 800128, 8000128, 800000128]\n",
    "fbid_1 =[1036, 1756, 8956, 80956, 800956, 80000956]\n",
    "dense_2 = [8128, 80128, 800128, 8000128, 80000128, 8000000128]\n",
    "fbid_2 = [7892, 8612, 15812, 87812, 807812, 80007812]\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(ds_lengths, dense_1, label='w/o FeaturesStorage - 10 locations', c=colors[0], ls=linestyle[0])\n",
    "plt.plot(ds_lengths, fbid_1, label='w/ FeaturesStorage - 10 locations', c=colors[1], ls=linestyle[1])\n",
    "plt.scatter(ds_lengths, dense_1, c=colors[0])\n",
    "plt.scatter(ds_lengths, fbid_1, c=colors[1])\n",
    "\n",
    "plt.plot(ds_lengths, dense_2, label='w/o FeaturesStorage - 100 locations', c=colors[2], ls=linestyle[2])\n",
    "plt.plot(ds_lengths, fbid_2, label='w/ FeaturesStorage - 100 locations', c=colors[3], ls=linestyle[3])\n",
    "plt.scatter(ds_lengths, dense_2, c=colors[2])\n",
    "plt.scatter(ds_lengths, fbid_2, c=colors[3])\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.legend(prop={'size': 8})\n",
    "\n",
    "plt.title(\"(a) Memory usage of our retail dataset \\n for different dataset sizes\", y=-.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "data_lengths = [100, 1000, 10000, 100000, 397618]\n",
    "cd_with = [85640, 771440, 7629440, 76209440, 302994356]\n",
    "cd_wo =  [220400, 2198600, 21980600, 219800600, 873964964]\n",
    "df_long =  [5521360, 52463080, 524667470, 5234198450, 20815361140]\n",
    "df_wide =  [6252516976, 6266664976, 6408144976, 7822944976, 12501499936]\n",
    "df_wide = [3503109, 31784709, 314600709, 3142760709, 12495108741]\n",
    "torch_choice = [1249040, 12481040, 124801040, 1248001040, 4962273680]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_lengths, cd_with, label=\"ChoiceDataset with FeaturesByIDs\", c=\"teal\")\n",
    "plt.plot(data_lengths, cd_wo, label=\"ChoiceDataset withOUT FeaturesByIDs\", c=\"turquoise\")\n",
    "plt.plot(data_lengths, df_long, label=\"pd.DataFrame Long format\", c=\"darkblue\")\n",
    "plt.plot(data_lengths, df_wide, label=\"pd.DataFrame Wide format\", c=\"cornflowerblue\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size (x1000)\")\n",
    "plt.ylabel(\"Memory Size (bytes)\")\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000],\n",
    "           [0, 50, 100, 150, 200, 250, 300, 350, 400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_lengths, cd_with, label=\"Choice-Learn\", c=\"teal\")\n",
    "plt.plot(data_lengths, torch_choice, label=\"Torch-Choice\", c=\"turquoise\")\n",
    "plt.plot(data_lengths, df_long, label=\"PyLogit (long format)\", c=\"darkblue\")\n",
    "plt.plot(data_lengths, df_wide, label=\"Biogeme (wide format)\", c=\"cornflowerblue\")\n",
    "plt.scatter(data_lengths, cd_with, c=\"teal\")\n",
    "plt.scatter(data_lengths, torch_choice, c=\"turquoise\")\n",
    "plt.scatter(data_lengths, df_long, c=\"darkblue\")\n",
    "plt.scatter(data_lengths, df_wide, c=\"cornflowerblue\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.title(\"(b) Memory usage of our retail dataset \\n for different dataset sizes\", y=-.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "sizes = [100, 1000, 10000.0, 100000.0, 1000000.0, 4789225]\n",
    "tc = [3933312, 4854912, 14070912, 106230912, 1027830912, 4907997312]\n",
    "# Wide \n",
    "wide = [629748, 5954312, 59178320, 591244240, 5914324260, 28317686484]\n",
    "# Long\n",
    "long = [729260, 7216256, 79819560, 1000640616, 10190708220, 47241911756]\n",
    "# CL\n",
    "cl = [163734, 526146, 3921306, 34453314, 319713662, 1546499942]\n",
    "\n",
    "plt.plot(sizes, cl, label=\"Choice-Learn\", c=colors[3], ls=linestyle[3])\n",
    "plt.plot(sizes, tc, label=\"Torch-Choice\", c=colors[1], ls=linestyle[1])\n",
    "plt.plot(sizes, long, label=\"PyLogit (long format)\", c=colors[0], ls=linestyle[0])\n",
    "plt.plot(sizes, wide, label=\"Biogeme (wide format)\", c=colors[2], ls=linestyle[2])\n",
    "plt.scatter(sizes, cl, c=colors[3])\n",
    "plt.scatter(sizes, tc, c=colors[1])\n",
    "plt.scatter(sizes, long, c=colors[0])\n",
    "plt.scatter(sizes, wide, c=colors[2])\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.title(\"(c) Memory usage of our retail dataset \\n for different dataset sizes\", y=-.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "n_stores = [0, 10, 100, 250, 692]\n",
    "cl_mem_usage = [352083911, 352084711, 352163911, 352583911, 355914823]\n",
    "other_mem = [1027830912, 1027831712, 1027910912, 1028330912, 1031661824]\n",
    "long_mem_usage = [4654663932, 4734664572, 5454670332, 6654679932, 10190708220]\n",
    "wide_mem_usage = [378279972, 458280612, 1178286372, 2378295972, 5914324260]\n",
    "\n",
    "plt.plot(n_stores, cl_mem_usage, label=\"Choice-Learn\", c=colors[3], ls=linestyle[3])\n",
    "plt.plot(n_stores, other_mem, label=\"Torch-Choice\", c=colors[1], ls=linestyle[1])\n",
    "plt.plot(n_stores, long_mem_usage, label=\"PyLogit (long format)\", c=colors[0], ls=linestyle[0])\n",
    "plt.plot(n_stores, wide_mem_usage, label=\"Biogeme (wide format)\", c=colors[2], ls=linestyle[2])\n",
    "plt.scatter(n_stores, cl_mem_usage, c=colors[3])\n",
    "plt.scatter(n_stores, other_mem, c=colors[1])\n",
    "plt.scatter(n_stores, long_mem_usage, c=colors[0])\n",
    "plt.scatter(n_stores, wide_mem_usage, c=colors[2])\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Stores Number\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.title(\"(d) Memory usage of our retail dataset \\n for different number of stores\", y=-.3)\n",
    "# plt.xticks([0, 50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000],\n",
    "#            [0, 50, 100, 150, 200, 250, 300, 350, 400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\"#e69f00\", \"#56b4e9\", \"#009e73\", \"#0072b2\", \"#d55e00\", \"#cc79a7\", \"#f0e442\"]\n",
    "linestyle=[\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\"]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "ds_lengths = [10, 100, 1000, 10000, 100000, 10000000]\n",
    "dense_1 = [928, 8128, 80128, 800128, 8000128, 800000128]\n",
    "fbid_1 =[1036, 1756, 8956, 80956, 800956, 80000956]\n",
    "dense_2 = [8128, 80128, 800128, 8000128, 80000128, 8000000128]\n",
    "fbid_2 = [7892, 8612, 15812, 87812, 807812, 80007812]\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(ds_lengths, dense_1, label='w/o FeaturesStorage - 10 locations', c=colors[0], ls=linestyle[0])\n",
    "plt.plot(ds_lengths, fbid_1, label='w/ FeaturesStorage - 10 locations', c=colors[1], ls=linestyle[1])\n",
    "plt.scatter(ds_lengths, dense_1, c=colors[0])\n",
    "plt.scatter(ds_lengths, fbid_1, c=colors[1])\n",
    "\n",
    "plt.plot(ds_lengths, dense_2, label='w/o FeaturesStorage - 100 locations', c=colors[2], ls=linestyle[2])\n",
    "plt.plot(ds_lengths, fbid_2, label='w/ FeaturesStorage - 100 locations', c=colors[3], ls=linestyle[3])\n",
    "plt.scatter(ds_lengths, dense_2, c=colors[2])\n",
    "plt.scatter(ds_lengths, fbid_2, c=colors[3])\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.legend(prop={'size': 8})\n",
    "\n",
    "plt.title(\"(a) Memory usage of our retail dataset \\n for different dataset sizes\", y=-.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "data_lengths = [100, 1000, 10000, 100000, 397618]\n",
    "cd_with = [85640, 771440, 7629440, 76209440, 302994356]\n",
    "cd_wo =  [220400, 2198600, 21980600, 219800600, 873964964]\n",
    "df_long =  [5521360, 52463080, 524667470, 5234198450, 20815361140]\n",
    "df_wide =  [6252516976, 6266664976, 6408144976, 7822944976, 12501499936]\n",
    "df_wide = [3503109, 31784709, 314600709, 3142760709, 12495108741]\n",
    "torch_choice = [1249040, 12481040, 124801040, 1248001040, 4962273680]\n",
    "\n",
    "plt.plot(data_lengths, cd_with, label=\"Choice-Learn\", c=colors[3], ls=linestyle[3])\n",
    "plt.plot(data_lengths, torch_choice, label=\"Torch-Choice\", c=colors[1], ls=linestyle[1])\n",
    "plt.plot(data_lengths, df_long, label=\"PyLogit (long format)\", c=colors[0], ls=linestyle[0])\n",
    "plt.plot(data_lengths, df_wide, label=\"Biogeme (wide format)\", c=colors[2], ls=linestyle[2])\n",
    "plt.scatter(data_lengths, cd_with, c=colors[3])\n",
    "plt.scatter(data_lengths, torch_choice, c=colors[1])\n",
    "plt.scatter(data_lengths, df_long, c=colors[0])\n",
    "plt.scatter(data_lengths, df_wide, c=colors[2])\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.title(\"(b) Memory usage of our retail dataset \\n for different dataset sizes\", y=-.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "sizes = [100, 1000, 10000.0, 100000.0, 1000000.0, 4789225]\n",
    "tc = [3933312, 4854912, 14070912, 106230912, 1027830912, 4907997312]\n",
    "# Wide \n",
    "wide = [629748, 5954312, 59178320, 591244240, 5914324260, 28317686484]\n",
    "# Long\n",
    "long = [729260, 7216256, 79819560, 1000640616, 10190708220, 47241911756]\n",
    "# CL\n",
    "cl = [163734, 526146, 3921306, 34453314, 319713662, 1546499942]\n",
    "\n",
    "plt.plot(sizes, cl, label=\"Choice-Learn\", c=colors[3], ls=linestyle[3])\n",
    "plt.plot(sizes, tc, label=\"Torch-Choice\", c=colors[1], ls=linestyle[1])\n",
    "plt.plot(sizes, long, label=\"PyLogit (long format)\", c=colors[0], ls=linestyle[0])\n",
    "plt.plot(sizes, wide, label=\"Biogeme (wide format)\", c=colors[2], ls=linestyle[2])\n",
    "plt.scatter(sizes, cl, c=colors[3])\n",
    "plt.scatter(sizes, tc, c=colors[1])\n",
    "plt.scatter(sizes, long, c=colors[0])\n",
    "plt.scatter(sizes, wide, c=colors[2])\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.title(\"(c) Memory usage of our retail dataset \\n for different dataset sizes\", y=-.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "n_stores = [0, 10, 100, 250, 692]\n",
    "cl_mem_usage = [352083911, 352084711, 352163911, 352583911, 355914823]\n",
    "other_mem = [1027830912, 1027831712, 1027910912, 1028330912, 1031661824]\n",
    "long_mem_usage = [4654663932, 4734664572, 5454670332, 6654679932, 10190708220]\n",
    "wide_mem_usage = [378279972, 458280612, 1178286372, 2378295972, 5914324260]\n",
    "\n",
    "plt.plot(n_stores, cl_mem_usage, label=\"Choice-Learn\", c=colors[3], ls=linestyle[3])\n",
    "plt.plot(n_stores, other_mem, label=\"Torch-Choice\", c=colors[1], ls=linestyle[1])\n",
    "plt.plot(n_stores, long_mem_usage, label=\"PyLogit (long format)\", c=colors[0], ls=linestyle[0])\n",
    "plt.plot(n_stores, wide_mem_usage, label=\"Biogeme (wide format)\", c=colors[2], ls=linestyle[2])\n",
    "plt.scatter(n_stores, cl_mem_usage, c=colors[3])\n",
    "plt.scatter(n_stores, other_mem, c=colors[1])\n",
    "plt.scatter(n_stores, long_mem_usage, c=colors[0])\n",
    "plt.scatter(n_stores, wide_mem_usage, c=colors[2])\n",
    "plt.legend(prop={'size': 8})\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Stores Number\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.title(\"(d) Memory usage of our retail dataset \\n for different number of stores\", y=-.3)\n",
    "# plt.xticks([0, 50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000],\n",
    "#            [0, 50, 100, 150, 200, 250, 300, 350, 400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeaturesByIDs Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fixed_features = 10\n",
    "n_different_values = 10\n",
    "\n",
    "normal_sizes = []\n",
    "cd_sizes = []\n",
    "ds_lengths = [10, 100, 1000, 10000, 100000, 10000000]\n",
    "for dataset_len in ds_lengths:\n",
    "\n",
    "    normal_dataset = np.ones((dataset_len, n_fixed_features))\n",
    "    cd_dataset = (np.ones((dataset_len, 1)), np.ones((n_different_values, n_fixed_features)))\n",
    "    \n",
    "    cd_sizes.append(sys.getsizeof(cd_dataset[0]) + sys.getsizeof(cd_dataset[1]))\n",
    "    normal_sizes.append(sys.getsizeof(normal_dataset))\n",
    "\n",
    "plt.plot(ds_lengths, normal_sizes, label='w/o FeaturesById - (10, 10)', c=\"darkblue\")\n",
    "plt.plot(ds_lengths, cd_sizes, label='w/ FeaturesById - (10, 10)', c=\"turquoise\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "\n",
    "n_fixed_features = 100\n",
    "n_different_values = 100\n",
    "\n",
    "normal_sizes = []\n",
    "cd_sizes = []\n",
    "ds_lengths = [10, 100, 1000, 10000, 100000, 10000000]\n",
    "for dataset_len in ds_lengths:\n",
    "\n",
    "    normal_dataset = np.ones((dataset_len, n_fixed_features))\n",
    "    cd_dataset = (np.ones((dataset_len, 1)), np.ones((n_different_values, n_fixed_features)))\n",
    "    \n",
    "    cd_sizes.append(sys.getsizeof(cd_dataset[0]) + sys.getsizeof(cd_dataset[1]))\n",
    "    normal_sizes.append(sys.getsizeof(normal_dataset))\n",
    "\n",
    "plt.plot(ds_lengths, normal_sizes, label='w/o FeaturesById - (100, 100)', c=\"cornflowerblue\")\n",
    "plt.plot(ds_lengths, cd_sizes, label='w/ FeaturesById - (100, 100)', c=\"teal\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Memory usage (bytes)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
