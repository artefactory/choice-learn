{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the implementation of ResLogit\n",
    "\n",
    "We reproduce in this notebook the results of the paper ResLogit: A residual neural network logit model for data-driven choice modelling [1].\n",
    "\n",
    "Inspired by https://github.com/LiTrans/reslogit/blob/master/main.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove/Add GPU use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from choice_learn.models import reslogit_bis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "choices = [\"low\", \"high\"]\n",
    "batch_size = 64 # fixed\n",
    "learning_rate = 0.001\n",
    "n_layers = 16\n",
    "n_epochs = 1000\n",
    "patience = 30\n",
    "patience_increase = 3\n",
    "improvement_threshold = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the same data preparation as in the original paper in order to get the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "raw_data = pd.read_csv(\"../../choice_learn/datasets/data/Pedestrian_Waittime.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sefine the inputs and output\n",
    "x_data = raw_data.iloc[:,5:21]\n",
    "y_data=raw_data[\"category\"]\n",
    "\n",
    "# List of explanatory variable names\n",
    "variables = list(x_data.columns)\n",
    "\n",
    "# Number of observations, variables and choices\n",
    "n_obs = raw_data.shape[0]\n",
    "n_vars = x_data.shape[1]\n",
    "n_choices = len(choices)\n",
    "\n",
    "# Slicing index for train and valid split\n",
    "slice = np.floor(0.7 * n_obs).astype(int)\n",
    "\n",
    "# Slices x and y datasets into train and valid datasets based on slice\n",
    "train_x_data, valid_x_data = x_data.iloc[:slice], x_data.iloc[slice:]\n",
    "train_y_data, valid_y_data = y_data.iloc[:slice], y_data.iloc[slice:]\n",
    "\n",
    "# Number of train and valid batches\n",
    "n_train_batches = train_y_data.shape[0] // batch_size\n",
    "n_valid_batches = valid_y_data.shape[0] // batch_size\n",
    "\n",
    "# Define ordinal levels for train and valid dataset\n",
    "train_level_data = pd.DataFrame(columns = [\"level1\"])\n",
    "for i in range (train_y_data.shape[0]):\n",
    "    label_train = y_data[i]\n",
    "    classifier_train = [1] * (label_train -1) + [0] * (n_choices - label_train)\n",
    "    train_level_data_lenght = len(train_level_data)\n",
    "    train_level_data.loc[train_level_data_lenght] = classifier_train\n",
    "\n",
    "valid_level_data =pd.DataFrame(columns = [\"level1\"])\n",
    "for i in range (valid_y_data.shape[0]):\n",
    "    label_valid = y_data[i + train_y_data.shape[0]]\n",
    "    classifier_valid = [1] * (label_valid -1) + [0] * (n_choices - label_valid)\n",
    "    valid_level_data_length = len(valid_level_data)\n",
    "    valid_level_data.loc[valid_level_data_length] = classifier_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow tensor\n",
    "train_x_tensor = tf.convert_to_tensor(train_x_data.values, dtype=tf.float64)\n",
    "train_y_tensor = tf.convert_to_tensor(train_y_data.values , dtype = tf.int64)\n",
    "valid_x_tensor = tf.convert_to_tensor(valid_x_data.values , dtype = tf.float64)\n",
    "valid_y_tensor = tf.convert_to_tensor(valid_y_data.values , dtype = tf.int64)\n",
    "train_level_tensor = tf.convert_to_tensor(train_level_data.values , dtype = tf.int64)\n",
    "valid_level_tensor = tf.convert_to_tensor(valid_level_data.values , dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ResNet model\n",
    "training_object = reslogit_bis.TrainingResLogit()\n",
    "training_object.main_model_reslogit(x=train_x_tensor, y=train_y_tensor, n_vars=n_vars, n_choices=n_choices, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] ResLogit: A residual neural network logit model for data-driven choice modelling, Wong, M.; Farooq, B (2021), Transportation Research Part C: Emerging Technologies 126\\\n",
    "(URL: https://doi.org/10.1016/j.trc.2021.103050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "epoch = 0\n",
    "valid_freq = min(200, n_train_batches)\n",
    "best_validation_ll = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "done_looping = False\n",
    "step = 0\n",
    "\n",
    "\n",
    "filename = \"{}{}_bestmodel.pkl\".format(\"reslogit\", n_layers)\n",
    "training_frame = pd.DataFrame(columns=[\"epoch\", \"minibatch\", \"batches\", \"train_ll\", \"valid_ll\", \"valid_err\"])\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    training_ll = 0\n",
    "\n",
    "    # Minibatch loop\n",
    "    for i in range(n_train_batches):\n",
    "        inputs = train_x_tensor[i * batch_size : (i+1) * batch_size]\n",
    "        choice = train_y_tensor[i * batch_size : (i+1) * batch_size]-1\n",
    "\n",
    "        minibatch_ll = training_object.train_model(inputs, choice).item()\n",
    "        training_ll = (training_ll * i + minibatch_ll)/(i + 1)\n",
    "\n",
    "        iteration = (epoch - 1) * n_train_batches + i\n",
    "\n",
    "        # If the current iter has reached the valid_freq then evaluate the validation loss on the validation batches\n",
    "        if (iteration + 1) % valid_freq == 0:\n",
    "            validation_ll = training_object.validate_model(\n",
    "                valid_x_tensor,(valid_y_tensor-1)).item()\n",
    "\n",
    "            #check prediction accuracy\n",
    "            error = training_object.error(valid_y_tensor).item()\n",
    "\n",
    "            #################################\n",
    "            # track and save training stats #\n",
    "            #################################\n",
    "            training_step = {\n",
    "                \"epoch\": epoch,\n",
    "                \"minibatch\": i + 1,\n",
    "                \"batches\": n_train_batches,\n",
    "                \"train_ll\": training_ll * n_train_batches,\n",
    "                \"valid_ll\": validation_ll,\n",
    "                \"valid_err\": error,\n",
    "            }\n",
    "            training_frame.loc[step] = training_step\n",
    "            #################################\n",
    "\n",
    "            if validation_ll < best_validation_ll:\n",
    "                print((\"epoch {:d}, minibatch {:d}/{:d}, \"\n",
    "                       \"validation likelihood {:.2f}\").format(\n",
    "                        epoch, i + 1, n_train_batches, validation_ll))\n",
    "\n",
    "                # improve patience if loss improvement is good enough\n",
    "                if validation_ll < best_validation_ll * improvement_threshold:\n",
    "                    patience = max(patience, iteration * patience_increase)\n",
    "\n",
    "                # set the best loss to the new current (good) validation loss\n",
    "                best_validation_ll = validation_ll\n",
    "\n",
    "                error = training_object.error(valid_y_tensor).item()\n",
    "                training_frame.loc[step, \"valid_err\"] = error\n",
    "                print(\"validation error  {:.2%}\".format(error))\n",
    "\n",
    "                # save the best model\n",
    "                # with open(filename, 'wb') as f:\n",
    "                #     pickle.dump([training_object, config], f)\n",
    "\n",
    "            step = step + 1\n",
    "\n",
    "        if epoch > 200:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "run_time = end_time - start_time\n",
    "print(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the accuracy of model\n",
    "y_pred_valid = training_object.predict_validate(valid_x_tensor)\n",
    "\n",
    "# Count the number alternatives\n",
    "num_low = tf.reduce_sum(y_pred_valid == 1)\n",
    "num_high = tf.reduce_sum(y_pred_valid == 2)\n",
    "\n",
    "print(\"Confusion_matrix for validation data:\")\n",
    "print(confusion_matrix(valid_y_tensor,y_pred_valid))\n",
    "print(classification_report(valid_y_tensor,y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
