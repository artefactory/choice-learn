{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the results of the authors of the ResLogit paper [1]\n",
    "\n",
    "We reproduce in this notebook the results of the authors of the ResLogit paper on the toy dataset *Pedestrian_Waittime.csv* (10 lines).\n",
    "\n",
    "The toy dataset is available here: https://github.com/LiTrans/reslogit/blob/master/Pedestrian_Waittime.csv.\n",
    "\n",
    "Code of the authors: https://github.com/LiTrans/reslogit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from choice_learn.datasets.base import download_from_url, get_path, DATA_MODULE\n",
    "from choice_learn.data import ChoiceDataset\n",
    "from choice_learn.models.reslogit import ResLogit\n",
    "\n",
    "import os\n",
    "\n",
    "# Remove/Add GPU use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toy dataset is downloaded from the URL if it's not already present in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://raw.githubusercontent.com/LiTrans/reslogit/refs/heads/master/Pedestrian_Waittime.csv\"\n",
    "data_file_name = download_from_url(url)\n",
    "full_path = get_path(data_file_name, module=DATA_MODULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_df = pd.read_csv(full_path)\n",
    "\n",
    "# Rename columns corresponding to items features to be able to call ChoiceDataset.from_single_wide_df() \n",
    "pedestrian_df = pedestrian_df.rename(columns={\"mode_active\": \"mode_1\", \"mode_Public Transit\": \"mode_2\"})\n",
    "\n",
    "print(f\"Shape: {pedestrian_df.shape}\")\n",
    "print(f\"Columns names: {pedestrian_df.columns}\")\n",
    "pedestrian_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **output** we want to predict is the \"category\" (1 or 2).\n",
    "\n",
    "The **inputs** are the columns: \n",
    "- \"low LaneWidth\",\n",
    "- \"high LaneWidth\",\n",
    "- \"density\",\n",
    "- \"mixed_TrafficCondition\",\n",
    "- \"fully_AutomatedCondition\",\n",
    "- \"Snowy\",\n",
    "- \"One way\",\n",
    "- \"Two way with median\",\n",
    "- \"Night\",\n",
    "- \"OverOneCar\",\n",
    "- \"Age_30 - 39\", \n",
    "- \"age_over50\", \n",
    "- \"Gender_Female\", \n",
    "- \"mode_1\", \n",
    "- \"walk_to_work\", \n",
    "- \"mode_2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame is in the wide format, so we can use the 'from_single_wide_df' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_dataset = ChoiceDataset.from_single_wide_df(\n",
    "    # The main DataFrame\n",
    "    df=pedestrian_df,\n",
    "    # The names of the items, will be used to find columns and organize them\n",
    "    items_id=[1, 2],\n",
    "\n",
    "    # Columns for shared_features_by_choice\n",
    "    shared_features_columns=[\"low LaneWidth\", \"high LaneWidth\", \"density\",\n",
    "                             \"mixed_TrafficCondition\", \"fully_AutomatedCondition\",\n",
    "                             \"Snowy\", \"One way\", \"Two way with median\", \"Night\",\n",
    "                             \"OverOneCar\", \"Age_30 - 39\", \"age_over50\", \"Gender_Female\",\n",
    "                             \"walk_to_work\",],\n",
    "    \n",
    "    # Columns for items_features_by_choice\n",
    "    # They will be reconstructed as \"prefix\" + \"item_id\" + \"delimiter\"\n",
    "    items_features_prefixes=[\"mode\"],\n",
    "    delimiter=\"_\",\n",
    "\n",
    "    # The column containing the choices\n",
    "    choices_column=\"category\",\n",
    "    # How the choices are encoded: items_id means that the names of the choices correspond to the items_id list\n",
    "    choice_format=\"items_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_dataset.summary()\n",
    "print(f\"\\n\\n{type(pedestrian_dataset)=}\")\n",
    "print(f\"\\n{np.shape(pedestrian_dataset.items_features_by_choice)=}\")\n",
    "print(f\"{np.shape(pedestrian_dataset.shared_features_by_choice)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResLogit modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = len(set(pedestrian_dataset.choices))\n",
    "n_items_features = np.shape(pedestrian_dataset.items_features_by_choice)[3]\n",
    "n_shared_features = np.shape(pedestrian_dataset.shared_features_by_choice)[2]\n",
    "n_vars = n_items_features + n_shared_features\n",
    "n_choices = len(np.unique(pedestrian_dataset.choices))\n",
    "\n",
    "print(f\"{n_items=}\\n{n_items_features=}\\n{n_shared_features=}\\n{n_vars, n_choices=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(pedestrian_dataset.choices)\n",
    "# Slicing index for train and valid split\n",
    "slice = np.floor(0.7 * n_samples).astype(int)\n",
    "train_indexes = np.arange(0, slice)\n",
    "test_indexes = np.arange(slice, n_samples)\n",
    "\n",
    "train_dataset = pedestrian_dataset[train_indexes]\n",
    "test_dataset = pedestrian_dataset[test_indexes]\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "model = ResLogit(intercept=None, n_layers=16, lr=0.001, epochs=1000, batch_size=64)\n",
    "\n",
    "losses = model.fit(choice_dataset=train_dataset, val_dataset=test_dataset)\n",
    "probas = model.predict_probas(test_dataset)\n",
    "\n",
    "test_eval = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_pred=probas, y_true=tf.one_hot(test_dataset.choices, n_items))\n",
    "print(f\"\\n{losses=}\\n{probas=}\\n{test_eval=}\\n\")\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(f\"Execution time for iteration: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is estimated, we can look at the weights with the .trainable_weights argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[\"train_loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Negative log likelihood\")\n",
    "plt.title(\"Training of the ResLogit model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can easily acces the negative log likelihood value for the training dataset or another one using the .evaluate() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Negative log likelihood: {model.evaluate(pedestrian_dataset)}\")\n",
    "print(f\"Negative log likelihood multiplied by the size of the dataset: {len(pedestrian_dataset) * model.evaluate(pedestrian_dataset)}\")\n",
    "print(\"Average LogLikeliHood on test:\", np.mean(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same metrics as the authors of ResLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Predictions:\")\n",
    "print(probas, \"\\n\")\n",
    "print(np.argmax(probas, axis=1), \"\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "matrix = confusion_matrix(test_dataset.choices, np.argmax(probas, axis=1))\n",
    "print(f\"Confusion Matrix:\\n {matrix}\\n\")\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_dataset.choices, np.argmax(probas, axis=1))\n",
    "print(f\"Classification Report:\\n {report}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] ResLogit: A residual neural network logit model for data-driven choice modelling, Wong, M.; Farooq, B (2021), Transportation Research Part C: Emerging Technologies 126\\\n",
    "(URL: https://doi.org/10.1016/j.trc.2021.103050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
