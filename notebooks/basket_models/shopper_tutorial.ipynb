{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to basket modelling with SHOPPER\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/artefactory/choice-learn/blob/main/notebooks/basket_models/shopper_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a synthetic dataset to demonstrate how to use the SHOPPER model [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary requirements\n",
    "\n",
    "# If you run this notebook on Google Colab, or in standalone mode, you need to install the required packages.\n",
    "# Uncomment the following lines:\n",
    "\n",
    "# !pip install choice-learn\n",
    "\n",
    "# If you run the notebook within the GitHub repository, you need to run the following lines, that can skipped otherwise:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from choice_learn.basket_models.trip_dataset import Trip, TripDataset\n",
    "from choice_learn.basket_models.shopper import Shopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all physical GPUs\n",
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(f\"Available physical GPUs: {physical_gpus}\")\n",
    "# Select GPUs to use\n",
    "selected_gpus = []  # Choose the GPUs you want to use ([] = CPU)\n",
    "# Set the selected GPUs to be visible\n",
    "tf.config.set_visible_devices(selected_gpus, \"GPU\")\n",
    "# Verify the visible GPUs\n",
    "visible_gpus = tf.config.get_visible_devices(\"GPU\")\n",
    "print(f\"Visible GPUs: {visible_gpus}\")\n",
    "\n",
    "# Limit GPU memory growth\n",
    "if physical_gpus:\n",
    "  try:\n",
    "    for gpu in physical_gpus:\n",
    "      # Allocate only as much GPU memory as needed for the runtime allocations\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a TripDataset with an interaction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = 9  # Include the checkout item 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_matrix(n: int) -> np.ndarray:\n",
    "    \"\"\"Generate a random interaction matrix of size n x n.\n",
    "    \n",
    "    The matrix is symmetric and the diagonal is filled with zeros.\n",
    "    The matrix is divided into two parts:\n",
    "    - Complementary pairs (positive values)\n",
    "    - Substitutable pairs (negative values)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n: int\n",
    "        Size of the square matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matrix: np.ndarray\n",
    "        The interaction matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1):\n",
    "            # Generate a random value from a normal distribution\n",
    "            matrix[i, j] = np.random.normal(loc=0.0, scale=1.0)\n",
    "\n",
    "            # Copy the lower triangle to the upper triangle\n",
    "            matrix[j, i] = matrix[i, j]\n",
    "\n",
    "        # Same item\n",
    "        matrix[i, i] = 0\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix = create_interaction_matrix(n_items)\n",
    "\n",
    "print(interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction_matrix(matrix: np.ndarray, half: bool = False) -> None:\n",
    "    \"\"\"Plot the interaction matrix.\n",
    "    \n",
    "    The matrix is displayed with a color map and the value\n",
    "    of each cell is displayed inside the cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix: np.ndarray\n",
    "        The interaction matrix to plot\n",
    "    half: bool, optional\n",
    "        If True, only the lower half of the matrix is displayed\n",
    "        By default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix: np.ndarray\n",
    "        The interaction matrix\n",
    "    \"\"\"\n",
    "    if half:\n",
    "        # Mask: elements below the k-th diagonal are set to False, rest to True\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n",
    "        matrix = np.ma.array(matrix, mask=mask)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(matrix, cmap='coolwarm', interpolation='none')\n",
    "    plt.colorbar(label='Interaction Value')\n",
    "    plt.title('Interaction Matrix')\n",
    "    plt.xlabel('Item B')\n",
    "    plt.ylabel('Item A')\n",
    "    plt.xticks(ticks=np.arange(matrix.shape[0]), labels=np.arange(matrix.shape[0]))\n",
    "    plt.yticks(ticks=np.arange(matrix.shape[1]), labels=np.arange(matrix.shape[1]))\n",
    "\n",
    "    # Display the value inside each case\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if not half or (half and j <= i):\n",
    "                plt.text(j, i, f'{matrix[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "    \n",
    "    # Color the cases in black for checkout item\n",
    "    for i in range(matrix.shape[0]):\n",
    "        plt.gca().add_patch(plt.Rectangle((i-0.5, -0.5), 1, 1, fill=True, color=\"black\", alpha=0.3))\n",
    "        plt.gca().add_patch(plt.Rectangle((-0.5, i-0.5), 1, 1, fill=True, color=\"black\", alpha=0.3))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interaction_matrix(interaction_matrix, half=True)  # 0 is the checkout item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interaction_matrix(interaction_matrix, half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baskets(\n",
    "    interaction_matrix: np.ndarray,\n",
    "    assortment: np.ndarray,\n",
    "    num_baskets: int\n",
    ") -> list:\n",
    "    \"\"\"Generate baskets based on the interaction matrix.\n",
    "\n",
    "    Size of the baskets: between 3 and  n_items - 4 (including the checkout item)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    interaction_matrix: np.ndarray\n",
    "        The interaction matrix to plot\n",
    "    assortment: np.ndarray\n",
    "        The assortment of items available in the store\n",
    "    num_baskets: int\n",
    "        The number of baskets to generate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    baskets: list\n",
    "        List of generated baskets\n",
    "    \"\"\"\n",
    "    n_items = interaction_matrix.shape[0]\n",
    "    \n",
    "    baskets = []\n",
    "    for _ in range(num_baskets):\n",
    "        # Build the list of available items from the assortment\n",
    "        available_items = np.array([item_id for item_id in range(interaction_matrix.shape[0]) if assortment[item_id] == 1])\n",
    "\n",
    "        # Start with a random item (different from the checkout item)\n",
    "        basket = np.array([np.random.choice(available_items[available_items != 0])])\n",
    "\n",
    "        # Complete the basket with a new item until the checkout item is selected\n",
    "        item_a, item_b = basket[-1], None\n",
    "        while item_b != 0:\n",
    "            # Get probabilities of the next item given the previous item\n",
    "            # by applying the softmax function to the interaction values\n",
    "            # of the row corresponding to the previous item\n",
    "            probabilities = np.exp(interaction_matrix[item_a, :])\n",
    "            probabilities /= probabilities.sum()\n",
    "\n",
    "            while (item_b is None) or (item_b == item_a) or (item_b not in available_items) or (item_b in basket):\n",
    "                # No duplicate items\n",
    "                item_b = np.random.choice(n_items, p=probabilities)\n",
    "            \n",
    "            basket = np.append(basket, item_b)\n",
    "\n",
    "            if len(basket) == n_items - 4:\n",
    "                # Don't generate baskets of size > n_items - 4 to enable negative sampling\n",
    "                # with 2 negative samples while considering different assortments\n",
    "                if basket[-1]:\n",
    "                    # Impose the checkout item as the last item\n",
    "                    basket[-1] = 0\n",
    "                break\n",
    "\n",
    "        baskets.append(basket)\n",
    "\n",
    "    return baskets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2= np.ones(n_items), np.ones(n_items)\n",
    "a1[3], a1[6] = 0, 0\n",
    "a2[2], a2[4] = 0, 0\n",
    "\n",
    "assortments = np.array([a1, a2])\n",
    "\n",
    "readable_assortment_1 = [f\"Item {i}\" for i in range(n_items) if a1[i] == 1]\n",
    "readable_assortment_2 = [f\"Item {i}\" for i in range(n_items) if a2[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Assortments are encoded as availability matrices indicating the availability (1) or not (0) of the products:\\n{assortments=}\\n\")\n",
    "print(\n",
    "    \"Here, the variable 'assortments' can be read as:\\n\",\n",
    "    f\"- Assortment 1 = {readable_assortment_1}\\n\",\n",
    "    f\"- Assortment 2 = {readable_assortment_2}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_baskets = 1000\n",
    "\n",
    "purchases_assortment_1 = generate_baskets(\n",
    "    interaction_matrix=interaction_matrix,\n",
    "    assortment=assortments[0],\n",
    "    # Half of the baskets with assortment 1\n",
    "    num_baskets=num_baskets // 2,\n",
    ")\n",
    "purchases_assortment_2 = generate_baskets(\n",
    "    interaction_matrix=interaction_matrix,\n",
    "    assortment=assortments[1],\n",
    "    # Half of the baskets with assortment 2\n",
    "    num_baskets=num_baskets // 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(purchases_assortment_1)=}, {len(purchases_assortment_2)=}\\n\")\n",
    "\n",
    "print(f\"First baskets from assortment {a1}: {purchases_assortment_1[:10]}\")\n",
    "print(f\"First baskets from assortment {a2}: {purchases_assortment_2[:10]}\\n\")\n",
    "\n",
    "min_length_purchases_assortment_1, max_length_purchases_assortment_1= min(len(basket) for basket in purchases_assortment_1), max(len(basket) for basket in purchases_assortment_1)\n",
    "print(f\"Minimum and maximum lengths of arrays in baskets from assortment {a1}: {min_length_purchases_assortment_1} & {max_length_purchases_assortment_1}\")\n",
    "min_length_purchases_assortment_2, max_length_purchases_assortment_2 = min(len(basket) for basket in purchases_assortment_2), max(len(basket) for basket in purchases_assortment_2)\n",
    "print(f\"Minimum and maximum lengths of arrays in baskets from assortment {a2}: {min_length_purchases_assortment_2} & {max_length_purchases_assortment_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "\n",
    "customer = 0\n",
    "week = 0\n",
    "prices = np.arange(1, n_items + 1) * 1.1\n",
    "\n",
    "trip_list_assortment_1 = list(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            [Trip(id=i, purchases=basket, customer=customer, week=week, prices=prices, assortment=0)\n",
    "            for i in range(k*nrows, (k + 1)*nrows)] for k, basket in enumerate(purchases_assortment_1)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "trip_list_assortment_2 = list(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            [Trip(id=i, purchases=basket, customer=customer, week=week, prices=prices, assortment=1)\n",
    "            for i in range(k*nrows, (k + 1)*nrows)] for k, basket in enumerate(purchases_assortment_2)\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "threshold_1, threshold_2 = int(len(trip_list_assortment_1) * split_ratio), int(len(trip_list_assortment_2) * split_ratio)\n",
    "trip_list_train = trip_list_assortment_1[:threshold_1] + trip_list_assortment_2[:threshold_2]\n",
    "trip_list_val = trip_list_assortment_1[threshold_1:] + trip_list_assortment_2[threshold_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dataset_train = TripDataset(trips=trip_list_train, assortments=assortments)\n",
    "trip_dataset_val = TripDataset(trips=trip_list_val, assortments=assortments)\n",
    "\n",
    "n_items_train, n_customers_train = trip_dataset_train.n_items, trip_dataset_train.n_customers\n",
    "n_items_val, n_customers_val = trip_dataset_val.n_items, trip_dataset_val.n_customers\n",
    "\n",
    "print(f\"{len(trip_dataset_train)=}, {n_items_train=}, {n_customers_train=}\\n\")\n",
    "print(f\"{len(trip_dataset_val)=}, {n_items_val=}, {n_customers_val=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Shopper models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit several SHOPPER models with different utility formulas taking into account different effects (price elasticity, seasonality, etc.). We will use the same learning rate and number of epochs for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_sizes = {\"preferences\": 4, \"price\": 3, \"season\": 3}\n",
    "n_negative_samples = 2\n",
    "optimizer = \"adam\"\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st model: basic utility formula\n",
    "shopper_1 = Shopper(\n",
    "    item_popularity=False,\n",
    "    price_effects=False,\n",
    "    seasonal_effects=False,\n",
    "    think_ahead=False,\n",
    "    latent_sizes=latent_sizes,\n",
    "    n_negative_samples=n_negative_samples,\n",
    "    optimizer=optimizer,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# 2nd model: price effects\n",
    "shopper_2 = Shopper(\n",
    "    item_popularity=False,\n",
    "    price_effects=True,\n",
    "    seasonal_effects=False,\n",
    "    think_ahead=False,\n",
    "    latent_sizes=latent_sizes,\n",
    "    n_negative_samples=n_negative_samples,\n",
    "    optimizer=optimizer,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# 3rd model: item popularity + price effects + seasonal effects\n",
    "shopper_3 = Shopper(\n",
    "    item_popularity=True,\n",
    "    price_effects=True,\n",
    "    seasonal_effects=True,\n",
    "    think_ahead=False,\n",
    "    latent_sizes=latent_sizes,\n",
    "    n_negative_samples=n_negative_samples,\n",
    "    optimizer=optimizer,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Feel free to explore other models by changing the hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "shopper_1.instantiate(n_items=n_items_train, n_customers=n_customers_train)\n",
    "shopper_2.instantiate(n_items=n_items_train, n_customers=n_customers_train)\n",
    "shopper_3.instantiate(n_items=n_items_train, n_customers=n_customers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "history_1, history_2, history_3 = shopper_1.fit(trip_dataset=trip_dataset_train), shopper_2.fit(trip_dataset=trip_dataset_train), shopper_3.fit(trip_dataset=trip_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1[\"train_loss\"], label=f\"1st model: basic utility formula\")\n",
    "plt.plot(history_2[\"train_loss\"], label=f\"2nd model: price effects\")\n",
    "plt.plot(history_3[\"train_loss\"], label=f\"3rd model: item popularity + price effects + seasonal effects\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training of SHOPPER models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display item popularity for the models that include this effect in their utility formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of each item in the datasets\n",
    "item_counts_train = np.zeros(n_items)\n",
    "for basket in purchases_assortment_1 + purchases_assortment_2:\n",
    "    for item in basket:\n",
    "        item_counts_train[item] += 1\n",
    "\n",
    "# Normalize the counts to get frequencies\n",
    "item_frequencies_train = item_counts_train / item_counts_train.sum()\n",
    "\n",
    "if shopper_1.item_popularity:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Plot ground truth item frequencies\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(item_frequencies_train)), item_frequencies_train)\n",
    "    plt.title('Ground truth item frequencies\\n(train dataset)')\n",
    "    plt.xlabel('Item')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(ticks=range(len(item_frequencies_train)), labels=range(len(item_frequencies_train)))\n",
    "\n",
    "    # Plot lambdas for 1st model\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(len(shopper_1.lambda_.numpy())), np.exp(shopper_1.lambda_.numpy()))\n",
    "    plt.axhline(y=1, color='r', linestyle='--')\n",
    "    plt.title('Item popularity for 1st model')\n",
    "    plt.xlabel('Item')\n",
    "    plt.ylabel('exp(lambda)')\n",
    "    plt.xticks(ticks=range(len(shopper_1.lambda_.numpy())), labels=range(len(shopper_1.lambda_.numpy())))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "if shopper_2.item_popularity:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Plot ground truth item frequencies\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(item_frequencies_train)), item_frequencies_train)\n",
    "    plt.title('Ground truth item frequencies\\n(train dataset)')\n",
    "    plt.xlabel('Item')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(ticks=range(len(item_frequencies_train)), labels=range(len(item_frequencies_train)))\n",
    "\n",
    "    # Plot lambdas for 2nd model\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(len(shopper_2.lambda_.numpy())), np.exp(shopper_2.lambda_.numpy()))\n",
    "    plt.axhline(y=1, color='r', linestyle='--')\n",
    "    plt.title('Item popularity for 2nd model')\n",
    "    plt.xlabel('Item')\n",
    "    plt.ylabel('exp(lambda)')\n",
    "    plt.xticks(ticks=range(len(shopper_2.lambda_.numpy())), labels=range(len(shopper_2.lambda_.numpy())))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "if shopper_3.item_popularity:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Plot ground truth item frequencies\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(item_frequencies_train)), item_frequencies_train)\n",
    "    plt.title('Ground truth item frequencies\\n(train dataset)')\n",
    "    plt.xlabel('Item')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(ticks=range(len(item_frequencies_train)), labels=range(len(item_frequencies_train)))\n",
    "\n",
    "    # Plot lambdas for 3rd model\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(len(shopper_3.lambda_.numpy())), np.exp(shopper_3.lambda_.numpy()))\n",
    "    plt.axhline(y=1, color='r', linestyle='--')\n",
    "    plt.title('Item popularity for 3rd model')\n",
    "    plt.xlabel('Item')\n",
    "    plt.ylabel('exp(lambda)')\n",
    "    plt.xticks(ticks=range(len(shopper_3.lambda_.numpy())), labels=range(len(shopper_3.lambda_.numpy())))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute t-SNE of alpha embeddings to plot them in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha embeddings (for each item)\n",
    "alpha_embeddings_1 = shopper_1.alpha.numpy()\n",
    "alpha_embeddings_2 = shopper_2.alpha.numpy()\n",
    "alpha_embeddings_3 = shopper_3.alpha.numpy()\n",
    "\n",
    "print(f\"{alpha_embeddings_1.shape=} {alpha_embeddings_2.shape=} {alpha_embeddings_3.shape=}\")  # Shape: (n_items, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE (for each item)\n",
    "perplexity = 5\n",
    "# early_exaggeration =\n",
    "# learning_rate =\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "\n",
    "alpha_tsne_1 = tsne.fit_transform(alpha_embeddings_1)\n",
    "alpha_tsne_2 = tsne.fit_transform(alpha_embeddings_2)\n",
    "alpha_tsne_3 = tsne.fit_transform(alpha_embeddings_3)\n",
    "\n",
    "print(f\"{alpha_tsne_1.shape=} {alpha_tsne_2.shape=} {alpha_tsne_3.shape=}\")  # Shape: (n_items, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot t-SNE embeddings for 1st model\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(alpha_tsne_1[:, 0], alpha_tsne_1[:, 1], label='t-SNE embeddings')\n",
    "for i in range(n_items_train):\n",
    "    plt.annotate(i, (alpha_tsne_1[i, 0], alpha_tsne_1[i, 1]))\n",
    "plt.title('t-SNE embeddings for 1st model')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "\n",
    "# Plot t-SNE embeddings for 2nd model\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(alpha_tsne_2[:, 0], alpha_tsne_2[:, 1], label='t-SNE embeddings')\n",
    "for i in range(n_items_train):\n",
    "    plt.annotate(i, (alpha_tsne_2[i, 0], alpha_tsne_2[i, 1]))\n",
    "plt.title('t-SNE embeddings for 2nd model')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "\n",
    "# Plot t-SNE embeddings for 3rd model\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(alpha_tsne_3[:, 0], alpha_tsne_3[:, 1], label='t-SNE embeddings')\n",
    "for i in range(n_items_train):\n",
    "    plt.annotate(i, (alpha_tsne_3[i, 0], alpha_tsne_3[i, 1]))\n",
    "plt.title('t-SNE embeddings for 3rd model')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with SHOPPER models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the models on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 3\n",
    "\n",
    "nll_1 = shopper_1.evaluate(trip_dataset_val, n_permutations)\n",
    "nll_2 = shopper_2.evaluate(trip_dataset_val, n_permutations)\n",
    "nll_3 = shopper_3.evaluate(trip_dataset_val, n_permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean negative log-likelihood on the validation set:\\n\",\n",
    "    f\"- 1st model: {nll_1:.4f}\\n\",\n",
    "    f\"- 2nd model: {nll_2:.4f}\\n\",\n",
    "    f\"- 3rd model: {nll_3:.4f}\",\n",
    ")\n",
    "\n",
    "print(\"\\nWe can see that the more complex the model, the lower the negative log-likelihood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute various utilities and probabilities. We are going to use the 3rd model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item utilities\n",
    "item_batch_inference=np.array([2, 0, 4])\n",
    "basket_inference = np.array([1, 3])\n",
    "customer_inference = 0\n",
    "week_inference = 0\n",
    "price_inference = 5.\n",
    "available_items_inference = np.ones(trip_dataset_train.n_items)\n",
    "available_items_inference[4] = 0  # Consider that item 4 is not available during inference\n",
    "assortment_inference = np.array(\n",
    "    [\n",
    "        item_id for item_id in trip_dataset_train.get_all_items() if available_items_inference[item_id] == 1\n",
    "    ]\n",
    ")\n",
    "\n",
    "item_utilities = shopper_3.compute_batch_utility(\n",
    "    item_batch=item_batch_inference,\n",
    "    basket_batch=np.tile(basket_inference, (3, 1)),\n",
    "    customer_batch=np.tile(customer_inference, 3),\n",
    "    week_batch=np.tile(week_inference, 3),\n",
    "    price_batch=np.tile(price_inference, 3),\n",
    "    available_item_batch=np.tile(available_items_inference, (3, 1)),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The customer n째{customer_inference} has already in this basket the items {basket_inference}.\\n\",\n",
    "    f\"He is shopping during week n째{week_inference} in a shop where the prices of the items are respectively {prices}\",\n",
    "    f\"and whose assortment (ie the set of available items) is {assortment_inference}).\\n\",\n",
    "    f\"Under these circumstances, the utility of the selected items are:\\n\"\n",
    ")\n",
    "for i, item_id in enumerate(item_batch_inference):\n",
    "    if item_id == 0:\n",
    "        print(f\"- Item {item_id} (checkout item): {item_utilities[i]:.4f}\")\n",
    "    else:\n",
    "        print(f\"- Item {item_id}: {item_utilities[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item likelihoods\n",
    "item_batch=np.array([2, 0, 4])\n",
    "item_likelihoods = shopper_3.compute_item_likelihood(\n",
    "    basket=basket_inference,\n",
    "    available_items=available_items_inference,  # Consider all items available\n",
    "    customer=customer_inference,\n",
    "    week=week_inference,\n",
    "    prices=prices,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The customer n째{customer_inference} has already in this basket the items {basket_inference}.\\n\",\n",
    "    f\"He is shopping during week n째{week_inference} in a shop where the prices of the items are respectively {prices}\",\n",
    "    f\"and whose assortment (ie the set of available items) is {assortment_inference}).\\n\",\n",
    "    f\"Under these circumstances, the likelihoods that each item will be the next item he is going to add to his basket are:\\n\"\n",
    ")\n",
    "for i, item_id in enumerate(trip_dataset_train.get_all_items()):\n",
    "    if item_id == 0:\n",
    "        print(f\"- Item {item_id} (checkout item, the customer decides to end his shopping trip): {item_likelihoods[i]:.4f}\")\n",
    "    else:\n",
    "        print(f\"- Item {item_id}: {item_likelihoods[i]:.4f}\")\n",
    "print(f\"\\nN.B.: The item likelihoods sum to {np.sum(item_likelihoods):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered basket likelihoods\n",
    "basket_ordered = np.array([1, 3, 0])\n",
    "basket_ordered_likelihood = shopper_3.compute_ordered_basket_likelihood(\n",
    "    basket=basket_ordered,\n",
    "    available_items=available_items_inference,  # Consider all items available\n",
    "    customer=customer_inference,\n",
    "    week=week_inference,\n",
    "    prices=prices,\n",
    ")\n",
    "\n",
    "print(f\"Likelihood for ordered basket {basket_ordered}: {basket_ordered_likelihood:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unordered basket likelihoods\n",
    "n_permutations = 2\n",
    "\n",
    "basket_unordered = np.array([1, 3, 0])\n",
    "basket_unordered_likelihood = shopper_3.compute_unordered_basket_likelihood(\n",
    "    basket=basket_unordered,\n",
    "    available_items=available_items_inference,  # Consider all items available\n",
    "    customer=customer_inference,\n",
    "    week=week_inference,\n",
    "    prices=prices,\n",
    "    n_permutations=n_permutations,\n",
    ")\n",
    "print(f\"Likelihood for unordered basket {basket_unordered}: {basket_unordered_likelihood:.4f} (with {n_permutations} permutations to approximate all possible orders)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements, Ruiz, F. J. R.; Athey, S.; Blei, D. M. (2019), Annals of Applied Statistic\n",
    "(URL: https://arxiv.org/abs/1711.03560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "choice-learn-private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
