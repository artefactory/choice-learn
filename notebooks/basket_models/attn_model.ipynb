{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Implementation of an attention-based model for item recommendation.\n",
    "\n",
    "Wang, Shoujin, Liang Hu, Longbing Cao, Xiaoshui Huang, Defu Lian, and Wei Liu.\n",
    "\"Attention-based transactional context embedding for next-item recommendation.\"\n",
    "In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"./../../\")\n",
    "from choice_learn.basket_models import TripDataset\n",
    "from choice_learn.basket_models.basic_attention_model import AttentionBasedContextEmbedding\n",
    "from choice_learn.basket_models.synthetic_dataset import SyntheticDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "epochs = 30\n",
    "n_baskets = 1000\n",
    "embedding_dim = 4\n",
    "n_negative_samples = 3\n",
    "full_assortment_matrix = np.array([[1,1,1,1,1,1,1,1]])\n",
    "n_items = full_assortment_matrix.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Synthetic Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = SyntheticDataGenerator(\n",
    "    proba_complementary_items=0.7,\n",
    "    proba_neutral_items=0.3,\n",
    "    noise_proba=0.15,\n",
    "    items_nest = {0:[0, 1, 2],\n",
    "                   1: [3, 4, 5],\n",
    "                   2: [6],\n",
    "                   3: [7]},\n",
    "    nests_interactions = [[\"\", \"compl\", \"neutral\", \"neutral\"],\n",
    "                          [\"compl\", \"\", \"neutral\", \"neutral\"],\n",
    "                          [\"neutral\", \"neutral\", \"\", \"neutral\"],\n",
    "                          [\"neutral\", \"neutral\", \"neutral\", \"\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Two functions to visualise distributions\n",
    "\n",
    "* `visualise_tripdataset_trips` : to show on a heatmap the conditional items distribution P(i|j) in a tripdataset\n",
    "* `get_model_representation` : to show on a heatmap the conditional items distribution P(i|j) when calling a model's predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_tripdataset_trips(dataset, n_items):\n",
    "    \"\"\"\n",
    "    Visualize the conditional probability P(i|j) of items co-occurring in baskets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : TripDataset\n",
    "        The dataset containing trips.\n",
    "    n_items : int\n",
    "        Number of unique items.\n",
    "    \"\"\"\n",
    "    distribution_matrix = np.zeros((n_items, n_items))\n",
    "    for trip in dataset.trips:\n",
    "        basket = trip.purchases\n",
    "        for i in basket:\n",
    "            for j in basket:\n",
    "                if i != j:\n",
    "                    distribution_matrix[i, j] += 1\n",
    "    row_sums = distribution_matrix.sum(axis=1, keepdims=True)\n",
    "    for i in range(len(row_sums)):\n",
    "        if row_sums[i] != 0:\n",
    "            distribution_matrix[i] = distribution_matrix[i]/row_sums[i]\n",
    "    \n",
    "    return distribution_matrix\n",
    "\n",
    "\n",
    "\n",
    "def get_model_representation(model, n_items, test_dataset=None, assortment_matrix=None):\n",
    "    \"\"\"\n",
    "    Visualize the model's conditional probability matrix and training loss history.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : AttentionBasedContextEmbedding\n",
    "        The trained model.\n",
    "    hist : dict\n",
    "        Training history with \"train_loss\" key.\n",
    "    n_items : int\n",
    "        Number of unique items.\n",
    "    test_dataset : TripDataset, optional\n",
    "        Dataset for evaluation. If None, uses single-item contexts.\n",
    "    assortment_matrix : np.ndarray, optional\n",
    "        Binary matrix indicating available items.\n",
    "    \"\"\"\n",
    "    if assortment_matrix is None:\n",
    "        assortment_matrix = np.ones((1, n_items), dtype=int)\n",
    "\n",
    "    if test_dataset is None:\n",
    "        available_items = assortment_matrix[0]\n",
    "        contexts = tf.constant([[i] for i in range(n_items)], dtype=tf.int32)\n",
    "        \n",
    "    else:\n",
    "        contexts = []\n",
    "        for batch in test_dataset.iter_batch(1, data_method=\"aleacarta\"):\n",
    "            contexts.append(batch[1][0])\n",
    "        contexts = tf.ragged.constant(\n",
    "            [row[row != -1] for row in contexts], dtype=tf.int32\n",
    "        )\n",
    "        available_items = batch[-1][0]\n",
    "\n",
    "    context_prediction = model.predict(contexts, available_items=available_items)\n",
    "    predicted_items = [np.argmax(context_prediction[i]) for i in range(context_prediction.shape[0])]\n",
    "\n",
    "    if test_dataset is None:\n",
    "        distribution_matrix = np.stack(context_prediction)\n",
    "        for i in range(len(available_items)):\n",
    "            if available_items[i] == 0:\n",
    "                distribution_matrix[i] *= 0\n",
    "    else:\n",
    "        distribution_matrix = np.zeros((n_items, n_items))\n",
    "        for i in range(contexts.shape[0]):\n",
    "            for j in contexts[i]:\n",
    "                distribution_matrix[predicted_items[i], j] += 1\n",
    "\n",
    "    row_sums = distribution_matrix.sum(axis=1, keepdims=True)\n",
    "    for i in range(len(row_sums)):\n",
    "        if row_sums[i] != 0:\n",
    "            distribution_matrix[i] = distribution_matrix[i]/row_sums[i]\n",
    "\n",
    "    return distribution_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Generate full assortments synthetic dataset : train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dataset_train = data_gen.generate_trip_dataset(n_baskets,full_assortment_matrix)\n",
    "trip_dataset_test = data_gen.generate_trip_dataset(n_baskets,full_assortment_matrix)\n",
    "\n",
    "    \n",
    "distribution_matrix = visualise_tripdataset_trips(trip_dataset_train,n_items)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(distribution_matrix, vmin=0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "plt.colorbar(label='P(i|j)')\n",
    "plt.title('Items distribution in the train dataset (A_full)')\n",
    "plt.xlabel('j')\n",
    "plt.ylabel('i')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Instantiate and train the model 1 on A_full\n",
    " -> The model uses the true NCE sampling distribution; items frequencies aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AttentionBasedContextEmbedding(\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_negative_samples=n_negative_samples)\n",
    "\n",
    "model1.instantiate(n_items=len(full_assortment_matrix[0]), use_true_nce_distribution=True)\n",
    "history1 = model1.fit(trip_dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Instantiate and train the model 2 on A_full\n",
    "-> The model uses a uniform sampling distribution for NCE (1/(n_items-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AttentionBasedContextEmbedding(\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_negative_samples=n_negative_samples)\n",
    "\n",
    "model2.instantiate(n_items=len(full_assortment_matrix[0]), use_true_nce_distribution = False)\n",
    "history2 = model2.fit(trip_dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Model 1 is using the true NCE distribution\n",
    "\n",
    "The following plot of the evaluation on the test dataset shows P(i|j) in the predictions for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = get_model_representation(model1, n_items)\n",
    "M2 = get_model_representation(model1, n_items, trip_dataset_test)\n",
    "M3 = visualise_tripdataset_trips(trip_dataset_train, n_items)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4), constrained_layout = True)\n",
    "\n",
    "im1 = axes[0].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[0].set_title(\"Model1 evaluated on [[0], [1], ...]\")\n",
    "\n",
    "im2 = axes[1].imshow(M2, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[1].set_title(\"Model1 evaluated on test_dataset\")\n",
    "\n",
    "im3 = axes[2].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[2].set_title(\"Training distribution\")\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\n",
    "cbar.set_label(\"Probability\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "loss_train_dataset = model1.evaluate(trip_dataset_train)\n",
    "loss_test_dataset = model1.evaluate(trip_dataset_test)\n",
    "print(f\"Loss of model1 on the train dataset {loss_train_dataset}\")\n",
    "print(f\"Loss of model1 on the test dataset {loss_test_dataset}\")\n",
    "print(\"Used loss for evaluation: NLL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Model 2 is using a uniform sampling distribution for NCE\n",
    "\n",
    "The following plot of the evaluation on the test dataset shows P(i|j) in the predictions for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = get_model_representation(model2, n_items)\n",
    "M2 = get_model_representation(model2, n_items, trip_dataset_test)\n",
    "M3 = visualise_tripdataset_trips(trip_dataset_train, n_items)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4), constrained_layout = True)\n",
    "\n",
    "im1 = axes[0].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[0].set_title(\"Model2 evaluated on [[0], [1], ...]\")\n",
    "\n",
    "im2 = axes[1].imshow(M2, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[1].set_title(\"Model2 evaluated on test_dataset\")\n",
    "\n",
    "im3 = axes[2].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[2].set_title(\"Training distribution\")\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\n",
    "cbar.set_label(\"Probability\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "loss_train_dataset = model2.evaluate(trip_dataset_train)\n",
    "loss_test_dataset = model2.evaluate(trip_dataset_test)\n",
    "print(f\"Loss of model2 on the train dataset {loss_train_dataset}\")\n",
    "print(f\"Loss of model2 on the test dataset {loss_test_dataset}\")\n",
    "print(\"Used loss for evaluation: NLL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "* The uniform sampling distribution yields a smaller loss.\n",
    "* The plot of the P(i|j) distribution, calculated after evaluation on the test dataset, differs from the base model\n",
    "\n",
    "  Maybe we should focus on P(i|baskets without {i}) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def plot_Pi_given_not_i(model, n_items, available_items=None, max_context_size=None):\n",
    "    \"\"\"\n",
    "    Plot the average probability P(i | context not containing i) for each item i,\n",
    "    averaged over all possible contexts that do not include i.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        The trained model with a .predict() method.\n",
    "    n_items : int\n",
    "        Number of unique items.\n",
    "    available_items : array-like or None, optional\n",
    "        Binary array indicating available items. If None, all items are available.\n",
    "    max_context_size : int or None, optional\n",
    "        Maximum context size to consider for tractability. If None, uses all sizes.\n",
    "    \"\"\"\n",
    "    Pi_given_not_i = np.zeros(n_items)\n",
    "    counts = np.zeros(n_items)\n",
    "\n",
    "    for i in range(n_items):\n",
    "        context_candidates = [j for j in range(n_items) if j != i]\n",
    "        if max_context_size is not None:\n",
    "            context_sizes = range(1, max_context_size + 1)\n",
    "        else:\n",
    "            context_sizes = range(1, n_items)\n",
    "        for k in context_sizes:\n",
    "            for context in itertools.combinations(context_candidates, k):\n",
    "                context_tensor = tf.ragged.constant([list(context)], dtype=tf.int32)\n",
    "                avail = np.ones(n_items, dtype=np.float32) if available_items is None else available_items\n",
    "                probas = model.predict(context_tensor, available_items=avail)\n",
    "                Pi_given_not_i[i] += probas[0, i]\n",
    "                counts[i] += 1\n",
    "\n",
    "    return Pi_given_not_i / np.maximum(counts, 1)\n",
    "\n",
    "def empirical_Pi_given_not_i(tripdataset, n_items, max_context_size=None):\n",
    "    \"\"\"\n",
    "    Compute empirical P(i | context not containing i) from a TripDataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tripdataset : TripDataset\n",
    "        The dataset containing trips.\n",
    "    n_items : int\n",
    "        Number of unique items.\n",
    "    max_context_size : int or None\n",
    "        If set, only consider contexts up to this size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of shape (n_items,) with empirical P(i | context not containing i).\n",
    "    \"\"\"\n",
    "    numerators = np.zeros(n_items)\n",
    "    denominators = np.zeros(n_items)\n",
    "\n",
    "    for trip in tripdataset.trips:\n",
    "        basket = list(trip.purchases)\n",
    "        for idx, target in enumerate(basket):\n",
    "            context = basket[:idx] + basket[idx+1:]\n",
    "            if target >= n_items:\n",
    "                continue\n",
    "            if max_context_size is not None and len(context) > max_context_size:\n",
    "                continue\n",
    "            # For every item, if it is NOT in the context, increment denominator\n",
    "            for i in range(n_items):\n",
    "                if i not in context:\n",
    "                    denominators[i] += 1\n",
    "                    if target == i:\n",
    "                        numerators[i] += 1\n",
    "\n",
    "    return numerators / np.maximum(denominators, 1)\n",
    "\n",
    "pi_not_i_train_dataset = empirical_Pi_given_not_i(trip_dataset_train, n_items)\n",
    "pi_not_i_model1 = plot_Pi_given_not_i(model1, n_items)\n",
    "pi_not_i_model2 = plot_Pi_given_not_i(model2, n_items)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes[0].bar(np.arange(n_items), pi_not_i_model1)\n",
    "axes[0].set_ylabel(r\"Average $P(i\\,|\\,\\mathrm{context\\ not\\ containing}\\ i)$\")\n",
    "axes[0].set_title(\"Model 1\")\n",
    "\n",
    "axes[1].bar(np.arange(n_items), pi_not_i_model2)\n",
    "axes[1].set_title(\"Model 2\")\n",
    "\n",
    "axes[2].bar(np.arange(n_items), pi_not_i_train_dataset)\n",
    "axes[2].set_title(\"Train dataset\")\n",
    "\n",
    "fig.supxlabel(\"Item index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Train on A1, A2, A3 and test on A4 cf J.D√©sir, V.Auriau, E. Malherbes paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortments definition\n",
    "assortment1 = np.array([[1,1,0,1,1,1,1,1]])\n",
    "assortment2 = np.array([[1,0,1,0,1,1,1,1]])\n",
    "assortment3 = np.array([[0,1,1,1,0,1,1,1]])\n",
    "assortment4 = np.array([[1,1,0,0,1,1,1,1]])\n",
    "assortment_full = np.array([[1,1,1,1,1,1,1,1]])\n",
    "\n",
    "n_baskets = 500\n",
    "\n",
    "trip_dataset_1 = data_gen.generate_trip_dataset(n_baskets,assortment1)\n",
    "trip_dataset_2 = data_gen.generate_trip_dataset(n_baskets,assortment2)\n",
    "trip_dataset_3 = data_gen.generate_trip_dataset(n_baskets,assortment3)\n",
    "\n",
    "\n",
    "paper_trip_dataset_train = trip_dataset_1.concatenate(trip_dataset_2).concatenate(trip_dataset_3)\n",
    "paper_trip_dataset_test_a4 = data_gen.generate_trip_dataset(10*n_baskets,assortment4)\n",
    "paper_trip_dataset_test_full = data_gen.generate_trip_dataset(10*n_baskets,assortment_full)\n",
    "\n",
    "\n",
    "M1 = visualise_tripdataset_trips(paper_trip_dataset_train,n_items)\n",
    "M2 = visualise_tripdataset_trips(paper_trip_dataset_test_a4,n_items)\n",
    "M3 = visualise_tripdataset_trips(paper_trip_dataset_test_full,n_items)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4), constrained_layout = True)\n",
    "\n",
    "im1 = axes[0].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[0].set_title(\"Train dataset (A1,A2,A3)\")\n",
    "\n",
    "im2 = axes[1].imshow(M2, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[1].set_title(\"Test dataset (A4)\")\n",
    "\n",
    "im3 = axes[2].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[2].set_title(\"Test dataset (A_full)\")\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\n",
    "cbar.set_label(\"Probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Instantiate and train model 3 on A1,A2,A3 with uniform sampling for NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = AttentionBasedContextEmbedding(\n",
    "    epochs=50,\n",
    "    lr=0.05,\n",
    "    embedding_dim=3,\n",
    "    n_negative_samples=3)\n",
    "\n",
    "model3.instantiate(n_items=n_items,use_true_nce_distribution = False)\n",
    "history3 = model3.fit(paper_trip_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = get_model_representation(model3, n_items, assortment_matrix = [[1,1,0,0,1,1,1,1]])\n",
    "M2 = get_model_representation(model3, n_items)\n",
    "M3 = visualise_tripdataset_trips(paper_trip_dataset_train,n_items)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4), constrained_layout = True)\n",
    "\n",
    "im1 = axes[0].imshow(M3, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[0].set_title(\"Train dataset (A1,A2,A3)\")\n",
    "\n",
    "im2 = axes[1].imshow(M1, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[1].set_title(\"Model3 evaluated on A4\")\n",
    "\n",
    "im3 = axes[2].imshow(M2, vmin=0.0, vmax=1, interpolation='nearest', cmap=\"coolwarm\")\n",
    "axes[2].set_title(\"Model3 evaluated on Afull\")\n",
    "\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=axes, orientation='vertical', shrink=0.8)\n",
    "cbar.set_label(\"Probability\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "loss_paper_train_dataset = model3.evaluate(paper_trip_dataset_train)\n",
    "loss_paper_test_dataset_a4 = model3.evaluate(paper_trip_dataset_test_a4)\n",
    "loss_paper_test_dataset_afull = model3.evaluate(paper_trip_dataset_test_full)\n",
    "print(f\"Loss of model3 on the train dataset {loss_paper_train_dataset}\")\n",
    "print(f\"Loss of model3 on the test dataset A4 {loss_paper_test_dataset_a4}\")\n",
    "print(f\"Loss of model2 on the test dataset Afull {loss_paper_test_dataset_afull}\")\n",
    "print(\"Used loss for evaluation : NLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_not_i_train_dataset = empirical_Pi_given_not_i(trip_dataset_train, n_items)\n",
    "pi_not_i_model3_a4 = plot_Pi_given_not_i(model3, n_items, available_items = [1,1,0,0,1,1,1,1])\n",
    "pi_not_i_model3_full = plot_Pi_given_not_i(model3, n_items)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes[0].bar(np.arange(n_items), pi_not_i_model3_full)\n",
    "axes[0].set_ylabel(r\"Average $P(i\\,|\\,\\mathrm{context\\ not\\ containing}\\ i)$\")\n",
    "axes[0].set_title(\"Model 3 on Afull\")\n",
    "\n",
    "axes[1].bar(np.arange(n_items), pi_not_i_model3_a4)\n",
    "axes[1].set_title(\"Model 3 on A4\")\n",
    "\n",
    "axes[2].bar(np.arange(n_items), pi_not_i_train_dataset)\n",
    "axes[2].set_title(\"Train dataset\")\n",
    "\n",
    "fig.supxlabel(\"Item index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Test save and load methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset\n",
    "eval_dataset = data_gen.generate_trip_dataset(100, full_assortment_matrix)\n",
    "\n",
    "# Evaluate model\n",
    "loss_eval_dataset_1 = model1.evaluate(eval_dataset)\n",
    "print(f\"Loss of model1 on the evaluation dataset {loss_eval_dataset_1}\")\n",
    "\n",
    "# Save model\n",
    "model1.save_model(\"attn_model.json\")\n",
    "\n",
    "# Create a model 3 without instantiating\n",
    "model3 = AttentionBasedContextEmbedding(\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_negative_samples=n_negative_samples)\n",
    "\n",
    "# Load first model and compare results on evaluation dataset\n",
    "model3.load_model(\"attn_model.json\")\n",
    "loss_eval_dataset_3 = model3.evaluate(eval_dataset)\n",
    "print(f\"Loss of model3 on the evaluation dataset {loss_eval_dataset_3}\")\n",
    "os.remove(\"attn_model.json\")\n",
    "os.remove(\"attn_model_empty_context_embedding.npy\")\n",
    "os.remove(\"attn_model_wa.npy\")\n",
    "os.remove(\"attn_model_Wi.npy\")\n",
    "os.remove(\"attn_model_Wo.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Why is True NCE not as good as we predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(predictions, targets, k=10):\n",
    "    \"\"\"Calculate Hit Rate @ K\"\"\"\n",
    "    hit_count = 0\n",
    "    for i, target in enumerate(targets):\n",
    "        top_k_items = np.argsort(predictions[i])[-k:]\n",
    "        if target in top_k_items:\n",
    "            hit_count += 1\n",
    "    return hit_count / len(targets)\n",
    "\n",
    "def calculate_mrr(predictions, targets, k=10):\n",
    "    \"\"\"Calculate Mean Reciprocal Rank @ K\"\"\"\n",
    "    mrr_sum = 0\n",
    "    for i, target in enumerate(targets):\n",
    "        top_k_items = np.argsort(predictions[i])[-k:][::-1]  # Sort descending\n",
    "        try:\n",
    "            rank = list(top_k_items).index(target) + 1\n",
    "            mrr_sum += 1.0 / rank\n",
    "        except ValueError:\n",
    "            pass  # Target not in top-k\n",
    "    return mrr_sum / len(targets)\n",
    "\n",
    "def calculate_ndcg(predictions, targets, k=10):\n",
    "    \"\"\"Calculate Normalized Discounted Cumulative Gain @ K\"\"\"\n",
    "    ndcg_sum = 0\n",
    "    for i, target in enumerate(targets):\n",
    "        top_k_items = np.argsort(predictions[i])[-k:][::-1]  # Sort descending\n",
    "        try:\n",
    "            rank = list(top_k_items).index(target) + 1\n",
    "            dcg = 1.0 / np.log2(rank + 1)\n",
    "            idcg = 1.0  # Perfect ranking for binary relevance\n",
    "            ndcg_sum += dcg / idcg\n",
    "        except ValueError:\n",
    "            pass  # Target not in top-k\n",
    "    return ndcg_sum / len(targets)\n",
    "\n",
    "def evaluate_model_comprehensive(model, test_dataset, k_values=[1, 5, 10, 20]):\n",
    "    \"\"\"Comprehensive evaluation of the model\"\"\"\n",
    "    results = {}\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    for batch in test_dataset.iter_batch(batch_size=100, shuffle=False, data_method=\"aleacarta\"):\n",
    "        target_items, context_items, _, _, _, _, available_items = batch\n",
    "        \n",
    "        # Convert to proper format for prediction\n",
    "        context_ragged = tf.ragged.constant([row[row != -1] for row in context_items], dtype=tf.int32)\n",
    "        predictions = model.predict(context_ragged, available_items)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(target_items)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    print(f\"Evaluating {len(all_targets)} predictions...\")\n",
    "    \n",
    "    # Calculate metrics for different k values\n",
    "    for k in k_values:\n",
    "        results[f'hit_rate@{k}'] = calculate_hit_rate(all_predictions, all_targets, k)\n",
    "        results[f'mrr@{k}'] = calculate_mrr(all_predictions, all_targets, k)\n",
    "        results[f'ndcg@{k}'] = calculate_ndcg(all_predictions, all_targets, k)\n",
    "    \n",
    "    return results, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(n_items=50, n_baskets_train=4000, n_baskets_test=500):\n",
    "    \"\"\"Create synthetic dataset for evaluation\"\"\"\n",
    "    \n",
    "    # Define item nests (product categories)\n",
    "    items_nest = {\n",
    "        0: list(range(0, 10)),      # Electronics\n",
    "        1: list(range(10, 20)),     # Clothing  \n",
    "        2: list(range(20, 30)),     # Food\n",
    "        3: list(range(30, 40)),     # Books\n",
    "        4: list(range(40, 50))      # Sports\n",
    "    }\n",
    "    \n",
    "    # Define nest interactions (complementary, neutral relationships)\n",
    "    nests_interactions = [\n",
    "        [\"\", \"neutral\", \"neutral\", \"compl\", \"neutral\"],      # Electronics\n",
    "        [\"neutral\", \"\", \"compl\", \"neutral\", \"compl\"],        # Clothing\n",
    "        [\"neutral\", \"compl\", \"\", \"neutral\", \"neutral\"],      # Food  \n",
    "        [\"compl\", \"neutral\", \"neutral\", \"\", \"neutral\"],      # Books\n",
    "        [\"neutral\", \"compl\", \"neutral\", \"neutral\", \"\"]       # Sports\n",
    "    ]\n",
    "    \n",
    "    # Create data generator\n",
    "    generator = SyntheticDataGenerator(\n",
    "        items_nest=items_nest,\n",
    "        nests_interactions=nests_interactions,\n",
    "        proba_complementary_items=0.7,\n",
    "        proba_neutral_items=0.3,\n",
    "        noise_proba=0.1,\n",
    "        plant_seed=42\n",
    "    )\n",
    "    \n",
    "    # Create assortment matrix (all items available in single assortment for simplicity)\n",
    "    assortments_matrix = np.ones((1, n_items), dtype=int)\n",
    "    \n",
    "    # Generate training dataset\n",
    "    print(\"Generating training dataset...\")\n",
    "    train_dataset = generator.generate_trip_dataset(\n",
    "        n_baskets=n_baskets_train, \n",
    "        assortments_matrix=assortments_matrix,\n",
    "        len_basket=None\n",
    "    )\n",
    "    \n",
    "    # Generate test dataset\n",
    "    print(\"Generating test dataset...\")\n",
    "    test_dataset = generator.generate_trip_dataset(\n",
    "        n_baskets=n_baskets_test, \n",
    "        assortments_matrix=assortments_matrix,\n",
    "        len_basket=None\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset: {len(train_dataset)} trips\")\n",
    "    print(f\"Test dataset: {len(test_dataset)} trips\")\n",
    "    print(f\"Max basket length: {train_dataset.max_length}\")\n",
    "    print(f\"Number of items: {train_dataset.n_items}\")\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Create datasets\n",
    "train_data, test_data = create_synthetic_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(train_dataset, test_dataset, use_true_nce=True, model_name=\"model\"):\n",
    "    \"\"\"Train and evaluate a single model configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name} (use_true_nce_distribution={use_true_nce})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    config = {\n",
    "        'epochs': 40,\n",
    "        'lr': 0.001,\n",
    "        'embedding_dim': 64,\n",
    "        'n_negative_samples': 10,\n",
    "        'batch_size': 64,\n",
    "        'optimizer': 'Adam'\n",
    "    }\n",
    "    \n",
    "    print(f\"Config: {config}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AttentionBasedContextEmbedding(**config)\n",
    "    \n",
    "    # Instantiate model\n",
    "    model.instantiate(\n",
    "        n_items=train_dataset.n_items,\n",
    "        use_true_nce_distribution=use_true_nce\n",
    "    )\n",
    "    \n",
    "    print(f\"Model instantiated with {train_dataset.n_items} items\")\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_dataset)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    # Evaluate model\n",
    "    start_time = time.time()\n",
    "    results, predictions, targets = evaluate_model_comprehensive(model, test_dataset)\n",
    "    eval_time = time.time() - start_time\n",
    "        \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'results': results,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets,\n",
    "        'config': config,\n",
    "        'training_time': training_time,\n",
    "        'eval_time': eval_time\n",
    "    }\n",
    "\n",
    "print(\"Training function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with true NCE distribution\n",
    "results_true_nce = train_and_evaluate_model(\n",
    "    train_data, test_data, \n",
    "    use_true_nce=True, \n",
    "    model_name=\"Model_TRUE_NCE\"\n",
    ")\n",
    "\n",
    "# Train model with uniform NCE distribution  \n",
    "results_false_nce = train_and_evaluate_model(\n",
    "    train_data, test_data, \n",
    "    use_true_nce=False, \n",
    "    model_name=\"Model_UNIFORM_NCE\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED FOR BOTH MODELS\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(results_true, results_false):\n",
    "    \"\"\"Create detailed comparison of results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"DETAILED RESULTS COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    metrics = ['hit_rate@1', 'hit_rate@5', 'hit_rate@10', 'hit_rate@20',\n",
    "               'mrr@1', 'mrr@5', 'mrr@10', 'mrr@20',\n",
    "               'ndcg@1', 'ndcg@5', 'ndcg@10', 'ndcg@20']\n",
    "    \n",
    "    comparison_data = []\n",
    "    for metric in metrics:\n",
    "        true_val = results_true['results'][metric]\n",
    "        false_val = results_false['results'][metric]\n",
    "        improvement = ((true_val - false_val) / false_val * 100) if false_val > 0 else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Metric': metric,\n",
    "            'True_NCE': true_val,\n",
    "            'Uniform_NCE': false_val,\n",
    "            'Improvement_%': improvement\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nMetrics Comparison:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Training time comparison\n",
    "    print(f\"\\nTraining Time Comparison:\")\n",
    "    print(f\"True NCE: {results_true['training_time']:.2f} seconds\")\n",
    "    print(f\"Uniform NCE: {results_false['training_time']:.2f} seconds\")\n",
    "    print(f\"Time difference: {results_true['training_time'] - results_false['training_time']:.2f} seconds\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "comparison_df = compare_results(results_true_nce, results_false_nce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive metrics comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Hit Rate comparison\n",
    "k_values = [1, 5, 10, 20]\n",
    "hit_rates_true = [results_true_nce['results'][f'hit_rate@{k}'] for k in k_values]\n",
    "hit_rates_false = [results_false_nce['results'][f'hit_rate@{k}'] for k in k_values]\n",
    "\n",
    "axes[0,0].bar(np.arange(len(k_values)) - 0.2, hit_rates_true, 0.4, \n",
    "              label='True NCE', color='blue', alpha=0.7)\n",
    "axes[0,0].bar(np.arange(len(k_values)) + 0.2, hit_rates_false, 0.4, \n",
    "              label='Uniform NCE', color='red', alpha=0.7)\n",
    "axes[0,0].set_xlabel('K Value')\n",
    "axes[0,0].set_ylabel('Hit Rate @ K')\n",
    "axes[0,0].set_title('Hit Rate Comparison')\n",
    "axes[0,0].set_xticks(range(len(k_values)))\n",
    "axes[0,0].set_xticklabels(k_values)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MRR comparison\n",
    "mrr_true = [results_true_nce['results'][f'mrr@{k}'] for k in k_values]\n",
    "mrr_false = [results_false_nce['results'][f'mrr@{k}'] for k in k_values]\n",
    "\n",
    "axes[0,1].bar(np.arange(len(k_values)) - 0.2, mrr_true, 0.4, \n",
    "              label='True NCE', color='blue', alpha=0.7)\n",
    "axes[0,1].bar(np.arange(len(k_values)) + 0.2, mrr_false, 0.4, \n",
    "              label='Uniform NCE', color='red', alpha=0.7)\n",
    "axes[0,1].set_xlabel('K Value')\n",
    "axes[0,1].set_ylabel('MRR @ K')\n",
    "axes[0,1].set_title('Mean Reciprocal Rank Comparison')\n",
    "axes[0,1].set_xticks(range(len(k_values)))\n",
    "axes[0,1].set_xticklabels(k_values)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# NDCG comparison\n",
    "ndcg_true = [results_true_nce['results'][f'ndcg@{k}'] for k in k_values]\n",
    "ndcg_false = [results_false_nce['results'][f'ndcg@{k}'] for k in k_values]\n",
    "\n",
    "axes[1,0].bar(np.arange(len(k_values)) - 0.2, ndcg_true, 0.4, \n",
    "              label='True NCE', color='blue', alpha=0.7)\n",
    "axes[1,0].bar(np.arange(len(k_values)) + 0.2, ndcg_false, 0.4, \n",
    "              label='Uniform NCE', color='red', alpha=0.7)\n",
    "axes[1,0].set_xlabel('K Value')\n",
    "axes[1,0].set_ylabel('NDCG @ K')\n",
    "axes[1,0].set_title('NDCG Comparison')\n",
    "axes[1,0].set_xticks(range(len(k_values)))\n",
    "axes[1,0].set_xticklabels(k_values)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement percentage\n",
    "improvements = []\n",
    "for k in k_values:\n",
    "    hr_imp = ((hit_rates_true[k_values.index(k)] - hit_rates_false[k_values.index(k)]) / \n",
    "              hit_rates_false[k_values.index(k)] * 100) if hit_rates_false[k_values.index(k)] > 0 else 0\n",
    "    improvements.append(hr_imp)\n",
    "\n",
    "axes[1,1].bar(range(len(k_values)), improvements, color='green', alpha=0.7)\n",
    "axes[1,1].set_xlabel('K Value')\n",
    "axes[1,1].set_ylabel('Improvement (%)')\n",
    "axes[1,1].set_title('Hit Rate Improvement: True NCE vs Uniform NCE')\n",
    "axes[1,1].set_xticks(range(len(k_values)))\n",
    "axes[1,1].set_xticklabels(k_values)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_embeddings(model_true, model_false, train_dataset):\n",
    "    \"\"\"Analyze and compare learned embeddings\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL INTERPRETABILITY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get embedding matrices\n",
    "    Wi_true = model_true.Wi.numpy()\n",
    "    Wo_true = model_true.Wo.numpy()\n",
    "    Wi_false = model_false.Wi.numpy() \n",
    "    Wo_false = model_false.Wo.numpy()\n",
    "    \n",
    "    print(f\"Input embedding dimensions: {Wi_true.shape}\")\n",
    "    print(f\"Output embedding dimensions: {Wo_true.shape}\")\n",
    "    \n",
    "    # Analyze embedding magnitudes\n",
    "    print(\"\\nEmbedding Magnitude Analysis:\")\n",
    "    print(f\"True NCE - Input embeddings mean norm: {np.mean(np.linalg.norm(Wi_true, axis=1)):.4f}\")\n",
    "    print(f\"True NCE - Output embeddings mean norm: {np.mean(np.linalg.norm(Wo_true, axis=1)):.4f}\")\n",
    "    print(f\"Uniform NCE - Input embeddings mean norm: {np.mean(np.linalg.norm(Wi_false, axis=1)):.4f}\")\n",
    "    print(f\"Uniform NCE - Output embeddings mean norm: {np.mean(np.linalg.norm(Wo_false, axis=1)):.4f}\")\n",
    "    \n",
    "    # Analyze embedding similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Calculate pairwise similarities\n",
    "    sim_Wi_true = cosine_similarity(Wi_true)\n",
    "    sim_Wo_true = cosine_similarity(Wo_true)\n",
    "    sim_Wi_false = cosine_similarity(Wi_false)\n",
    "    sim_Wo_false = cosine_similarity(Wo_false)\n",
    "    \n",
    "    print(f\"\\nEmbedding Similarity Analysis:\")\n",
    "    print(f\"True NCE - Input embeddings mean similarity: {np.mean(sim_Wi_true[np.triu_indices(len(sim_Wi_true), k=1)]):.4f}\")\n",
    "    print(f\"True NCE - Output embeddings mean similarity: {np.mean(sim_Wo_true[np.triu_indices(len(sim_Wo_true), k=1)]):.4f}\")\n",
    "    print(f\"Uniform NCE - Input embeddings mean similarity: {np.mean(sim_Wi_false[np.triu_indices(len(sim_Wi_false), k=1)]):.4f}\")\n",
    "    print(f\"Uniform NCE - Output embeddings mean similarity: {np.mean(sim_Wo_false[np.triu_indices(len(sim_Wo_false), k=1)]):.4f}\")\n",
    "    \n",
    "    # Visualize embedding similarities\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # True NCE embeddings\n",
    "    im1 = axes[0,0].imshow(sim_Wi_true, cmap='viridis', vmin=-1, vmax=1)\n",
    "    axes[0,0].set_title('True NCE - Input Embeddings Similarity')\n",
    "    axes[0,0].set_xlabel('Item ID')\n",
    "    axes[0,0].set_ylabel('Item ID')\n",
    "    plt.colorbar(im1, ax=axes[0,0])\n",
    "    \n",
    "    im2 = axes[0,1].imshow(sim_Wo_true, cmap='viridis', vmin=-1, vmax=1)\n",
    "    axes[0,1].set_title('True NCE - Output Embeddings Similarity')\n",
    "    axes[0,1].set_xlabel('Item ID')\n",
    "    axes[0,1].set_ylabel('Item ID')\n",
    "    plt.colorbar(im2, ax=axes[0,1])\n",
    "    \n",
    "    # Uniform NCE embeddings\n",
    "    im3 = axes[1,0].imshow(sim_Wi_false, cmap='viridis', vmin=-1, vmax=1)\n",
    "    axes[1,0].set_title('Uniform NCE - Input Embeddings Similarity')\n",
    "    axes[1,0].set_xlabel('Item ID')\n",
    "    axes[1,0].set_ylabel('Item ID')\n",
    "    plt.colorbar(im3, ax=axes[1,0])\n",
    "    \n",
    "    im4 = axes[1,1].imshow(sim_Wo_false, cmap='viridis', vmin=-1, vmax=1)\n",
    "    axes[1,1].set_title('Uniform NCE - Output Embeddings Similarity')\n",
    "    axes[1,1].set_xlabel('Item ID')\n",
    "    axes[1,1].set_ylabel('Item ID')\n",
    "    plt.colorbar(im4, ax=axes[1,1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'Wi_true': Wi_true, 'Wo_true': Wo_true,\n",
    "        'Wi_false': Wi_false, 'Wo_false': Wo_false,\n",
    "        'similarities': {\n",
    "            'Wi_true': sim_Wi_true, 'Wo_true': sim_Wo_true,\n",
    "            'Wi_false': sim_Wi_false, 'Wo_false': sim_Wo_false\n",
    "        }\n",
    "    }\n",
    "\n",
    "embedding_analysis = analyze_model_embeddings(\n",
    "    results_true_nce['model'], \n",
    "    results_false_nce['model'], \n",
    "    train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(results_true, results_false, significance_results):\n",
    "    \"\"\"Generate comprehensive final report\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"FINAL EVALUATION REPORT: ATTENTION-BASED MODEL COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(\"\\nüìä EXECUTIVE SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Best performing model\n",
    "    hr10_true = results_true['results']['hit_rate@10']\n",
    "    hr10_false = results_false['results']['hit_rate@10']\n",
    "    best_model = \"True NCE Distribution\" if hr10_true > hr10_false else \"Uniform NCE Distribution\"\n",
    "    improvement = abs(hr10_true - hr10_false) / min(hr10_true, hr10_false) * 100\n",
    "    \n",
    "    print(f\"Best performing model: {best_model}\")\n",
    "    print(f\"Performance improvement: {improvement:.2f}%\")\n",
    "    print(f\"Statistical significance: {'Yes' if significance_results['paired_t_test']['p_value'] < 0.05 else 'No'}\")\n",
    "    \n",
    "    print(\"\\nüìà KEY METRICS COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    metrics_summary = [\n",
    "        (\"Hit Rate@10\", hr10_true, hr10_false),\n",
    "        (\"MRR@10\", results_true['results']['mrr@10'], results_false['results']['mrr@10']),\n",
    "        (\"NDCG@10\", results_true['results']['ndcg@10'], results_false['results']['ndcg@10'])\n",
    "    ]\n",
    "    \n",
    "    for metric, true_val, false_val in metrics_summary:\n",
    "        better = \"TRUE NCE\" if true_val > false_val else \"UNIFORM NCE\"\n",
    "        diff = abs(true_val - false_val)\n",
    "        print(f\"{metric:12} | True NCE: {true_val:.4f} | Uniform NCE: {false_val:.4f} | Best: {better} (+{diff:.4f})\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è  EFFICIENCY COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Training time (True NCE):    {results_true['training_time']:.2f} seconds\")\n",
    "    print(f\"Training time (Uniform NCE): {results_false['training_time']:.2f} seconds\")\n",
    "    print(f\"Time overhead: {results_true['training_time'] - results_false['training_time']:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nüî¨ STATISTICAL ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"P-value (paired t-test): {significance_results['paired_t_test']['p_value']:.6f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Generate final report\n",
    "generate_final_report(results_true_nce, results_false_nce, significance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
