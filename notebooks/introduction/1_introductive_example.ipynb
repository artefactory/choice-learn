{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"../../docs/illustrations/logos/logo_choice_learn.png\" width=\"256\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice-Learn is a Python package designed to help building discrete choice models. In particular you will find:\n",
    "\n",
    "- Optimized **Data** handling with the ChoiceDataset object and ready-to-use datasets\n",
    "- **Modelling** tools with:\n",
    "    - Efficient well-known choice models\n",
    "    - Customizable class ChoiceModel to build your own model\n",
    "    - Estimation options such as choosing the method (LBFGS, Gradient Descent, etc...)\n",
    "- Divers **Tools** revolving around choice models such as an Assortment Optimizer\n",
    "\n",
    "\n",
    "### Discrete Choice Modelling\n",
    "Discrete choice models aim at explaining or predicting a choice from a set of alternatives. Well known use-cases include analyzing people choice of mean of transport or products purchases in stores.\n",
    "\n",
    "If you are new to choice modelling, you can check this [resource](https://www.publichealth.columbia.edu/research/population-health-methods/discrete-choice-model-and-analysis). \n",
    "\n",
    "### Tutorial\n",
    "In this notebook we will describe step-by-step the estimation of a choice model.\n",
    "\n",
    "- [Data Handling](#Data:-items,-features-and-choices)\n",
    "- [Modelling](#modelling:-estimation-and-choice-probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: items, features and choices\n",
    "\n",
    "The data structure for choice modelling is somehow different than usual prediction use-cases.\n",
    "We consider a set of variable size of different alternatives. Each alternative is described by features and one is chosen among the set. Some contexts features (describing a customer, or time) can also affect the choice.\n",
    "Let's take an example where we want to predict a customer's next purchase.\n",
    "\n",
    "Three different items, i<sub>1</sub>, i<sub>2</sub> and i<sub>3</sub> are sold and we have gathered a small dataset:\n",
    "\n",
    "<table>\n",
    "<tr><th>1st Purchase: </th><th>2nd Purchase:</th><th>3rd Purchase:</th></tr>\n",
    "\n",
    "<tr><td>\n",
    "\n",
    "**Shelf**:\n",
    "\n",
    "| Item           | Price   | Promotion |\n",
    "| -------------- | ------- | --------- |\n",
    "| i<sub>1</sub>  | $100    | no        |\n",
    "| i<sub>2</sub>  | $140    | no        |\n",
    "| i<sub>3</sub>  | $200    | no        |\n",
    "\n",
    "**Customer Purchase:** i<sub>1</sub>\n",
    "\n",
    "</td><td>\n",
    "\n",
    "**Shelf**:\n",
    "\n",
    "| Item           | Price   | Promotion |\n",
    "| -------------- | ------- | --------- |\n",
    "| i<sub>1</sub>  | $100    | no        |\n",
    "| i<sub>2</sub>  | $120    | yes       |\n",
    "| i<sub>3</sub>  | $200    | no        |\n",
    "\n",
    "**Customer Purchase:** i<sub>2</sub>\n",
    "\n",
    "</td><td>\n",
    "\n",
    "**Shelf**:\n",
    "\n",
    "| Item           | Price        | Promotion    |\n",
    "| -------------- | ------------ | ------------ |\n",
    "| i<sub>1</sub>  | $100         | no           |\n",
    "| i<sub>2</sub>  | Out-Of-Stock | Out-Of-Stock |\n",
    "| i<sub>3</sub>  | $180         | yes          |\n",
    "\n",
    "**Customer Purchase:** i<sub>3</sub>\n",
    "\n",
    "</td></tr> </table>\n",
    "\n",
    "Indexing the items in the same order, we create the ChoiceDataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [0, 1, 2] # Indexes of the items chosen\n",
    "\n",
    "items_features_by_choice =  [\n",
    "    [\n",
    "        [100., 0.], # choice 1, Item 1 [price, promotion]\n",
    "        [140., 0.], # choice 1, Item 2 [price, promotion]\n",
    "        [200., 0.], # choice 1, Item 3 [price, promotion]\n",
    "    ],\n",
    "    [\n",
    "        [100., 0.], # choice 2, Item 1 [price, promotion]\n",
    "        [120., 1.], # choice 2, Item 2 [price, promotion]\n",
    "        [200., 0.], # choice 2, Item 3 [price, promotion]\n",
    "    ],\n",
    "    [\n",
    "        [100., 0.], # choice 3, Item 1 [price, promotion]\n",
    "        [120., 1.], # choice 3, Item 2 [price, promotion]\n",
    "        [180., 1.], # choice 3, Item 3 [price, promotion]\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item i<sub>2</sub> was out of stock during the last choice. Thus it could not have been chosen. In order to keep this information we create a matric indicating which items were available during each of the choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_items_by_choice = [\n",
    "    [1, 1, 1], # All items available for choice 1\n",
    "    [1, 1, 1], # All items available for choice 2\n",
    "    [1, 0, 1], # Item 2 not available for choice 3\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's create the ChoiceDataset! We can also specify the features names if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.data import ChoiceDataset\n",
    "\n",
    "dataset = ChoiceDataset(\n",
    "    choices=choices,\n",
    "    items_features_by_choice=items_features_by_choice,\n",
    "    items_features_by_choice_names=[\"price\", \"promotion\"],\n",
    "    available_items_by_choice=available_items_by_choice,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling: Estimation and choice probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first and simple model to predict a customer choice is the Multinomial Logit.\n",
    "\n",
    "We consider that customers attribute a utility to each product and that he chooses the product with hightest utility.\n",
    "\n",
    "We formulate the utility as a linear function of our features:\n",
    "\n",
    "$$U(i) = \\alpha_i +  \\beta \\cdot price(i) + \\gamma \\cdot promotion(i)$$\n",
    "\n",
    "Considering that this estimation is noisy, we use the softmax function over the available products to get the purchase probability. For example using our first data sample we obtain:\n",
    "\n",
    "$$\\mathbb{P}(i_1) = \\frac{e^{U(i_1)}}{e^{U(i_1)} + e^{U(i_2)} + e^{U(i_3)}}$$\n",
    "\n",
    "For the third sample only two items are still available, making the probability:\n",
    "$$\\mathbb{P}(i_1) = \\frac{e^{U(i_1)}}{e^{U(i_1)} + e^{U(i_3)}}$$\n",
    "\n",
    "The parameters $\\alpha_i$, $\\beta$ and $\\gamma$ are estimated by maximizing the Negative Log-Likelihood. Here is how it goes with Choice-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid value for argument `reduction`. Expected one of {None, 'none', 'sum', 'sum_over_batch_size'}. Received: reduction=auto",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchoice_learn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleMNL\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleMNL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "File \u001b[0;32m~/Documents/GitHub/choice-learn/notebooks/introduction/../../choice_learn/models/simple_mnl.py:36\u001b[0m, in \u001b[0;36mSimpleMNL.__init__\u001b[0;34m(self, add_exit_choice, intercept, optimizer, lr, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     18\u001b[0m     add_exit_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     23\u001b[0m ):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize of Simple-MNL.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m        Learning Rate to be used with optimizer.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madd_exit_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_exit_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstantiated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept \u001b[38;5;241m=\u001b[39m intercept\n",
      "File \u001b[0;32m~/Documents/GitHub/choice-learn/notebooks/introduction/../../choice_learn/models/base_model.py:72\u001b[0m, in \u001b[0;36mChoiceModel.__init__\u001b[0;34m(self, label_smoothing, add_exit_choice, optimizer, tolerance, callbacks, lr, epochs, batch_size, regularization, regularization_strength)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Loss function wrapping tf.keras.losses.CategoricalCrossEntropy\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# with smoothing and normalization options\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m tf_ops\u001b[38;5;241m.\u001b[39mCustomCategoricalCrossEntropy(\n\u001b[1;32m     70\u001b[0m     from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_nll \u001b[38;5;241m=\u001b[39m \u001b[43mtf_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCustomCategoricalCrossEntropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexact_categorical_crossentropy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCallbackList(callbacks, add_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mset_model(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/choice-learn/notebooks/introduction/../../choice_learn/tf_ops.py:108\u001b[0m, in \u001b[0;36mCustomCategoricalCrossEntropy.__init__\u001b[0;34m(self, from_logits, sparse, label_smoothing, axis, epsilon, name, reduction)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     77\u001b[0m     from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_over_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     84\u001b[0m ):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize function.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Follows structure of tf.keras.losses.CategoricalCrossentropy.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m        Reduction function - here to follow tf.keras.losses.Loss signature\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing \u001b[38;5;241m=\u001b[39m label_smoothing\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_logits \u001b[38;5;241m=\u001b[39m from_logits\n",
      "File \u001b[0;32m~/Documents/GitHub/choice-learn/.conda/lib/python3.11/site-packages/keras/src/losses/loss.py:29\u001b[0m, in \u001b[0;36mLoss.__init__\u001b[0;34m(self, name, reduction, dtype)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_over_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize_reduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mor\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mfloatx()\n",
      "File \u001b[0;32m~/Documents/GitHub/choice-learn/.conda/lib/python3.11/site-packages/keras/src/losses/loss.py:80\u001b[0m, in \u001b[0;36mstandardize_reduction\u001b[0;34m(reduction)\u001b[0m\n\u001b[1;32m     78\u001b[0m allowed \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_over_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for argument `reduction`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduction=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduction\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid value for argument `reduction`. Expected one of {None, 'none', 'sum', 'sum_over_batch_size'}. Received: reduction=auto"
     ]
    }
   ],
   "source": [
    "from choice_learn.models import SimpleMNL\n",
    "\n",
    "model = SimpleMNL(intercept=\"item\")\n",
    "history = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the weights estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features coefficients are:\n",
      "<tf.Variable 'Weights_items_features:0' shape=(2,) dtype=float32, numpy=array([ -6.0389166, 471.84808  ], dtype=float32)>\n",
      "Items intercepts:\n",
      "[0] and <tf.Variable 'Intercept:0' shape=(2,) dtype=float32, numpy=array([-191.80449,  171.00584], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(\"Features coefficients are:\")\n",
    "print(model.trainable_weights[0])\n",
    "print(\"Items intercepts:\")\n",
    "print([0], \"and\", model.trainable_weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the average Negative Log-Likelihood of the model, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.001363e-05>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now acces estimated choice probabilities using a ChoiceDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities are:\n",
      "tf.Tensor(\n",
      "[[0.99999 0.      0.     ]\n",
      " [0.      0.99999 0.     ]\n",
      " [0.      0.      0.99999]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "probabilities = model.predict_probas(dataset)\n",
    "print(\"Probabilities are:\")\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Jupyter Notebooks\n",
    "\n",
    "If you want to go further, here are a few useful Jupyter Notebooks:\n",
    "\n",
    "*Data:*\n",
    "- A more complete example [here]()\n",
    "- A detailed use of FeaturesByIDs [here]() if you want to minimize your RAM footprint\n",
    "\n",
    "*Modelling:*\n",
    "- A more complete example using the Conditional-MNL [here]()\n",
    "- An example to easily build custom models [here]()\n",
    "\n",
    "*Tools:*\n",
    "- An example of assortment optimization using a choice model and Gurobi [here]()\n",
    "\n",
    "Here are complementary Notebooks that might interest you:\n",
    "- A comparison with the R package mlogit [here]()\n",
    "- A reconstruction of the experiments of the RUMnet paper [here]()\n",
    "- An example of estimatation of a Latent Class MNL [here]()\n",
    "- A reconstruction using Choice-Learn of a Logistic Regression with the scikit-learn package [here]()\n",
    "\n",
    "### Documentation\n",
    "The [full documentation]() also hosts a lot of useful details and information.\n",
    "\n",
    "### Additional Questions, Requests, etc...\n",
    "If you have ideas, questions, features request or any other input, do not hesitate to reach out by opening an issue on [GitHub]()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
