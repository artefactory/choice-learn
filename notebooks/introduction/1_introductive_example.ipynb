{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"../../docs/illustrations/logos/logo_choice_learn.png\" width=\"256\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice-Learn is a Python package designed to help building discrete choice models. In particular you will find:\n",
    "\n",
    "- Optimized **Data** handling with the ChoiceDataset object and ready-to-use datasets\n",
    "- **Modelling** tools with:\n",
    "    - Efficient well-known choice models\n",
    "    - Customizable class ChoiceModel to build your own model\n",
    "    - Estimation options such as choosing the method (LBFGS, Gradient Descent, etc...)\n",
    "- Divers **Tools** revolving around choice models such as an Assortment Optimizer\n",
    "\n",
    "\n",
    "### Discrete Choice Modelling\n",
    "Discrete choice models aim at explaining or predicting a choice from a set of alternatives. Well known use-cases include analyzing people choice of mean of transport or products purchases in stores.\n",
    "\n",
    "If you are new to choice modelling, you can check this [resource](https://www.publichealth.columbia.edu/research/population-health-methods/discrete-choice-model-and-analysis). \n",
    "\n",
    "### Tutorial\n",
    "In this notebook we will describe step-by-step the estimation of a choice model.\n",
    "\n",
    "- [Data Handling](#Data)\n",
    "- [Modelling](#Modelling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Items, features and choices\n",
    "\n",
    "The data structure for choice modelling is somehow different than usual prediction use-cases.\n",
    "We consider a set of variable size of different alternatives. Each alternative is described by features and one is chosen among the set. Some contexts features (describing a customer, or time) can also affect the choice.\n",
    "Let's take an example where we want to predict a customer's next purchase.\n",
    "\n",
    "Three different items, i<sub>1</sub>, i<sub>2</sub> and i<sub>3</sub> are sold and we have gathered a small dataset:\n",
    "\n",
    "<table>\n",
    "<tr><th>1st Purchase: </th><th>2nd Purchase:</th><th>3rd Purchase:</th></tr>\n",
    "\n",
    "<tr><td>\n",
    "\n",
    "**Shelf**:\n",
    "\n",
    "| Item           | Price   | Promotion |\n",
    "| -------------- | ------- | --------- |\n",
    "| i<sub>1</sub>  | $100    | no        |\n",
    "| i<sub>2</sub>  | $140    | no        |\n",
    "| i<sub>3</sub>  | $200    | no        |\n",
    "\n",
    "**Customer Purchase:** i<sub>1</sub>\n",
    "\n",
    "</td><td>\n",
    "\n",
    "**Shelf**:\n",
    "\n",
    "| Item           | Price   | Promotion |\n",
    "| -------------- | ------- | --------- |\n",
    "| i<sub>1</sub>  | $100    | no        |\n",
    "| i<sub>2</sub>  | $120    | yes       |\n",
    "| i<sub>3</sub>  | $200    | no        |\n",
    "\n",
    "**Customer Purchase:** i<sub>2</sub>\n",
    "\n",
    "</td><td>\n",
    "\n",
    "**Shelf**:\n",
    "\n",
    "| Item           | Price        | Promotion    |\n",
    "| -------------- | ------------ | ------------ |\n",
    "| i<sub>1</sub>  | $100         | no           |\n",
    "| i<sub>2</sub>  | Out-Of-Stock | Out-Of-Stock |\n",
    "| i<sub>3</sub>  | $180         | yes          |\n",
    "\n",
    "**Customer Purchase:** i<sub>3</sub>\n",
    "\n",
    "</td></tr> </table>\n",
    "\n",
    "Indexing the items in the same order, we create the ChoiceDataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [0, 1, 2] # Indexes of the items chosen\n",
    "\n",
    "items_features_by_choice =  [\n",
    "    [\n",
    "        [100., 0.], # choice 1, Item 1 [price, promotion]\n",
    "        [140., 0.], # choice 1, Item 2 [price, promotion]\n",
    "        [200., 0.], # choice 1, Item 3 [price, promotion]\n",
    "    ],\n",
    "    [\n",
    "        [100., 0.], # choice 2, Item 1 [price, promotion]\n",
    "        [120., 1.], # choice 2, Item 2 [price, promotion]\n",
    "        [200., 0.], # choice 2, Item 3 [price, promotion]\n",
    "    ],\n",
    "    [\n",
    "        [100., 0.], # choice 3, Item 1 [price, promotion]\n",
    "        [120., 1.], # choice 3, Item 2 [price, promotion]\n",
    "        [180., 1.], # choice 3, Item 3 [price, promotion]\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item i<sub>2</sub> was out of stock during the last choice. Thus it could not have been chosen. In order to keep this information we create a matric indicating which items were available during each of the choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_items_by_choice = [\n",
    "    [1, 1, 1], # All items available for choice 1\n",
    "    [1, 1, 1], # All items available for choice 2\n",
    "    [1, 0, 1], # Item 2 not available for choice 3\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's create the ChoiceDataset! We can also specify the features names if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.data import ChoiceDataset\n",
    "\n",
    "dataset = ChoiceDataset(\n",
    "    choices=choices,\n",
    "    items_features_by_choice=items_features_by_choice,\n",
    "    items_features_by_choice_names=[\"price\", \"promotion\"],\n",
    "    available_items_by_choice=available_items_by_choice,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "### Estimation and choice probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first and simple model to predict a customer choice is the Multinomial Logit.\n",
    "\n",
    "We consider that customers attribute a utility to each product and that he chooses the product with hightest utility.\n",
    "\n",
    "We formulate the utility as a linear function of our features:\n",
    "\n",
    "$$U(i) = \\alpha_i +  \\beta \\cdot price(i) + \\gamma \\cdot promotion(i)$$\n",
    "\n",
    "Considering that this estimation is noisy, we use the softmax function over the available products to get the purchase probability. For example using our first data sample we obtain:\n",
    "\n",
    "$$\\mathbb{P}(i_1) = \\frac{e^{U(i_1)}}{e^{U(i_1)} + e^{U(i_2)} + e^{U(i_3)}}$$\n",
    "\n",
    "For the third sample only two items are still available, making the probability:\n",
    "$$\\mathbb{P}(i_1) = \\frac{e^{U(i_1)}}{e^{U(i_1)} + e^{U(i_3)}}$$\n",
    "\n",
    "The parameters $\\alpha_i$, $\\beta$ and $\\gamma$ are estimated by maximizing the Negative Log-Likelihood. Here is how it goes with Choice-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.models import SimpleMNL\n",
    "\n",
    "model = SimpleMNL(intercept=\"item\")\n",
    "history = model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the weights estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features coefficients are:\n",
      "<tf.Variable 'Weights_items_features:0' shape=(2,) dtype=float32, numpy=array([-0.37710273, 40.983475  ], dtype=float32)>\n",
      "Items intercepts:\n",
      "[0] and <tf.Variable 'Intercept:0' shape=(2,) dtype=float32, numpy=array([-11.027451,  12.578588], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(\"Features coefficients are:\")\n",
    "print(model.trainable_weights[0])\n",
    "print(\"Items intercepts:\")\n",
    "print([0], \"and\", model.trainable_weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the average Negative Log-Likelihood of the model, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.001363e-05>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now acces estimated choice probabilities using a ChoiceDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities are:\n",
      "tf.Tensor(\n",
      "[[9.9998999e-01 4.5697122e-12 1.2174261e-11]\n",
      " [1.8438762e-10 9.9998999e-01 2.2448054e-21]\n",
      " [6.9211727e-11 0.0000000e+00 9.9998999e-01]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "probabilities = model.predict_probas(dataset)\n",
    "print(\"Probabilities are:\")\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Jupyter Notebooks\n",
    "\n",
    "If you want to go further, here are a few useful Jupyter Notebooks:\n",
    "\n",
    "*Data:*\n",
    "- A more complete example [here](./2_data_handling.ipynb)\n",
    "- A detailed use of FeaturesByIDs [here](./../data/features_byID_examples.ipynb) if you want to minimize your RAM footprint\n",
    "\n",
    "*Modelling:*\n",
    "- A more complete example using the Conditional-MNL [here](./3_model_clogit.ipynb)\n",
    "- An example to easily build custom models [here](./4_model_customization.ipynb)\n",
    "\n",
    "*Tools:*\n",
    "- An example of assortment optimization using a choice model and Gurobi [here](./../auxiliary_tools/assortment_example.ipynb)\n",
    "\n",
    "Here are complementary Notebooks that might interest you:\n",
    "- A comparison with the R package mlogit [here](./../models/simple_mnl.ipynb)\n",
    "- A reconstruction of the experiments of the RUMnet paper [here](./../models/rumnet.ipynb)\n",
    "- An example of estimation of a Latent Class MNL [here](./../models/latent_class_model.ipynb)\n",
    "- An example of estimation of the Nested Logit model [here](./../models/nested_logit.ipynb)\n",
    "- A reconstruction using Choice-Learn of scikit-learn's Logistic Regression tutorial [here](./../models/logistic_regression.ipynb)\n",
    "\n",
    "### Documentation\n",
    "The [full documentation](https://artefactory.github.io/choice-learn) also hosts a lot of useful details and information.\n",
    "\n",
    "### Additional Questions, Requests, etc...\n",
    "If you have ideas, questions, features request or any other input, do not hesitate to reach out by opening an issue on [GitHub](https://github.com/artefactory/choice-learn/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
