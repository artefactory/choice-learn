{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to choice-learn's modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- [Example 1: ConditionalMNL with ModeCanada](#getting-started-with-the-conditionalmnl)\n",
    "    - [A few words on c-MNL formulation](#conditional-mnl-formulation)\n",
    "    - [Instantiation and estimation with Choice-Learn](#instantiation--estimation-with-choice-learn)\n",
    "- [Example 2: ConditionalMNL with SwissMetro](#example-2-swissmetro)\n",
    "\n",
    "For model customization and more explanation on ChoiceModel and the endpoints, you can go [here](./custom_model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with the ConditionalMNL\n",
    "\n",
    "The choice-learn package offers a high level API to conceive and estimate discrete choice models. Several models are ready to be used, you can check the list [here](../README.md). If you want to create your own model or another one that is not in the list, the lower level API can help you. Check the notebook [here](./custom_model.ipynb).\n",
    "\n",
    "We begin this tutorial with the estimation of a Conditional Logit Model on the ModeCanada dataset[1]. We try to reproduce the example from [Torch-Choice](https://gsbdbi.github.io/torch-choice/conditional_logit_model_mode_canada/).\n",
    "Another example from [PyLogit](https://github.com/timothyb0912/pylogit/blob/master/examples/notebooks/Main%20PyLogit%20Example.ipynb) is [here](#example-2-swissmetro).\n",
    "\n",
    "First, we download our data as a ChoiceDataset. See the [data management tutorial](./choice_learn_introduction_data.ipynb) first if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to check what's in the dataset:\n",
    "from choice_learn.datasets import load_modecanada\n",
    "\n",
    "transport_df = load_modecanada(as_frame=True)\n",
    "transport_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the ChoiceDataset\n",
    "from choice_learn.data import ChoiceDataset\n",
    "dataset = load_modecanada(as_frame=False, preprocessing=\"tutorial\")\n",
    "\n",
    "print(dataset.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import the model from choice_learn.models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional MNL formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional MNL [2] specifies a linear utility for each item i during the choice c with regards to the features:\n",
    "$$\n",
    "U(i, c) = \\sum_{features} a(i, c) * feature(i, c)\n",
    "$$\n",
    "\n",
    "We will define a ConditionalMNL model with regards to our ChoiceDataset.\n",
    "For each feature in the choice dataset we can specify how it must be specified in the utility.\n",
    "\n",
    "Let's re-use a common example: the ModeCanada [1] dataset:\n",
    "$$\n",
    "U(i, c) = \\beta^{inter}_i + \\beta^{price} \\cdot price(i, c) + \\beta^{freq} \\cdot freq(i, c) + \\beta^{ovt} \\cdot ovt(i, c) + \\beta^{income}_i \\cdot income(c) + \\beta^{ivt}_i \\cdot ivt(i, c) + \\epsilon(i, c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we want to estimate:\n",
    "\n",
    "- one $\\beta^{price}$, $\\beta^{freq}$ and $\\beta^{ovt}$ coefficient. They are **shared** by all items.\n",
    "- one $\\beta^{ivt}$ coefficient for **each** item.\n",
    "- one $\\beta^{inter}$ and $\\beta^{income}$ coefficient for **each** item, with **additional constraint** to be 0 for the first item (air).\n",
    "\n",
    "One notes that it makes sense to include an intercept $\\beta^{inter}$ for each item since $ivt(i, c)$ and $income(c)$ depends on each choice $c$.\n",
    "\n",
    "To build a model with the right utility function, we need to specify for each weight:\n",
    "- a unique name\n",
    "- the name of the feature it goes with:\n",
    "    - it must match the feature name in the ChoiceDataset\n",
    "    - \"intercept\" is the standardized name used for intercept, pay attention not to override it\n",
    "- items_indexes: the items concerned, as indexed in the ChoiceDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation & estimation with Choice-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.models import ConditionalMNL\n",
    "\n",
    "# Initialization of the model\n",
    "model = ConditionalMNL()\n",
    "\n",
    "# Creation of the different weights:\n",
    "\n",
    "# shared_coefficient add one coefficient that is used for all items specified in the items_indexes:\n",
    "# Here, cost, freq and ovt coefficients are shared between all items\n",
    "model.add_shared_coefficient(feature_name=\"cost\", items_indexes=[0, 1, 2, 3])\n",
    "# You can specify you own coefficient name\n",
    "model.add_shared_coefficient(feature_name=\"freq\",\n",
    "                             coefficient_name=\"beta_frequence\",\n",
    "                             items_indexes=[0, 1, 2, 3])\n",
    "model.add_shared_coefficient(feature_name=\"ovt\", items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# ivt is added for each item:\n",
    "model.add_coefficients(feature_name=\"ivt\", items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# add_coefficients adds one coefficient for each specified item_index\n",
    "# intercept, and income are added for each item except the first one that needs to be zeroed\n",
    "model.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2, 3])\n",
    "model.add_coefficients(feature_name=\"income\", items_indexes=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can instantiate our ConditionalMNL from the specification. We use LBFGS as the estimation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the the coefficients values, use the .fit method with the ChoiceDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, get_report=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the estimated coefficients with the .trainable_weights argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative loglikelihood can be estimated using .evaluate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average neg-loglikelihood is: 0.6744666\n",
      "The total neg-loglikelihood is: 1874.3427090644836\n"
     ]
    }
   ],
   "source": [
    "print(\"The average neg-loglikelihood is:\", model.evaluate(dataset).numpy())\n",
    "print(\"The total neg-loglikelihood is:\", model.evaluate(dataset).numpy()*len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model automatically creates a report for each of the coefficient, with its estimation, its standard deviation and more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient Name</th>\n",
       "      <th>Coefficient Estimation</th>\n",
       "      <th>Std. Err</th>\n",
       "      <th>z_value</th>\n",
       "      <th>P(.&gt;z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beta_cost:0_0</td>\n",
       "      <td>-0.033339</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>-4.698975</td>\n",
       "      <td>2.622604e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beta_frequence:0_0</td>\n",
       "      <td>0.092529</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>18.151848</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beta_ovt:0_0</td>\n",
       "      <td>-0.043004</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>-13.335655</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beta_ivt:0_0</td>\n",
       "      <td>0.059509</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>5.907977</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beta_ivt:0_1</td>\n",
       "      <td>-0.006784</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>-1.530147</td>\n",
       "      <td>1.259804e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beta_ivt:0_2</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>-3.403007</td>\n",
       "      <td>6.664991e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beta_ivt:0_3</td>\n",
       "      <td>-0.001450</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>-1.221385</td>\n",
       "      <td>2.219402e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>beta_intercept:0_0</td>\n",
       "      <td>0.698379</td>\n",
       "      <td>1.280196</td>\n",
       "      <td>0.545525</td>\n",
       "      <td>5.853922e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beta_intercept:0_1</td>\n",
       "      <td>1.844094</td>\n",
       "      <td>0.708432</td>\n",
       "      <td>2.603063</td>\n",
       "      <td>9.239554e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>beta_intercept:0_2</td>\n",
       "      <td>3.274180</td>\n",
       "      <td>0.624344</td>\n",
       "      <td>5.244195</td>\n",
       "      <td>1.192093e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>beta_income:0_0</td>\n",
       "      <td>-0.089087</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>-4.855653</td>\n",
       "      <td>1.192093e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>beta_income:0_1</td>\n",
       "      <td>-0.027993</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>-7.228698</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>beta_income:0_2</td>\n",
       "      <td>-0.038146</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>-9.342754</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coefficient Name  Coefficient Estimation  Std. Err    z_value  \\\n",
       "0        beta_cost:0_0               -0.033339  0.007095  -4.698975   \n",
       "1   beta_frequence:0_0                0.092529  0.005098  18.151848   \n",
       "2         beta_ovt:0_0               -0.043004  0.003225 -13.335655   \n",
       "3         beta_ivt:0_0                0.059509  0.010073   5.907977   \n",
       "4         beta_ivt:0_1               -0.006784  0.004433  -1.530147   \n",
       "5         beta_ivt:0_2               -0.006460  0.001898  -3.403007   \n",
       "6         beta_ivt:0_3               -0.001450  0.001187  -1.221385   \n",
       "7   beta_intercept:0_0                0.698379  1.280196   0.545525   \n",
       "8   beta_intercept:0_1                1.844094  0.708432   2.603063   \n",
       "9   beta_intercept:0_2                3.274180  0.624344   5.244195   \n",
       "10     beta_income:0_0               -0.089087  0.018347  -4.855653   \n",
       "11     beta_income:0_1               -0.027993  0.003872  -7.228698   \n",
       "12     beta_income:0_2               -0.038146  0.004083  -9.342754   \n",
       "\n",
       "          P(.>z)  \n",
       "0   2.622604e-06  \n",
       "1   0.000000e+00  \n",
       "2   0.000000e+00  \n",
       "3   0.000000e+00  \n",
       "4   1.259804e-01  \n",
       "5   6.664991e-04  \n",
       "6   2.219402e-01  \n",
       "7   5.853922e-01  \n",
       "8   9.239554e-03  \n",
       "9   1.192093e-07  \n",
       "10  1.192093e-06  \n",
       "11  0.000000e+00  \n",
       "12  0.000000e+00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faster specification can be done using a dictionnary. It follows torch-choice method to create conditional logit models.\n",
    "The parameters dict needs to be as follows:\n",
    "- The key is the feature name\n",
    "- The value is the mode. Currently three modes are available:\n",
    "    - constant: the learned coefficient is shared by all items\n",
    "    - item: one coefficient by item is estimated, the value for the item at index 0 is set to 0\n",
    "    - item-full: one coefficient by item is estimated\n",
    "\n",
    "In order to create the same model for the ModeCanada dataset, it looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n"
     ]
    }
   ],
   "source": [
    "# Instantiation with the coefficients dictionnary\n",
    "coefficients = {\"income\": \"item\",\n",
    " \"cost\": \"constant\",\n",
    " \"freq\": \"constant\",\n",
    " \"ovt\": \"constant\",\n",
    " \"ivt\": \"item-full\",\n",
    " \"intercept\": \"item\"}\n",
    "\n",
    "# Instantiation of the model\n",
    "cmnl = ConditionalMNL(coefficients=coefficients, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cmnl.fit(dataset)\n",
    "print(cmnl.trainable_weights)\n",
    "print(cmnl.evaluate(dataset).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the estimated coefficients and the negative log-likelihood obtained in torch-choice example, and it is similar !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n",
      "'Ground Truth' Negative LogLikelihood: tf.Tensor(1874.3427, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Here are the values obtained in the references:\n",
    "gt_weights = [\n",
    "    tf.constant([[-0.0890796, -0.0279925, -0.038146]]),\n",
    "    tf.constant([[-0.0333421]]),\n",
    "    tf.constant([[0.0925304]]),\n",
    "    tf.constant([[-0.0430032]]),\n",
    "    tf.constant([[0.0595089, -0.00678188, -0.00645982, -0.00145029]]),\n",
    "    tf.constant([[0.697311, 1.8437, 3.27381]]),\n",
    "]\n",
    "gt_model = ConditionalMNL(coefficients=coefficients)\n",
    "gt_model.instantiate(dataset)\n",
    "\n",
    "# Here we estimate the negative log-likelihood with these coefficients (also, we obtain same value as in those papers):\n",
    "gt_model.trainable_weights = gt_weights\n",
    "print(\"'Ground Truth' Negative LogLikelihood:\", gt_model.evaluate(dataset) * len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the utilities, use the .predict_utility() method. In order to estimate the probabilities, use the .compute_probabilities() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchase probability of each item for the first 5 sessions: tf.Tensor(\n",
      "[[0.19061336 0.00353294 0.40536717 0.4004825 ]\n",
      " [0.34869507 0.00069691 0.36830768 0.28229663]\n",
      " [0.14418297 0.00651323 0.40567806 0.44362125]\n",
      " [0.34869507 0.00069691 0.36830768 0.28229663]\n",
      " [0.34869507 0.00069691 0.36830768 0.28229663]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# print(\"Utilities of each item for the first 5 sessions:\", cmnl.predict_utility(dataset)[:5])\n",
    "print(\"Purchase probability of each item for the first 5 sessions:\", cmnl.predict_probas(dataset)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large datasets that do not fit entirely in the memory, the LBFGS method might not be the best choice. Here we can use the power of the Tensorflow library to use stochastic gradient descent optimizers.\n",
    "\n",
    "In this case, it is possible to obtain the same coefficients estimation, also it is a little tricky to get it quickly. We need to adjust the learning rate over time for the optimization not to be too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl = ConditionalMNL(coefficients=coefficients, optimizer=\"Adam\", epochs=2000, batch_size=-1)\n",
    "history = cmnl.fit(dataset)\n",
    "cmnl.optimizer.lr = cmnl.optimizer.lr / 5\n",
    "cmnl.epochs = 4000\n",
    "history2 = cmnl.fit(dataset)\n",
    "cmnl.optimizer.lr = cmnl.optimizer.lr  / 10\n",
    "cmnl.epochs = 20000\n",
    "history3 = cmnl.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to look at the loss (negative loglikelyhood) over time to see how the estimation goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"])\n",
    "plt.title(\"First part of the gradient descent.\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history2[\"train_loss\"] + history3[\"train_loss\"])\n",
    "plt.title(\"Second and third part of the gradient descent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'income_w_0:0' shape=(1, 3) dtype=float32, numpy=array([[-0.08402906, -0.02359901, -0.03233592]], dtype=float32)>,\n",
       " <tf.Variable 'cost_w_1:0' shape=(1, 1) dtype=float32, numpy=array([[-0.05140896]], dtype=float32)>,\n",
       " <tf.Variable 'freq_w_2:0' shape=(1, 1) dtype=float32, numpy=array([[0.09645308]], dtype=float32)>,\n",
       " <tf.Variable 'ovt_w_3:0' shape=(1, 1) dtype=float32, numpy=array([[-0.04099102]], dtype=float32)>,\n",
       " <tf.Variable 'ivt_w_4:0' shape=(1, 4) dtype=float32, numpy=\n",
       " array([[ 0.05871329, -0.00726112, -0.00368666, -0.00105637]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'intercept_w_5:0' shape=(1, 3) dtype=float32, numpy=array([[-1.6874433 , -0.39637026,  1.1344589 ]], dtype=float32)>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmnl.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.67664886>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmnl.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faster specification can be done using a dictionnary. It follows torch-choice \\ref{} method to create conditional logit models.\n",
    "The parameters dict needs to be as follows:\n",
    "- The key is the feature name\n",
    "- The value is the mode. Currently three modes are available:\n",
    "    - constant: the learned coefficient is shared by all items\n",
    "    - item: one coefficient by item is estimated, the value for the item at index 0 is set to 0\n",
    "    - item-full: one coefficient by item is estimated\n",
    "\n",
    "In order to create the same model for the ModeCanada dataset, it looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the parameters dictionnary\n",
    "coefficients = {\"income\": \"item\",\n",
    " \"cost\": \"constant\",\n",
    " \"freq\": \"constant\",\n",
    " \"ovt\": \"constant\",\n",
    " \"ivt\": \"item-full\",\n",
    " \"intercept\": \"item\"}\n",
    "\n",
    "# Instantiation of the model\n",
    "cmnl = ConditionalMNL(coefficients=coefficients, optimizer=\"lbfgs\", epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'income_w_0:0' shape=(1, 3) dtype=float32, numpy=array([[-0.08908693, -0.02799302, -0.0381465 ]], dtype=float32)>\n",
      "<tf.Variable 'cost_w_1:0' shape=(1, 1) dtype=float32, numpy=array([[-0.03333883]], dtype=float32)>\n",
      "<tf.Variable 'freq_w_2:0' shape=(1, 1) dtype=float32, numpy=array([[0.09252924]], dtype=float32)>\n",
      "<tf.Variable 'ovt_w_3:0' shape=(1, 1) dtype=float32, numpy=array([[-0.0430035]], dtype=float32)>\n",
      "<tf.Variable 'ivt_w_4:0' shape=(1, 4) dtype=float32, numpy=\n",
      "array([[ 0.05950952, -0.00678374, -0.00646028, -0.00145036]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'intercept_w_5:0' shape=(1, 3) dtype=float32, numpy=array([[0.698383 , 1.8441006, 3.2741847]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "history = cmnl.fit(dataset)\n",
    "for weight in cmnl.trainable_weights:\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: SwissMetro\n",
    "\n",
    "We reproduce the [PyLogit](https://github.com/timothyb0912/pylogit/blob/master/examples/notebooks/Main%20PyLogit%20Example.ipynb) example of ConditionalMNL, that is reproduction of a Biogeme example. It uses the SwissMetro dataset[3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.datasets import load_swissmetro\n",
    "swiss_dataset = load_swissmetro(as_frame=False, preprocessing=\"tutorial\")\n",
    "print(swiss_dataset.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the model\n",
    "swiss_model = ConditionalMNL(optimizer=\"lbfgs\", epochs=10000)\n",
    "\n",
    "swiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 1])\n",
    "swiss_model.add_shared_coefficient(feature_name=\"travel_time\",\n",
    "                                   items_indexes=[0, 1],\n",
    "                                   coefficient_name=\"beta_tt_transit\")\n",
    "swiss_model.add_coefficients(feature_name=\"travel_time\",\n",
    "                             items_indexes=[2],\n",
    "                             coefficient_name=\"beta_tt_car\")\n",
    "swiss_model.add_coefficients(feature_name=\"cost\",\n",
    "                             items_indexes=[0, 1, 2],\n",
    "                             coefficient_name=\"beta_tc\")\n",
    "swiss_model.add_coefficients(feature_name=\"headway\",\n",
    "                             items_indexes=[0, 1],\n",
    "                             coefficient_name=\"beta_he\")\n",
    "swiss_model.add_coefficients(feature_name=\"seats\", items_indexes=[1])\n",
    "swiss_model.add_shared_coefficient(feature_name=\"train_survey\",\n",
    "                                   items_indexes=[0, 1],\n",
    "                                   coefficient_name=\"beta_survey\")\n",
    "swiss_model.add_coefficients(feature_name=\"regular_class\",\n",
    "                             items_indexes=[0],\n",
    "                             coefficient_name=\"beta_first_class\")\n",
    "swiss_model.add_coefficients(feature_name=\"single_luggage_piece\",\n",
    "                             items_indexes=[2],\n",
    "                             coefficient_name=\"beta_luggage=1\")\n",
    "swiss_model.add_coefficients(feature_name=\"multiple_luggage_piece\",\n",
    "                             items_indexes=[2],\n",
    "                             coefficient_name=\"beta_luggage>1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = swiss_model.fit(swiss_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[-1.2929306 , -0.50257486]], dtype=float32)>,\n",
       " <tf.Variable 'beta_tt_transit:0' shape=(1, 1) dtype=float32, numpy=array([[-0.69901353]], dtype=float32)>,\n",
       " <tf.Variable 'beta_tt_car:0' shape=(1, 1) dtype=float32, numpy=array([[-0.72298324]], dtype=float32)>,\n",
       " <tf.Variable 'beta_tc:0' shape=(1, 3) dtype=float32, numpy=array([[-0.5617619 , -0.28167555, -0.51384664]], dtype=float32)>,\n",
       " <tf.Variable 'beta_he:0' shape=(1, 2) dtype=float32, numpy=array([[-0.31433576, -0.3773172 ]], dtype=float32)>,\n",
       " <tf.Variable 'beta_seats:0' shape=(1, 1) dtype=float32, numpy=array([[-0.7824475]], dtype=float32)>,\n",
       " <tf.Variable 'beta_survey:0' shape=(1, 1) dtype=float32, numpy=array([[2.5424762]], dtype=float32)>,\n",
       " <tf.Variable 'beta_first_class:0' shape=(1, 1) dtype=float32, numpy=array([[0.5650172]], dtype=float32)>,\n",
       " <tf.Variable 'beta_luggage=1:0' shape=(1, 1) dtype=float32, numpy=array([[0.4227602]], dtype=float32)>,\n",
       " <tf.Variable 'beta_luggage>1:0' shape=(1, 1) dtype=float32, numpy=array([[1.413982]], dtype=float32)>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swiss_model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5156.3345>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(swiss_dataset) * swiss_model.evaluate(swiss_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the same results (estimation of parameters and negative log-likelihood) as the PyLogit package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] ModeCanada dataset in *Application and interpretation of nested logit models of intercity mode choice*, Christophier, V. F.; Koppelman, S. (1993)\\\n",
    "[2] Conditional MultinomialLogit, Train, K.; McFadden, D.; Ben-Akiva, M. (1987)\\\n",
    "[3] Siwssmetro dataset in *The acceptance of modal innovation: The case of Swissmetro*, Bierlaire, M.; Axhausen, K.; Abay, G (2001)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
